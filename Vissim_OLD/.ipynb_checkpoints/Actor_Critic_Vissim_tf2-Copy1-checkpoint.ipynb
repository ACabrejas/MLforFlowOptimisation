{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import math\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from Actor_Critic_Agents import ACAgent\n",
    "\n",
    "\n",
    "\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PER\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes \n",
    "episodes = 400\n",
    "partial_save_at = 25 #50\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "Surtrac = False\n",
    "AC = True\n",
    "PER_activated = False\n",
    "\n",
    "\n",
    "alpha   = 0.0000005\n",
    "gamma   = 0.85  #0.85 # 0.99\n",
    "entropy = 0.000001 # exploration\n",
    "value = 5 #0.5 # weight attributed in the value loss during gradient descent\n",
    "\n",
    "n_sample = 10 # number of sample for the value check\n",
    "horizon = 100 # horizon of the value check, the number of step forward uses to compute the return\n",
    "\n",
    "# In order to reduce entropy during training (not implemented yet)\n",
    "reduce_entropy = False\n",
    "reduce_entropy_every = 1000\n",
    "\n",
    "n_step_size = 8 #16 # number of step in the n step learning 32\n",
    "# Do not work if n_step_size is aver 31\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1 # worked with 2400\n",
    "\n",
    "\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = (2,4,11)  #5 # 4 queues or 5 queues + signal state    #49 53 (1,8,6) for conv\n",
    "action_size = 2\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP) # worked with 600\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":600, 'm':300, 'l':150}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\n"
     ]
    }
   ],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"retraining\" = continue the training of previous agent\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Network Model Parameters\n",
    "model_name  = 'Single_Cross_Mod2'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Triple_Mod'\n",
    "# 'Single_Cross_Mod2'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "agent_type = 'AC' # DQN, DuelingDQN, DDQN, DuelingDDQN AC\n",
    "reward_type = 'Queues'\n",
    "\n",
    "state_type  = 'CellsT'\n",
    "#CellsSpeedOccSig'    # 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig' CellsSpeedSig \n",
    "#CellsSpeedOccSig 'CellsOccSig' 'CellsT'\n",
    "# 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig'\n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = True\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Loading the best agent during demo and training\n",
    "best = True\n",
    "\n",
    "\n",
    "\n",
    "# Session ID\n",
    "#Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "\n",
    "# Adding the state type to the Session_ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_State_\"+state_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "print(Session_ID)\n",
    "\n",
    "if mode == 'demo' :\n",
    "    simulation_length = 3601\n",
    "    demand_list = [[demand['l'], demand['l']]]\n",
    "    demand_change_timesteps = simulation_length\n",
    "\n",
    "if mode == 'test' : \n",
    "    simulation_length = 3601\n",
    "    demand_change_timesteps = 450\n",
    "    demand = {\"h\":800, 'm':400, 'l':200}\n",
    "    demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "                  [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "                  [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "                  [demand['l'], demand['h']], [demand['l'], demand['m']]]\n",
    "    Random_Seed = 1\n",
    "    # Loading the best agent\n",
    "    best = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Mod2.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"modelconv\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value_conv1 (Conv2D)         multiple                  3200      \n",
      "_________________________________________________________________\n",
      "value_conv2 (Conv2D)         multiple                  4624      \n",
      "_________________________________________________________________\n",
      "value1 (Dense)               multiple                  4128      \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  528       \n",
      "_________________________________________________________________\n",
      "value_last (Dense)           multiple                  17        \n",
      "_________________________________________________________________\n",
      "value_flat (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "policy_conv1 (Conv2D)        multiple                  1600      \n",
      "_________________________________________________________________\n",
      "policy_conv2 (Conv2D)        multiple                  2320      \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  4128      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  528       \n",
      "_________________________________________________________________\n",
      "policy_last (Dense)          multiple                  34        \n",
      "_________________________________________________________________\n",
      "policy_flat (Flatten)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 21,107\n",
      "Trainable params: 21,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corected\n",
      "Deployed 1 agent(s) of the Class AC.\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6899b8eed71d48c8a504d53c4144d321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=400)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/400, Epsilon:0, Average reward: -47.24\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -4.0, -3.0, -2.0, 1.0, -7.0, -6.0, -7.0, -0.0, -6.0] \n",
      " [-53.0, -281.0, -33.0, -55.0, -53.0, -80.0, -144.0, -121.0, -1776.0, -140.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.87, 0.05], [2.09, 3.29], [2.42, -0.42], [0.26, 1.84], [2.64, -1.93], [2.56, 1.82], [-0.15, 0.41], [-1.74, 4.55], [-1.61, 1.93], [3.5, -0.64]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 2/400, Epsilon:0, Average reward: -30.12\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -3.0, -2.0, -1.0, -5.0, -7.0, -9.0, -6.0, -7.0, -3.0] \n",
      " [-322.0, -328.0, -178.0, -36.0, -75.0, -163.0, -476.0, -171.0, -134.0, -90.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.82, 0.44], [-0.52, 1.59], [1.64, 1.0], [3.27, -0.99], [-0.99, -0.64], [-1.66, 5.4], [4.89, -2.45], [0.61, 2.64], [1.35, 2.41], [0.8, 1.39]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 3/400, Epsilon:0, Average reward: -35.6\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -2.0, -6.0, -5.0, -7.0, -4.0, -8.0, 1.0, -4.0, -8.0] \n",
      " [-325.0, -605.0, -118.0, -62.0, -133.0, -165.0, -310.0, -205.0, -95.0, -290.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-1.7, 2.88], [-1.78, 1.95], [1.21, 0.64], [-4.99, 3.76], [4.06, -0.81], [0.28, 2.08], [2.43, -1.28], [3.05, -0.16], [3.73, -0.48], [2.71, 2.22]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 4/400, Epsilon:0, Average reward: -34.91\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -16.0, -3.0, -13.0, -8.0, -5.0, -3.0, -1.0, -4.0, -6.0] \n",
      " [-50.0, -155.0, -35.0, -205.0, -385.0, -72.0, -1248.0, -24.0, -222.0, -56.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.74, -1.36], [-0.98, 5.68], [-2.52, 4.09], [-1.01, 5.87], [2.23, 6.97], [3.41, 0.17], [0.46, 1.64], [-1.7, 2.6], [7.1, -1.76], [0.21, 0.56]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 5/400, Epsilon:0, Average reward: -31.78\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -1.0, -2.0, -10.0, -10.0, -14.0, -11.0, -4.0, -16.0, -6.0] \n",
      " [-451.0, -33.0, -107.0, -111.0, -176.0, -483.0, -178.0, -336.0, -920.0, -257.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.09, 3.94], [2.82, -1.84], [0.26, 1.71], [3.46, 0.4], [-1.52, -0.46], [1.91, 6.33], [1.72, 0.15], [3.86, -0.53], [2.92, 3.91], [4.9, 1.45]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 6/400, Epsilon:0, Average reward: -35.67\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14.0, -10.0, -8.0, -0.0, -12.0, -2.0, -5.0, -12.0, 1.0, -4.0] \n",
      " [-177.0, -53.0, -151.0, -86.0, -689.0, -18.0, -94.0, -750.0, -117.0, -31.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.19, 4.18], [3.55, -2.69], [1.68, 4.65], [2.51, 1.53], [1.95, 1.8], [1.27, -0.44], [5.72, -1.27], [0.83, 1.9], [0.06, 3.21], [3.52, -0.28]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 7/400, Epsilon:0, Average reward: -42.77\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10.0, -9.0, -4.0, -0.0, -9.0, -4.0, -12.0, -9.0, -16.0, -8.0] \n",
      " [-204.0, -339.0, -337.0, -18.0, -40.0, -79.0, -277.0, -268.0, -1655.0, -133.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[2.01, 1.06], [0.37, 3.17], [-0.03, 1.59], [0.38, 3.33], [-3.01, 4.1], [3.3, 0.38], [-1.92, 2.58], [-3.68, 8.37], [4.24, 1.84], [5.3, 1.54]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 8/400, Epsilon:0, Average reward: -42.32\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -5.0, -8.0, -0.0, -11.0, -18.0, -0.0, -8.0, -6.0, -3.0] \n",
      " [-162.0, -41.0, -1120.0, -45.0, -371.0, -817.0, -35.0, -40.0, -606.0, -44.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.37, 2.31], [3.15, -0.79], [1.89, 1.04], [0.01, 0.0], [3.27, 0.14], [6.88, -2.85], [0.02, 0.02], [0.73, 1.6], [1.98, 3.04], [2.49, -0.55]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 9/400, Epsilon:0, Average reward: -35.51\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -5.0, -12.0, -5.0, -13.0, -3.0, -3.0, -9.0, -4.0, -10.0] \n",
      " [-120.0, -86.0, -61.0, -230.0, -151.0, -509.0, -258.0, -69.0, -48.0, -146.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.79, 1.13], [1.53, 0.1], [-0.09, 3.41], [3.32, -1.75], [-4.34, 2.38], [1.32, 2.01], [3.37, 1.63], [-2.91, 3.94], [2.65, -0.34], [0.28, 1.71]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 10/400, Epsilon:0, Average reward: -39.96\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -9.0, -13.0, -9.0, -11.0, -1.0, -15.0, -1.0, -15.0, -6.0] \n",
      " [-102.0, -429.0, -780.0, -300.0, -190.0, -69.0, -258.0, -174.0, -433.0, -362.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.24, 0.53], [-0.71, 1.7], [3.06, 0.27], [-0.94, 0.44], [3.29, 2.48], [2.35, -0.37], [0.48, 5.31], [5.58, -0.94], [5.9, 2.28], [3.16, 2.31]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 11/400, Epsilon:0, Average reward: -36.71\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -7.0, -17.0, -9.0, -15.0, -12.0, -3.0, -8.0, -13.0, -2.0] \n",
      " [-207.0, -152.0, -1172.0, -123.0, -440.0, -416.0, -9.0, -350.0, -736.0, -372.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.1, 0.35], [5.94, -1.47], [-6.07, 10.22], [1.23, 3.26], [1.59, -1.17], [3.16, 4.08], [2.31, -0.04], [2.57, 0.8], [-2.27, 3.56], [0.39, 1.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 12/400, Epsilon:0, Average reward: -33.37\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7.0, -13.0, -6.0, -6.0, -14.0, -10.0, -5.0, -15.0, -10.0, -13.0] \n",
      " [-35.0, -151.0, -305.0, -124.0, -387.0, -331.0, -250.0, -319.0, -177.0, -522.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.03, 2.15], [2.4, 1.0], [3.87, -0.25], [4.75, -0.71], [-2.04, 3.99], [-2.94, 5.77], [-2.32, 2.52], [7.29, -1.97], [-0.88, 2.8], [0.34, -1.28]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 13/400, Epsilon:0, Average reward: -34.23\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -11.0, -17.0, -14.0, -19.0, -15.0, -12.0, -7.0, -16.0, -14.0] \n",
      " [-345.0, -130.0, -65.0, -129.0, -294.0, -317.0, -943.0, -84.0, -145.0, -226.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.4, 1.48], [3.8, 1.17], [-4.83, 4.34], [2.0, 0.87], [0.01, 2.84], [1.88, 1.73], [5.38, 3.41], [1.25, 1.33], [1.22, 3.46], [-6.83, 6.73]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 14/400, Epsilon:0, Average reward: -29.83\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -7.0, -3.0, -19.0, -5.0, -3.0, -3.0, -7.0, -18.0, -13.0] \n",
      " [-359.0, -90.0, -144.0, -145.0, -38.0, -74.0, -586.0, -88.0, -651.0, -538.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.89, 3.54], [3.25, 0.18], [-1.69, 1.36], [-2.0, 5.96], [-0.88, 1.95], [2.46, 0.11], [3.26, 3.07], [-3.03, 3.25], [5.15, -1.48], [2.09, 3.32]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 15/400, Epsilon:0, Average reward: -37.6\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-22.0, -0.0, -24.0, -24.0, -9.0, -21.0, -10.0, -13.0, -18.0, -14.0] \n",
      " [-37.0, -22.0, -116.0, -116.0, -591.0, -311.0, -59.0, -58.0, -313.0, -359.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[4.49, -1.29], [-1.07, 3.16], [1.19, 4.85], [1.19, 4.85], [-0.62, 1.36], [-1.97, 2.22], [6.37, -2.07], [2.41, 0.1], [-1.88, 3.08], [2.15, -1.2]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 16/400, Epsilon:0, Average reward: -30.58\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -5.0, -19.0, -3.0, -19.0, -1.0, -18.0, -8.0, -19.0, -15.0] \n",
      " [-178.0, -252.0, -675.0, -95.0, -356.0, -61.0, -479.0, -148.0, -237.0, -354.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.53, -0.57], [1.31, 1.62], [1.83, 1.58], [1.29, 4.76], [5.7, -2.15], [0.62, -0.39], [3.34, 1.86], [2.27, -0.11], [6.25, -0.57], [-2.33, 1.55]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 17/400, Epsilon:0, Average reward: -35.67\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -22.0, -5.0, -13.0, -21.0, -18.0, -13.0, -6.0, -20.0, -15.0] \n",
      " [-74.0, -274.0, -10.0, -514.0, -438.0, -97.0, -36.0, -85.0, -244.0, -164.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.69, -0.68], [9.29, -4.33], [2.14, -0.81], [2.58, 0.44], [0.7, 5.32], [3.14, 0.63], [6.23, -1.92], [2.4, 0.63], [1.41, 0.01], [-0.22, 5.39]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 18/400, Epsilon:0, Average reward: -45.03\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -17.0, -31.0, -12.0, -20.0, -17.0, -28.0, -18.0, -12.0, -7.0] \n",
      " [-61.0, -113.0, -746.0, -102.0, -610.0, -131.0, -193.0, -871.0, -189.0, -194.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.38, 1.86], [3.84, -0.65], [2.09, -1.07], [1.91, 0.59], [1.93, 7.14], [-1.64, 2.29], [-4.25, 4.36], [1.26, -1.0], [4.0, -0.99], [2.78, -0.65]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 19/400, Epsilon:0, Average reward: -33.04\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -12.0, -18.0, -12.0, -5.0, -16.0, -3.0, -15.0, -7.0, -32.0] \n",
      " [-142.0, -151.0, -82.0, -152.0, -72.0, -108.0, -63.0, -97.0, -58.0, -460.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.54, 3.49], [-2.94, 2.88], [3.86, 4.52], [-1.96, 4.82], [1.65, 1.91], [4.19, -1.26], [1.32, 0.72], [2.44, 0.29], [0.74, 0.04], [-1.6, 0.49]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 20/400, Epsilon:0, Average reward: -36.02\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-26.0, -22.0, -12.0, -31.0, -25.0, -21.0, -21.0, -8.0, -14.0, -11.0] \n",
      " [-52.0, -178.0, -332.0, -492.0, -50.0, -544.0, -141.0, -69.0, -57.0, -75.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.12, 1.79], [1.81, -1.09], [2.57, -0.64], [-5.96, 4.86], [0.22, 0.42], [4.2, 0.77], [2.0, 0.19], [2.61, 0.25], [1.06, 0.71], [-0.42, 1.51]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 21/400, Epsilon:0, Average reward: -40.12\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -6.0, -8.0, -20.0, -11.0, -13.0, -10.0, -23.0, -22.0, -16.0] \n",
      " [-114.0, -114.0, -48.0, -278.0, -74.0, -97.0, -143.0, -499.0, -695.0, -42.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.04, -0.63], [3.04, -0.63], [1.26, 1.91], [1.45, 1.29], [-4.1, 1.5], [1.19, 1.56], [0.49, 1.9], [1.21, -1.81], [0.61, 2.4], [3.97, 1.02]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 22/400, Epsilon:0, Average reward: -34.23\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18.0, -16.0, -22.0, -8.0, -23.0, -18.0, -14.0, -9.0, -20.0, -25.0] \n",
      " [-166.0, -308.0, -53.0, -135.0, -176.0, -104.0, -36.0, -114.0, -263.0, -250.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-1.8, 4.05], [5.42, 0.23], [-0.31, 2.02], [2.7, 1.56], [0.96, 0.89], [4.47, 0.31], [3.59, 1.77], [1.59, 1.23], [5.18, 3.94], [-1.96, 3.35]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 23/400, Epsilon:0, Average reward: -37.82\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -16.0, -11.0, -7.0, -22.0, -24.0, -15.0, -14.0, -11.0, -13.0] \n",
      " [-84.0, -54.0, -26.0, -70.0, -141.0, -626.0, -651.0, -121.0, -30.0, -71.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[4.11, -0.86], [4.67, -2.16], [-1.56, 1.73], [-2.27, 1.39], [1.6, 1.76], [-1.76, 4.8], [-0.09, 1.73], [-0.95, 1.45], [1.74, -0.38], [2.85, 0.95]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 24/400, Epsilon:0, Average reward: -40.61\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7.0, -20.0, -18.0, -10.0, -29.0, -15.0, -24.0, -9.0, -28.0, -30.0] \n",
      " [-26.0, -514.0, -843.0, -106.0, -333.0, -46.0, -333.0, -83.0, -501.0, -526.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.16, 1.36], [4.46, -1.04], [4.46, 1.89], [3.95, -2.43], [6.19, 2.88], [8.13, -3.17], [2.84, 0.05], [-0.9, 2.35], [6.57, -1.19], [3.51, 3.9]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 25/400, Epsilon:0, Average reward: -30.86\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -20.0, -18.0, -21.0, -33.0, -29.0, -23.0, -27.0, -24.0, -11.0] \n",
      " [-100.0, -20.0, -66.0, -360.0, -586.0, -590.0, -297.0, -527.0, -303.0, -48.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.98, -0.66], [-0.1, 2.85], [1.12, 2.44], [4.16, -1.39], [2.67, 1.89], [-1.73, 4.66], [1.11, 1.13], [4.06, 1.23], [-5.5, 7.76], [0.43, 3.61]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 25.\n",
      "Episode: 26/400, Epsilon:0, Average reward: -34.79\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -15.0, -24.0, -19.0, -21.0, -43.0, -29.0, -11.0, -32.0, -25.0] \n",
      " [-126.0, -59.0, -180.0, -452.0, -561.0, -478.0, -635.0, -198.0, -125.0, -385.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.96, 2.28], [0.34, 4.02], [-0.82, 4.97], [-0.11, 4.04], [0.92, 0.3], [1.72, -0.74], [2.38, -0.32], [5.0, -0.75], [2.42, 2.58], [0.99, 1.48]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 27/400, Epsilon:0, Average reward: -31.13\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-23.0, -31.0, -32.0, -28.0, -39.0, -21.0, -30.0, -34.0, -10.0, -32.0] \n",
      " [-97.0, -119.0, -126.0, -433.0, -370.0, -245.0, -213.0, -511.0, -294.0, -615.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.38, 1.25], [1.59, 2.83], [0.25, 3.14], [3.63, 1.04], [-0.07, 3.66], [2.79, 3.74], [4.47, -0.86], [-1.31, 8.01], [4.47, -1.48], [0.27, 4.07]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 28/400, Epsilon:0, Average reward: -46.32\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14.0, -14.0, -5.0, -32.0, -21.0, -40.0, -34.0, -31.0, -27.0, -33.0] \n",
      " [-7.0, -287.0, -5.0, -469.0, -198.0, -190.0, -153.0, -715.0, -452.0, -1075.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[2.58, 1.53], [2.89, 0.06], [1.77, -0.31], [-0.29, 2.31], [2.09, -1.07], [3.06, -0.69], [2.41, 1.4], [0.44, 1.84], [1.41, 3.77], [4.17, -3.33]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 29/400, Epsilon:0, Average reward: -25.98\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -50.0, -30.0, -1.0, -29.0, -26.0, -19.0, -40.0, -23.0, -25.0] \n",
      " [-365.0, -390.0, -203.0, -28.0, -51.0, -98.0, -533.0, -329.0, -228.0, -90.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[2.14, 0.37], [0.1, 2.55], [-1.52, 4.94], [-0.25, 0.3], [-1.59, 2.19], [4.76, -0.31], [0.6, 1.55], [-0.92, 8.5], [-5.53, 5.56], [0.29, 1.7]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 30/400, Epsilon:0, Average reward: -29.98\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-31.0, -44.0, -20.0, -30.0, -40.0, -25.0, -25.0, -55.0, -20.0, -25.0] \n",
      " [-347.0, -140.0, -454.0, -180.0, -770.0, -223.0, -232.0, -432.0, -157.0, -263.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.71, 2.7], [-1.61, 1.55], [5.04, -1.18], [2.55, 1.15], [4.25, -0.46], [-3.86, 4.51], [0.29, 3.28], [2.31, 3.31], [3.92, -0.32], [2.72, 1.15]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 31/400, Epsilon:0, Average reward: -37.02\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-31.0, -31.0, -2.0, -8.0, -17.0, -33.0, -18.0, -19.0, -46.0, -35.0] \n",
      " [-153.0, -291.0, -202.0, -33.0, -75.0, -262.0, -294.0, -124.0, -437.0, -424.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.37, 1.65], [5.02, -1.62], [0.67, 0.15], [-0.8, 1.55], [1.96, 1.33], [-0.09, 3.49], [-0.7, 0.52], [-0.32, 2.29], [1.55, 5.23], [3.09, 0.59]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 32/400, Epsilon:0, Average reward: -32.45\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-26.0, -31.0, -14.0, -17.0, -28.0, -48.0, -48.0, -10.0, -31.0, -47.0] \n",
      " [-62.0, -103.0, -22.0, -181.0, -199.0, -583.0, -583.0, -17.0, -176.0, -211.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.63, 1.6], [3.53, 0.95], [5.39, -1.85], [0.14, -1.23], [1.47, 1.7], [-1.99, 6.74], [-1.99, 6.74], [-0.11, 1.06], [1.52, 0.97], [6.14, -2.29]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 33/400, Epsilon:0, Average reward: -38.52\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21.0, -34.0, -48.0, -33.0, -25.0, -36.0, -24.0, -28.0, -24.0, -49.0] \n",
      " [-618.0, -121.0, -691.0, -422.0, -348.0, -130.0, -72.0, -136.0, -71.0, -353.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.67, -0.29], [2.63, -0.68], [0.09, 3.16], [1.26, 1.61], [3.62, 0.55], [-0.72, 2.19], [-0.43, 1.94], [6.1, -0.13], [1.17, 1.18], [4.11, 3.21]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 34/400, Epsilon:0, Average reward: -59.16\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-29.0, -40.0, -39.0, -41.0, -19.0, -37.0, -14.0, -22.0, -44.0, -66.0] \n",
      " [-332.0, -200.0, -683.0, -250.0, -210.0, -53.0, -1889.0, -279.0, -569.0, -811.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[5.43, -3.44], [2.36, 3.31], [4.89, -2.2], [4.35, -1.21], [3.69, 1.02], [2.15, 1.06], [1.11, 0.19], [-1.31, 3.03], [-2.27, 7.79], [-2.38, 7.19]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 35/400, Epsilon:0, Average reward: -30.64\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-30.0, -57.0, -35.0, -52.0, -17.0, -50.0, -30.0, -40.0, -31.0, -12.0] \n",
      " [-138.0, -634.0, -346.0, -189.0, -219.0, -155.0, -205.0, -478.0, -156.0, -53.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.78, 0.76], [-0.97, 4.01], [3.05, 0.36], [-1.77, 2.37], [1.69, 0.29], [5.12, 3.39], [1.92, -2.38], [1.15, -0.65], [-1.82, 2.9], [2.38, -0.29]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 36/400, Epsilon:0, Average reward: -40.45\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -50.0, -12.0, -17.0, -12.0, -21.0, -30.0, -6.0, -35.0, -11.0] \n",
      " [-78.0, -288.0, -64.0, -29.0, -64.0, -245.0, -219.0, -60.0, -650.0, -27.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[2.7, -0.15], [3.19, 0.61], [3.41, 2.35], [-1.32, 1.57], [3.41, 2.35], [4.3, -0.27], [2.16, 1.61], [0.62, 0.76], [3.3, 4.29], [0.64, 1.61]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 37/400, Epsilon:0, Average reward: -36.02\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -39.0, -10.0, -55.0, -9.0, -45.0, -42.0, -2.0, -57.0, -34.0] \n",
      " [-272.0, -411.0, -174.0, -264.0, -33.0, -265.0, -132.0, -63.0, -713.0, -267.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.13, 2.82], [2.52, 2.83], [-1.04, 0.86], [-2.7, 1.01], [0.99, 0.85], [3.93, 1.33], [0.29, 1.49], [0.41, -0.28], [0.82, 1.25], [5.61, 2.35]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 38/400, Epsilon:0, Average reward: -32.22\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -28.0, -27.0, -65.0, -46.0, -50.0, -19.0, -35.0, -11.0, -36.0] \n",
      " [-53.0, -57.0, -562.0, -171.0, -1076.0, -289.0, -21.0, -508.0, -40.0, -82.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.54, 0.01], [-1.85, 1.63], [0.4, 1.41], [-2.47, 1.0], [-5.81, 6.31], [4.12, 0.23], [1.07, 2.51], [0.8, 2.1], [0.84, 0.63], [4.1, 2.38]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 39/400, Epsilon:0, Average reward: -37.42\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-22.0, -40.0, -40.0, -39.0, -45.0, -41.0, -30.0, -65.0, -20.0, -32.0] \n",
      " [-38.0, -1283.0, -111.0, -193.0, -241.0, -112.0, -230.0, -338.0, -158.0, -1242.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.89, 3.67], [5.13, -0.58], [-3.1, 5.06], [5.12, -1.23], [0.12, 3.8], [1.05, -0.74], [4.57, -1.17], [-7.9, 6.42], [3.1, -0.76], [-2.1, 2.35]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 40/400, Epsilon:0, Average reward: -39.95\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-71.0, -43.0, -41.0, -61.0, -45.0, -33.0, -54.0, -49.0, -31.0, -43.0] \n",
      " [-1081.0, -107.0, -134.0, -790.0, -251.0, -1864.0, -631.0, -115.0, -238.0, -180.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.18, 3.13], [-3.5, 4.71], [-0.52, 3.35], [-3.34, 5.54], [2.26, -0.25], [0.09, 1.68], [0.52, 1.8], [5.95, -2.59], [6.5, -3.07], [-2.95, 5.29]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 41/400, Epsilon:0, Average reward: -32.85\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -50.0, -51.0, -56.0, -50.0, -59.0, -24.0, -33.0, -40.0, -35.0] \n",
      " [-113.0, -418.0, -162.0, -177.0, -234.0, -589.0, -191.0, -130.0, -172.0, -103.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[4.79, -0.87], [0.97, 1.32], [3.64, -1.9], [2.22, -1.54], [-2.53, 4.52], [5.16, 0.48], [3.47, -0.43], [2.2, 3.19], [0.47, 1.29], [0.62, 0.73]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 42/400, Epsilon:0, Average reward: -29.67\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-64.0, -58.0, -45.0, -35.0, -33.0, -27.0, -73.0, -50.0, -48.0, -52.0] \n",
      " [-849.0, -335.0, -751.0, -178.0, -78.0, -97.0, -400.0, -108.0, -194.0, -54.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.57, 2.11], [0.22, 2.15], [-0.42, 0.2], [1.83, 5.02], [4.51, 0.92], [-0.49, 0.18], [-9.81, 8.67], [5.53, -0.05], [2.43, -0.0], [-0.76, 3.36]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 43/400, Epsilon:0, Average reward: -35.44\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-27.0, -31.0, -50.0, -63.0, -5.0, -23.0, -26.0, -76.0, -54.0, -20.0] \n",
      " [-23.0, -476.0, -256.0, -708.0, -58.0, -83.0, -26.0, -530.0, -732.0, -31.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.0, 0.57], [-2.51, 4.69], [-0.39, 4.64], [0.28, -0.85], [-0.55, 0.63], [2.7, 1.6], [3.37, 0.56], [-6.93, 5.54], [1.13, 2.94], [2.89, -0.8]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 44/400, Epsilon:0, Average reward: -28.35\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-69.0, -58.0, -46.0, -40.0, -19.0, -40.0, -54.0, -37.0, -32.0, -33.0] \n",
      " [-605.0, -221.0, -215.0, -230.0, -48.0, -64.0, -293.0, -75.0, -406.0, -75.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.63, 1.59], [7.95, -2.09], [-2.99, 6.24], [7.69, -0.8], [-1.31, 1.96], [6.83, -5.92], [-2.08, 3.93], [4.93, -2.02], [-0.07, 4.34], [6.96, -3.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 45/400, Epsilon:0, Average reward: -37.33\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-58.0, -0.0, -21.0, -34.0, -26.0, -30.0, -52.0, -80.0, -42.0, -58.0] \n",
      " [-320.0, -35.0, -92.0, -181.0, -385.0, -431.0, -626.0, -719.0, -688.0, -258.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[6.68, -1.67], [-0.0, 0.0], [1.76, 1.47], [0.84, 1.28], [2.38, 0.67], [2.59, -0.56], [2.01, 4.66], [1.49, 1.51], [-1.35, 3.45], [2.52, 3.6]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 46/400, Epsilon:0, Average reward: -29.78\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-32.0, -80.0, -30.0, -36.0, -20.0, -68.0, -48.0, -7.0, -18.0, -30.0] \n",
      " [-22.0, -165.0, -136.0, -38.0, -53.0, -1046.0, -152.0, -26.0, -27.0, -21.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-2.8, 3.29], [4.4, -0.75], [2.91, -0.89], [-3.44, 5.0], [5.18, -0.93], [2.41, 2.76], [3.65, 1.43], [0.55, 0.85], [4.65, -1.82], [-1.3, 1.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 47/400, Epsilon:0, Average reward: -38.07\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-49.0, -66.0, -56.0, -57.0, -51.0, -34.0, -36.0, -43.0, -56.0, -53.0] \n",
      " [-166.0, -138.0, -81.0, -482.0, -196.0, -442.0, -338.0, -462.0, -404.0, -377.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[2.99, -1.27], [-6.21, 6.79], [3.08, 1.55], [2.19, 3.91], [2.82, 2.68], [1.93, 1.64], [-0.02, 3.11], [-0.59, 4.9], [1.38, 0.9], [2.72, -0.68]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 48/400, Epsilon:0, Average reward: -37.95\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-36.0, -42.0, -43.0, -23.0, -65.0, -17.0, -47.0, -50.0, -45.0, -59.0] \n",
      " [-19.0, -209.0, -58.0, -45.0, -280.0, -28.0, -283.0, -559.0, -251.0, -328.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.68, 1.48], [4.55, -0.91], [-1.42, 3.11], [0.57, 1.57], [2.92, -1.6], [1.13, -0.2], [3.43, -0.46], [1.81, 2.38], [-2.44, 2.49], [-1.77, 2.52]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 49/400, Epsilon:0, Average reward: -33.84\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-57.0, -58.0, -16.0, -44.0, -61.0, -54.0, -44.0, -43.0, -35.0, -44.0] \n",
      " [-215.0, -114.0, -73.0, -477.0, -197.0, -622.0, -273.0, -744.0, -45.0, -295.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[3.39, 2.88], [-0.77, 2.15], [1.2, 0.61], [0.49, 2.23], [3.14, -1.43], [0.3, 5.14], [2.95, 1.22], [1.54, -0.19], [6.17, -2.11], [-1.27, 6.66]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 50/400, Epsilon:0, Average reward: -36.09\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-40.0, -23.0, -50.0, -87.0, -46.0, -109.0, -65.0, -35.0, -58.0, -50.0] \n",
      " [-134.0, -98.0, -205.0, -805.0, -131.0, -662.0, -162.0, -56.0, -463.0, -84.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[2.59, 0.81], [1.37, 1.91], [4.07, -1.17], [-2.29, 9.37], [-1.23, 2.22], [-1.95, 1.64], [6.4, 0.94], [-0.51, 2.69], [2.81, 1.28], [0.44, 1.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51/400, Epsilon:0, Average reward: -52.99\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-67.0, -45.0, -73.0, -55.0, -78.0, -44.0, -53.0, -33.0, -26.0, -45.0] \n",
      " [-431.0, -136.0, -1179.0, -240.0, -936.0, -308.0, -109.0, -51.0, -53.0, -136.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.94, 0.95], [3.36, 0.08], [-0.43, 2.53], [6.01, -2.86], [3.54, -1.53], [1.99, 3.11], [4.24, -0.47], [3.05, -0.72], [3.22, -0.01], [3.36, 0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 52/400, Epsilon:0, Average reward: -43.35\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-37.0, -50.0, -78.0, -91.0, -66.0, -54.0, -60.0, -73.0, -73.0, -80.0] \n",
      " [-35.0, -223.0, -1439.0, -953.0, -215.0, -203.0, -244.0, -233.0, -478.0, -300.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.51, 4.41], [4.41, 0.56], [4.14, 2.61], [3.03, -1.64], [-0.01, 4.23], [0.95, 6.1], [2.36, 2.13], [1.45, -0.26], [2.28, -0.9], [3.84, -2.61]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 53/400, Epsilon:0, Average reward: -30.71\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-65.0, -90.0, -48.0, -73.0, -32.0, -74.0, -43.0, -39.0, -78.0, -64.0] \n",
      " [-391.0, -383.0, -60.0, -434.0, -80.0, -759.0, -147.0, -63.0, -159.0, -303.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[4.96, 1.34], [2.73, -1.4], [1.87, 3.4], [0.66, 1.26], [3.33, -1.71], [3.53, -0.95], [-1.5, 0.53], [5.11, 0.46], [1.82, 0.68], [7.1, -3.58]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 54/400, Epsilon:0, Average reward: -30.48\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-32.0, -62.0, -48.0, -42.0, -66.0, -38.0, -25.0, -60.0, -58.0, -51.0] \n",
      " [-131.0, -455.0, -26.0, -114.0, -152.0, -47.0, -122.0, -137.0, -212.0, -73.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[4.87, -0.13], [5.68, -1.67], [3.63, -0.3], [-1.41, 2.43], [1.93, -0.02], [-1.06, 3.99], [1.91, -0.1], [2.95, 1.8], [5.67, -0.7], [5.37, -1.72]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 55/400, Epsilon:0, Average reward: -33.18\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-32.0, -65.0, -43.0, -83.0, -63.0, -41.0, -105.0, -65.0, -71.0, -38.0] \n",
      " [-56.0, -421.0, -154.0, -484.0, -74.0, -128.0, -517.0, -733.0, -282.0, -112.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.89, 0.55], [0.56, 1.51], [1.59, 0.58], [1.67, 4.26], [1.16, 0.27], [2.67, 0.28], [0.91, 0.73], [-0.88, 8.23], [7.48, -0.25], [0.21, 2.41]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 56/400, Epsilon:0, Average reward: -40.98\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-70.0, -97.0, -39.0, -124.0, -35.0, -62.0, -30.0, -87.0, -68.0, -78.0] \n",
      " [-204.0, -895.0, -238.0, -596.0, -203.0, -142.0, -161.0, -129.0, -458.0, -171.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.44, 2.36], [-0.97, 6.25], [-0.47, 1.62], [-1.36, 3.88], [0.77, 1.98], [2.74, 0.21], [2.08, 1.88], [-3.63, 4.17], [-0.17, -3.1], [7.96, -2.32]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 57/400, Epsilon:0, Average reward: -36.88\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-91.0, -50.0, -54.0, -32.0, -65.0, -60.0, -45.0, -44.0, -99.0, -75.0] \n",
      " [-164.0, -42.0, -162.0, -82.0, -95.0, -90.0, -973.0, -192.0, -233.0, -125.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.92, 3.18], [1.76, -0.95], [-3.52, 2.05], [1.7, 0.26], [1.35, 2.09], [-1.53, 0.85], [0.85, 3.52], [0.34, 1.46], [4.19, 0.66], [3.14, 0.52]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n",
      "Episode: 58/400, Epsilon:0, Average reward: -35.32\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-25.0, -14.0, -82.0, -44.0, -99.0, -76.0, -45.0, -41.0, -51.0, -61.0] \n",
      " [-80.0, -54.0, -853.0, -204.0, -265.0, -527.0, -43.0, -122.0, -195.0, -159.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.37, 2.06], [2.7, 1.01], [4.88, -2.57], [-2.94, 1.51], [4.48, -0.58], [-0.34, 0.87], [-1.15, 3.37], [-0.44, 2.17], [-0.26, 2.6], [0.07, 1.77]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Have to find a way to reduce entropy over time entropy = exploration\n",
    "\n",
    "## Converging network\n",
    "# - reward queue, state queue\n",
    "# converging with updates every steps and entropy = 0.00001 and 1 core layer of 42\n",
    "# converging well with updates every steps and entropy = 0.00001 and no core\n",
    "\n",
    "# - reward queue state queues + sig\n",
    "# converging well with updates every steps and entropy = 0.00001 and no core\n",
    "\n",
    "# 64 is a good number\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                simulation_length, timesteps_per_second,\\\n",
    "                                                                delete_results = True, verbose = True)\n",
    "    SF.Select_Vissim_Mode(Vissim,mode)\n",
    "    \n",
    "    runflag = True\n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['AC'] :\n",
    "        Agents = [ACAgent(state_size, action_size, ID, state_type, npa, n_step_size, gamma, alpha, entropy, value, Vissim) for ID in npa.signal_controllers_ids] \n",
    "        for agent in Agents:\n",
    "            # to initialise the computational graph ot the model (I am sure there is a better way to to this)\n",
    "            agent.test()\n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents , reward_storage = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = best)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0\n",
    "                \n",
    "    # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        \n",
    "        \n",
    "        Vissim = None\n",
    "    \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\" or mode == \"retraining\":\n",
    "        print(\"Training\")\n",
    "        \n",
    "            \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                \n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "        \n",
    "            # Run Episode at maximum speed\n",
    "            \n",
    "            SF.Select_Vissim_Mode(Vissim, mode)\n",
    "            \n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated,Surtrac = Surtrac)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "            \n",
    "            for index, agent in enumerate(Agents):\n",
    "                predicted_values, true_values, logit0, logits = agent.value_check(horizon, n_sample)\n",
    "                print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(index, predicted_values, true_values))\n",
    "                print (\"Agent {} : Logits on those states : \\n {}\" .format(index, logits))\n",
    "                print (\"Agent {} : Logits on the 0 state : \\n {}\" .format(index, logit0))\n",
    "               \n",
    "        \n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "            \n",
    "            # line to reduce the entropy of the actor_critic.\n",
    "            if reduce_entropy:\n",
    "                pass\n",
    "            \n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()\n",
    "\n",
    "print(reward_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
