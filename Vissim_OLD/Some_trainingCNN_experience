#################################################
#                                               #
#                   TEST 1                      #
#                                               #
#################################################


Working Directory set to: C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\
Generating Cache...
Cache generated.

****************************
*   COM Server dispatched  *
****************************

Attempting to load Model File: Single_Cross_Mod2.inpx ...
Load process successful
Simulation length set to 3601 seconds.
Results from Previous Simulations: Deleted. Fresh Start Available.
Fetched and containerized Simulation Object
Fetched and containerized Network Object 

*******************************************************
*                                                     *
*                 SETUP COMPLETE                      *
*                                                     *
*******************************************************

Random seed set in simulator. Random Seed = 42
NetworkParser has succesfully crawled the model network.
Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! 
Model: "modelconv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
value_conv1 (Conv2D)         multiple                  3200      
_________________________________________________________________
value2 (Dense)               multiple                  4112      
_________________________________________________________________
value3 (Dense)               multiple                  272       
_________________________________________________________________
value_last (Dense)           multiple                  17        
_________________________________________________________________
value_flat (Flatten)         multiple                  0         
_________________________________________________________________
policy_conv1 (Conv2D)        multiple                  3200      
_________________________________________________________________
policy_logits2 (Dense)       multiple                  4112      
_________________________________________________________________
policy_logits3 (Dense)       multiple                  136       
_________________________________________________________________
policy_last (Dense)          multiple                  18        
_________________________________________________________________
policy_flat (Flatten)        multiple                  0         
_________________________________________________________________
probability_distribution (Pr multiple                  0         
=================================================================
Total params: 15,067
Trainable params: 15,067
Non-trainable params: 0
_________________________________________________________________
To be corected
Deployed 1 agent(s) of the Class AC.
Training
Items: 400
Episode: 1/400, Epsilon:0, Average reward: -281.35
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-3.0, -3.0, -9.0, -3.0, -4.0, -2.0, -3.0, -3.0, -2.0, -9.0] 
 [-221.0, -194.0, -250.0, -203.0, -3414.0, -791.0, -422.0, -209.0, -29.0, -3484.0]
Episode: 2/400, Epsilon:0, Average reward: -263.43
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-13.0, -13.0, -19.0, -2.0, -8.0, -13.0, -3.0, -6.0, -3.0, -8.0] 
 [-189.0, -138.0, -686.0, -3390.0, -563.0, -3359.0, -345.0, -984.0, -3389.0, -3382.0]
Episode: 3/400, Epsilon:0, Average reward: -274.42
Agent 0 : Predicted Values and True Return : 
 [-18.0, -13.0, -17.0, -7.0, -9.0, -16.0, -13.0, -16.0, -23.0, -11.0] 
 [-3368.0, -3384.0, -39.0, -38.0, -3385.0, -2631.0, -2683.0, -131.0, -3510.0, -3468.0]
Episode: 4/400, Epsilon:0, Average reward: -251.52
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-27.0, -6.0, -40.0, -12.0, -15.0, -28.0, -4.0, -30.0, -8.0, -31.0] 
 [-1119.0, -61.0, -1092.0, -248.0, -3412.0, -3323.0, -94.0, -95.0, -92.0, -3295.0]
Episode: 5/400, Epsilon:0, Average reward: -95.85
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-77.0, -21.0, -57.0, -0.0, -15.0, -21.0, -34.0, -18.0, -42.0, -39.0] 
 [-320.0, -282.0, -1367.0, -25.0, -131.0, -210.0, -284.0, -42.0, -818.0, -1551.0]
Episode: 6/400, Epsilon:0, Average reward: -91.44
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-63.0, -26.0, -28.0, -33.0, -58.0, -57.0, -53.0, -69.0, -73.0, -43.0] 
 [-268.0, -123.0, -2617.0, -47.0, -183.0, -74.0, -204.0, -742.0, -75.0, -59.0]
Episode: 7/400, Epsilon:0, Average reward: -46.41
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-66.0, -20.0, -106.0, -127.0, -53.0, -116.0, -120.0, -103.0, -55.0, -107.0] 
 [-1188.0, -96.0, -210.0, -794.0, -150.0, -371.0, -613.0, -332.0, -34.0, -549.0]
Episode: 8/400, Epsilon:0, Average reward: -92.75
Agent 0 : Predicted Values and True Return : 
 [-144.0, -105.0, -50.0, -94.0, -64.0, -37.0, -120.0, -93.0, -94.0, -79.0] 
 [-1021.0, -559.0, -2496.0, -2554.0, -55.0, -71.0, -232.0, -139.0, -1287.0, -2429.0]
Episode: 9/400, Epsilon:0, Average reward: -88.4
Agent 0 : Predicted Values and True Return : 
 [-123.0, -84.0, -78.0, -137.0, -165.0, -46.0, -86.0, -170.0, -118.0, -98.0] 
 [-129.0, -305.0, -338.0, -355.0, -1116.0, -6.0, -86.0, -1115.0, -956.0, -474.0]
Episode: 10/400, Epsilon:0, Average reward: -87.64
Agent 0 : Predicted Values and True Return : 
 [-142.0, -128.0, -27.0, -148.0, -128.0, -82.0, -64.0, -105.0, -99.0, -81.0] 
 [-216.0, -211.0, -77.0, -293.0, -88.0, -1189.0, -145.0, -135.0, -336.0, -203.0]
Episode: 11/400, Epsilon:0, Average reward: -179.21
Agent 0 : Predicted Values and True Return : 
 [-116.0, -154.0, -111.0, -207.0, -124.0, -227.0, -107.0, -174.0, -83.0, -216.0] 
 [-140.0, -102.0, -3263.0, -316.0, -182.0, -1777.0, -135.0, -1117.0, -64.0, -3199.0]
Episode: 12/400, Epsilon:0, Average reward: -185.9
Agent 0 : Predicted Values and True Return : 
 [-261.0, -131.0, -58.0, -154.0, -334.0, -210.0, -121.0, -185.0, -142.0, -189.0] 
 [-500.0, -277.0, -94.0, -248.0, -346.0, -130.0, -3048.0, -520.0, -188.0, -406.0]
Episode: 13/400, Epsilon:0, Average reward: -229.89
Agent 0 : Predicted Values and True Return : 
 [-185.0, -265.0, -203.0, -81.0, -178.0, -117.0, -400.0, -133.0, -334.0, -205.0] 
 [-2935.0, -534.0, -3408.0, -118.0, -121.0, -3319.0, -212.0, -402.0, -1680.0, -117.0]
Episode: 14/400, Epsilon:0, Average reward: -168.27
Agent 0 : Predicted Values and True Return : 
 [-257.0, -241.0, -199.0, -75.0, -259.0, -222.0, -84.0, -257.0, -241.0, -313.0] 
 [-947.0, -62.0, -1228.0, -34.0, -2990.0, -2230.0, -45.0, -947.0, -86.0, -241.0]
Episode: 15/400, Epsilon:0, Average reward: -135.29
Agent 0 : Predicted Values and True Return : 
 [-151.0, -379.0, -237.0, -361.0, -210.0, -230.0, -526.0, -204.0, -203.0, -54.0] 
 [-3358.0, -1509.0, -152.0, -2438.0, -160.0, -3008.0, -879.0, -341.0, -261.0, -49.0]
Episode: 16/400, Epsilon:0, Average reward: -242.41
Agent 0 : Predicted Values and True Return : 
 [-125.0, -260.0, -219.0, -60.0, -321.0, -155.0, -31.0, -160.0, -235.0, -324.0] 
 [-74.0, -87.0, -3388.0, -86.0, -75.0, -119.0, -65.0, -3400.0, -1882.0, -202.0]
Episode: 17/400, Epsilon:0, Average reward: -191.41
Agent 0 : Predicted Values and True Return : 
 [-78.0, -263.0, -174.0, -239.0, -150.0, -270.0, -253.0, -230.0, -294.0, -3.0] 
 [-118.0, -93.0, -3407.0, -3397.0, -3405.0, -136.0, -235.0, -3351.0, -124.0, -66.0]
Episode: 18/400, Epsilon:0, Average reward: -158.67
Agent 0 : Predicted Values and True Return : 
 [-210.0, -296.0, -363.0, -169.0, -343.0, -107.0, -178.0, -336.0, -336.0, -66.0] 
 [-3091.0, -266.0, -1666.0, -58.0, -1226.0, -73.0, -33.0, -149.0, -149.0, -30.0]
Episode: 19/400, Epsilon:0, Average reward: -144.31
Agent 0 : Predicted Values and True Return : 
 [-369.0, -134.0, -254.0, -478.0, -47.0, -304.0, -10.0, -137.0, -324.0, -151.0] 
 [-3300.0, -142.0, -3199.0, -226.0, -54.0, -3144.0, -66.0, -3358.0, -2667.0, -125.0]
Episode: 20/400, Epsilon:0, Average reward: -128.29
Agent 0 : Predicted Values and True Return : 
 [-219.0, -2.0, -204.0, -143.0, -208.0, -80.0, -79.0, -299.0, -138.0, -193.0] 
 [-122.0, -6.0, -193.0, -57.0, -38.0, -20.0, -56.0, -133.0, -145.0, -164.0]
Episode: 21/400, Epsilon:0, Average reward: -200.42
Agent 0 : Predicted Values and True Return : 
 [-432.0, -211.0, -223.0, -164.0, -102.0, -306.0, -487.0, -165.0, -250.0, -196.0] 
 [-125.0, -3391.0, -2988.0, -33.0, -63.0, -84.0, -3303.0, -47.0, -3253.0, -3345.0]
Episode: 22/400, Epsilon:0, Average reward: -159.9
Agent 0 : Predicted Values and True Return : 
 [-215.0, -346.0, -227.0, -267.0, -300.0, -177.0, -205.0, -212.0, -72.0, -250.0] 
 [-40.0, -220.0, -2099.0, -2701.0, -2158.0, -504.0, -155.0, -103.0, -154.0, -1199.0]
Episode: 23/400, Epsilon:0, Average reward: -69.25
Agent 0 : Predicted Values and True Return : 
 [-149.0, -209.0, -105.0, -439.0, -329.0, -232.0, -224.0, -327.0, -147.0, -358.0] 
 [-343.0, -1779.0, -20.0, -206.0, -194.0, -293.0, -371.0, -266.0, -204.0, -1540.0]
Episode: 24/400, Epsilon:0, Average reward: -171.38
Agent 0 : Predicted Values and True Return : 
 [-51.0, -463.0, -3.0, -79.0, -162.0, -376.0, -103.0, -170.0, -199.0, -503.0] 
 [-83.0, -2013.0, -10.0, -70.0, -437.0, -252.0, -23.0, -66.0, -550.0, -1186.0]
Episode: 25/400, Epsilon:0, Average reward: -109.99
Agent 0 : Predicted Values and True Return : 
 [-465.0, -389.0, -563.0, -221.0, -235.0, -441.0, -604.0, -335.0, -430.0, -262.0] 
 [-2015.0, -194.0, -1331.0, -248.0, -378.0, -107.0, -413.0, -884.0, -390.0, -647.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 25.
Episode: 26/400, Epsilon:0, Average reward: -185.15
Agent 0 : Predicted Values and True Return : 
 [-289.0, -406.0, -274.0, -258.0, -544.0, -471.0, -440.0, -389.0, -406.0, -456.0] 
 [-414.0, -3041.0, -121.0, -269.0, -2748.0, -3094.0, -107.0, -166.0, -3041.0, -458.0]
Episode: 27/400, Epsilon:0, Average reward: -241.28
Agent 0 : Predicted Values and True Return : 
 [-446.0, -411.0, -211.0, -378.0, -448.0, -146.0, -371.0, -637.0, -72.0, -416.0] 
 [-2937.0, -202.0, -81.0, -152.0, -2271.0, -3355.0, -72.0, -2896.0, -165.0, -215.0]
Episode: 28/400, Epsilon:0, Average reward: -205.97
Agent 0 : Predicted Values and True Return : 
 [-127.0, -266.0, -551.0, -547.0, -98.0, -163.0, -284.0, -406.0, -353.0, -343.0] 
 [-394.0, -191.0, -3334.0, -394.0, -33.0, -2988.0, -370.0, -3476.0, -77.0, -3367.0]
Episode: 29/400, Epsilon:0, Average reward: -123.02
Agent 0 : Predicted Values and True Return : 
 [-300.0, -283.0, -245.0, -375.0, -493.0, -515.0, -287.0, -125.0, -405.0, -332.0] 
 [-69.0, -447.0, -2401.0, -98.0, -85.0, -338.0, -2066.0, -88.0, -416.0, -81.0]
Episode: 30/400, Epsilon:0, Average reward: -217.23
Agent 0 : Predicted Values and True Return : 
 [-612.0, -339.0, -100.0, -633.0, -333.0, -633.0, -265.0, -298.0, -177.0, -275.0] 
 [-2915.0, -191.0, -38.0, -1807.0, -3041.0, -1807.0, -2800.0, -139.0, -133.0, -237.0]
Episode: 31/400, Epsilon:0, Average reward: -215.05
Agent 0 : Predicted Values and True Return : 
 [-604.0, -514.0, -95.0, -535.0, -312.0, -438.0, -257.0, -166.0, -453.0, -407.0] 
 [-2316.0, -187.0, -12.0, -205.0, -3412.0, -181.0, -3356.0, -80.0, -2867.0, -199.0]
Episode: 32/400, Epsilon:0, Average reward: -135.54
Agent 0 : Predicted Values and True Return : 
 [-183.0, -523.0, -484.0, -391.0, -141.0, -230.0, -429.0, -490.0, -484.0, -375.0] 
 [-37.0, -155.0, -2583.0, -2413.0, -536.0, -65.0, -2819.0, -244.0, -2583.0, -132.0]
Episode: 33/400, Epsilon:0, Average reward: -233.15
Agent 0 : Predicted Values and True Return : 
 [-580.0, -181.0, -74.0, -257.0, -92.0, -88.0, -192.0, -328.0, -577.0, -265.0] 
 [-202.0, -24.0, -109.0, -131.0, -62.0, -3399.0, -3095.0, -3214.0, -223.0, -56.0]
Episode: 34/400, Epsilon:0, Average reward: -59.47
Agent 0 : Predicted Values and True Return : 
 [-238.0, -153.0, -642.0, -234.0, -56.0, -254.0, -212.0, -584.0, -218.0, -293.0] 
 [-1415.0, -779.0, -421.0, -59.0, -17.0, -69.0, -1024.0, -139.0, -188.0, -191.0]
Episode: 35/400, Epsilon:0, Average reward: -72.3
Agent 0 : Predicted Values and True Return : 
 [-478.0, -414.0, -382.0, -430.0, -260.0, -362.0, -305.0, -275.0, -839.0, -160.0] 
 [-681.0, -63.0, -322.0, -116.0, -2073.0, -1803.0, -807.0, -230.0, -359.0, -395.0]
Episode: 36/400, Epsilon:0, Average reward: -95.82
Agent 0 : Predicted Values and True Return : 
 [-288.0, -246.0, -216.0, -241.0, -476.0, -369.0, -105.0, -160.0, -105.0, -219.0] 
 [-130.0, -137.0, -1820.0, -76.0, -1484.0, -570.0, -2373.0, -70.0, -2373.0, -1827.0]
Episode: 37/400, Epsilon:0, Average reward: -148.95
Agent 0 : Predicted Values and True Return : 
 [-398.0, -176.0, -242.0, -391.0, -363.0, -26.0, -85.0, -305.0, -426.0, -483.0] 
 [-2581.0, -77.0, -95.0, -216.0, -3015.0, -19.0, -48.0, -168.0, -349.0, -1525.0]
Episode: 38/400, Epsilon:0, Average reward: -80.12
Agent 0 : Predicted Values and True Return : 
 [-135.0, -284.0, -242.0, -351.0, -220.0, -437.0, -373.0, -237.0, -662.0, -394.0] 
 [-69.0, -2285.0, -253.0, -907.0, -460.0, -576.0, -1048.0, -2121.0, -1066.0, -181.0]
Episode: 39/400, Epsilon:0, Average reward: -80.71
Agent 0 : Predicted Values and True Return : 
 [-199.0, -510.0, -238.0, -312.0, -351.0, -275.0, -147.0, -3.0, -514.0, -109.0] 
 [-55.0, -143.0, -2340.0, -88.0, -240.0, -205.0, -97.0, -23.0, -1957.0, -187.0]
Episode: 40/400, Epsilon:0, Average reward: -141.35
Agent 0 : Predicted Values and True Return : 
 [-68.0, -217.0, -365.0, -178.0, -296.0, -67.0, -305.0, -190.0, -516.0, -179.0] 
 [-44.0, -2107.0, -592.0, -44.0, -91.0, -3385.0, -2751.0, -114.0, -449.0, -75.0]
Episode: 41/400, Epsilon:0, Average reward: -161.6
Agent 0 : Predicted Values and True Return : 
 [-301.0, -208.0, -89.0, -378.0, -244.0, -225.0, -57.0, -3.0, -195.0, -284.0] 
 [-2672.0, -2221.0, -58.0, -317.0, -2455.0, -111.0, -53.0, -50.0, -274.0, -85.0]
Episode: 42/400, Epsilon:0, Average reward: -44.51
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-419.0, -308.0, -197.0, -301.0, -439.0, -151.0, -364.0, -168.0, -222.0, -228.0] 
 [-668.0, -116.0, -480.0, -255.0, -237.0, -385.0, -570.0, -196.0, -160.0, -75.0]
Episode: 43/400, Epsilon:0, Average reward: -62.64
Agent 0 : Predicted Values and True Return : 
 [-80.0, -98.0, -334.0, -417.0, -354.0, -174.0, -169.0, -130.0, -161.0, -142.0] 
 [-37.0, -1623.0, -56.0, -250.0, -1555.0, -750.0, -132.0, -34.0, -71.0, -1387.0]
Episode: 44/400, Epsilon:0, Average reward: -71.47
Agent 0 : Predicted Values and True Return : 
 [-329.0, -319.0, -343.0, -279.0, -335.0, -381.0, -353.0, -189.0, -197.0, -227.0] 
 [-908.0, -221.0, -160.0, -641.0, -573.0, -966.0, -739.0, -1007.0, -108.0, -108.0]
Episode: 45/400, Epsilon:0, Average reward: -87.46
Agent 0 : Predicted Values and True Return : 
 [-526.0, -164.0, -320.0, -352.0, -164.0, -254.0, -119.0, -499.0, -3.0, -292.0] 
 [-787.0, -8.0, -636.0, -781.0, -1495.0, -1517.0, -16.0, -268.0, -32.0, -290.0]
Episode: 46/400, Epsilon:0, Average reward: -123.09
Agent 0 : Predicted Values and True Return : 
 [-200.0, -261.0, -3.0, -297.0, -244.0, -390.0, -324.0, -401.0, -191.0, -151.0] 
 [-82.0, -3262.0, -40.0, -69.0, -3317.0, -281.0, -3184.0, -125.0, -123.0, -3258.0]
Episode: 47/400, Epsilon:0, Average reward: -47.48
Agent 0 : Predicted Values and True Return : 
 [-156.0, -207.0, -198.0, -109.0, -325.0, -133.0, -77.0, -465.0, -97.0, -310.0] 
 [-114.0, -127.0, -1177.0, -43.0, -298.0, -276.0, -37.0, -995.0, -38.0, -78.0]
Episode: 48/400, Epsilon:0, Average reward: -40.02
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-144.0, -256.0, -414.0, -91.0, -171.0, -171.0, -42.0, -391.0, -332.0, -304.0] 
 [-41.0, -332.0, -337.0, -45.0, -316.0, -316.0, -62.0, -446.0, -683.0, -243.0]
Episode: 49/400, Epsilon:0, Average reward: -93.39
Agent 0 : Predicted Values and True Return : 
 [-99.0, -51.0, -160.0, -274.0, -240.0, -310.0, -238.0, -255.0, -233.0, -128.0] 
 [-1195.0, -66.0, -262.0, -156.0, -129.0, -782.0, -3313.0, -652.0, -208.0, -76.0]
Episode: 50/400, Epsilon:0, Average reward: -31.4
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-99.0, -209.0, -142.0, -334.0, -109.0, -260.0, -108.0, -61.0, -53.0, -166.0] 
 [-90.0, -112.0, -24.0, -486.0, -70.0, -360.0, -71.0, -118.0, -223.0, -304.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 50.
Episode: 51/400, Epsilon:0, Average reward: -42.09
Agent 0 : Predicted Values and True Return : 
 [-108.0, -434.0, -242.0, -129.0, -148.0, -3.0, -298.0, -193.0, -64.0, -265.0] 
 [-88.0, -682.0, -237.0, -182.0, -279.0, -54.0, -1335.0, -169.0, -111.0, -233.0]
Episode: 52/400, Epsilon:0, Average reward: -42.19
Agent 0 : Predicted Values and True Return : 
 [-241.0, -235.0, -293.0, -291.0, -385.0, -141.0, -242.0, -170.0, -392.0, -243.0] 
 [-542.0, -421.0, -125.0, -473.0, -447.0, -453.0, -813.0, -197.0, -385.0, -318.0]
Episode: 53/400, Epsilon:0, Average reward: -49.87
Agent 0 : Predicted Values and True Return : 
 [-208.0, -79.0, -166.0, -86.0, -150.0, -174.0, -99.0, -134.0, -211.0, -387.0] 
 [-237.0, -45.0, -93.0, -130.0, -121.0, -95.0, -159.0, -51.0, -53.0, -699.0]
Episode: 54/400, Epsilon:0, Average reward: -27.9
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-113.0, -164.0, -57.0, -180.0, -110.0, -333.0, -165.0, -200.0, -138.0, -159.0] 
 [-43.0, -156.0, -91.0, -75.0, -73.0, -413.0, -91.0, -231.0, -101.0, -97.0]
Episode: 55/400, Epsilon:0, Average reward: -55.07
Agent 0 : Predicted Values and True Return : 
 [-123.0, -170.0, -39.0, -93.0, -145.0, -185.0, -196.0, -2.0, -67.0, -168.0] 
 [-27.0, -88.0, -74.0, -33.0, -307.0, -651.0, -199.0, -38.0, -86.0, -225.0]
Episode: 56/400, Epsilon:0, Average reward: -83.54
Agent 0 : Predicted Values and True Return : 
 [-131.0, -70.0, -125.0, -187.0, -292.0, -190.0, -98.0, -227.0, -113.0, -231.0] 
 [-26.0, -352.0, -91.0, -1127.0, -1287.0, -157.0, -59.0, -547.0, -105.0, -210.0]
Episode: 57/400, Epsilon:0, Average reward: -51.31
Agent 0 : Predicted Values and True Return : 
 [-154.0, -81.0, -231.0, -186.0, -193.0, -269.0, -190.0, -44.0, -161.0, -175.0] 
 [-118.0, -40.0, -304.0, -118.0, -215.0, -217.0, -312.0, -28.0, -108.0, -425.0]
Episode: 58/400, Epsilon:0, Average reward: -34.1
Agent 0 : Predicted Values and True Return : 
 [-45.0, -128.0, -187.0, -152.0, -172.0, -122.0, -124.0, -200.0, -155.0, -114.0] 
 [-138.0, -17.0, -143.0, -202.0, -245.0, -590.0, -125.0, -791.0, -179.0, -14.0]
Episode: 59/400, Epsilon:0, Average reward: -33.93
Agent 0 : Predicted Values and True Return : 
 [-123.0, -120.0, -180.0, -175.0, -158.0, -284.0, -225.0, -221.0, -36.0, -202.0] 
 [-46.0, -45.0, -127.0, -457.0, -540.0, -565.0, -218.0, -170.0, -54.0, -97.0]
Episode: 60/400, Epsilon:0, Average reward: -39.21
Agent 0 : Predicted Values and True Return : 
 [-173.0, -119.0, -91.0, -71.0, -169.0, -64.0, -157.0, -69.0, -204.0, -113.0] 
 [-973.0, -76.0, -101.0, -68.0, -208.0, -66.0, -80.0, -67.0, -615.0, -63.0]
Episode: 61/400, Epsilon:0, Average reward: -49.95
Agent 0 : Predicted Values and True Return : 
 [-192.0, -139.0, -130.0, -123.0, -169.0, -253.0, -241.0, -173.0, -140.0, -228.0] 
 [-146.0, -36.0, -97.0, -69.0, -154.0, -173.0, -436.0, -96.0, -931.0, -1989.0]
Episode: 62/400, Epsilon:0, Average reward: -36.52
Agent 0 : Predicted Values and True Return : 
 [-179.0, -102.0, -184.0, -209.0, -235.0, -98.0, -210.0, -126.0, -203.0, -76.0] 
 [-242.0, -326.0, -107.0, -136.0, -765.0, -56.0, -1067.0, -1029.0, -280.0, -73.0]
Episode: 63/400, Epsilon:0, Average reward: -47.74
Agent 0 : Predicted Values and True Return : 
 [-121.0, -170.0, -173.0, -187.0, -162.0, -188.0, -86.0, -15.0, -128.0, -185.0] 
 [-76.0, -756.0, -1593.0, -233.0, -17.0, -67.0, -28.0, -57.0, -64.0, -1321.0]
Episode: 64/400, Epsilon:0, Average reward: -93.38
Agent 0 : Predicted Values and True Return : 
 [-92.0, -92.0, -96.0, -264.0, -168.0, -160.0, -138.0, -300.0, -159.0, -180.0] 
 [-56.0, -592.0, -92.0, -284.0, -1602.0, -206.0, -2898.0, -298.0, -107.0, -716.0]
Episode: 65/400, Epsilon:0, Average reward: -97.94
Agent 0 : Predicted Values and True Return : 
 [-151.0, -83.0, -198.0, -332.0, -48.0, -99.0, -175.0, -35.0, -75.0, -260.0] 
 [-114.0, -3324.0, -86.0, -179.0, -17.0, -75.0, -3222.0, -81.0, -67.0, -1190.0]
Episode: 66/400, Epsilon:0, Average reward: -44.89
Agent 0 : Predicted Values and True Return : 
 [-129.0, -3.0, -113.0, -80.0, -118.0, -118.0, -75.0, -255.0, -146.0, -227.0] 
 [-157.0, -94.0, -65.0, -91.0, -161.0, -161.0, -126.0, -958.0, -206.0, -231.0]
Episode: 67/400, Epsilon:0, Average reward: -36.61
Agent 0 : Predicted Values and True Return : 
 [-157.0, -159.0, -97.0, -200.0, -163.0, -148.0, -86.0, -209.0, -15.0, -278.0] 
 [-166.0, -273.0, -192.0, -193.0, -232.0, -56.0, -64.0, -224.0, -42.0, -734.0]
Episode: 68/400, Epsilon:0, Average reward: -37.51
Agent 0 : Predicted Values and True Return : 
 [-113.0, -208.0, -103.0, -216.0, -169.0, -201.0, -181.0, -164.0, -110.0, -188.0] 
 [-117.0, -250.0, -180.0, -223.0, -160.0, -305.0, -232.0, -51.0, -75.0, -1161.0]
Episode: 69/400, Epsilon:0, Average reward: -32.92
Agent 0 : Predicted Values and True Return : 
 [-59.0, -2.0, -120.0, -59.0, -256.0, -208.0, -195.0, -255.0, -178.0, -87.0] 
 [-47.0, -45.0, -102.0, -161.0, -718.0, -185.0, -1173.0, -785.0, -260.0, -144.0]
Episode: 70/400, Epsilon:0, Average reward: -44.76
Agent 0 : Predicted Values and True Return : 
 [-87.0, -143.0, -63.0, -103.0, -239.0, -66.0, -244.0, -131.0, -80.0, -147.0] 
 [-663.0, -93.0, -690.0, -163.0, -604.0, -29.0, -655.0, -314.0, -12.0, -68.0]
Episode: 71/400, Epsilon:0, Average reward: -38.34
Agent 0 : Predicted Values and True Return : 
 [-44.0, -105.0, -119.0, -82.0, -145.0, -5.0, -266.0, -84.0, -179.0, -114.0] 
 [-35.0, -182.0, -743.0, -49.0, -20.0, -53.0, -368.0, -21.0, -688.0, -129.0]
Episode: 72/400, Epsilon:0, Average reward: -32.9
Agent 0 : Predicted Values and True Return : 
 [-206.0, -174.0, -125.0, -137.0, -120.0, -162.0, -63.0, -239.0, -123.0, -173.0] 
 [-680.0, -187.0, -194.0, -159.0, -139.0, -56.0, -71.0, -215.0, -165.0, -141.0]
Episode: 73/400, Epsilon:0, Average reward: -47.64
Agent 0 : Predicted Values and True Return : 
 [-73.0, -201.0, -208.0, -109.0, -96.0, -137.0, -109.0, -126.0, -174.0, -227.0] 
 [-65.0, -1447.0, -759.0, -43.0, -48.0, -543.0, -43.0, -56.0, -252.0, -1375.0]
Episode: 74/400, Epsilon:0, Average reward: -27.94
Agent 0 : Predicted Values and True Return : 
 [-130.0, -170.0, -175.0, -234.0, -243.0, -98.0, -153.0, -137.0, -200.0, -193.0] 
 [-109.0, -258.0, -142.0, -743.0, -304.0, -65.0, -146.0, -178.0, -270.0, -127.0]
Episode: 75/400, Epsilon:0, Average reward: -33.59
Agent 0 : Predicted Values and True Return : 
 [-187.0, -219.0, -67.0, -194.0, -2.0, -191.0, -120.0, -199.0, -2.0, -201.0] 
 [-530.0, -424.0, -129.0, -171.0, -28.0, -910.0, -79.0, -214.0, -16.0, -878.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 75.
Episode: 76/400, Epsilon:0, Average reward: -30.24
Agent 0 : Predicted Values and True Return : 
 [-211.0, -79.0, -249.0, -130.0, -25.0, -130.0, -69.0, -180.0, -240.0, -242.0] 
 [-173.0, -80.0, -346.0, -504.0, -39.0, -337.0, -41.0, -252.0, -382.0, -299.0]
Episode: 77/400, Epsilon:0, Average reward: -28.78
Agent 0 : Predicted Values and True Return : 
 [-168.0, -115.0, -59.0, -64.0, -146.0, -67.0, -181.0, -155.0, -2.0, -233.0] 
 [-100.0, -82.0, -68.0, -26.0, -153.0, -17.0, -783.0, -260.0, -20.0, -784.0]
Episode: 78/400, Epsilon:0, Average reward: -36.3
Agent 0 : Predicted Values and True Return : 
 [-168.0, -41.0, -122.0, -106.0, -271.0, -283.0, -191.0, -74.0, -45.0, -59.0] 
 [-99.0, -10.0, -186.0, -90.0, -787.0, -887.0, -302.0, -35.0, -14.0, -56.0]
Episode: 79/400, Epsilon:0, Average reward: -27.52
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-150.0, -142.0, -173.0, -236.0, -227.0, -162.0, -74.0, -224.0, -59.0, -129.0] 
 [-245.0, -383.0, -299.0, -275.0, -409.0, -268.0, -42.0, -492.0, -67.0, -27.0]
Episode: 80/400, Epsilon:0, Average reward: -27.22
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-135.0, -174.0, -131.0, -264.0, -179.0, -139.0, -300.0, -202.0, -214.0, -204.0] 
 [-98.0, -592.0, -81.0, -371.0, -341.0, -158.0, -318.0, -186.0, -319.0, -361.0]
Episode: 81/400, Epsilon:0, Average reward: -74.46
Agent 0 : Predicted Values and True Return : 
 [-3.0, -223.0, -48.0, -155.0, -72.0, -145.0, -242.0, -115.0, -68.0, -177.0] 
 [-53.0, -119.0, -44.0, -69.0, -29.0, -98.0, -2630.0, -615.0, -275.0, -573.0]
Episode: 82/400, Epsilon:0, Average reward: -31.16
Agent 0 : Predicted Values and True Return : 
 [-170.0, -138.0, -94.0, -42.0, -226.0, -96.0, -2.0, -159.0, -123.0, -158.0] 
 [-738.0, -111.0, -91.0, -26.0, -346.0, -92.0, -19.0, -312.0, -103.0, -181.0]
Episode: 83/400, Epsilon:0, Average reward: -48.35
Agent 0 : Predicted Values and True Return : 
 [-158.0, -202.0, -267.0, -195.0, -82.0, -234.0, -241.0, -168.0, -128.0, -86.0] 
 [-129.0, -298.0, -264.0, -182.0, -85.0, -271.0, -203.0, -604.0, -946.0, -64.0]
Episode: 84/400, Epsilon:0, Average reward: -63.22
Agent 0 : Predicted Values and True Return : 
 [-164.0, -76.0, -155.0, -147.0, -124.0, -85.0, -76.0, -185.0, -112.0, -233.0] 
 [-135.0, -29.0, -422.0, -103.0, -75.0, -135.0, -141.0, -330.0, -100.0, -689.0]
Episode: 85/400, Epsilon:0, Average reward: -55.51
Agent 0 : Predicted Values and True Return : 
 [-224.0, -44.0, -56.0, -133.0, -252.0, -121.0, -252.0, -46.0, -158.0, -142.0] 
 [-307.0, -31.0, -107.0, -63.0, -115.0, -103.0, -115.0, -25.0, -218.0, -50.0]
Episode: 86/400, Epsilon:0, Average reward: -32.76
Agent 0 : Predicted Values and True Return : 
 [-175.0, -131.0, -99.0, -107.0, -166.0, -181.0, -62.0, -87.0, -166.0, -96.0] 
 [-500.0, -84.0, -38.0, -723.0, -192.0, -320.0, -77.0, -352.0, -84.0, -44.0]
Episode: 87/400, Epsilon:0, Average reward: -44.52
Agent 0 : Predicted Values and True Return : 
 [-101.0, -195.0, -82.0, -71.0, -189.0, -125.0, -152.0, -55.0, -117.0, -90.0] 
 [-103.0, -946.0, -83.0, -23.0, -304.0, -990.0, -217.0, -20.0, -38.0, -26.0]
Episode: 88/400, Epsilon:0, Average reward: -58.35
Agent 0 : Predicted Values and True Return : 
 [-254.0, -155.0, -106.0, -68.0, -177.0, -202.0, -133.0, -161.0, -166.0, -107.0] 
 [-1008.0, -64.0, -455.0, -44.0, -131.0, -102.0, -1250.0, -88.0, -1080.0, -35.0]
Episode: 89/400, Epsilon:0, Average reward: -72.0
Agent 0 : Predicted Values and True Return : 
 [-144.0, -83.0, -225.0, -102.0, -159.0, -117.0, -92.0, -70.0, -161.0, -152.0] 
 [-601.0, -169.0, -1290.0, -32.0, -209.0, -94.0, -247.0, -8.0, -247.0, -126.0]
Episode: 90/400, Epsilon:0, Average reward: -28.56
Agent 0 : Predicted Values and True Return : 
 [-182.0, -185.0, -148.0, -100.0, -125.0, -129.0, -189.0, -75.0, -57.0, -140.0] 
 [-58.0, -356.0, -104.0, -49.0, -139.0, -44.0, -596.0, -52.0, -65.0, -31.0]
Episode: 91/400, Epsilon:0, Average reward: -94.25
Agent 0 : Predicted Values and True Return : 
 [-185.0, -68.0, -176.0, -47.0, -78.0, -67.0, -197.0, -2.0, -87.0, -292.0] 
 [-2269.0, -35.0, -575.0, -87.0, -45.0, -20.0, -3054.0, -14.0, -17.0, -983.0]
Episode: 92/400, Epsilon:0, Average reward: -82.25
Agent 0 : Predicted Values and True Return : 
 [-18.0, -111.0, -111.0, -86.0, -113.0, -53.0, -84.0, -109.0, -110.0, -131.0] 
 [-21.0, -74.0, -3101.0, -105.0, -102.0, -54.0, -17.0, -110.0, -247.0, -70.0]
Episode: 93/400, Epsilon:0, Average reward: -86.57
Agent 0 : Predicted Values and True Return : 
 [-167.0, -238.0, -68.0, -156.0, -91.0, -144.0, -103.0, -111.0, -62.0, -80.0] 
 [-247.0, -401.0, -2.0, -3358.0, -27.0, -3370.0, -233.0, -205.0, -1.0, -2.0]
Episode: 94/400, Epsilon:0, Average reward: -37.43
Agent 0 : Predicted Values and True Return : 
 [-53.0, -280.0, -112.0, -198.0, -149.0, -115.0, -250.0, -273.0, -123.0, -70.0] 
 [-59.0, -482.0, -45.0, -250.0, -79.0, -67.0, -367.0, -380.0, -27.0, -29.0]
Episode: 95/400, Epsilon:0, Average reward: -29.17
Agent 0 : Predicted Values and True Return : 
 [-97.0, -109.0, -181.0, -98.0, -178.0, -86.0, -8.0, -182.0, -105.0, -20.0] 
 [-35.0, -347.0, -504.0, -62.0, -226.0, -38.0, -20.0, -732.0, -189.0, -28.0]
Episode: 96/400, Epsilon:0, Average reward: -31.07
Agent 0 : Predicted Values and True Return : 
 [-115.0, -76.0, -203.0, -75.0, -234.0, -187.0, -64.0, -118.0, -142.0, -171.0] 
 [-85.0, -16.0, -162.0, -156.0, -115.0, -454.0, -189.0, -163.0, -93.0, -603.0]
Episode: 97/400, Epsilon:0, Average reward: -23.27
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-180.0, -95.0, -59.0, -129.0, -153.0, -133.0, -133.0, -116.0, -63.0, -84.0] 
 [-218.0, -44.0, -21.0, -136.0, -166.0, -160.0, -160.0, -610.0, -38.0, -20.0]
Episode: 98/400, Epsilon:0, Average reward: -63.56
Agent 0 : Predicted Values and True Return : 
 [-87.0, -245.0, -192.0, -79.0, -195.0, -153.0, -209.0, -115.0, -138.0, -64.0] 
 [-2962.0, -517.0, -305.0, -2878.0, -510.0, -194.0, -321.0, -71.0, -2505.0, -44.0]
Episode: 99/400, Epsilon:0, Average reward: -63.07
Agent 0 : Predicted Values and True Return : 
 [-213.0, -219.0, -66.0, -146.0, -108.0, -139.0, -148.0, -148.0, -57.0, -72.0] 
 [-552.0, -480.0, -56.0, -139.0, -3004.0, -266.0, -79.0, -401.0, -62.0, -39.0]
Episode: 100/400, Epsilon:0, Average reward: -52.83
Agent 0 : Predicted Values and True Return : 
 [-140.0, -74.0, -113.0, -215.0, -144.0, -90.0, -160.0, -259.0, -150.0, -36.0] 
 [-329.0, -64.0, -125.0, -989.0, -114.0, -159.0, -1954.0, -929.0, -1710.0, -44.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 100.
Episode: 101/400, Epsilon:0, Average reward: -57.45
Agent 0 : Predicted Values and True Return : 
 [-222.0, -145.0, -111.0, -165.0, -200.0, -59.0, -84.0, -103.0, -80.0, -92.0] 
 [-2147.0, -10.0, -173.0, -108.0, -1241.0, -221.0, -15.0, -74.0, -9.0, -76.0]
Episode: 102/400, Epsilon:0, Average reward: -32.61
Agent 0 : Predicted Values and True Return : 
 [-199.0, -209.0, -131.0, -172.0, -32.0, -110.0, -157.0, -214.0, -183.0, -66.0] 
 [-169.0, -232.0, -81.0, -141.0, -67.0, -146.0, -162.0, -995.0, -203.0, -35.0]
Episode: 103/400, Epsilon:0, Average reward: -41.52
Agent 0 : Predicted Values and True Return : 
 [-133.0, -3.0, -146.0, -227.0, -51.0, -161.0, -85.0, -122.0, -166.0, -182.0] 
 [-852.0, -34.0, -122.0, -172.0, -42.0, -847.0, -25.0, -26.0, -1012.0, -755.0]
Episode: 104/400, Epsilon:0, Average reward: -31.51
Agent 0 : Predicted Values and True Return : 
 [-204.0, -202.0, -168.0, -120.0, -76.0, -174.0, -111.0, -160.0, -127.0, -187.0] 
 [-879.0, -585.0, -254.0, -224.0, -48.0, -597.0, -127.0, -73.0, -70.0, -202.0]
Episode: 105/400, Epsilon:0, Average reward: -23.34
Agent 0 : Predicted Values and True Return : 
 [-62.0, -160.0, -101.0, -207.0, -163.0, -169.0, -174.0, -51.0, -148.0, -117.0] 
 [-25.0, -226.0, -174.0, -299.0, -386.0, -197.0, -113.0, -101.0, -110.0, -19.0]
Episode: 106/400, Epsilon:0, Average reward: -30.21
Agent 0 : Predicted Values and True Return : 
 [-197.0, -148.0, -84.0, -148.0, -171.0, -39.0, -104.0, -210.0, -98.0, -3.0] 
 [-767.0, -118.0, -124.0, -26.0, -174.0, -24.0, -51.0, -163.0, -44.0, -22.0]
Episode: 107/400, Epsilon:0, Average reward: -35.4
Agent 0 : Predicted Values and True Return : 
 [-204.0, -189.0, -62.0, -118.0, -115.0, -162.0, -250.0, -163.0, -243.0, -49.0] 
 [-391.0, -255.0, -129.0, -207.0, -44.0, -301.0, -529.0, -427.0, -822.0, -43.0]
Episode: 108/400, Epsilon:0, Average reward: -185.61
Agent 0 : Predicted Values and True Return : 
 [-178.0, -131.0, -250.0, -161.0, -134.0, -199.0, -246.0, -203.0, -188.0, -122.0] 
 [-2357.0, -51.0, -241.0, -3323.0, -77.0, -237.0, -183.0, -2606.0, -2285.0, -123.0]
Episode: 109/400, Epsilon:0, Average reward: -37.15
Agent 0 : Predicted Values and True Return : 
 [-249.0, -114.0, -102.0, -42.0, -247.0, -157.0, -159.0, -110.0, -120.0, -200.0] 
 [-407.0, -214.0, -64.0, -17.0, -88.0, -86.0, -602.0, -216.0, -181.0, -249.0]
Episode: 110/400, Epsilon:0, Average reward: -35.43
Agent 0 : Predicted Values and True Return : 
 [-195.0, -22.0, -86.0, -148.0, -42.0, -142.0, -156.0, -149.0, -133.0, -158.0] 
 [-155.0, -47.0, -65.0, -96.0, -89.0, -123.0, -114.0, -122.0, -300.0, -196.0]
Episode: 111/400, Epsilon:0, Average reward: -59.34
Agent 0 : Predicted Values and True Return : 
 [-50.0, -125.0, -186.0, -133.0, -180.0, -182.0, -108.0, -122.0, -59.0, -198.0] 
 [-230.0, -108.0, -322.0, -47.0, -115.0, -414.0, -65.0, -3255.0, -30.0, -1367.0]
Episode: 112/400, Epsilon:0, Average reward: -160.54
Agent 0 : Predicted Values and True Return : 
 [-90.0, -108.0, -164.0, -164.0, -255.0, -87.0, -115.0, -96.0, -56.0, -118.0] 
 [-22.0, -17.0, -65.0, -65.0, -258.0, -32.0, -44.0, -53.0, -39.0, -85.0]
Episode: 113/400, Epsilon:0, Average reward: -246.28
Agent 0 : Predicted Values and True Return : 
 [-197.0, -201.0, -110.0, -223.0, -240.0, -100.0, -190.0, -104.0, -121.0, -45.0] 
 [-145.0, -3436.0, -22.0, -3391.0, -230.0, -3413.0, -69.0, -42.0, -105.0, -60.0]
Episode: 114/400, Epsilon:0, Average reward: -150.76
Agent 0 : Predicted Values and True Return : 
 [-162.0, -272.0, -71.0, -167.0, -127.0, -123.0, -81.0, -177.0, -135.0, -190.0] 
 [-47.0, -384.0, -41.0, -3341.0, -88.0, -3432.0, -93.0, -63.0, -3442.0, -3317.0]
Episode: 115/400, Epsilon:0, Average reward: -36.53
Agent 0 : Predicted Values and True Return : 
 [-245.0, -177.0, -200.0, -4.0, -102.0, -80.0, -193.0, -97.0, -121.0, -146.0] 
 [-267.0, -62.0, -442.0, -32.0, -228.0, -39.0, -170.0, -56.0, -37.0, -90.0]
Episode: 116/400, Epsilon:0, Average reward: -26.02
Agent 0 : Predicted Values and True Return : 
 [-47.0, -92.0, -256.0, -23.0, -207.0, -151.0, -338.0, -295.0, -160.0, -151.0] 
 [-34.0, -47.0, -428.0, -126.0, -170.0, -110.0, -561.0, -407.0, -321.0, -182.0]
Episode: 117/400, Epsilon:0, Average reward: -21.07
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-288.0, -159.0, -226.0, -123.0, -177.0, -85.0, -249.0, -136.0, -91.0, -105.0] 
 [-405.0, -64.0, -255.0, -163.0, -73.0, -78.0, -145.0, -277.0, -64.0, -67.0]
Episode: 118/400, Epsilon:0, Average reward: -168.91
Agent 0 : Predicted Values and True Return : 
 [-248.0, -199.0, -84.0, -24.0, -76.0, -153.0, -3.0, -22.0, -73.0, -78.0] 
 [-786.0, -2847.0, -100.0, -31.0, -56.0, -3152.0, -24.0, -46.0, -129.0, -68.0]
Episode: 119/400, Epsilon:0, Average reward: -32.49
Agent 0 : Predicted Values and True Return : 
 [-144.0, -192.0, -88.0, -324.0, -125.0, -64.0, -160.0, -88.0, -68.0, -168.0] 
 [-164.0, -210.0, -694.0, -511.0, -500.0, -66.0, -188.0, -84.0, -64.0, -574.0]
Episode: 120/400, Epsilon:0, Average reward: -34.9
Agent 0 : Predicted Values and True Return : 
 [-106.0, -257.0, -69.0, -122.0, -78.0, -106.0, -25.0, -61.0, -35.0, -166.0] 
 [-25.0, -606.0, -41.0, -109.0, -13.0, -93.0, -13.0, -26.0, -26.0, -1788.0]
Episode: 121/400, Epsilon:0, Average reward: -33.1
Agent 0 : Predicted Values and True Return : 
 [-166.0, -121.0, -221.0, -175.0, -133.0, -124.0, -170.0, -106.0, -118.0, -221.0] 
 [-247.0, -182.0, -256.0, -161.0, -209.0, -271.0, -311.0, -29.0, -63.0, -203.0]
Episode: 122/400, Epsilon:0, Average reward: -82.61
Agent 0 : Predicted Values and True Return : 
 [-228.0, -79.0, -142.0, -84.0, -154.0, -93.0, -175.0, -197.0, -166.0, -133.0] 
 [-758.0, -52.0, -3371.0, -89.0, -223.0, -219.0, -424.0, -259.0, -335.0, -282.0]
Episode: 123/400, Epsilon:0, Average reward: -62.15
Agent 0 : Predicted Values and True Return : 
 [-51.0, -167.0, -136.0, -94.0, -80.0, -152.0, -122.0, -139.0, -74.0, -58.0] 
 [-40.0, -209.0, -563.0, -70.0, -42.0, -179.0, -150.0, -2627.0, -123.0, -159.0]
Episode: 124/400, Epsilon:0, Average reward: -137.6
Agent 0 : Predicted Values and True Return : 
 [-127.0, -203.0, -144.0, -176.0, -212.0, -174.0, -214.0, -239.0, -211.0, -196.0] 
 [-3151.0, -1783.0, -309.0, -3415.0, -1751.0, -15.0, -163.0, -284.0, -403.0, -577.0]
Episode: 125/400, Epsilon:0, Average reward: -55.33
Agent 0 : Predicted Values and True Return : 
 [-110.0, -296.0, -146.0, -86.0, -185.0, -211.0, -91.0, -85.0, -108.0, -86.0] 
 [-299.0, -1033.0, -216.0, -31.0, -29.0, -159.0, -68.0, -90.0, -89.0, -54.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 125.
Episode: 126/400, Epsilon:0, Average reward: -22.82
Agent 0 : Predicted Values and True Return : 
 [-24.0, -144.0, -160.0, -190.0, -193.0, -137.0, -62.0, -89.0, -164.0, -187.0] 
 [-40.0, -261.0, -249.0, -292.0, -176.0, -242.0, -95.0, -71.0, -166.0, -192.0]
Episode: 127/400, Epsilon:0, Average reward: -30.43
Agent 0 : Predicted Values and True Return : 
 [-114.0, -200.0, -173.0, -198.0, -204.0, -180.0, -227.0, -168.0, -112.0, -104.0] 
 [-214.0, -318.0, -501.0, -553.0, -530.0, -182.0, -760.0, -352.0, -17.0, -88.0]
Episode: 128/400, Epsilon:0, Average reward: -183.21
Agent 0 : Predicted Values and True Return : 
 [-184.0, -214.0, -93.0, -149.0, -47.0, -211.0, -179.0, -133.0, -125.0, -92.0] 
 [-309.0, -3003.0, -160.0, -3048.0, -30.0, -70.0, -3053.0, -24.0, -2858.0, -57.0]
Episode: 129/400, Epsilon:0, Average reward: -54.31
Agent 0 : Predicted Values and True Return : 
 [-117.0, -78.0, -75.0, -79.0, -91.0, -353.0, -233.0, -194.0, -215.0, -4.0] 
 [-885.0, -75.0, -33.0, -100.0, -26.0, -148.0, -337.0, -293.0, -322.0, -31.0]
Episode: 130/400, Epsilon:0, Average reward: -31.41
Agent 0 : Predicted Values and True Return : 
 [-60.0, -95.0, -184.0, -196.0, -200.0, -175.0, -80.0, -227.0, -98.0, -81.0] 
 [-27.0, -109.0, -221.0, -233.0, -167.0, -127.0, -59.0, -205.0, -123.0, -70.0]
Episode: 131/400, Epsilon:0, Average reward: -48.28
Agent 0 : Predicted Values and True Return : 
 [-137.0, -194.0, -85.0, -225.0, -23.0, -231.0, -228.0, -82.0, -140.0, -57.0] 
 [-118.0, -300.0, -23.0, -1844.0, -25.0, -830.0, -280.0, -687.0, -91.0, -84.0]
Episode: 132/400, Epsilon:0, Average reward: -38.3
Agent 0 : Predicted Values and True Return : 
 [-129.0, -148.0, -127.0, -4.0, -257.0, -222.0, -124.0, -76.0, -210.0, -166.0] 
 [-133.0, -121.0, -122.0, -78.0, -548.0, -498.0, -1084.0, -53.0, -241.0, -524.0]
Episode: 133/400, Epsilon:0, Average reward: -28.42
Agent 0 : Predicted Values and True Return : 
 [-74.0, -148.0, -97.0, -188.0, -67.0, -152.0, -171.0, -185.0, -99.0, -159.0] 
 [-34.0, -358.0, -364.0, -1086.0, -156.0, -90.0, -312.0, -342.0, -119.0, -99.0]
Episode: 134/400, Epsilon:0, Average reward: -23.53
Agent 0 : Predicted Values and True Return : 
 [-115.0, -121.0, -183.0, -121.0, -137.0, -187.0, -163.0, -112.0, -131.0, -175.0] 
 [-225.0, -290.0, -553.0, -28.0, -266.0, -281.0, -123.0, -322.0, -106.0, -597.0]
Episode: 135/400, Epsilon:0, Average reward: -43.95
Agent 0 : Predicted Values and True Return : 
 [-117.0, -49.0, -43.0, -110.0, -114.0, -189.0, -117.0, -107.0, -3.0, -141.0] 
 [-91.0, -35.0, -22.0, -39.0, -122.0, -119.0, -93.0, -92.0, -25.0, -325.0]
Episode: 136/400, Epsilon:0, Average reward: -28.87
Agent 0 : Predicted Values and True Return : 
 [-112.0, -211.0, -125.0, -169.0, -184.0, -140.0, -150.0, -211.0, -71.0, -150.0] 
 [-234.0, -245.0, -221.0, -244.0, -367.0, -233.0, -263.0, -245.0, -41.0, -263.0]
Episode: 137/400, Epsilon:0, Average reward: -72.58
Agent 0 : Predicted Values and True Return : 
 [-101.0, -60.0, -145.0, -3.0, -121.0, -113.0, -225.0, -134.0, -113.0, -195.0] 
 [-2628.0, -38.0, -2103.0, -1.0, -1722.0, -66.0, -787.0, -215.0, -2950.0, -606.0]
Episode: 138/400, Epsilon:0, Average reward: -27.29
Agent 0 : Predicted Values and True Return : 
 [-201.0, -189.0, -193.0, -70.0, -68.0, -200.0, -170.0, -58.0, -76.0, -70.0] 
 [-188.0, -713.0, -297.0, -19.0, -69.0, -110.0, -647.0, -32.0, -223.0, -98.0]
Episode: 139/400, Epsilon:0, Average reward: -32.23
Agent 0 : Predicted Values and True Return : 
 [-188.0, -130.0, -137.0, -175.0, -51.0, -184.0, -207.0, -132.0, -79.0, -33.0] 
 [-95.0, -26.0, -260.0, -230.0, -45.0, -249.0, -446.0, -145.0, -178.0, -18.0]
Episode: 140/400, Epsilon:0, Average reward: -38.71
Agent 0 : Predicted Values and True Return : 
 [-70.0, -152.0, -152.0, -213.0, -160.0, -140.0, -151.0, -118.0, -55.0, -123.0] 
 [-14.0, -90.0, -124.0, -224.0, -237.0, -221.0, -158.0, -17.0, -140.0, -292.0]
Episode: 141/400, Epsilon:0, Average reward: -29.74
Agent 0 : Predicted Values and True Return : 
 [-281.0, -101.0, -231.0, -160.0, -123.0, -64.0, -44.0, -221.0, -254.0, -165.0] 
 [-514.0, -32.0, -375.0, -240.0, -346.0, -235.0, -36.0, -421.0, -409.0, -362.0]
Episode: 142/400, Epsilon:0, Average reward: -23.47
Agent 0 : Predicted Values and True Return : 
 [-257.0, -164.0, -108.0, -136.0, -113.0, -74.0, -43.0, -189.0, -115.0, -131.0] 
 [-189.0, -405.0, -185.0, -44.0, -460.0, -23.0, -49.0, -311.0, -89.0, -63.0]
Episode: 143/400, Epsilon:0, Average reward: -28.93
Agent 0 : Predicted Values and True Return : 
 [-162.0, -187.0, -106.0, -212.0, -70.0, -199.0, -162.0, -137.0, -66.0, -125.0] 
 [-581.0, -202.0, -123.0, -566.0, -99.0, -580.0, -294.0, -168.0, -34.0, -103.0]
Episode: 144/400, Epsilon:0, Average reward: -35.21
Agent 0 : Predicted Values and True Return : 
 [-83.0, -110.0, -178.0, -99.0, -91.0, -167.0, -123.0, -159.0, -178.0, -93.0] 
 [-34.0, -25.0, -414.0, -54.0, -8.0, -119.0, -557.0, -28.0, -414.0, -36.0]
Episode: 145/400, Epsilon:0, Average reward: -36.92
Agent 0 : Predicted Values and True Return : 
 [-183.0, -165.0, -99.0, -84.0, -198.0, -173.0, -81.0, -48.0, -117.0, -85.0] 
 [-1225.0, -150.0, -157.0, -84.0, -572.0, -401.0, -139.0, -19.0, -155.0, -44.0]
Episode: 146/400, Epsilon:0, Average reward: -26.19
Agent 0 : Predicted Values and True Return : 
 [-94.0, -252.0, -274.0, -163.0, -126.0, -119.0, -236.0, -220.0, -175.0, -93.0] 
 [-86.0, -225.0, -500.0, -78.0, -321.0, -138.0, -228.0, -386.0, -225.0, -49.0]
Episode: 147/400, Epsilon:0, Average reward: -131.17
Agent 0 : Predicted Values and True Return : 
 [-93.0, -86.0, -226.0, -178.0, -215.0, -104.0, -113.0, -260.0, -92.0, -148.0] 
 [-104.0, -113.0, -608.0, -419.0, -400.0, -3398.0, -113.0, -1260.0, -31.0, -563.0]
Episode: 148/400, Epsilon:0, Average reward: -25.91
Agent 0 : Predicted Values and True Return : 
 [-234.0, -67.0, -146.0, -115.0, -129.0, -220.0, -74.0, -157.0, -114.0, -144.0] 
 [-756.0, -44.0, -94.0, -126.0, -339.0, -636.0, -10.0, -194.0, -67.0, -222.0]
Episode: 149/400, Epsilon:0, Average reward: -36.73
Agent 0 : Predicted Values and True Return : 
 [-74.0, -125.0, -270.0, -91.0, -119.0, -167.0, -55.0, -119.0, -236.0, -66.0] 
 [-99.0, -143.0, -456.0, -137.0, -196.0, -118.0, -56.0, -196.0, -1214.0, -39.0]
Episode: 150/400, Epsilon:0, Average reward: -31.01
Agent 0 : Predicted Values and True Return : 
 [-146.0, -194.0, -102.0, -62.0, -108.0, -35.0, -121.0, -119.0, -146.0, -146.0] 
 [-210.0, -221.0, -178.0, -108.0, -189.0, -93.0, -156.0, -230.0, -257.0, -1316.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 150.
Episode: 151/400, Epsilon:0, Average reward: -24.96
Agent 0 : Predicted Values and True Return : 
 [-142.0, -142.0, -173.0, -90.0, -58.0, -115.0, -50.0, -143.0, -182.0, -162.0] 
 [-90.0, -90.0, -352.0, -25.0, -45.0, -114.0, -26.0, -36.0, -162.0, -148.0]
Episode: 152/400, Epsilon:0, Average reward: -62.73
Agent 0 : Predicted Values and True Return : 
 [-68.0, -164.0, -68.0, -152.0, -233.0, -90.0, -174.0, -78.0, -83.0, -228.0] 
 [-47.0, -231.0, -115.0, -487.0, -582.0, -64.0, -142.0, -70.0, -55.0, -834.0]
Episode: 153/400, Epsilon:0, Average reward: -28.83
Agent 0 : Predicted Values and True Return : 
 [-141.0, -86.0, -98.0, -122.0, -52.0, -177.0, -53.0, -84.0, -165.0, -189.0] 
 [-406.0, -130.0, -804.0, -203.0, -40.0, -193.0, -20.0, -157.0, -167.0, -127.0]
Episode: 154/400, Epsilon:0, Average reward: -62.16
Agent 0 : Predicted Values and True Return : 
 [-57.0, -162.0, -124.0, -177.0, -104.0, -93.0, -171.0, -126.0, -57.0, -132.0] 
 [-15.0, -70.0, -249.0, -224.0, -60.0, -148.0, -293.0, -471.0, -15.0, -39.0]
Episode: 155/400, Epsilon:0, Average reward: -70.25
Agent 0 : Predicted Values and True Return : 
 [-129.0, -104.0, -110.0, -31.0, -144.0, -92.0, -165.0, -135.0, -43.0, -148.0] 
 [-131.0, -3153.0, -153.0, -27.0, -1547.0, -61.0, -1780.0, -1304.0, -45.0, -1756.0]
Episode: 156/400, Epsilon:0, Average reward: -125.99
Agent 0 : Predicted Values and True Return : 
 [-203.0, -124.0, -169.0, -95.0, -202.0, -109.0, -121.0, -159.0, -208.0, -155.0] 
 [-231.0, -116.0, -3348.0, -41.0, -425.0, -196.0, -3386.0, -391.0, -1989.0, -70.0]
Episode: 157/400, Epsilon:0, Average reward: -69.16
Agent 0 : Predicted Values and True Return : 
 [-240.0, -150.0, -45.0, -217.0, -87.0, -71.0, -111.0, -146.0, -94.0, -177.0] 
 [-101.0, -21.0, -52.0, -210.0, -10.0, -135.0, -88.0, -158.0, -36.0, -102.0]
Episode: 158/400, Epsilon:0, Average reward: -72.2
Agent 0 : Predicted Values and True Return : 
 [-146.0, -148.0, -240.0, -188.0, -106.0, -85.0, -4.0, -167.0, -167.0, -169.0] 
 [-138.0, -85.0, -427.0, -2285.0, -87.0, -95.0, -22.0, -291.0, -106.0, -1046.0]
Episode: 159/400, Epsilon:0, Average reward: -67.81
Agent 0 : Predicted Values and True Return : 
 [-151.0, -164.0, -165.0, -74.0, -74.0, -194.0, -134.0, -23.0, -227.0, -212.0] 
 [-406.0, -403.0, -836.0, -29.0, -48.0, -222.0, -123.0, -60.0, -465.0, -574.0]
Episode: 160/400, Epsilon:0, Average reward: -49.7
Agent 0 : Predicted Values and True Return : 
 [-146.0, -230.0, -193.0, -153.0, -25.0, -144.0, -81.0, -38.0, -167.0, -96.0] 
 [-196.0, -241.0, -117.0, -146.0, -36.0, -1906.0, -48.0, -53.0, -195.0, -44.0]
Episode: 161/400, Epsilon:0, Average reward: -26.42
Agent 0 : Predicted Values and True Return : 
 [-146.0, -174.0, -42.0, -135.0, -159.0, -90.0, -32.0, -152.0, -168.0, -151.0] 
 [-81.0, -396.0, -15.0, -59.0, -143.0, -37.0, -28.0, -437.0, -95.0, -214.0]
Episode: 162/400, Epsilon:0, Average reward: -39.15
Agent 0 : Predicted Values and True Return : 
 [-51.0, -50.0, -170.0, -103.0, -120.0, -152.0, -204.0, -167.0, -133.0, -181.0] 
 [-27.0, -69.0, -647.0, -19.0, -90.0, -303.0, -226.0, -1093.0, -289.0, -344.0]
Episode: 163/400, Epsilon:0, Average reward: -20.77
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-127.0, -162.0, -147.0, -228.0, -198.0, -177.0, -94.0, -185.0, -219.0, -108.0] 
 [-97.0, -183.0, -179.0, -312.0, -268.0, -188.0, -104.0, -151.0, -251.0, -24.0]
Episode: 164/400, Epsilon:0, Average reward: -41.0
Agent 0 : Predicted Values and True Return : 
 [-104.0, -3.0, -183.0, -69.0, -141.0, -91.0, -191.0, -69.0, -225.0, -191.0] 
 [-124.0, -11.0, -278.0, -12.0, -101.0, -146.0, -233.0, -76.0, -324.0, -212.0]
Episode: 165/400, Epsilon:0, Average reward: -41.03
Agent 0 : Predicted Values and True Return : 
 [-78.0, -109.0, -102.0, -196.0, -150.0, -68.0, -96.0, -150.0, -100.0, -167.0] 
 [-49.0, -54.0, -165.0, -209.0, -297.0, -159.0, -75.0, -148.0, -124.0, -273.0]
Episode: 166/400, Epsilon:0, Average reward: -88.51
Agent 0 : Predicted Values and True Return : 
 [-55.0, -71.0, -126.0, -59.0, -59.0, -70.0, -178.0, -129.0, -140.0, -122.0] 
 [-32.0, -25.0, -3187.0, -3215.0, -3215.0, -76.0, -463.0, -180.0, -3393.0, -386.0]
Episode: 167/400, Epsilon:0, Average reward: -26.07
Agent 0 : Predicted Values and True Return : 
 [-162.0, -216.0, -128.0, -101.0, -152.0, -78.0, -151.0, -176.0, -135.0, -142.0] 
 [-524.0, -280.0, -141.0, -307.0, -279.0, -222.0, -199.0, -251.0, -356.0, -65.0]
Episode: 168/400, Epsilon:0, Average reward: -30.48
Agent 0 : Predicted Values and True Return : 
 [-120.0, -182.0, -204.0, -55.0, -251.0, -144.0, -128.0, -33.0, -97.0, -212.0] 
 [-97.0, -233.0, -599.0, -33.0, -281.0, -117.0, -36.0, -25.0, -54.0, -675.0]
Episode: 169/400, Epsilon:0, Average reward: -43.77
Agent 0 : Predicted Values and True Return : 
 [-162.0, -90.0, -102.0, -121.0, -60.0, -118.0, -147.0, -148.0, -85.0, -77.0] 
 [-869.0, -51.0, -139.0, -10.0, -28.0, -75.0, -911.0, -273.0, -75.0, -458.0]
Episode: 170/400, Epsilon:0, Average reward: -28.22
Agent 0 : Predicted Values and True Return : 
 [-199.0, -176.0, -123.0, -247.0, -195.0, -199.0, -185.0, -133.0, -146.0, -184.0] 
 [-367.0, -259.0, -17.0, -370.0, -268.0, -512.0, -260.0, -36.0, -545.0, -289.0]
Episode: 171/400, Epsilon:0, Average reward: -46.74
Agent 0 : Predicted Values and True Return : 
 [-248.0, -152.0, -136.0, -160.0, -149.0, -4.0, -194.0, -65.0, -211.0, -211.0] 
 [-1806.0, -53.0, -276.0, -380.0, -30.0, -12.0, -198.0, -31.0, -1002.0, -1002.0]
Episode: 172/400, Epsilon:0, Average reward: -173.81
Agent 0 : Predicted Values and True Return : 
 [-117.0, -203.0, -93.0, -202.0, -259.0, -194.0, -168.0, -165.0, -164.0, -183.0] 
 [-141.0, -3320.0, -3331.0, -2691.0, -162.0, -3120.0, -3317.0, -2845.0, -2758.0, -3258.0]
Episode: 173/400, Epsilon:0, Average reward: -179.78
Agent 0 : Predicted Values and True Return : 
 [-124.0, -210.0, -5.0, -63.0, -140.0, -169.0, -105.0, -55.0, -107.0, -223.0] 
 [-147.0, -3415.0, -29.0, -44.0, -3198.0, -3416.0, -26.0, -50.0, -3286.0, -3427.0]
Episode: 174/400, Epsilon:0, Average reward: -33.64
Agent 0 : Predicted Values and True Return : 
 [-198.0, -149.0, -185.0, -233.0, -170.0, -285.0, -232.0, -200.0, -67.0, -198.0] 
 [-246.0, -113.0, -316.0, -271.0, -107.0, -294.0, -149.0, -121.0, -25.0, -246.0]
Episode: 175/400, Epsilon:0, Average reward: -61.72
Agent 0 : Predicted Values and True Return : 
 [-4.0, -169.0, -50.0, -186.0, -262.0, -25.0, -160.0, -300.0, -220.0, -99.0] 
 [-21.0, -43.0, -49.0, -2877.0, -386.0, -48.0, -127.0, -1576.0, -180.0, -246.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 175.
Episode: 176/400, Epsilon:0, Average reward: -111.22
Agent 0 : Predicted Values and True Return : 
 [-335.0, -59.0, -206.0, -277.0, -128.0, -379.0, -6.0, -144.0, -395.0, -170.0] 
 [-1944.0, -20.0, -161.0, -2931.0, -3393.0, -619.0, -21.0, -53.0, -651.0, -78.0]
Episode: 177/400, Epsilon:0, Average reward: -25.59
Agent 0 : Predicted Values and True Return : 
 [-191.0, -229.0, -200.0, -179.0, -210.0, -43.0, -4.0, -213.0, -231.0, -51.0] 
 [-393.0, -295.0, -204.0, -195.0, -187.0, -60.0, -52.0, -136.0, -411.0, -33.0]
Episode: 178/400, Epsilon:0, Average reward: -45.52
Agent 0 : Predicted Values and True Return : 
 [-174.0, -152.0, -165.0, -69.0, -148.0, -306.0, -139.0, -166.0, -172.0, -83.0] 
 [-108.0, -222.0, -2107.0, -71.0, -46.0, -141.0, -115.0, -41.0, -85.0, -12.0]
Episode: 179/400, Epsilon:0, Average reward: -73.28
Agent 0 : Predicted Values and True Return : 
 [-249.0, -212.0, -130.0, -131.0, -169.0, -176.0, -175.0, -46.0, -240.0, -154.0] 
 [-1562.0, -2197.0, -111.0, -2199.0, -321.0, -1626.0, -997.0, -46.0, -1512.0, -57.0]
Episode: 180/400, Epsilon:0, Average reward: -33.67
Agent 0 : Predicted Values and True Return : 
 [-132.0, -120.0, -185.0, -163.0, -218.0, -149.0, -147.0, -156.0, -84.0, -188.0] 
 [-422.0, -72.0, -168.0, -305.0, -789.0, -947.0, -401.0, -82.0, -195.0, -684.0]
Episode: 181/400, Epsilon:0, Average reward: -80.49
Agent 0 : Predicted Values and True Return : 
 [-122.0, -69.0, -76.0, -106.0, -131.0, -125.0, -141.0, -132.0, -141.0, -140.0] 
 [-42.0, -67.0, -7.0, -3237.0, -1787.0, -26.0, -817.0, -93.0, -817.0, -166.0]
Episode: 182/400, Epsilon:0, Average reward: -99.14
Agent 0 : Predicted Values and True Return : 
 [-206.0, -219.0, -260.0, -148.0, -101.0, -198.0, -126.0, -112.0, -83.0, -88.0] 
 [-1503.0, -2577.0, -184.0, -40.0, -75.0, -514.0, -34.0, -2147.0, -40.0, -22.0]
Episode: 183/400, Epsilon:0, Average reward: -46.89
Agent 0 : Predicted Values and True Return : 
 [-106.0, -88.0, -269.0, -133.0, -260.0, -84.0, -112.0, -179.0, -230.0, -166.0] 
 [-12.0, -25.0, -751.0, -27.0, -288.0, -20.0, -161.0, -185.0, -442.0, -494.0]
Episode: 184/400, Epsilon:0, Average reward: -36.8
Agent 0 : Predicted Values and True Return : 
 [-4.0, -171.0, -76.0, -177.0, -86.0, -178.0, -119.0, -99.0, -264.0, -241.0] 
 [-76.0, -591.0, -363.0, -535.0, -113.0, -867.0, -68.0, -61.0, -863.0, -89.0]
Episode: 185/400, Epsilon:0, Average reward: -93.91
Agent 0 : Predicted Values and True Return : 
 [-215.0, -25.0, -105.0, -63.0, -149.0, -144.0, -229.0, -42.0, -145.0, -144.0] 
 [-2370.0, -14.0, -37.0, -34.0, -241.0, -331.0, -285.0, -40.0, -47.0, -316.0]
Episode: 186/400, Epsilon:0, Average reward: -24.29
Agent 0 : Predicted Values and True Return : 
 [-37.0, -148.0, -103.0, -232.0, -180.0, -228.0, -89.0, -96.0, -163.0, -200.0] 
 [-42.0, -72.0, -93.0, -341.0, -139.0, -488.0, -20.0, -53.0, -337.0, -431.0]
Episode: 187/400, Epsilon:0, Average reward: -88.45
Agent 0 : Predicted Values and True Return : 
 [-66.0, -175.0, -232.0, -98.0, -161.0, -153.0, -98.0, -131.0, -98.0, -127.0] 
 [-58.0, -591.0, -309.0, -55.0, -787.0, -2572.0, -55.0, -484.0, -55.0, -20.0]
Episode: 188/400, Epsilon:0, Average reward: -22.87
Agent 0 : Predicted Values and True Return : 
 [-97.0, -127.0, -4.0, -83.0, -133.0, -81.0, -137.0, -206.0, -128.0, -156.0] 
 [-79.0, -76.0, -85.0, -104.0, -92.0, -138.0, -112.0, -81.0, -167.0, -128.0]
Episode: 189/400, Epsilon:0, Average reward: -32.32
Agent 0 : Predicted Values and True Return : 
 [-138.0, -150.0, -113.0, -136.0, -165.0, -95.0, -150.0, -150.0, -112.0, -185.0] 
 [-53.0, -989.0, -13.0, -158.0, -122.0, -44.0, -182.0, -93.0, -91.0, -81.0]
Episode: 190/400, Epsilon:0, Average reward: -36.66
Agent 0 : Predicted Values and True Return : 
 [-64.0, -36.0, -53.0, -120.0, -162.0, -153.0, -90.0, -103.0, -43.0, -128.0] 
 [-22.0, -20.0, -28.0, -842.0, -197.0, -235.0, -91.0, -31.0, -15.0, -806.0]
Episode: 191/400, Epsilon:0, Average reward: -22.4
Agent 0 : Predicted Values and True Return : 
 [-111.0, -72.0, -70.0, -163.0, -21.0, -131.0, -126.0, -96.0, -77.0, -90.0] 
 [-174.0, -101.0, -183.0, -170.0, -12.0, -45.0, -16.0, -52.0, -10.0, -230.0]
Episode: 192/400, Epsilon:0, Average reward: -30.28
Agent 0 : Predicted Values and True Return : 
 [-67.0, -83.0, -67.0, -43.0, -110.0, -143.0, -100.0, -141.0, -96.0, -75.0] 
 [-170.0, -65.0, -67.0, -19.0, -109.0, -804.0, -60.0, -678.0, -134.0, -31.0]
Episode: 193/400, Epsilon:0, Average reward: -28.55
Agent 0 : Predicted Values and True Return : 
 [-87.0, -60.0, -220.0, -66.0, -103.0, -58.0, -112.0, -137.0, -62.0, -221.0] 
 [-120.0, -46.0, -241.0, -47.0, -147.0, -79.0, -69.0, -283.0, -242.0, -315.0]
Episode: 194/400, Epsilon:0, Average reward: -21.85
Agent 0 : Predicted Values and True Return : 
 [-179.0, -213.0, -160.0, -125.0, -123.0, -61.0, -223.0, -123.0, -100.0, -141.0] 
 [-305.0, -303.0, -241.0, -202.0, -74.0, -9.0, -525.0, -247.0, -100.0, -26.0]
Episode: 195/400, Epsilon:0, Average reward: -28.5
Agent 0 : Predicted Values and True Return : 
 [-92.0, -60.0, -133.0, -155.0, -143.0, -131.0, -98.0, -119.0, -137.0, -91.0] 
 [-154.0, -44.0, -335.0, -250.0, -137.0, -156.0, -17.0, -214.0, -285.0, -52.0]
Episode: 196/400, Epsilon:0, Average reward: -35.56
Agent 0 : Predicted Values and True Return : 
 [-107.0, -81.0, -94.0, -135.0, -127.0, -45.0, -102.0, -71.0, -39.0, -110.0] 
 [-184.0, -58.0, -64.0, -924.0, -1002.0, -55.0, -45.0, -745.0, -34.0, -55.0]
Episode: 197/400, Epsilon:0, Average reward: -23.14
Agent 0 : Predicted Values and True Return : 
 [-161.0, -142.0, -101.0, -168.0, -45.0, -115.0, -108.0, -21.0, -120.0, -30.0] 
 [-550.0, -235.0, -341.0, -484.0, -41.0, -16.0, -163.0, -27.0, -19.0, -4.0]
Episode: 198/400, Epsilon:0, Average reward: -64.94
Agent 0 : Predicted Values and True Return : 
 [-95.0, -105.0, -186.0, -58.0, -83.0, -105.0, -82.0, -66.0, -109.0, -75.0] 
 [-296.0, -177.0, -104.0, -25.0, -79.0, -1809.0, -318.0, -49.0, -114.0, -68.0]
Episode: 199/400, Epsilon:0, Average reward: -31.15
Agent 0 : Predicted Values and True Return : 
 [-102.0, -217.0, -102.0, -64.0, -35.0, -142.0, -43.0, -188.0, -110.0, -73.0] 
 [-105.0, -242.0, -378.0, -132.0, -142.0, -671.0, -51.0, -794.0, -394.0, -66.0]
Episode: 200/400, Epsilon:0, Average reward: -26.95
Agent 0 : Predicted Values and True Return : 
 [-168.0, -174.0, -76.0, -120.0, -150.0, -110.0, -33.0, -140.0, -4.0, -174.0] 
 [-117.0, -584.0, -51.0, -90.0, -131.0, -153.0, -36.0, -111.0, -61.0, -739.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 200.
Episode: 201/400, Epsilon:0, Average reward: -35.67
Agent 0 : Predicted Values and True Return : 
 [-149.0, -42.0, -92.0, -114.0, -111.0, -125.0, -124.0, -120.0, -165.0, -81.0] 
 [-931.0, -45.0, -70.0, -248.0, -463.0, -992.0, -651.0, -44.0, -745.0, -35.0]
Episode: 202/400, Epsilon:0, Average reward: -25.1
Agent 0 : Predicted Values and True Return : 
 [-127.0, -23.0, -45.0, -194.0, -128.0, -83.0, -108.0, -74.0, -149.0, -94.0] 
 [-205.0, -12.0, -16.0, -196.0, -341.0, -31.0, -64.0, -423.0, -439.0, -65.0]
Episode: 203/400, Epsilon:0, Average reward: -26.71
Agent 0 : Predicted Values and True Return : 
 [-158.0, -167.0, -131.0, -67.0, -148.0, -81.0, -70.0, -100.0, -66.0, -60.0] 
 [-251.0, -188.0, -459.0, -23.0, -294.0, -38.0, -96.0, -97.0, -48.0, -54.0]
Episode: 204/400, Epsilon:0, Average reward: -76.22
Agent 0 : Predicted Values and True Return : 
 [-110.0, -42.0, -163.0, -113.0, -21.0, -103.0, -209.0, -4.0, -84.0, -48.0] 
 [-100.0, -18.0, -133.0, -197.0, -8.0, -216.0, -1575.0, -17.0, -1336.0, -33.0]
Episode: 205/400, Epsilon:0, Average reward: -23.42
Agent 0 : Predicted Values and True Return : 
 [-61.0, -98.0, -89.0, -210.0, -120.0, -210.0, -94.0, -99.0, -149.0, -196.0] 
 [-47.0, -128.0, -168.0, -351.0, -364.0, -349.0, -78.0, -105.0, -122.0, -396.0]
Episode: 206/400, Epsilon:0, Average reward: -65.53
Agent 0 : Predicted Values and True Return : 
 [-122.0, -109.0, -55.0, -128.0, -135.0, -206.0, -86.0, -167.0, -58.0, -153.0] 
 [-115.0, -46.0, -50.0, -2998.0, -953.0, -315.0, -52.0, -1564.0, -159.0, -183.0]
Episode: 207/400, Epsilon:0, Average reward: -24.37
Agent 0 : Predicted Values and True Return : 
 [-142.0, -192.0, -114.0, -40.0, -149.0, -107.0, -111.0, -90.0, -108.0, -114.0] 
 [-394.0, -390.0, -202.0, -86.0, -143.0, -316.0, -140.0, -65.0, -54.0, -618.0]
Episode: 208/400, Epsilon:0, Average reward: -60.31
Agent 0 : Predicted Values and True Return : 
 [-24.0, -130.0, -97.0, -242.0, -44.0, -134.0, -131.0, -171.0, -105.0, -155.0] 
 [-50.0, -77.0, -2936.0, -436.0, -10.0, -193.0, -189.0, -546.0, -3300.0, -245.0]
Episode: 209/400, Epsilon:0, Average reward: -36.42
Agent 0 : Predicted Values and True Return : 
 [-101.0, -125.0, -136.0, -136.0, -93.0, -89.0, -55.0, -100.0, -226.0, -69.0] 
 [-273.0, -45.0, -103.0, -104.0, -83.0, -87.0, -74.0, -1025.0, -460.0, -40.0]
Episode: 210/400, Epsilon:0, Average reward: -31.65
Agent 0 : Predicted Values and True Return : 
 [-71.0, -142.0, -246.0, -86.0, -198.0, -179.0, -89.0, -118.0, -48.0, -96.0] 
 [-224.0, -252.0, -379.0, -43.0, -121.0, -450.0, -202.0, -76.0, -48.0, -14.0]
Episode: 211/400, Epsilon:0, Average reward: -23.21
Agent 0 : Predicted Values and True Return : 
 [-43.0, -181.0, -46.0, -153.0, -158.0, -172.0, -76.0, -71.0, -46.0, -110.0] 
 [-212.0, -329.0, -4.0, -372.0, -161.0, -889.0, -97.0, -42.0, -4.0, -7.0]
Episode: 212/400, Epsilon:0, Average reward: -24.47
Agent 0 : Predicted Values and True Return : 
 [-114.0, -157.0, -176.0, -126.0, -166.0, -27.0, -91.0, -102.0, -161.0, -75.0] 
 [-34.0, -101.0, -156.0, -488.0, -589.0, -17.0, -359.0, -93.0, -224.0, -47.0]
Episode: 213/400, Epsilon:0, Average reward: -30.14
Agent 0 : Predicted Values and True Return : 
 [-251.0, -109.0, -153.0, -72.0, -177.0, -103.0, -143.0, -51.0, -146.0, -112.0] 
 [-570.0, -76.0, -206.0, -39.0, -560.0, -227.0, -296.0, -70.0, -698.0, -177.0]
Episode: 214/400, Epsilon:0, Average reward: -35.54
Agent 0 : Predicted Values and True Return : 
 [-128.0, -242.0, -139.0, -255.0, -142.0, -24.0, -54.0, -165.0, -86.0, -126.0] 
 [-912.0, -352.0, -199.0, -699.0, -61.0, -37.0, -40.0, -428.0, -856.0, -50.0]
Episode: 215/400, Epsilon:0, Average reward: -23.62
Agent 0 : Predicted Values and True Return : 
 [-176.0, -78.0, -156.0, -208.0, -112.0, -89.0, -185.0, -169.0, -152.0, -89.0] 
 [-279.0, -204.0, -373.0, -385.0, -175.0, -305.0, -214.0, -315.0, -203.0, -144.0]
Episode: 216/400, Epsilon:0, Average reward: -30.41
Agent 0 : Predicted Values and True Return : 
 [-124.0, -93.0, -180.0, -149.0, -149.0, -84.0, -5.0, -159.0, -79.0, -54.0] 
 [-181.0, -39.0, -98.0, -168.0, -168.0, -63.0, -37.0, -249.0, -124.0, -39.0]
Episode: 217/400, Epsilon:0, Average reward: -28.24
Agent 0 : Predicted Values and True Return : 
 [-119.0, -4.0, -4.0, -138.0, -138.0, -175.0, -52.0, -132.0, -274.0, -132.0] 
 [-165.0, -47.0, -47.0, -98.0, -84.0, -408.0, -31.0, -280.0, -498.0, -280.0]
Episode: 218/400, Epsilon:0, Average reward: -43.4
Agent 0 : Predicted Values and True Return : 
 [-97.0, -321.0, -169.0, -183.0, -18.0, -31.0, -25.0, -183.0, -75.0, -86.0] 
 [-26.0, -414.0, -303.0, -1735.0, -14.0, -22.0, -44.0, -846.0, -44.0, -20.0]
Episode: 219/400, Epsilon:0, Average reward: -30.95
Agent 0 : Predicted Values and True Return : 
 [-93.0, -155.0, -43.0, -86.0, -52.0, -103.0, -38.0, -221.0, -222.0, -130.0] 
 [-215.0, -498.0, -29.0, -22.0, -18.0, -135.0, -7.0, -725.0, -202.0, -168.0]
Episode: 220/400, Epsilon:0, Average reward: -25.03
Agent 0 : Predicted Values and True Return : 
 [-135.0, -97.0, -127.0, -152.0, -118.0, -121.0, -119.0, -152.0, -198.0, -107.0] 
 [-77.0, -22.0, -474.0, -143.0, -173.0, -32.0, -165.0, -134.0, -515.0, -70.0]
Episode: 221/400, Epsilon:0, Average reward: -33.01
Agent 0 : Predicted Values and True Return : 
 [-146.0, -74.0, -82.0, -125.0, -101.0, -121.0, -109.0, -155.0, -175.0, -156.0] 
 [-136.0, -51.0, -156.0, -887.0, -121.0, -531.0, -35.0, -259.0, -251.0, -1389.0]
Episode: 222/400, Epsilon:0, Average reward: -90.64
Agent 0 : Predicted Values and True Return : 
 [-87.0, -59.0, -84.0, -174.0, -108.0, -105.0, -122.0, -138.0, -106.0, -134.0] 
 [-653.0, -141.0, -28.0, -545.0, -173.0, -37.0, -130.0, -124.0, -336.0, -648.0]
Episode: 223/400, Epsilon:0, Average reward: -70.37
Agent 0 : Predicted Values and True Return : 
 [-36.0, -82.0, -170.0, -106.0, -66.0, -88.0, -171.0, -57.0, -155.0, -86.0] 
 [-53.0, -108.0, -126.0, -32.0, -60.0, -2155.0, -1551.0, -18.0, -1223.0, -2460.0]
Episode: 224/400, Epsilon:0, Average reward: -22.75
Agent 0 : Predicted Values and True Return : 
 [-104.0, -118.0, -148.0, -141.0, -79.0, -148.0, -66.0, -153.0, -103.0, -132.0] 
 [-53.0, -315.0, -143.0, -74.0, -112.0, -143.0, -27.0, -307.0, -25.0, -118.0]
Episode: 225/400, Epsilon:0, Average reward: -55.98
Agent 0 : Predicted Values and True Return : 
 [-134.0, -206.0, -52.0, -116.0, -82.0, -110.0, -132.0, -114.0, -158.0, -131.0] 
 [-1164.0, -381.0, -42.0, -156.0, -1997.0, -108.0, -131.0, -12.0, -622.0, -252.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 225.
Episode: 226/400, Epsilon:0, Average reward: -25.41
Agent 0 : Predicted Values and True Return : 
 [-89.0, -138.0, -135.0, -154.0, -75.0, -89.0, -206.0, -52.0, -90.0, -90.0] 
 [-78.0, -351.0, -299.0, -210.0, -53.0, -59.0, -88.0, -28.0, -89.0, -64.0]
Episode: 227/400, Epsilon:0, Average reward: -25.96
Agent 0 : Predicted Values and True Return : 
 [-101.0, -33.0, -125.0, -70.0, -102.0, -58.0, -142.0, -196.0, -77.0, -134.0] 
 [-32.0, -17.0, -304.0, -20.0, -88.0, -37.0, -117.0, -427.0, -1.0, -61.0]
Episode: 228/400, Epsilon:0, Average reward: -21.51
Agent 0 : Predicted Values and True Return : 
 [-237.0, -135.0, -39.0, -71.0, -125.0, -219.0, -189.0, -36.0, -163.0, -79.0] 
 [-476.0, -107.0, -170.0, -206.0, -71.0, -338.0, -195.0, -37.0, -265.0, -6.0]
Episode: 229/400, Epsilon:0, Average reward: -31.14
Agent 0 : Predicted Values and True Return : 
 [-106.0, -91.0, -81.0, -46.0, -155.0, -122.0, -42.0, -113.0, -157.0, -107.0] 
 [-310.0, -54.0, -146.0, -32.0, -699.0, -112.0, -4.0, -449.0, -123.0, -308.0]
Episode: 230/400, Epsilon:0, Average reward: -47.15
Agent 0 : Predicted Values and True Return : 
 [-27.0, -126.0, -118.0, -130.0, -90.0, -111.0, -59.0, -98.0, -110.0, -143.0] 
 [-38.0, -44.0, -64.0, -139.0, -31.0, -709.0, -47.0, -58.0, -41.0, -43.0]
Episode: 231/400, Epsilon:0, Average reward: -33.26
Agent 0 : Predicted Values and True Return : 
 [-94.0, -41.0, -54.0, -116.0, -154.0, -163.0, -208.0, -116.0, -224.0, -122.0] 
 [-547.0, -9.0, -4.0, -389.0, -398.0, -221.0, -203.0, -389.0, -196.0, -54.0]
Episode: 232/400, Epsilon:0, Average reward: -79.75
Agent 0 : Predicted Values and True Return : 
 [-40.0, -55.0, -212.0, -108.0, -153.0, -97.0, -108.0, -155.0, -154.0, -110.0] 
 [-6.0, -3164.0, -358.0, -453.0, -496.0, -502.0, -1440.0, -163.0, -516.0, -613.0]
Episode: 233/400, Epsilon:0, Average reward: -119.12
Agent 0 : Predicted Values and True Return : 
 [-27.0, -139.0, -21.0, -139.0, -316.0, -209.0, -109.0, -109.0, -180.0, -62.0] 
 [-27.0, -136.0, -3302.0, -651.0, -482.0, -1104.0, -572.0, -58.0, -744.0, -26.0]
Episode: 234/400, Epsilon:0, Average reward: -101.21
Agent 0 : Predicted Values and True Return : 
 [-136.0, -127.0, -151.0, -195.0, -107.0, -132.0, -63.0, -263.0, -61.0, -94.0] 
 [-165.0, -102.0, -557.0, -228.0, -86.0, -1957.0, -84.0, -875.0, -18.0, -73.0]
Episode: 235/400, Epsilon:0, Average reward: -67.31
Agent 0 : Predicted Values and True Return : 
 [-103.0, -287.0, -128.0, -152.0, -86.0, -147.0, -97.0, -171.0, -76.0, -186.0] 
 [-38.0, -269.0, -103.0, -145.0, -49.0, -107.0, -131.0, -1461.0, -1051.0, -131.0]
Episode: 236/400, Epsilon:0, Average reward: -136.33
Agent 0 : Predicted Values and True Return : 
 [-103.0, -167.0, -142.0, -187.0, -145.0, -142.0, -91.0, -118.0, -86.0, -136.0] 
 [-57.0, -147.0, -529.0, -144.0, -3354.0, -488.0, -119.0, -3425.0, -77.0, -125.0]
Episode: 237/400, Epsilon:0, Average reward: -64.72
Agent 0 : Predicted Values and True Return : 
 [-73.0, -163.0, -71.0, -92.0, -165.0, -72.0, -114.0, -37.0, -95.0, -188.0] 
 [-72.0, -83.0, -38.0, -44.0, -1439.0, -77.0, -93.0, -9.0, -371.0, -244.0]
Episode: 238/400, Epsilon:0, Average reward: -28.77
Agent 0 : Predicted Values and True Return : 
 [-153.0, -90.0, -211.0, -142.0, -40.0, -177.0, -127.0, -104.0, -150.0, -163.0] 
 [-567.0, -17.0, -307.0, -270.0, -26.0, -769.0, -91.0, -7.0, -561.0, -95.0]
Episode: 239/400, Epsilon:0, Average reward: -46.99
Agent 0 : Predicted Values and True Return : 
 [-36.0, -4.0, -28.0, -228.0, -122.0, -35.0, -144.0, -67.0, -213.0, -165.0] 
 [-12.0, -7.0, -14.0, -91.0, -1633.0, -6.0, -150.0, -27.0, -209.0, -149.0]
Episode: 240/400, Epsilon:0, Average reward: -61.39
Agent 0 : Predicted Values and True Return : 
 [-56.0, -157.0, -123.0, -93.0, -109.0, -98.0, -154.0, -213.0, -123.0, -141.0] 
 [-47.0, -161.0, -1633.0, -111.0, -2295.0, -445.0, -252.0, -928.0, -592.0, -2140.0]
Episode: 241/400, Epsilon:0, Average reward: -28.34
Agent 0 : Predicted Values and True Return : 
 [-57.0, -196.0, -80.0, -184.0, -179.0, -160.0, -97.0, -186.0, -135.0, -190.0] 
 [-13.0, -201.0, -64.0, -339.0, -164.0, -528.0, -19.0, -227.0, -57.0, -135.0]
Episode: 242/400, Epsilon:0, Average reward: -40.87
Agent 0 : Predicted Values and True Return : 
 [-149.0, -196.0, -102.0, -184.0, -106.0, -170.0, -147.0, -121.0, -181.0, -177.0] 
 [-1352.0, -103.0, -443.0, -1336.0, -1384.0, -97.0, -604.0, -1429.0, -206.0, -1562.0]
Episode: 243/400, Epsilon:0, Average reward: -17.0
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-130.0, -72.0, -109.0, -252.0, -66.0, -146.0, -56.0, -77.0, -138.0, -59.0] 
 [-105.0, -82.0, -76.0, -339.0, -25.0, -262.0, -33.0, -33.0, -127.0, -36.0]
Episode: 244/400, Epsilon:0, Average reward: -20.92
Agent 0 : Predicted Values and True Return : 
 [-72.0, -164.0, -213.0, -115.0, -142.0, -143.0, -107.0, -68.0, -100.0, -125.0] 
 [-115.0, -245.0, -320.0, -410.0, -258.0, -269.0, -62.0, -55.0, -57.0, -97.0]
Episode: 245/400, Epsilon:0, Average reward: -71.6
Agent 0 : Predicted Values and True Return : 
 [-149.0, -103.0, -120.0, -84.0, -40.0, -59.0, -166.0, -140.0, -74.0, -35.0] 
 [-3227.0, -104.0, -115.0, -68.0, -5.0, -64.0, -1369.0, -100.0, -8.0, -35.0]
Episode: 246/400, Epsilon:0, Average reward: -29.86
Agent 0 : Predicted Values and True Return : 
 [-27.0, -85.0, -62.0, -134.0, -73.0, -141.0, -111.0, -192.0, -106.0, -88.0] 
 [-13.0, -205.0, -60.0, -515.0, -226.0, -313.0, -344.0, -151.0, -232.0, -246.0]
Episode: 247/400, Epsilon:0, Average reward: -56.03
Agent 0 : Predicted Values and True Return : 
 [-79.0, -66.0, -90.0, -124.0, -122.0, -61.0, -45.0, -61.0, -128.0, -123.0] 
 [-45.0, -41.0, -88.0, -45.0, -171.0, -9.0, -15.0, -39.0, -2322.0, -439.0]
Episode: 248/400, Epsilon:0, Average reward: -22.51
Agent 0 : Predicted Values and True Return : 
 [-74.0, -105.0, -76.0, -154.0, -92.0, -155.0, -110.0, -99.0, -178.0, -85.0] 
 [-83.0, -111.0, -58.0, -205.0, -61.0, -111.0, -128.0, -483.0, -657.0, -48.0]
Episode: 249/400, Epsilon:0, Average reward: -25.35
Agent 0 : Predicted Values and True Return : 
 [-100.0, -106.0, -178.0, -81.0, -104.0, -86.0, -119.0, -182.0, -177.0, -131.0] 
 [-877.0, -992.0, -756.0, -91.0, -463.0, -30.0, -238.0, -175.0, -161.0, -45.0]
Episode: 250/400, Epsilon:0, Average reward: -33.92
Agent 0 : Predicted Values and True Return : 
 [-100.0, -133.0, -94.0, -20.0, -20.0, -128.0, -137.0, -149.0, -144.0, -148.0] 
 [-67.0, -77.0, -32.0, -59.0, -59.0, -243.0, -174.0, -384.0, -232.0, -208.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 250.
Episode: 251/400, Epsilon:0, Average reward: -28.5
Agent 0 : Predicted Values and True Return : 
 [-194.0, -105.0, -195.0, -198.0, -187.0, -113.0, -64.0, -97.0, -69.0, -117.0] 
 [-412.0, -56.0, -205.0, -432.0, -264.0, -117.0, -52.0, -61.0, -116.0, -273.0]
Episode: 252/400, Epsilon:0, Average reward: -72.91
Agent 0 : Predicted Values and True Return : 
 [-106.0, -103.0, -118.0, -114.0, -134.0, -65.0, -33.0, -113.0, -18.0, -52.0] 
 [-277.0, -1809.0, -100.0, -191.0, -335.0, -3056.0, -63.0, -2891.0, -19.0, -15.0]
Episode: 253/400, Epsilon:0, Average reward: -78.85
Agent 0 : Predicted Values and True Return : 
 [-59.0, -131.0, -101.0, -126.0, -87.0, -142.0, -34.0, -80.0, -97.0, -123.0] 
 [-24.0, -515.0, -3389.0, -250.0, -86.0, -347.0, -24.0, -248.0, -253.0, -372.0]
Episode: 254/400, Epsilon:0, Average reward: -27.4
Agent 0 : Predicted Values and True Return : 
 [-151.0, -181.0, -161.0, -172.0, -121.0, -145.0, -47.0, -219.0, -122.0, -32.0] 
 [-611.0, -565.0, -486.0, -145.0, -12.0, -435.0, -38.0, -633.0, -146.0, -11.0]
Episode: 255/400, Epsilon:0, Average reward: -61.58
Agent 0 : Predicted Values and True Return : 
 [-97.0, -86.0, -137.0, -156.0, -143.0, -135.0, -143.0, -199.0, -212.0, -128.0] 
 [-138.0, -124.0, -112.0, -2760.0, -1015.0, -1110.0, -273.0, -1173.0, -225.0, -186.0]
Episode: 256/400, Epsilon:0, Average reward: -24.23
Agent 0 : Predicted Values and True Return : 
 [-95.0, -33.0, -156.0, -129.0, -78.0, -41.0, -128.0, -33.0, -171.0, -132.0] 
 [-189.0, -4.0, -274.0, -81.0, -131.0, -21.0, -184.0, -4.0, -443.0, -326.0]
Episode: 257/400, Epsilon:0, Average reward: -22.88
Agent 0 : Predicted Values and True Return : 
 [-54.0, -189.0, -166.0, -89.0, -95.0, -119.0, -178.0, -109.0, -155.0, -126.0] 
 [-28.0, -136.0, -343.0, -117.0, -68.0, -160.0, -435.0, -108.0, -153.0, -447.0]
Episode: 258/400, Epsilon:0, Average reward: -78.95
Agent 0 : Predicted Values and True Return : 
 [-98.0, -51.0, -4.0, -78.0, -146.0, -115.0, -82.0, -190.0, -129.0, -87.0] 
 [-148.0, -39.0, -16.0, -1728.0, -269.0, -200.0, -354.0, -242.0, -40.0, -42.0]
Episode: 259/400, Epsilon:0, Average reward: -43.79
Agent 0 : Predicted Values and True Return : 
 [-72.0, -72.0, -248.0, -228.0, -196.0, -151.0, -171.0, -140.0, -162.0, -148.0] 
 [-47.0, -47.0, -1089.0, -152.0, -101.0, -198.0, -161.0, -359.0, -120.0, -104.0]
Episode: 260/400, Epsilon:0, Average reward: -31.11
Agent 0 : Predicted Values and True Return : 
 [-209.0, -90.0, -104.0, -192.0, -25.0, -158.0, -182.0, -49.0, -89.0, -72.0] 
 [-354.0, -327.0, -368.0, -127.0, -17.0, -295.0, -146.0, -18.0, -141.0, -376.0]
Episode: 261/400, Epsilon:0, Average reward: -21.53
Agent 0 : Predicted Values and True Return : 
 [-149.0, -114.0, -123.0, -71.0, -59.0, -122.0, -42.0, -110.0, -89.0, -82.0] 
 [-319.0, -59.0, -26.0, -128.0, -59.0, -163.0, -13.0, -334.0, -51.0, -36.0]
Episode: 262/400, Epsilon:0, Average reward: -24.55
Agent 0 : Predicted Values and True Return : 
 [-142.0, -125.0, -164.0, -124.0, -18.0, -126.0, -32.0, -147.0, -180.0, -100.0] 
 [-294.0, -183.0, -337.0, -228.0, -53.0, -202.0, -47.0, -84.0, -674.0, -201.0]
Episode: 263/400, Epsilon:0, Average reward: -113.42
Agent 0 : Predicted Values and True Return : 
 [-120.0, -68.0, -99.0, -123.0, -77.0, -103.0, -128.0, -118.0, -134.0, -144.0] 
 [-640.0, -419.0, -3149.0, -215.0, -62.0, -822.0, -100.0, -73.0, -1877.0, -1679.0]
Episode: 264/400, Epsilon:0, Average reward: -23.89
Agent 0 : Predicted Values and True Return : 
 [-91.0, -131.0, -171.0, -92.0, -65.0, -104.0, -116.0, -161.0, -139.0, -62.0] 
 [-39.0, -73.0, -634.0, -116.0, -43.0, -699.0, -589.0, -479.0, -138.0, -128.0]
Episode: 265/400, Epsilon:0, Average reward: -39.12
Agent 0 : Predicted Values and True Return : 
 [-5.0, -171.0, -4.0, -154.0, -128.0, -54.0, -95.0, -147.0, -190.0, -117.0] 
 [-16.0, -355.0, -3.0, -610.0, -93.0, -68.0, -280.0, -717.0, -355.0, -90.0]
Episode: 266/400, Epsilon:0, Average reward: -95.19
Agent 0 : Predicted Values and True Return : 
 [-178.0, -112.0, -147.0, -82.0, -196.0, -130.0, -176.0, -164.0, -133.0, -218.0] 
 [-2217.0, -497.0, -145.0, -50.0, -387.0, -259.0, -511.0, -296.0, -63.0, -681.0]
Episode: 267/400, Epsilon:0, Average reward: -109.56
Agent 0 : Predicted Values and True Return : 
 [-98.0, -137.0, -158.0, -175.0, -35.0, -174.0, -49.0, -70.0, -262.0, -5.0] 
 [-100.0, -3307.0, -1159.0, -151.0, -44.0, -154.0, -42.0, -3392.0, -678.0, -27.0]
Episode: 268/400, Epsilon:0, Average reward: -17.29
Agent 0 : Predicted Values and True Return : 
 [-133.0, -67.0, -133.0, -35.0, -118.0, -5.0, -141.0, -125.0, -178.0, -196.0] 
 [-112.0, -72.0, -112.0, -11.0, -164.0, -16.0, -116.0, -74.0, -126.0, -383.0]
Episode: 269/400, Epsilon:0, Average reward: -62.23
Agent 0 : Predicted Values and True Return : 
 [-174.0, -144.0, -36.0, -138.0, -124.0, -170.0, -234.0, -68.0, -73.0, -61.0] 
 [-62.0, -1714.0, -51.0, -2619.0, -1489.0, -1741.0, -1634.0, -24.0, -87.0, -52.0]
Episode: 270/400, Epsilon:0, Average reward: -120.82
Agent 0 : Predicted Values and True Return : 
 [-117.0, -87.0, -276.0, -127.0, -4.0, -117.0, -87.0, -84.0, -125.0, -138.0] 
 [-3384.0, -39.0, -510.0, -3375.0, -47.0, -3376.0, -30.0, -3392.0, -99.0, -3092.0]
Episode: 271/400, Epsilon:0, Average reward: -30.68
Agent 0 : Predicted Values and True Return : 
 [-137.0, -78.0, -154.0, -156.0, -125.0, -100.0, -85.0, -119.0, -137.0, -98.0] 
 [-255.0, -133.0, -1313.0, -196.0, -262.0, -721.0, -39.0, -191.0, -64.0, -57.0]
Episode: 272/400, Epsilon:0, Average reward: -43.67
Agent 0 : Predicted Values and True Return : 
 [-37.0, -160.0, -140.0, -71.0, -97.0, -75.0, -90.0, -137.0, -137.0, -146.0] 
 [-35.0, -164.0, -911.0, -94.0, -127.0, -51.0, -37.0, -283.0, -157.0, -1269.0]
Episode: 273/400, Epsilon:0, Average reward: -18.8
Agent 0 : Predicted Values and True Return : 
 [-107.0, -90.0, -50.0, -41.0, -55.0, -153.0, -112.0, -140.0, -121.0, -127.0] 
 [-152.0, -72.0, -102.0, -18.0, -16.0, -129.0, -406.0, -114.0, -12.0, -170.0]
Episode: 274/400, Epsilon:0, Average reward: -25.37
Agent 0 : Predicted Values and True Return : 
 [-190.0, -200.0, -132.0, -37.0, -130.0, -32.0, -141.0, -109.0, -12.0, -91.0] 
 [-521.0, -434.0, -213.0, -46.0, -149.0, -16.0, -210.0, -177.0, -76.0, -172.0]
Episode: 275/400, Epsilon:0, Average reward: -38.53
Agent 0 : Predicted Values and True Return : 
 [-5.0, -148.0, -112.0, -173.0, -131.0, -120.0, -109.0, -30.0, -93.0, -89.0] 
 [-22.0, -454.0, -70.0, -1055.0, -44.0, -617.0, -1909.0, -12.0, -1266.0, -72.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 275.
Episode: 276/400, Epsilon:0, Average reward: -29.99
Agent 0 : Predicted Values and True Return : 
 [-206.0, -67.0, -118.0, -112.0, -130.0, -77.0, -94.0, -127.0, -100.0, -49.0] 
 [-344.0, -163.0, -109.0, -91.0, -57.0, -232.0, -128.0, -244.0, -44.0, -82.0]
Episode: 277/400, Epsilon:0, Average reward: -25.35
Agent 0 : Predicted Values and True Return : 
 [-147.0, -96.0, -79.0, -34.0, -95.0, -91.0, -134.0, -164.0, -83.0, -53.0] 
 [-434.0, -404.0, -45.0, -32.0, -33.0, -71.0, -62.0, -282.0, -80.0, -36.0]
Episode: 278/400, Epsilon:0, Average reward: -20.0
Agent 0 : Predicted Values and True Return : 
 [-71.0, -84.0, -46.0, -107.0, -41.0, -254.0, -112.0, -54.0, -138.0, -167.0] 
 [-164.0, -82.0, -72.0, -52.0, -21.0, -301.0, -123.0, -30.0, -174.0, -202.0]
Episode: 279/400, Epsilon:0, Average reward: -44.72
Agent 0 : Predicted Values and True Return : 
 [-36.0, -112.0, -144.0, -140.0, -127.0, -116.0, -147.0, -118.0, -111.0, -98.0] 
 [-40.0, -128.0, -95.0, -184.0, -188.0, -1144.0, -62.0, -118.0, -114.0, -260.0]
Episode: 280/400, Epsilon:0, Average reward: -93.57
Agent 0 : Predicted Values and True Return : 
 [-33.0, -120.0, -76.0, -96.0, -117.0, -70.0, -134.0, -153.0, -110.0, -122.0] 
 [-9.0, -3469.0, -52.0, -2566.0, -314.0, -132.0, -2096.0, -666.0, -47.0, -81.0]
Episode: 281/400, Epsilon:0, Average reward: -74.01
Agent 0 : Predicted Values and True Return : 
 [-136.0, -87.0, -40.0, -201.0, -4.0, -109.0, -176.0, -40.0, -73.0, -90.0] 
 [-231.0, -2751.0, -17.0, -832.0, -10.0, -1391.0, -852.0, -4.0, -41.0, -2490.0]
Episode: 282/400, Epsilon:0, Average reward: -111.56
Agent 0 : Predicted Values and True Return : 
 [-140.0, -138.0, -126.0, -91.0, -105.0, -83.0, -141.0, -44.0, -5.0, -205.0] 
 [-54.0, -51.0, -930.0, -799.0, -3089.0, -3115.0, -2648.0, -11.0, -6.0, -480.0]
Episode: 283/400, Epsilon:0, Average reward: -108.94
Agent 0 : Predicted Values and True Return : 
 [-131.0, -93.0, -89.0, -61.0, -97.0, -130.0, -154.0, -124.0, -61.0, -216.0] 
 [-25.0, -108.0, -3196.0, -22.0, -2728.0, -2054.0, -1599.0, -98.0, -22.0, -166.0]
Episode: 284/400, Epsilon:0, Average reward: -23.75
Agent 0 : Predicted Values and True Return : 
 [-135.0, -88.0, -89.0, -138.0, -25.0, -38.0, -81.0, -155.0, -89.0, -133.0] 
 [-73.0, -41.0, -32.0, -342.0, -17.0, -131.0, -3.0, -199.0, -81.0, -398.0]
Episode: 285/400, Epsilon:0, Average reward: -22.64
Agent 0 : Predicted Values and True Return : 
 [-122.0, -135.0, -80.0, -71.0, -148.0, -170.0, -30.0, -119.0, -63.0, -170.0] 
 [-85.0, -183.0, -118.0, -40.0, -259.0, -746.0, -11.0, -61.0, -28.0, -746.0]
Episode: 286/400, Epsilon:0, Average reward: -20.78
Agent 0 : Predicted Values and True Return : 
 [-109.0, -123.0, -47.0, -35.0, -99.0, -43.0, -80.0, -129.0, -99.0, -115.0] 
 [-126.0, -75.0, -38.0, -26.0, -78.0, -26.0, -29.0, -74.0, -34.0, -44.0]
Episode: 287/400, Epsilon:0, Average reward: -92.79
Agent 0 : Predicted Values and True Return : 
 [-77.0, -25.0, -106.0, -12.0, -153.0, -100.0, -110.0, -159.0, -102.0, -121.0] 
 [-9.0, -15.0, -259.0, -27.0, -137.0, -70.0, -38.0, -213.0, -1158.0, -48.0]
Episode: 288/400, Epsilon:0, Average reward: -18.66
Agent 0 : Predicted Values and True Return : 
 [-71.0, -117.0, -127.0, -141.0, -89.0, -52.0, -93.0, -87.0, -84.0, -107.0] 
 [-84.0, -118.0, -344.0, -181.0, -41.0, -12.0, -37.0, -18.0, -98.0, -192.0]
Episode: 289/400, Epsilon:0, Average reward: -78.19
Agent 0 : Predicted Values and True Return : 
 [-105.0, -99.0, -94.0, -104.0, -99.0, -78.0, -101.0, -54.0, -64.0, -72.0] 
 [-79.0, -74.0, -116.0, -2977.0, -74.0, -42.0, -77.0, -3.0, -69.0, -789.0]
Episode: 290/400, Epsilon:0, Average reward: -23.3
Agent 0 : Predicted Values and True Return : 
 [-119.0, -152.0, -99.0, -100.0, -92.0, -53.0, -152.0, -79.0, -140.0, -164.0] 
 [-191.0, -311.0, -139.0, -205.0, -69.0, -11.0, -199.0, -45.0, -363.0, -412.0]
Episode: 291/400, Epsilon:0, Average reward: -16.37
Saving architecture, weights, optimizer state for best agent-0
New best agent found. Saved in C:\Users\Rzhang\Desktop\MLforFlowOptimisationOrigine\Vissim\Single_Cross_Mod2\Agents_Results\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\BestAgent0_Memory.p
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Agent 0 : Predicted Values and True Return : 
 [-117.0, -154.0, -94.0, -87.0, -188.0, -15.0, -27.0, -21.0, -56.0, -91.0] 
 [-119.0, -545.0, -75.0, -103.0, -166.0, -32.0, -22.0, -33.0, -31.0, -74.0]
Episode: 292/400, Epsilon:0, Average reward: -21.22
Agent 0 : Predicted Values and True Return : 
 [-96.0, -113.0, -71.0, -116.0, -113.0, -48.0, -26.0, -39.0, -21.0, -80.0] 
 [-25.0, -138.0, -130.0, -146.0, -138.0, -17.0, -88.0, -30.0, -25.0, -23.0]
Episode: 293/400, Epsilon:0, Average reward: -24.33
Agent 0 : Predicted Values and True Return : 
 [-122.0, -103.0, -131.0, -116.0, -122.0, -53.0, -18.0, -118.0, -62.0, -151.0] 
 [-197.0, -208.0, -692.0, -49.0, -720.0, -50.0, -8.0, -441.0, -97.0, -211.0]
Episode: 294/400, Epsilon:0, Average reward: -85.89
Agent 0 : Predicted Values and True Return : 
 [-84.0, -85.0, -62.0, -34.0, -112.0, -74.0, -87.0, -95.0, -132.0, -92.0] 
 [-3369.0, -3111.0, -25.0, -24.0, -123.0, -60.0, -125.0, -122.0, -345.0, -25.0]
Episode: 295/400, Epsilon:0, Average reward: -44.1
Agent 0 : Predicted Values and True Return : 
 [-33.0, -183.0, -70.0, -85.0, -71.0, -60.0, -144.0, -151.0, -159.0, -141.0] 
 [-15.0, -313.0, -852.0, -112.0, -74.0, -11.0, -54.0, -1222.0, -758.0, -351.0]
Episode: 296/400, Epsilon:0, Average reward: -22.26
Agent 0 : Predicted Values and True Return : 
 [-108.0, -37.0, -19.0, -154.0, -88.0, -95.0, -123.0, -133.0, -146.0, -89.0] 
 [-257.0, -19.0, -44.0, -476.0, -48.0, -563.0, -114.0, -175.0, -156.0, -97.0]
Episode: 297/400, Epsilon:0, Average reward: -99.55
Agent 0 : Predicted Values and True Return : 
 [-143.0, -132.0, -93.0, -70.0, -110.0, -84.0, -90.0, -92.0, -178.0, -75.0] 
 [-1721.0, -60.0, -76.0, -27.0, -577.0, -3321.0, -2668.0, -21.0, -520.0, -40.0]
Episode: 298/400, Epsilon:0, Average reward: -80.45
Agent 0 : Predicted Values and True Return : 
 [-260.0, -112.0, -173.0, -143.0, -81.0, -122.0, -65.0, -144.0, -130.0, -117.0] 
 [-1123.0, -176.0, -238.0, -113.0, -3386.0, -3003.0, -37.0, -296.0, -153.0, -200.0]
Episode: 299/400, Epsilon:0, Average reward: -23.71
Agent 0 : Predicted Values and True Return : 
 [-125.0, -80.0, -210.0, -100.0, -102.0, -88.0, -114.0, -40.0, -103.0, -81.0] 
 [-175.0, -148.0, -587.0, -33.0, -61.0, -26.0, -110.0, -129.0, -87.0, -83.0]
Episode: 300/400, Epsilon:0, Average reward: -18.19
Agent 0 : Predicted Values and True Return : 
 [-222.0, -36.0, -113.0, -128.0, -132.0, -68.0, -51.0, -48.0, -71.0, -83.0] 
 [-243.0, -20.0, -272.0, -287.0, -686.0, -60.0, -79.0, -13.0, -155.0, -40.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 300.
Episode: 301/400, Epsilon:0, Average reward: -23.68
Agent 0 : Predicted Values and True Return : 
 [-120.0, -96.0, -120.0, -5.0, -76.0, -82.0, -97.0, -89.0, -58.0, -123.0] 
 [-151.0, -19.0, -151.0, -17.0, -241.0, -302.0, -236.0, -89.0, -280.0, -91.0]
Episode: 302/400, Epsilon:0, Average reward: -24.2
Agent 0 : Predicted Values and True Return : 
 [-168.0, -59.0, -163.0, -161.0, -47.0, -6.0, -116.0, -121.0, -80.0, -80.0] 
 [-433.0, -45.0, -331.0, -230.0, -61.0, -42.0, -42.0, -147.0, -125.0, -21.0]
Episode: 303/400, Epsilon:0, Average reward: -22.97
Agent 0 : Predicted Values and True Return : 
 [-112.0, -100.0, -77.0, -137.0, -109.0, -58.0, -96.0, -172.0, -103.0, -130.0] 
 [-191.0, -89.0, -145.0, -119.0, -186.0, -47.0, -115.0, -231.0, -133.0, -467.0]
Episode: 304/400, Epsilon:0, Average reward: -20.46
Agent 0 : Predicted Values and True Return : 
 [-169.0, -197.0, -106.0, -85.0, -121.0, -118.0, -69.0, -106.0, -100.0, -128.0] 
 [-192.0, -156.0, -222.0, -118.0, -132.0, -94.0, -11.0, -92.0, -86.0, -103.0]
Episode: 305/400, Epsilon:0, Average reward: -27.46
Agent 0 : Predicted Values and True Return : 
 [-164.0, -129.0, -51.0, -126.0, -95.0, -63.0, -92.0, -140.0, -126.0, -95.0] 
 [-195.0, -653.0, -36.0, -259.0, -48.0, -68.0, -54.0, -112.0, -259.0, -270.0]
Episode: 306/400, Epsilon:0, Average reward: -30.46
Agent 0 : Predicted Values and True Return : 
 [-98.0, -53.0, -81.0, -94.0, -115.0, -4.0, -50.0, -120.0, -22.0, -112.0] 
 [-75.0, -17.0, -20.0, -205.0, -413.0, -27.0, -19.0, -67.0, -25.0, -73.0]
Episode: 307/400, Epsilon:0, Average reward: -23.81
Agent 0 : Predicted Values and True Return : 
 [-46.0, -119.0, -103.0, -117.0, -124.0, -66.0, -142.0, -112.0, -157.0, -119.0] 
 [-14.0, -189.0, -162.0, -137.0, -228.0, -98.0, -81.0, -954.0, -209.0, -310.0]
Episode: 308/400, Epsilon:0, Average reward: -83.5
Agent 0 : Predicted Values and True Return : 
 [-65.0, -115.0, -117.0, -62.0, -221.0, -29.0, -86.0, -75.0, -109.0, -115.0] 
 [-22.0, -85.0, -55.0, -2.0, -282.0, -19.0, -76.0, -12.0, -1500.0, -1684.0]
Episode: 309/400, Epsilon:0, Average reward: -23.67
Agent 0 : Predicted Values and True Return : 
 [-130.0, -69.0, -48.0, -144.0, -121.0, -35.0, -109.0, -137.0, -101.0, -184.0] 
 [-152.0, -22.0, -12.0, -248.0, -383.0, -19.0, -152.0, -498.0, -52.0, -315.0]
Episode: 310/400, Epsilon:0, Average reward: -54.34
Agent 0 : Predicted Values and True Return : 
 [-5.0, -82.0, -62.0, -66.0, -173.0, -141.0, -160.0, -93.0, -101.0, -113.0] 
 [-29.0, -22.0, -150.0, -146.0, -1178.0, -1957.0, -487.0, -1551.0, -92.0, -1062.0]
Episode: 311/400, Epsilon:0, Average reward: -22.69
Agent 0 : Predicted Values and True Return : 
 [-127.0, -73.0, -47.0, -66.0, -145.0, -175.0, -120.0, -83.0, -174.0, -20.0] 
 [-171.0, -24.0, -40.0, -46.0, -241.0, -691.0, -271.0, -116.0, -341.0, -17.0]
Episode: 312/400, Epsilon:0, Average reward: -19.56
Agent 0 : Predicted Values and True Return : 
 [-71.0, -56.0, -163.0, -88.0, -43.0, -30.0, -108.0, -37.0, -102.0, -136.0] 
 [-52.0, -105.0, -80.0, -88.0, -55.0, -3.0, -133.0, -120.0, -220.0, -422.0]
Episode: 313/400, Epsilon:0, Average reward: -21.53
Agent 0 : Predicted Values and True Return : 
 [-111.0, -61.0, -77.0, -101.0, -113.0, -66.0, -145.0, -54.0, -94.0, -38.0] 
 [-236.0, -80.0, -14.0, -152.0, -198.0, -42.0, -136.0, -16.0, -112.0, -31.0]
Episode: 314/400, Epsilon:0, Average reward: -19.01
Agent 0 : Predicted Values and True Return : 
 [-105.0, -71.0, -105.0, -120.0, -124.0, -144.0, -105.0, -102.0, -106.0, -43.0] 
 [-271.0, -222.0, -280.0, -244.0, -115.0, -285.0, -173.0, -133.0, -271.0, -25.0]
Episode: 315/400, Epsilon:0, Average reward: -32.78
Agent 0 : Predicted Values and True Return : 
 [-116.0, -141.0, -111.0, -115.0, -90.0, -159.0, -165.0, -156.0, -137.0, -135.0] 
 [-45.0, -491.0, -157.0, -254.0, -150.0, -441.0, -526.0, -189.0, -192.0, -206.0]
Episode: 316/400, Epsilon:0, Average reward: -19.63
Agent 0 : Predicted Values and True Return : 
 [-141.0, -72.0, -74.0, -4.0, -161.0, -74.0, -90.0, -108.0, -4.0, -90.0] 
 [-151.0, -93.0, -24.0, -12.0, -157.0, -146.0, -111.0, -85.0, -10.0, -73.0]
Episode: 317/400, Epsilon:0, Average reward: -21.27
Agent 0 : Predicted Values and True Return : 
 [-113.0, -74.0, -72.0, -68.0, -120.0, -137.0, -75.0, -101.0, -71.0, -111.0] 
 [-134.0, -30.0, -108.0, -38.0, -128.0, -444.0, -382.0, -122.0, -142.0, -137.0]
Episode: 318/400, Epsilon:0, Average reward: -19.54
Agent 0 : Predicted Values and True Return : 
 [-75.0, -98.0, -115.0, -29.0, -92.0, -59.0, -103.0, -104.0, -92.0, -68.0] 
 [-72.0, -19.0, -247.0, -25.0, -367.0, -27.0, -104.0, -142.0, -79.0, -148.0]
Episode: 319/400, Epsilon:0, Average reward: -27.5
Agent 0 : Predicted Values and True Return : 
 [-133.0, -65.0, -156.0, -86.0, -94.0, -49.0, -50.0, -69.0, -134.0, -188.0] 
 [-150.0, -180.0, -128.0, -707.0, -86.0, -55.0, -54.0, -16.0, -128.0, -815.0]
Episode: 320/400, Epsilon:0, Average reward: -19.52
Agent 0 : Predicted Values and True Return : 
 [-76.0, -41.0, -96.0, -26.0, -137.0, -92.0, -81.0, -136.0, -104.0, -84.0] 
 [-38.0, -21.0, -106.0, -13.0, -452.0, -69.0, -39.0, -189.0, -89.0, -127.0]
Episode: 321/400, Epsilon:0, Average reward: -27.66
Agent 0 : Predicted Values and True Return : 
 [-125.0, -63.0, -110.0, -102.0, -59.0, -89.0, -55.0, -172.0, -169.0, -84.0] 
 [-403.0, -225.0, -111.0, -208.0, -27.0, -131.0, -12.0, -373.0, -450.0, -66.0]
Episode: 322/400, Epsilon:0, Average reward: -106.35
Agent 0 : Predicted Values and True Return : 
 [-153.0, -79.0, -139.0, -241.0, -104.0, -47.0, -126.0, -85.0, -111.0, -86.0] 
 [-291.0, -362.0, -2903.0, -1064.0, -2914.0, -4.0, -3309.0, -24.0, -2748.0, -3371.0]
Episode: 323/400, Epsilon:0, Average reward: -28.45
Agent 0 : Predicted Values and True Return : 
 [-58.0, -98.0, -167.0, -75.0, -102.0, -119.0, -99.0, -108.0, -99.0, -87.0] 
 [-16.0, -190.0, -1263.0, -117.0, -36.0, -78.0, -29.0, -93.0, -217.0, -184.0]
Episode: 324/400, Epsilon:0, Average reward: -23.04
Agent 0 : Predicted Values and True Return : 
 [-49.0, -116.0, -157.0, -20.0, -113.0, -74.0, -150.0, -161.0, -201.0, -71.0] 
 [-56.0, -112.0, -234.0, -21.0, -137.0, -67.0, -238.0, -418.0, -496.0, -20.0]
Episode: 325/400, Epsilon:0, Average reward: -19.48
Agent 0 : Predicted Values and True Return : 
 [-28.0, -134.0, -135.0, -124.0, -105.0, -74.0, -100.0, -115.0, -142.0, -123.0] 
 [-17.0, -192.0, -64.0, -104.0, -108.0, -66.0, -112.0, -156.0, -103.0, -153.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 325.
Episode: 326/400, Epsilon:0, Average reward: -20.69
Agent 0 : Predicted Values and True Return : 
 [-92.0, -27.0, -113.0, -5.0, -117.0, -41.0, -84.0, -117.0, -99.0, -100.0] 
 [-38.0, -19.0, -265.0, -12.0, -50.0, -18.0, -102.0, -301.0, -72.0, -36.0]
Episode: 327/400, Epsilon:0, Average reward: -24.51
Agent 0 : Predicted Values and True Return : 
 [-182.0, -25.0, -94.0, -70.0, -131.0, -146.0, -182.0, -42.0, -104.0, -69.0] 
 [-313.0, -30.0, -78.0, -193.0, -79.0, -106.0, -319.0, -13.0, -32.0, -87.0]
Episode: 328/400, Epsilon:0, Average reward: -31.91
Agent 0 : Predicted Values and True Return : 
 [-163.0, -61.0, -134.0, -124.0, -108.0, -70.0, -103.0, -172.0, -62.0, -154.0] 
 [-379.0, -74.0, -305.0, -120.0, -1472.0, -68.0, -1147.0, -966.0, -31.0, -1456.0]
Episode: 329/400, Epsilon:0, Average reward: -97.11
Agent 0 : Predicted Values and True Return : 
 [-126.0, -61.0, -86.0, -123.0, -151.0, -85.0, -76.0, -48.0, -102.0, -105.0] 
 [-3438.0, -60.0, -3456.0, -165.0, -3389.0, -28.0, -83.0, -37.0, -2494.0, -249.0]
Episode: 330/400, Epsilon:0, Average reward: -32.93
Agent 0 : Predicted Values and True Return : 
 [-63.0, -5.0, -103.0, -58.0, -90.0, -120.0, -127.0, -83.0, -36.0, -33.0] 
 [-141.0, -19.0, -308.0, -55.0, -125.0, -76.0, -41.0, -68.0, -12.0, -20.0]
Episode: 331/400, Epsilon:0, Average reward: -20.32
Agent 0 : Predicted Values and True Return : 
 [-84.0, -152.0, -58.0, -118.0, -96.0, -69.0, -130.0, -148.0, -78.0, -56.0] 
 [-140.0, -179.0, -66.0, -198.0, -80.0, -38.0, -400.0, -237.0, -112.0, -160.0]
Episode: 332/400, Epsilon:0, Average reward: -31.25
Agent 0 : Predicted Values and True Return : 
 [-57.0, -73.0, -87.0, -89.0, -51.0, -111.0, -153.0, -118.0, -147.0, -67.0] 
 [-64.0, -241.0, -89.0, -494.0, -29.0, -1375.0, -700.0, -139.0, -1243.0, -46.0]
Episode: 333/400, Epsilon:0, Average reward: -23.1
Agent 0 : Predicted Values and True Return : 
 [-114.0, -112.0, -78.0, -109.0, -102.0, -73.0, -73.0, -162.0, -89.0, -88.0] 
 [-396.0, -86.0, -177.0, -178.0, -161.0, -37.0, -150.0, -167.0, -32.0, -129.0]
Episode: 334/400, Epsilon:0, Average reward: -23.84
Agent 0 : Predicted Values and True Return : 
 [-129.0, -165.0, -70.0, -126.0, -41.0, -80.0, -154.0, -106.0, -103.0, -94.0] 
 [-174.0, -296.0, -156.0, -298.0, -72.0, -229.0, -236.0, -40.0, -88.0, -160.0]
Episode: 335/400, Epsilon:0, Average reward: -82.15
Agent 0 : Predicted Values and True Return : 
 [-114.0, -106.0, -102.0, -102.0, -64.0, -91.0, -181.0, -130.0, -83.0, -107.0] 
 [-145.0, -43.0, -61.0, -139.0, -52.0, -499.0, -998.0, -2846.0, -211.0, -162.0]
Episode: 336/400, Epsilon:0, Average reward: -87.06
Agent 0 : Predicted Values and True Return : 
 [-83.0, -90.0, -76.0, -40.0, -60.0, -120.0, -92.0, -146.0, -144.0, -37.0] 
 [-53.0, -3220.0, -49.0, -56.0, -42.0, -1196.0, -17.0, -78.0, -126.0, -25.0]
Episode: 337/400, Epsilon:0, Average reward: -47.81
Agent 0 : Predicted Values and True Return : 
 [-107.0, -43.0, -117.0, -144.0, -41.0, -94.0, -130.0, -213.0, -55.0, -5.0] 
 [-245.0, -14.0, -65.0, -313.0, -49.0, -1578.0, -109.0, -1151.0, -26.0, -15.0]
Episode: 338/400, Epsilon:0, Average reward: -71.04
Agent 0 : Predicted Values and True Return : 
 [-60.0, -116.0, -91.0, -70.0, -80.0, -103.0, -144.0, -6.0, -138.0, -217.0] 
 [-68.0, -74.0, -88.0, -253.0, -14.0, -94.0, -251.0, -20.0, -77.0, -1823.0]
Episode: 339/400, Epsilon:0, Average reward: -122.79
Agent 0 : Predicted Values and True Return : 
 [-24.0, -98.0, -142.0, -5.0, -40.0, -186.0, -125.0, -165.0, -60.0, -126.0] 
 [-28.0, -20.0, -3202.0, -18.0, -285.0, -366.0, -70.0, -2495.0, -15.0, -1632.0]
Episode: 340/400, Epsilon:0, Average reward: -110.89
Agent 0 : Predicted Values and True Return : 
 [-129.0, -211.0, -166.0, -225.0, -146.0, -116.0, -85.0, -204.0, -120.0, -88.0] 
 [-118.0, -164.0, -425.0, -410.0, -1284.0, -200.0, -2424.0, -1308.0, -399.0, -175.0]
Episode: 341/400, Epsilon:0, Average reward: -24.54
Agent 0 : Predicted Values and True Return : 
 [-169.0, -32.0, -45.0, -37.0, -29.0, -24.0, -123.0, -138.0, -163.0, -157.0] 
 [-808.0, -13.0, -38.0, -5.0, -116.0, -17.0, -226.0, -284.0, -259.0, -779.0]
Episode: 342/400, Epsilon:0, Average reward: -24.0
Agent 0 : Predicted Values and True Return : 
 [-162.0, -118.0, -151.0, -157.0, -81.0, -70.0, -90.0, -149.0, -53.0, -147.0] 
 [-129.0, -222.0, -95.0, -378.0, -98.0, -473.0, -52.0, -409.0, -14.0, -237.0]
Episode: 343/400, Epsilon:0, Average reward: -69.57
Agent 0 : Predicted Values and True Return : 
 [-90.0, -120.0, -83.0, -83.0, -90.0, -105.0, -139.0, -83.0, -164.0, -113.0] 
 [-454.0, -544.0, -569.0, -569.0, -122.0, -30.0, -1207.0, -41.0, -177.0, -405.0]
Episode: 344/400, Epsilon:0, Average reward: -182.44
Agent 0 : Predicted Values and True Return : 
 [-235.0, -143.0, -178.0, -99.0, -160.0, -138.0, -74.0, -268.0, -154.0, -99.0] 
 [-2630.0, -2721.0, -116.0, -28.0, -2406.0, -3217.0, -35.0, -1388.0, -3015.0, -89.0]
Episode: 345/400, Epsilon:0, Average reward: -17.67
Agent 0 : Predicted Values and True Return : 
 [-178.0, -167.0, -57.0, -96.0, -193.0, -59.0, -276.0, -99.0, -215.0, -93.0] 
 [-202.0, -238.0, -24.0, -339.0, -57.0, -134.0, -406.0, -91.0, -366.0, -18.0]
Episode: 346/400, Epsilon:0, Average reward: -24.09
Agent 0 : Predicted Values and True Return : 
 [-227.0, -95.0, -83.0, -176.0, -62.0, -154.0, -130.0, -129.0, -108.0, -146.0] 
 [-311.0, -532.0, -40.0, -376.0, -58.0, -237.0, -103.0, -244.0, -118.0, -476.0]
Episode: 347/400, Epsilon:0, Average reward: -24.04
Agent 0 : Predicted Values and True Return : 
 [-253.0, -85.0, -149.0, -60.0, -74.0, -72.0, -211.0, -138.0, -46.0, -148.0] 
 [-245.0, -79.0, -112.0, -22.0, -48.0, -64.0, -221.0, -203.0, -16.0, -388.0]
Episode: 348/400, Epsilon:0, Average reward: -21.36
Agent 0 : Predicted Values and True Return : 
 [-47.0, -122.0, -76.0, -70.0, -67.0, -64.0, -144.0, -175.0, -135.0, -122.0] 
 [-14.0, -125.0, -51.0, -56.0, -9.0, -45.0, -514.0, -486.0, -51.0, -125.0]
Episode: 349/400, Epsilon:0, Average reward: -72.67
Agent 0 : Predicted Values and True Return : 
 [-91.0, -121.0, -153.0, -62.0, -105.0, -46.0, -32.0, -121.0, -80.0, -110.0] 
 [-3255.0, -101.0, -55.0, -32.0, -150.0, -45.0, -14.0, -170.0, -48.0, -83.0]
Episode: 350/400, Epsilon:0, Average reward: -20.68
Agent 0 : Predicted Values and True Return : 
 [-144.0, -137.0, -79.0, -94.0, -93.0, -41.0, -87.0, -65.0, -129.0, -161.0] 
 [-180.0, -84.0, -109.0, -161.0, -100.0, -3.0, -146.0, -66.0, -240.0, -151.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 350.
Episode: 351/400, Epsilon:0, Average reward: -28.75
Agent 0 : Predicted Values and True Return : 
 [-108.0, -59.0, -108.0, -138.0, -161.0, -49.0, -76.0, -119.0, -132.0, -113.0] 
 [-66.0, -65.0, -71.0, -122.0, -188.0, -78.0, -28.0, -79.0, -35.0, -66.0]
Episode: 352/400, Epsilon:0, Average reward: -26.58
Agent 0 : Predicted Values and True Return : 
 [-186.0, -113.0, -85.0, -147.0, -60.0, -65.0, -120.0, -79.0, -193.0, -112.0] 
 [-327.0, -902.0, -39.0, -587.0, -22.0, -105.0, -816.0, -106.0, -480.0, -217.0]
Episode: 353/400, Epsilon:0, Average reward: -56.8
Agent 0 : Predicted Values and True Return : 
 [-46.0, -100.0, -84.0, -113.0, -5.0, -104.0, -99.0, -72.0, -158.0, -105.0] 
 [-3.0, -77.0, -54.0, -192.0, -19.0, -36.0, -74.0, -204.0, -191.0, -81.0]
Episode: 354/400, Epsilon:0, Average reward: -27.54
Agent 0 : Predicted Values and True Return : 
 [-174.0, -132.0, -160.0, -22.0, -68.0, -98.0, -108.0, -145.0, -118.0, -105.0] 
 [-682.0, -115.0, -164.0, -10.0, -20.0, -89.0, -129.0, -80.0, -108.0, -23.0]
Episode: 355/400, Epsilon:0, Average reward: -23.66
Agent 0 : Predicted Values and True Return : 
 [-129.0, -127.0, -77.0, -119.0, -148.0, -138.0, -114.0, -112.0, -78.0, -25.0] 
 [-43.0, -174.0, -65.0, -675.0, -75.0, -157.0, -126.0, -261.0, -115.0, -37.0]
Episode: 356/400, Epsilon:0, Average reward: -54.5
Agent 0 : Predicted Values and True Return : 
 [-47.0, -69.0, -157.0, -84.0, -75.0, -102.0, -94.0, -74.0, -49.0, -102.0] 
 [-106.0, -52.0, -548.0, -101.0, -82.0, -51.0, -195.0, -86.0, -47.0, -93.0]
Episode: 357/400, Epsilon:0, Average reward: -102.87
Agent 0 : Predicted Values and True Return : 
 [-121.0, -110.0, -89.0, -137.0, -56.0, -40.0, -113.0, -162.0, -79.0, -150.0] 
 [-229.0, -244.0, -10.0, -313.0, -3259.0, -14.0, -193.0, -296.0, -19.0, -179.0]
Episode: 358/400, Epsilon:0, Average reward: -62.85
Agent 0 : Predicted Values and True Return : 
 [-98.0, -108.0, -103.0, -175.0, -93.0, -34.0, -120.0, -103.0, -108.0, -66.0] 
 [-2904.0, -177.0, -210.0, -280.0, -49.0, -16.0, -3057.0, -211.0, -2874.0, -44.0]
Episode: 359/400, Epsilon:0, Average reward: -24.81
Agent 0 : Predicted Values and True Return : 
 [-94.0, -133.0, -68.0, -117.0, -81.0, -123.0, -98.0, -52.0, -159.0, -42.0] 
 [-49.0, -410.0, -44.0, -188.0, -116.0, -175.0, -143.0, -13.0, -95.0, -35.0]
Episode: 360/400, Epsilon:0, Average reward: -24.4
Agent 0 : Predicted Values and True Return : 
 [-60.0, -121.0, -43.0, -64.0, -141.0, -130.0, -29.0, -95.0, -162.0, -32.0] 
 [-3.0, -318.0, -40.0, -39.0, -232.0, -301.0, -40.0, -518.0, -239.0, -5.0]
Episode: 361/400, Epsilon:0, Average reward: -24.37
Agent 0 : Predicted Values and True Return : 
 [-43.0, -161.0, -43.0, -63.0, -121.0, -67.0, -22.0, -71.0, -110.0, -76.0] 
 [-3.0, -727.0, -53.0, -39.0, -342.0, -39.0, -23.0, -114.0, -88.0, -30.0]
Episode: 362/400, Epsilon:0, Average reward: -67.01
Agent 0 : Predicted Values and True Return : 
 [-177.0, -41.0, -88.0, -77.0, -142.0, -54.0, -184.0, -99.0, -205.0, -41.0] 
 [-500.0, -6.0, -281.0, -24.0, -166.0, -3029.0, -189.0, -128.0, -669.0, -44.0]
Episode: 363/400, Epsilon:0, Average reward: -56.72
Agent 0 : Predicted Values and True Return : 
 [-112.0, -29.0, -54.0, -100.0, -89.0, -146.0, -178.0, -50.0, -133.0, -132.0] 
 [-120.0, -15.0, -29.0, -1610.0, -51.0, -189.0, -1757.0, -15.0, -68.0, -180.0]
Episode: 364/400, Epsilon:0, Average reward: -64.02
Agent 0 : Predicted Values and True Return : 
 [-94.0, -59.0, -101.0, -125.0, -95.0, -126.0, -80.0, -94.0, -94.0, -89.0] 
 [-46.0, -27.0, -553.0, -328.0, -324.0, -322.0, -3106.0, -46.0, -46.0, -39.0]
Episode: 365/400, Epsilon:0, Average reward: -57.98
Agent 0 : Predicted Values and True Return : 
 [-139.0, -130.0, -129.0, -82.0, -97.0, -119.0, -116.0, -131.0, -70.0, -75.0] 
 [-1461.0, -586.0, -338.0, -358.0, -119.0, -353.0, -766.0, -1126.0, -29.0, -344.0]
Episode: 366/400, Epsilon:0, Average reward: -19.32
Agent 0 : Predicted Values and True Return : 
 [-56.0, -55.0, -142.0, -79.0, -19.0, -80.0, -79.0, -41.0, -68.0, -88.0] 
 [-74.0, -79.0, -87.0, -78.0, -28.0, -28.0, -149.0, -44.0, -77.0, -76.0]
Episode: 367/400, Epsilon:0, Average reward: -81.46
Agent 0 : Predicted Values and True Return : 
 [-28.0, -80.0, -129.0, -167.0, -99.0, -108.0, -181.0, -117.0, -59.0, -87.0] 
 [-72.0, -60.0, -247.0, -1306.0, -85.0, -202.0, -529.0, -3383.0, -70.0, -64.0]
Episode: 368/400, Epsilon:0, Average reward: -37.21
Agent 0 : Predicted Values and True Return : 
 [-149.0, -102.0, -78.0, -188.0, -65.0, -86.0, -110.0, -121.0, -111.0, -120.0] 
 [-1250.0, -64.0, -56.0, -269.0, -53.0, -165.0, -104.0, -347.0, -177.0, -230.0]
Episode: 369/400, Epsilon:0, Average reward: -68.36
Agent 0 : Predicted Values and True Return : 
 [-129.0, -95.0, -92.0, -124.0, -42.0, -103.0, -34.0, -64.0, -61.0, -141.0] 
 [-1836.0, -76.0, -180.0, -98.0, -6.0, -1835.0, -9.0, -59.0, -108.0, -142.0]
Episode: 370/400, Epsilon:0, Average reward: -29.83
Agent 0 : Predicted Values and True Return : 
 [-47.0, -46.0, -55.0, -139.0, -162.0, -92.0, -104.0, -150.0, -92.0, -52.0] 
 [-6.0, -16.0, -89.0, -258.0, -195.0, -148.0, -87.0, -974.0, -120.0, -172.0]
Episode: 371/400, Epsilon:0, Average reward: -46.05
Agent 0 : Predicted Values and True Return : 
 [-90.0, -86.0, -98.0, -60.0, -192.0, -140.0, -120.0, -146.0, -110.0, -53.0] 
 [-43.0, -101.0, -242.0, -21.0, -247.0, -1222.0, -117.0, -727.0, -161.0, -143.0]
Episode: 372/400, Epsilon:0, Average reward: -62.24
Agent 0 : Predicted Values and True Return : 
 [-96.0, -96.0, -125.0, -86.0, -115.0, -29.0, -126.0, -86.0, -149.0, -92.0] 
 [-68.0, -2369.0, -110.0, -22.0, -151.0, -20.0, -934.0, -3078.0, -122.0, -75.0]
Episode: 373/400, Epsilon:0, Average reward: -56.21
Agent 0 : Predicted Values and True Return : 
 [-29.0, -39.0, -118.0, -188.0, -133.0, -110.0, -140.0, -86.0, -49.0, -59.0] 
 [-52.0, -51.0, -1923.0, -127.0, -39.0, -1868.0, -123.0, -52.0, -23.0, -32.0]
Episode: 374/400, Epsilon:0, Average reward: -31.52
Agent 0 : Predicted Values and True Return : 
 [-80.0, -156.0, -129.0, -84.0, -137.0, -44.0, -23.0, -137.0, -60.0, -116.0] 
 [-936.0, -412.0, -430.0, -83.0, -219.0, -56.0, -21.0, -219.0, -65.0, -829.0]
Episode: 375/400, Epsilon:0, Average reward: -32.05
Agent 0 : Predicted Values and True Return : 
 [-100.0, -96.0, -121.0, -68.0, -141.0, -156.0, -121.0, -140.0, -23.0, -32.0] 
 [-1498.0, -4.0, -386.0, -20.0, -137.0, -226.0, -610.0, -239.0, -31.0, -13.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 375.
Episode: 376/400, Epsilon:0, Average reward: -100.49
Agent 0 : Predicted Values and True Return : 
 [-146.0, -152.0, -72.0, -110.0, -131.0, -93.0, -72.0, -53.0, -74.0, -37.0] 
 [-60.0, -1361.0, -24.0, -145.0, -753.0, -3380.0, -373.0, -41.0, -74.0, -37.0]
Episode: 377/400, Epsilon:0, Average reward: -19.98
Agent 0 : Predicted Values and True Return : 
 [-114.0, -135.0, -124.0, -94.0, -107.0, -128.0, -79.0, -37.0, -69.0, -139.0] 
 [-144.0, -87.0, -163.0, -290.0, -239.0, -28.0, -90.0, -135.0, -28.0, -585.0]
Episode: 378/400, Epsilon:0, Average reward: -32.82
Agent 0 : Predicted Values and True Return : 
 [-127.0, -88.0, -84.0, -111.0, -124.0, -73.0, -74.0, -41.0, -62.0, -137.0] 
 [-361.0, -114.0, -154.0, -298.0, -273.0, -285.0, -7.0, -39.0, -19.0, -236.0]
Episode: 379/400, Epsilon:0, Average reward: -20.05
Agent 0 : Predicted Values and True Return : 
 [-110.0, -98.0, -98.0, -119.0, -103.0, -130.0, -78.0, -86.0, -188.0, -110.0] 
 [-128.0, -122.0, -11.0, -136.0, -157.0, -262.0, -121.0, -80.0, -312.0, -128.0]
Episode: 380/400, Epsilon:0, Average reward: -18.69
Agent 0 : Predicted Values and True Return : 
 [-180.0, -81.0, -5.0, -5.0, -96.0, -82.0, -66.0, -111.0, -111.0, -152.0] 
 [-308.0, -11.0, -1.0, -23.0, -45.0, -175.0, -53.0, -35.0, -35.0, -215.0]
Episode: 381/400, Epsilon:0, Average reward: -51.5
Agent 0 : Predicted Values and True Return : 
 [-86.0, -116.0, -93.0, -112.0, -137.0, -45.0, -90.0, -84.0, -84.0, -82.0] 
 [-95.0, -121.0, -71.0, -65.0, -85.0, -6.0, -117.0, -87.0, -2199.0, -94.0]
Episode: 382/400, Epsilon:0, Average reward: -120.17
Agent 0 : Predicted Values and True Return : 
 [-116.0, -88.0, -67.0, -261.0, -126.0, -70.0, -46.0, -141.0, -127.0, -116.0] 
 [-3216.0, -3318.0, -3112.0, -425.0, -3160.0, -757.0, -24.0, -2518.0, -776.0, -54.0]
Episode: 383/400, Epsilon:0, Average reward: -53.42
Agent 0 : Predicted Values and True Return : 
 [-108.0, -112.0, -74.0, -115.0, -87.0, -111.0, -81.0, -146.0, -174.0, -22.0] 
 [-99.0, -167.0, -340.0, -105.0, -331.0, -133.0, -55.0, -205.0, -157.0, -98.0]
Episode: 384/400, Epsilon:0, Average reward: -116.25
Agent 0 : Predicted Values and True Return : 
 [-95.0, -155.0, -132.0, -80.0, -111.0, -68.0, -55.0, -64.0, -31.0, -165.0] 
 [-96.0, -474.0, -483.0, -3346.0, -178.0, -39.0, -42.0, -28.0, -61.0, -94.0]
Episode: 385/400, Epsilon:0, Average reward: -143.22
Agent 0 : Predicted Values and True Return : 
 [-55.0, -72.0, -117.0, -224.0, -73.0, -115.0, -33.0, -65.0, -105.0, -97.0] 
 [-3381.0, -72.0, -1254.0, -984.0, -3312.0, -485.0, -1325.0, -21.0, -2324.0, -118.0]
Episode: 386/400, Epsilon:0, Average reward: -24.22
Agent 0 : Predicted Values and True Return : 
 [-150.0, -20.0, -114.0, -88.0, -74.0, -154.0, -167.0, -196.0, -62.0, -34.0] 
 [-595.0, -51.0, -24.0, -44.0, -138.0, -427.0, -154.0, -99.0, -73.0, -22.0]
Episode: 387/400, Epsilon:0, Average reward: -58.81
Agent 0 : Predicted Values and True Return : 
 [-106.0, -160.0, -120.0, -116.0, -29.0, -122.0, -45.0, -96.0, -100.0, -209.0] 
 [-53.0, -162.0, -1836.0, -3156.0, -20.0, -404.0, -51.0, -57.0, -54.0, -641.0]
Episode: 388/400, Epsilon:0, Average reward: -31.95
Agent 0 : Predicted Values and True Return : 
 [-146.0, -157.0, -81.0, -129.0, -81.0, -41.0, -102.0, -33.0, -135.0, -83.0] 
 [-56.0, -1509.0, -43.0, -51.0, -273.0, -30.0, -135.0, -42.0, -87.0, -132.0]
Episode: 389/400, Epsilon:0, Average reward: -194.86
Agent 0 : Predicted Values and True Return : 
 [-57.0, -199.0, -133.0, -79.0, -104.0, -97.0, -31.0, -161.0, -45.0, -89.0] 
 [-3398.0, -162.0, -10.0, -3384.0, -3395.0, -57.0, -3438.0, -65.0, -3423.0, -70.0]
Episode: 390/400, Epsilon:0, Average reward: -135.39
Agent 0 : Predicted Values and True Return : 
 [-120.0, -240.0, -87.0, -128.0, -216.0, -6.0, -105.0, -69.0, -216.0, -106.0] 
 [-3406.0, -459.0, -3131.0, -13.0, -343.0, -17.0, -1998.0, -52.0, -343.0, -306.0]
Episode: 391/400, Epsilon:0, Average reward: -107.69
Agent 0 : Predicted Values and True Return : 
 [-262.0, -80.0, -240.0, -105.0, -77.0, -162.0, -106.0, -132.0, -102.0, -202.0] 
 [-1105.0, -36.0, -3363.0, -125.0, -89.0, -105.0, -156.0, -42.0, -58.0, -101.0]
Episode: 392/400, Epsilon:0, Average reward: -171.24
Agent 0 : Predicted Values and True Return : 
 [-106.0, -279.0, -88.0, -37.0, -67.0, -137.0, -245.0, -261.0, -195.0, -109.0] 
 [-431.0, -709.0, -43.0, -3.0, -8.0, -2269.0, -1555.0, -860.0, -2095.0, -1987.0]
Episode: 393/400, Epsilon:0, Average reward: -84.0
Agent 0 : Predicted Values and True Return : 
 [-127.0, -299.0, -112.0, -116.0, -191.0, -108.0, -142.0, -78.0, -149.0, -167.0] 
 [-110.0, -326.0, -61.0, -97.0, -168.0, -170.0, -42.0, -50.0, -173.0, -271.0]
Episode: 394/400, Epsilon:0, Average reward: -25.98
Agent 0 : Predicted Values and True Return : 
 [-135.0, -179.0, -183.0, -116.0, -198.0, -160.0, -27.0, -120.0, -79.0, -63.0] 
 [-101.0, -496.0, -192.0, -283.0, -455.0, -30.0, -32.0, -216.0, -26.0, -18.0]
Episode: 395/400, Epsilon:0, Average reward: -52.45
Agent 0 : Predicted Values and True Return : 
 [-155.0, -98.0, -122.0, -162.0, -112.0, -107.0, -112.0, -56.0, -112.0, -135.0] 
 [-753.0, -191.0, -495.0, -366.0, -1225.0, -850.0, -487.0, -1598.0, -221.0, -41.0]
Episode: 396/400, Epsilon:0, Average reward: -96.52
Agent 0 : Predicted Values and True Return : 
 [-105.0, -188.0, -80.0, -132.0, -91.0, -38.0, -89.0, -44.0, -86.0, -117.0] 
 [-28.0, -212.0, -62.0, -2485.0, -41.0, -55.0, -22.0, -44.0, -67.0, -106.0]
Episode: 397/400, Epsilon:0, Average reward: -67.17
Agent 0 : Predicted Values and True Return : 
 [-165.0, -113.0, -128.0, -32.0, -145.0, -179.0, -99.0, -174.0, -54.0, -98.0] 
 [-117.0, -2050.0, -28.0, -5.0, -1807.0, -104.0, -35.0, -249.0, -26.0, -51.0]
Episode: 398/400, Epsilon:0, Average reward: -112.86
Agent 0 : Predicted Values and True Return : 
 [-317.0, -123.0, -126.0, -189.0, -158.0, -94.0, -6.0, -230.0, -131.0, -166.0] 
 [-321.0, -3372.0, -1567.0, -123.0, -3239.0, -136.0, -51.0, -205.0, -3358.0, -162.0]
Episode: 399/400, Epsilon:0, Average reward: -56.31
Agent 0 : Predicted Values and True Return : 
 [-116.0, -137.0, -117.0, -267.0, -212.0, -192.0, -125.0, -151.0, -110.0, -207.0] 
 [-144.0, -5.0, -1870.0, -345.0, -2034.0, -560.0, -139.0, -1112.0, -42.0, -183.0]
Episode: 400/400, Epsilon:0, Average reward: -21.71
Agent 0 : Predicted Values and True Return : 
 [-148.0, -125.0, -79.0, -110.0, -80.0, -98.0, -56.0, -100.0, -57.0, -199.0] 
 [-204.0, -639.0, -79.0, -232.0, -43.0, -85.0, -30.0, -47.0, -136.0, -314.0]
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Saved Partial results at the end of episode 400.
Saving architecture, weights, optimizer state for agent-0
Dumping agent-0 memory into pickle file
Dumping Training Results into pickle file.
Dumping Loss Results into pickle file.
Model, architecture, weights, optimizer, memory and training results succesfully saved.            Succesfully Terminated.









#################################################
#                                               #
#                   TEST 2                      #
#                                               #
#################################################