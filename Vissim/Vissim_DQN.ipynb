{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Libraries\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Libraries\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "## Data Management Libraries\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "## Other Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Model Parameters\n",
    "Random_Seed = 42\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "program = 'DQN'\n",
    "reward_type = 'Delay'\n",
    "state_type  = 'Queues' \n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "## Load trained model?\n",
    "load_trained = False\n",
    "Quickmode = True\n",
    "# Random demand\n",
    "Random_Demand = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling flags\n",
    "# Flag for restarting the COM Server\n",
    "reset_flag = True\n",
    "# If a fresh start is needed, all previous results from simulations are deleted\n",
    "Start_Fresh = True\n",
    "# Debug action\n",
    "debug_action = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## State-Action Parameters\n",
    "state_size = 4\n",
    "action_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations\n",
    "episodes = 15\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec)\n",
    "simulation_length = 36000*2\n",
    "# Memory Size\n",
    "memory_size = 1000\n",
    "# Batch Size\n",
    "batch_size = 64\n",
    "# Learning Rate\n",
    "alpha   = 0.001\n",
    "# Discount Factor\n",
    "gamma   = 0.9\n",
    "# Exploration Schedule\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "epsilon_decay = np.power(epsilon_start/epsilon_end, 1./episodes) # Geometric decay\n",
    "# Demand Schedule\n",
    "demands = [100,200, 400, 600, 800, 1000, 1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user defined functions\n"
     ]
    }
   ],
   "source": [
    "## Basic User Defined Functions\n",
    "# Function to convert a nested tuple to a nested list\n",
    "def toList(NestedTuple):\n",
    "    return list(map(toList, NestedTuple)) if isinstance(NestedTuple, (list, tuple)) else NestedTuple\n",
    "print ('Loaded user defined functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN Agent Class\n",
    "# To access memory of agent i for data about time t:\n",
    "# s_t     = Agents[i].memory[t][0]\n",
    "# a_t     = Agents[i].memory[t][1]\n",
    "# r_t     = Agents[i].memory[t][2]\n",
    "# s_(t+1) = Agents[i].memory[t][3]\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, ID):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.gamma = gamma                    # discount rate\n",
    "        self.epsilon = epsilon_start          # starting exploration rate\n",
    "        self.epsilon_min = epsilon_end        # final exploration rate\n",
    "        self.epsilon_decay = epsilon_decay    # decay of exploration rate\n",
    "        self.learning_rate = alpha            # learning rate\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        self.state = np.reshape([0,0,0,0], [1,4])\n",
    "        self.newstate = np.reshape([0,0,0,0], [1,4])\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.episode_reward = []\n",
    "        \n",
    "    def update_IDS(self, ID):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "    \n",
    "    # DNN definition\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    # Obtain the state based on different state definitions\n",
    "    def get_state(self, state_type = state_type):\n",
    "        if state_type == 'Queues':\n",
    "            #Obtain Queue Values (average value over the last period)\n",
    "            East_Queue  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)')\n",
    "            South_Queue = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)')\n",
    "            West_Queue  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)')\n",
    "            North_Queue = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)')\n",
    "            state = [East_Queue, South_Queue, West_Queue, North_Queue]\n",
    "            state = np.reshape(state, [1,4])\n",
    "            return(state)\n",
    "        elif state_type == 'Delay':\n",
    "            pass\n",
    "        elif state_type == 'MaxFlow':\n",
    "            pass\n",
    "        elif state_type == 'FuelConsumption':\n",
    "            pass\n",
    "        elif state_type == 'NOx':\n",
    "            pass\n",
    "        elif state_type == \"COM\":\n",
    "            pass\n",
    "    \n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "        return(self.memory)\n",
    "    \n",
    "    # Choosing actions\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size) \n",
    "            self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "            #print('Chosen Random Action {}'.format(action+1))\n",
    "            return action\n",
    "        act_values = self.model.predict(state)\n",
    "        action = np.argmax(act_values[0]) \n",
    "        self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "        #print('Chosen Not-Random Action {}'.format(action+1))\n",
    "        return action  # returns action\n",
    "    \n",
    "    def get_reward(self):\n",
    "        reward = -np.absolute((self.newstate[0][0]-self.newstate[0][2])-(self.newstate[0][1]-self.newstate[0][3]))\n",
    "        self.episode_reward.append(reward)\n",
    "        return reward\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * \\\n",
    "                       np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Parser (Crawler) class definition\n",
    "class NetworkParser:\n",
    "    \n",
    "    ######################################################################################################################\n",
    "    ## Nested data structure:\n",
    "    ## \n",
    "    ## Signal Controllers = signal_controllers[signal_controller_ids]\n",
    "    ## Signal Groups      = signal_groups     [signal_controller_ids] [signal_group_id]\n",
    "    ## Signal Heads       = signal_heads      [signal_controller_ids] [signal_heads_id]\n",
    "    ## Lanes              = lanes             [signal_controller_ids] [signal_heads_id] [lane_id]\n",
    "    ##\n",
    "    ######################################################################################################################\n",
    "    ##\n",
    "    ## Accessing attributes:\n",
    "    ##\n",
    "    ## AttValue('AttName(X,Y,bla)')\n",
    "    ##\n",
    "    ## X = Simulation Number.      Values: 1,2,3.. 'Current' [single case], Avg, StdDev, Min, Max [over several sims]\n",
    "    ## Y = Time Interval Number    Values: 1,2,3, 'Current', 'Last', Avg, StdDev, Min, Max, Total\n",
    "    ## All = All vehicle classes   Values: 10, 20, All\n",
    "    ######################################################################################################################\n",
    "\n",
    "    def __init__(self):\n",
    "        ## Get all SignalControllers\n",
    "        self.signal_controllers     = toList(Vissim.Net.SignalControllers.GetAll())\n",
    "        self.signal_controllers_ids = range(len(self.signal_controllers)) #Vissim count starts at 1\n",
    "                 \n",
    "        ## Create SignalGroupContainers and unpack the SignalGroups into a list by SignalController\n",
    "        self.signal_groups = [[] for _ in self.signal_controllers_ids]\n",
    "        for SC in self.signal_controllers_ids:\n",
    "            for SG in range(1,self.signal_controllers[SC].SGs.Count+1):\n",
    "                self.signal_groups[SC].append(self.signal_controllers[SC].SGs.ItemByKey(SG))\n",
    "                \n",
    "        ## Create SignalHeadsCollection and unpack the SignalHeads into a list by SignalController\n",
    "        self.signal_heads = [[] for _ in self.signal_controllers_ids]\n",
    "        for SC in self.signal_controllers_ids:\n",
    "            for SG in range(self.signal_controllers[SC].SGs.Count):\n",
    "                self.signal_heads[SC].append(toList(self.signal_groups[SC][SG].SigHeads.GetAll())[0])\n",
    "                \n",
    "        self.lanes = [[[] for b in range(len(self.signal_heads[a])) ] for a in self.signal_controllers_ids]\n",
    "        for SC in self.signal_controllers_ids:\n",
    "            for SH in range(len(self.signal_heads[SC])):\n",
    "                self.lanes[SC][SH].append(self.signal_heads[SC][SH].Lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COMServerDispatch(reset_flag):\n",
    "    ## Connecting the COM Server => Open a new Vissim Window:\n",
    "    # Server should only be dispatched in first run. Otherwise reload model.\n",
    "    # Setting Working Directory\n",
    "    vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "    print ('Working Directory set to: ' + vissim_working_directory)\n",
    "    # Check Chache\n",
    "    try:\n",
    "        print ('Checking Presence of Pregenerated Cache.')\n",
    "        cache_flag\n",
    "    # Re-generate Cache\n",
    "    except:\n",
    "        print ('Cache NOT Present.')\n",
    "        print ('Generating Cache...')\n",
    "        Vissim = com.gencache.EnsureDispatch(\"Vissim.Vissim\") \n",
    "        print ('Cache generated.\\n')\n",
    "        cache_flag = True\n",
    "        print ('****************************')\n",
    "        print ('*   COM Server dispatched  *')\n",
    "        print ('****************************\\n')\n",
    "    # Dispatch without re-generating Cache.\n",
    "    else:\n",
    "        print ('Previous Cache Found. Dispatching...\\n')\n",
    "        Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "        print ('****************************')\n",
    "        print ('*   COM Server dispatched  *')\n",
    "        print ('****************************\\n')\n",
    "\n",
    "    ## Load the Network:\n",
    "    Filename = os.path.join(vissim_working_directory, model_name, (model_name+'.inpx'))\n",
    "    print ('Model File: ' + model_name+'.inpx')\n",
    "\n",
    "    # Additional Files\n",
    "    if flag_read_additionally == False:\n",
    "        print ('No additional files will be loaded')\n",
    "    print ('Loading...')\n",
    "    Vissim.LoadNet(Filename, flag_read_additionally)\n",
    "    print ('Load process successful')\n",
    "\n",
    "    ## Setting Simulation End\n",
    "    Vissim.Simulation.SetAttValue('SimPeriod', simulation_length)\n",
    "    print ('Simulation length set to '+str(simulation_length/10) + ' seconds.')\n",
    "    \n",
    "    ## If a fresh start is needed\n",
    "    if reset_flag == True:\n",
    "        if Start_Fresh == True:\n",
    "            # Delete all previous simulation runs first:\n",
    "            for simRun in Vissim.Net.SimulationRuns:\n",
    "                Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "            print ('Results from Previous Simulations: Deleted. Fresh Start Available.')\n",
    "\n",
    "    #Pre-fetch objects for stability\n",
    "    Simulation = Vissim.Simulation\n",
    "    print ('Fetched and containerized Simulation Object')\n",
    "    Network = Vissim.Net\n",
    "    print ('Fetched and containerized Network Object \\n')\n",
    "    print ('*******************************************************')\n",
    "    print ('*                                                     *')\n",
    "    print ('*                 SETUP COMPLETE                      *')\n",
    "    print ('*                                                     *')\n",
    "    print ('*******************************************************\\n')\n",
    "    return(Vissim,Simulation,Network, cache_flag)\n",
    "\n",
    "def COMServerReload(Vissim, reset_flag):\n",
    "    ## Connecting the COM Server => Open a new Vissim Window:\n",
    "    # Server should only be dispatched in first run. Otherwise reload model.\n",
    "    # Setting Working Directory\n",
    "    vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "    ## Load the Network:\n",
    "    Filename = os.path.join(vissim_working_directory, model_name, (model_name+'.inpx'))\n",
    "\n",
    "    print('Reoading...')\n",
    "    Vissim.LoadNet(Filename, flag_read_additionally)\n",
    "\n",
    "    ## Setting Simulation End\n",
    "    Vissim.Simulation.SetAttValue('SimPeriod', simulation_length)\n",
    "\n",
    "    ## If a fresh start is needed\n",
    "    if reset_flag == True:\n",
    "        if Start_Fresh == True:\n",
    "            # Delete all previous simulation runs first:\n",
    "            for simRun in Vissim.Net.SimulationRuns:\n",
    "                Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "    \n",
    "    #Pre-fetch objects for stability\n",
    "    Simulation = Vissim.Simulation\n",
    "    Network = Vissim.Net\n",
    "    return(Simulation,Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Checking Presence of Pregenerated Cache.\n",
      "Cache NOT Present.\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Model File: Single_Cross_Straight.inpx\n",
      "No additional files will be loaded\n",
      "Loading...\n",
      "Load process successful\n",
      "Simulation length set to 7200.0 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Episode: 1/15, Average reward: -38.52515141960498\n",
      "Prediction for [50,0,50,0] is: [[  9.31344   -20.172968    9.636351   -5.2798433   8.902988 ]]\n",
      "Reoading...\n",
      "Episode: 2/15, Average reward: -36.119620848179935\n",
      "Prediction for [50,0,50,0] is: [[  0.40898174 -11.170195     1.8239694   -6.9915595   -0.34960887]]\n",
      "Reoading...\n",
      "Episode: 3/15, Average reward: -35.42392280034489\n",
      "Prediction for [50,0,50,0] is: [[-3.3956194 -8.991299  -5.037511  -8.106263  -2.1363137]]\n",
      "Reoading...\n",
      "Episode: 4/15, Average reward: -29.389708880386507\n",
      "Prediction for [50,0,50,0] is: [[-5.389573 -6.902258 -9.230926 -5.857324 -8.400287]]\n",
      "Reoading...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize simulation\n",
    "    if 'Vissim' not in globals() or Vissim == None:\n",
    "        Vissim, Simulation, Network, cache_flag = COMServerDispatch(reset_flag = True)\n",
    "    else:\n",
    "        Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "        Simulation, Network = COMServerReload(Vissim, reset_flag = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser()\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    Agents = [DQNAgent(state_size, action_size, ID) for ID in npa.signal_controllers_ids] \n",
    "    \n",
    "    # Load previous trained data\n",
    "    if load_trained:\n",
    "        print('Loading Pre-Trained Data')\n",
    "        for index, agent in enumerate(Agents):\n",
    "            Filename = os.path.join(vissim_working_directory, model_name, 'Agent'+str(index)+'_'+model_name+'.h5')\n",
    "            agent.model = load_model(Filename)\n",
    "    \n",
    "    # Iterations of the simulation\n",
    "    for e in range(episodes):\n",
    "        done = False\n",
    "        # If not the first episode, reset state at the start\n",
    "        if e != 0:\n",
    "            Simulation, Network = COMServerReload(Vissim, reset_flag = False)\n",
    "            npa = NetworkParser() \n",
    "            for index, agent in enumerate(Agents):\n",
    "                agent.update_IDS(npa.signal_controllers_ids[index])\n",
    "                agent.episode_reward = []\n",
    "        \n",
    "        # Change demand for every episode\n",
    "        if Random_Demand:\n",
    "            for vehicle_input in range(1,5):\n",
    "                Vissim.Net.VehicleInputs.ItemByKey(vehicle_input).SetAttValue('Volume(1)', demands[np.random.randint(0,6)])\n",
    "        \n",
    "        # Use max speed for Simulator\n",
    "        if Quickmode:\n",
    "            # Set speed parameters in Vissim\n",
    "            Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)\n",
    "            Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",1)\n",
    "            Vissim.SuspendUpdateGUI()\n",
    "\n",
    "        # Set cycle time to start of cycle\n",
    "        cycle_t = 0\n",
    "\n",
    "        # time_t represents each timestep of the simulation\n",
    "        for time_t in range(simulation_length+1):\n",
    "            # If the cycle for the current program is over\n",
    "            if cycle_t == 900:\n",
    "                for agent in Agents:\n",
    "                    agent.newstate = agent.get_state()\n",
    "                    agent.action   = agent.act(agent.newstate)\n",
    "                    agent.reward   = agent.get_reward()\n",
    "                    agent.memory   = agent.remember(agent.state, agent.action, agent.reward, agent.newstate)\n",
    "                    agent.state    = agent.newstate\n",
    "                cycle_t = 0\n",
    "            else:\n",
    "                cycle_t += 1\n",
    "\n",
    "            # Advance the game to the next frame based on the action.\n",
    "            Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "        # Stop the simulation    \n",
    "        Vissim.Simulation.Stop()\n",
    "\n",
    "        # Calculate episode average reward\n",
    "        average_reward = []\n",
    "        for agent in Agents:\n",
    "            average_agent_reward = np.average(agent.episode_reward)\n",
    "            average_reward.append(average_agent_reward)\n",
    "        average_reward = np.average(average_reward)\n",
    "\n",
    "        if len(Agents)>1:\n",
    "            # Print the score and break out of the loop\n",
    "            print(\"Episode: {}/{}, Average reward: {}\".format(e+1, episodes, average_reward))\n",
    "            print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape(state, [1,4]))))\n",
    "            for agent in enumerate(Agents):\n",
    "                print(\"Agent {}, Average agent reward: {}\".format(agent, average_agent_reward[agent]))\n",
    "        else:\n",
    "            print(\"Episode: {}/{}, Average reward: {}\".format(e+1, episodes, average_reward))\n",
    "            print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))))\n",
    "\n",
    "        done = True\n",
    "        # Train agent with experience of episode (indicated batch size)\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "        if e%200 == 0:\n",
    "            for index, agent in enumerate(Agents):\n",
    "                Filename = os.path.join(vissim_working_directory, model_name, 'PartialSave_'+str(e)+'_Agent'+str(index)+'_'+model_name+'.h5')\n",
    "                agent.model.save(Filename)\n",
    "\n",
    "\n",
    "    #Saving agents memory, weights and optimizer\n",
    "    for index,agent in enumerate(Agents):    \n",
    "        Filename = os.path.join(vissim_working_directory, model_name, 'Agent'+str(index)+'_'+model_name+'.h5')\n",
    "        print('Saving architecture, weights and optimizer state for agent{}'.format(index))\n",
    "        agent.model.save(Filename)\n",
    "    print('Model Trained and Saved. Succesfully Terminated.')\n",
    "    \n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6310063298620945"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agents[0].epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =[100,0,100,0]\n",
    "b = np.reshape(a,[1,4])\n",
    "a1=Agents[0].model.predict(b)\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=Agents[0].model.predict(np.reshape([0,0,0,0], [1,4]))\n",
    "a2=Agents[0].model.predict(np.reshape([0,30,0,30], [1,4]))\n",
    "a1-a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([0,20,0,20])\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = random.sample(Agents[0].memory, 3)\n",
    "for state, action, reward, next_state in minibatch:\n",
    "    st = state\n",
    "    st = np.reshape(st, [1,4])\n",
    "\n",
    "print(type(st))\n",
    "st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving agents memory\n",
    "    for index,agent in enumerate(Agents):    \n",
    "        Filename = os.path.join(vissim_working_directory, model_name, 'Agent'+str(index)+'_'+model_name+'.h5')\n",
    "        print('Saving architecture, weights and optimizer state for agent{}'.format(index))\n",
    "        agent.model.save(Filename)\n",
    "    print('Model Trained and Saved. Succesfully Terminated.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
