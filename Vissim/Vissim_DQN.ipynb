{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import math\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from Simulator_Functions import Select_Vissim_Mode\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PER\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 400\n",
    "partial_save_at =  100\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "Surtrac = False\n",
    "PER_activated = True\n",
    "batch_size = 64\n",
    "memory_size = 1024\n",
    "alpha   = 0.000065\n",
    "gamma   = 0.95\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = batch_size*seconds_per_green*2 +1\n",
    "if PER_activated:\n",
    "    memory_population_length = int(memory_size*seconds_per_green*1.6) +1\n",
    "\n",
    "# Vissim autosave the result of the simulation    \n",
    "delete_results = True\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = 4\n",
    "action_size = 8\n",
    "\n",
    "\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP)\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":600, 'm':300, 'l':150}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep_400_A_DDQN_State_Queues_Act_phases_Rew_Queues\n"
     ]
    }
   ],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"populate\" = population of memory, generation of initial memory file\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "if mode == 'demo' :\n",
    "    simulation_length = 3600\n",
    "    demand_list = [[demand['l'], demand['l']]]\n",
    "    demand_change_timesteps = simulation_length\n",
    "    \n",
    "    \n",
    "if mode == 'test' : \n",
    "    simulation_length = 3600\n",
    "    demand_change_timesteps = 450\n",
    "    demand = {\"h\":800, 'm':400, 'l':200}\n",
    "    demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "                  [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "                  [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "                  [demand['l'], demand['h']], [demand['l'], demand['m']]]\n",
    "    delete_results = False\n",
    "\n",
    "model_name  = 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'  #'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "agent_type = 'DDQN'        # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'   \n",
    "# 'Queues'          Sum of the queues for all lanes in intersection\n",
    "# 'QueuesDiff'      Difference in queue lengths in last timestep\n",
    "# 'QueuesDiffSC'    10000* QueuesDiff - Queues^2\n",
    "# 'TotalDelayDiff'\n",
    "\n",
    "state_type  = 'Queues'    # 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig'\n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Session ID\n",
    "#Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "\n",
    "# Adding the state type to the Session_ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_State_\"+state_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "print(Session_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_phases = [[1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                  [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                  [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                  [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                  [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                  [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                  [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                  [0,0,0,0,1,1,0,0,0,0,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEyCAYAAADjpUkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8VXW9//HXmwPHiUkFhwTFMSVz4khZDlSCiAN4nTBRM9Nr5XBtulqZWvaz7JraoKk5W5pZEl5xyvFqOYAiIk6EGCgqTuCUAn5+f3zXie3hDOvg2Wft4f18PNZj77X2Wmt/1lnoZ3+/6zsoIjAzM7Pq16PoAMzMzKxrOKmbmZnVCCd1MzOzGuGkbmZmViOc1M3MzGqEk7qZmVmNcFI3qxCSLpN0ejd+302SDuuu72uPpLskfaWLznWqpKu6el+zauCkbtZJkuZIelfSWyXLr4qOqz2tJa+I2D0iLi8qJjPrej2LDsCsSu0VEX8tOggAST0jYknRcZhZ8VxSN+tCks6XdF3J+k8l3a5khKR5kr4r6ZWsxH9wO+c6UtIsSa9JmiTpYyWfhaSvS3oGeCbbdq6kuZIWSZoqaads+2jgu8CBWa3Co9n2f1d5S+oh6fuSnpP0sqQrJPXLPhuSfd9hkv6Zxf69duIeI2mmpDclPS/pWyWfjZU0LYvxH1lszTaQdF923K2SBpQc92lJf5P0hqRHJY0o+WxDSXdnx90GlB43QtK8FvHNkbRrG7G3+T1m1cBJ3axrfRPYStKXsqR6BHBYLBuPeR1S0lkPOAy4UNLHW55E0ueBM4ADgHWB54BrWuw2DvgUMDRbfwjYBlgD+D3wR0krR8TNwP8D/hARvSNi61bi/lK2fA7YCOgNtHyksCPwceALwA8kbdHG3+Bi4D8jog+wJXBHdk3DgSuAbwP9gZ2BOSXHfRE4HFgLaAS+lR23HnAjcHp2bd8C/iRpYHbc74GppL/rj0h/107L8T1mFc9J3WzFTMxKc83LkQAR8Q4wAfg5cBVwbETMa3HsyRHxXkTcTUoiB7Ry/oOBSyLi4Yh4DzgJ2EHSkJJ9zoiI1yLi3ey7r4qIVyNiSUScBaxESsJ5HAz8PCJmR8Rb2feNl1T6iO60iHg3Ih4FHgVa+3EAsBgYKqlvRLweEQ9n24/Irum2iPggIp6PiCdLjrs0Ip7Oruda0g8USH/PyRExOTvuNmAKMEbS+sD2LPub3gPckPOaW2rze1bwfGbdzkndbMWMi4j+JctFzR9ExIPAbECk5FTq9Yh4u2T9OeBjLO9j2WfN53wLeJVUwm82t/QASd+U9ISkhZLeAPpRUhXdgQ99X/a+J7B2ybYXS96/QyrNt2ZfUiJ8LqsW3yHbPhj4RzsxtHX+DYD9S39EkWoN1s3ibu1vuiLa+x6zquCkbtbFJH2dVEp+AfhOi49Xl7Rayfr62X4tvUBKMs3nXA1YE3i+ZJ8o+Xwn4L9Jpf7VI6I/sJD0w+JD+7bhQ9+XxbUEeKmD45YTEQ9FxFhSNfpElv2wmQts3NnzZcdd2eJH1GoR8RNgPq3/TZu9DazavCKpAWirOr297zGrCk7qZl1I0makZ7ITgEOA70japsVup0lqzBLxnsAfWznV74HDJW0jaSXSM/EHImJOG1/dh5SEFwA9Jf0A6Fvy+UvAEElt/Td/NXBC1uisN8uewXeqVX12XQdL6hcRi4FFwNLs44uza/pC1jBvPUmb5zjtVcBeknaT1CBp5awB3KCIeI5URd78N90R2Kvk2KeBlSXtIakX8H3SD65OfU9n/gZmRXJSN1sxN+jD/dSvz54/XwX8NCIejYhnSK3Or8wSM6Qq5tdJJePfAUe3eK4MQETcDpwM/IlUGt0YGN9OPLcAN5GS2HPAv/hw9XzzD4dXJT3M8i4BrgTuAZ7Njj+2oz9CGw4B5khaBBxN+oHT/FjicOBsUi3C3Xy4dqBVETEXGEv6Wy4gXde3Wfb/ry+SGgy+BpxCaozXfOxC4GvAb0m1HG8DLds45P0es4qnZY1yzaycsu5RV0WES35mVhb+BWpmZlYjnNTNzMxqhKvfzczMaoRL6mZmZjXCSd3MzKxGVN0sbQMGDIghQ4YUHYaZmVm3mDp16isRkWsOgqpL6kOGDGHKlClFh2FmZtYtJOUe+tjV72ZmZjXCSd3MzKxGOKmbmZnVCCd1MzOzGuGkbmZmViPKltQlXSLpZUkz2vhckn4haZak6ZK2K1csZmZm9aCcJfXLgNHtfL47sGm2HAWcX8ZYzMzMal7ZknpE3EOa37gtY4ErIrkf6C9p3XLFY2ZmVuuKfKa+HjC3ZH1etq3bzJwJP/whfPBBd36rmZlZeRSZ1NXKtlanjJN0lKQpkqYsWLCgywJ4+GE45RSYNq3LTmlmZlaYIpP6PGBwyfog4IXWdoyICyOiKSKaBg7MNfxtLrvuml5vvbXLTmlmZlaYIpP6JODQrBX8p4GFETG/OwNYZx3YemsndTMzqw1lm9BF0tXACGCApHnAKUAvgIj4DTAZGAPMAt4BDi9XLO0ZNQrOOQfefhtWW62ICMzMzLpG2ZJ6RBzUwecBfL1c35/XqFHws5/B3XfDmDFFR2NmZrbi6n5EuR13hJVXdhW8mZlVv7pP6iuvDLvs4qRuZmbVr+6TOsDIkfDEEzB3bsf7mpmZVSonddJzdYDbbis2DjMzs4/CSR3YcsvUvc1V8GZmVs2c1AEpldb/+lcPGWtmZtXLST0zahS8+io88kjRkZiZma0YJ/WMh4w1M7Nq56SeWXtt2GYbJ3UzM6teTuolRo2C++6Dt94qOhIzM7POc1IvMWoULF4Md9xRdCRmZmad56ReYqedoE8fuPHGoiMxMzPrPCf1Eo2NaXS5yZMhouhozMzMOsdJvYU99oB582D69KIjMTMz6xwn9Raap191FbyZmVUbJ/UW1lkHhg1zUjczs+rjpN6KPfaA++9PI8yZmZlVCyf1VowZk8aAv/nmoiMxMzPLz0m9FdtvDwMHugrezMyqi5N6K3r0gN13TyX1JUuKjsbMzCwfJ/U27LEHvP46PPBA0ZGYmZnl46TehlGjoKHBVfBmZlY9nNTb0L8/7Lijk7qZmVUPJ/V27LFHGllu7tyiIzEzM+uYk3o79torvd5wQ7FxmJmZ5eGk3o7NN4ePfxyuv77oSMzMzDrmpN6BcePgrrtSS3gzM7NK5qTegX32SX3VJ08uOhIzM7P2Oal3YPvtYd11XQVvZmaVz0m9Az16wNixaXS5d98tOhozM7O2OannsM8+8PbbcPvtRUdiZmbWNif1HEaMgH79XAVvZmaVzUk9h8bGNBDNpEmwdGnR0ZiZmbXOST2ncePglVfgvvuKjsTMzKx1Tuo5jR4NK60EEycWHYmZmVnrnNRz6tMHdt01JfWIoqMxMzNbXlmTuqTRkp6SNEvSia18vr6kOyU9Imm6pDHljOejGjcOnn02TfJiZmZWacqW1CU1AL8GdgeGAgdJGtpit+8D10bEtsB44LxyxdMV9t479Vu/7rqiIzEzM1teh0ld0n9IekbSQkmLJL0paVGOcw8HZkXE7Ih4H7gGGNtinwD6Zu/7AS90JvjuttZa8LnPwbXXugrezMwqT56S+pnA3hHRLyL6RkSfiOjb4VGwHlA6E/m8bFupU4EJkuYBk4Fjc5y3UAccAE8/7Sp4MzOrPHmS+ksR8cQKnFutbGtZvj0IuCwiBgFjgCslLReTpKMkTZE0ZcGCBSsQStfZZx9oaEildTMzs0qSJ6lPkfQHSQdlVfH/Iek/chw3Dxhcsj6I5avXjwCuBYiIvwMrAwNanigiLoyIpohoGjhwYI6vLp+BA+Hzn3cVvJmZVZ48Sb0v8A4wCtgrW/bMcdxDwKaSNpTUSGoIN6nFPv8EvgAgaQtSUi+2KJ7DAQfArFkwbVrRkZiZmS3Ts6MdIuLwFTlxRCyRdAxwC9AAXBIRj0v6ITAlIiYB3wQuknQCqWr+SxGVX/7dZx84+uhUWt9226KjMTMzS9RRDpU0CPgl8FlS4r0XOD4i5pU/vOU1NTXFlClTivjqDxk9Gp55JpXY1VrrATMzsy4gaWpENOXZN0/1+6WkavOPkVqv35Btq2sHHACzZ8PDDxcdiZmZWZInqQ+MiEsjYkm2XAYU21qtAowbBz17uhW8mZlVjjxJ/RVJEyQ1ZMsE4NVyB1bp1lgDRo50K3gzM6sceZL6l4EDgBeB+cB+2ba6d8ABMGcOPPRQ0ZGYmZnla/3+T2Dvboil6owdC42NcPXVMHx40dGYmVm9azOpS/pORJwp6ZcsPxIcEXFcWSOrAquvDnvskZL6z36WnrGbmZkVpb3q9+ahYacAU1tZDDjkEHjpJbj99qIjMTOzetdm2TIibsjevhMRfyz9TNL+ZY2qiowZA/37w5VXwm67FR2NmZnVszwN5U7Kua0urbRSajB3/fXw1ltFR2NmZvWszaQuaffsefp6kn5RslwGLOm2CKvAhAnwzjswcWLRkZiZWT1rr6T+Aul5+r/48LP0SYArmkt89rMwZAhcdVXRkZiZWT1r75n6o8Cjkn4fEYu7Maaq06MHHHwwnHEGzJ8P665bdERmZlaP8jxTHyLpOkkzJc1uXsoeWZU5+GD44AO45pqiIzEzs3qVd0KX80nP0T8HXAFcWc6gqtEWW8CwYa6CNzOz4uRJ6qtExO2kaVqfi4hTgc+XN6zqdMghada2mTOLjsTMzOpRnqT+L0k9gGckHSNpH2CtMsdVlcaPh4YGuOKKoiMxM7N6lCep/xewKnAcMAyYABxWzqCq1dprp8FoLr8clrjTn5mZdbN2k7qkBuCAiHgrIuZFxOERsW9E3N9N8VWdI46AF1+EyZOLjsTMzOpNu0k9IpYCwySpm+KpemPGpBL7xRcXHYmZmdWbPPOKPQL8RdIfgbebN0bEn8sWVRXr1QsOOwzOOiuV2NdZp+iIzMysXuR5pr4G8Cqpxfte2bJnOYOqdocfDkuXusGcmZl1L0UsN1V6RWtqaoopU6YUHUaHdtwRFiyAJ58EP7wwM7MVJWlqRDTl2bfDkrqkQZKul/SypJck/UnSoI8eZm074gh4+mm4776iIzEzs3qRd0S5ScDHgPWAG7Jt1o7994fevd1gzszMuk+epD4wIi6NiCXZchkwsMxxVb3evdNgNNdeC4sWFR2NmZnVgzxJ/RVJEyQ1ZMsEUsM568ARR6R51v/wh6IjMTOzepAnqX8ZOAB4EZgP7Jdtsw586lOw5ZZw/vlQZe0RzcysCnWY1CPinxGxd0QMjIi1ImJcRDzXHcFVOwm+9jV45BF48MGiozEzs1rX5uAzkn4JtFm+jIjjyhJRjZkwAb7zHTjvvFRyNzMzK5f2RpSr/M7gVaBPHzj00NQK/qyzYMCAoiMyM7Na1WZSj4jLS9cl9U2b482yR1VjvvrVVFK/9FL49reLjsbMzGpVnsFnmiQ9BkwHZkh6VNKw8odWO7bcEnbeOTWY++CDoqMxM7Nalaf1+yXA1yJiSERsAHwdDz7TaV/7Gjz7LNxyS9GRmJlZrcqT1N+MiP9rXomIewFXwXfSPvukKVnPO6/oSMzMrFblSeoPSrpA0ghJu0g6D7hL0naStit3gLWisRGOPBJuvBHmzCk6GjMzq0V5kvo2wGbAKcCpwBbAZ4CzgP9p70BJoyU9JWmWpBPb2OcASTMlPS7p952KvsocdVTqu/6b3xQdiZmZ1aKyTb0qqQF4GhgJzAMeAg6KiJkl+2wKXAt8PiJel7RWRLzc3nmrZerVtuy3H9xxB8ydC6utVnQ0ZmZW6bp66tUrJfUrWd9A0u05zj0cmBURsyPifeAaYGyLfY4Efh0RrwN0lNBrwQknwOuvw+WXd7yvmZlZZ+Spfr8XeEDSGElHArcB5+Q4bj1gbsn6vGxbqc2AzSTdJ+l+SaPzBF3NPvMZGD4czjnH3dvMzKxrtTeiHAARcYGkx4E7gVeAbSPixRznVmuna+X7NwVGAIOA/5O0ZUS88aETSUcBRwGsv/76Ob66ckmptH7QQanR3F57FR2RmZnVijzV74eQ+qofClwGTJa0dY5zzwMGl6wPAl5oZZ+/RMTiiHgWeIqU5D8kIi6MiKaIaBo4sPqnct93Xxg8GM4+u+hIzMysluSpft8X2DEiro6Ik4CjgTxPhB8CNpW0oaRGYDwwqcU+E4HPAUgaQKqOn503+GrVqxcceyzceSdMm1Z0NGZmVivyTL06rrQBW0Q8SGoE19FxS4BjgFuAJ4BrI+JxST+UtHe22y3Aq5Jmkqr3vx0Rr67AdVSdI49Mrd9dWjczs67SYZc2SZsB5wNrR8SWkrYC9o6I07sjwJaqvUtbqeOOS33Wn3sO1l236GjMzKwSdWmXNuAi4CRgMUBETCdVpdtHdPzxsGQJ/OpXRUdiZma1IE9SXzWrci+1pBzB1JuNN05jwp93HixaVHQ0ZmZW7fIk9VckbUzWHU3SfsD8skZVR046Cd54w0PHmpnZR5cnqX8duADYXNLzwH+RWsBbF2hqgpEj4ec/h3ffLToaMzOrZnlav8+OiF2BgcDmEbFjRDxX/tDqx3e/Cy+9BJddVnQkZmZWzfKU1AGIiLcjwvOol8Euu8AOO8CZZ8LixUVHY2Zm1Sp3UrfykVJpfc4cuOaaoqMxM7Nq5aReIfbYAz75SfjJTzzRi5mZrZgOJ3TJ5kXfAxhSun9E/Lx8YdUfKbWE/+IXYdIkGDeu6IjMzKza5Cmp3wB8CVgT6FOyWBfbf//Ud/3006GDgf7MzMyW02FJHRgUEVuVPRKjZ0/4/vfh8MNTaX3s2KIjMjOzapKnpH6TpFFlj8QAmDABNt0UTjnFz9bNzKxz8iT1+4HrJb0raZGkNyV5UNMy6dkzJfRHH4U//7noaMzMrJrkSepnATuQxoDvGxF9IqJvmeOqa+PHwxZbpOS+dGnR0ZiZWbXIk9SfAWZER3O0WpdpaIBTT4WZM+GPfyw6GjMzqxZ55lO/DNgIuAl4r3l7UV3aamk+9fZ88AFsvXUaYW7GjFQtb2Zm9aer51N/FrgdaMRd2rpNjx5w2mnw1FNw9dVFR2NmZtWgw5L6v3eU+gAREW+VN6T21UtJHVJf9WHDYOFCeOIJaGwsOiIzM+tuXVpSl7SlpEeAGcDjkqZK+sRHDdI6JsEZZ8Ds2Z5v3czMOpan+v1C4BsRsUFEbAB8E7iovGFZs1GjYNdd4Yc/TCV2MzOztuRJ6qtFxJ3NKxFxF7Ba2SKyD5HSlKyvvgo//WnR0ZiZWSXLk9RnSzpZ0pBs+T6p8Zx1k223hYMPhrPPhnnzio7GzMwqVZ6k/mVgIPBn4Prs/eHlDMqWd/rpqZvbKacUHYmZmVWqDpN6RLweEcdFxHYRsW1EHB8Rr3dHcLbMkCFw7LFw2WXw2GNFR2NmZpWozS5tkm4A2uzvFhF7lyuo9tRTl7aWXnstTc26ww4weXLR0ZiZWXfoqi5t/0Ma9/1Z4F1Si/eLgLdI3dusm62xRpqa9aabnNTNzGx5eYaJvScidu5oW3ep55I6wPvvw1ZbpefrM2Z4QBozs1rX1cPEDpS0UcnJNyQ1lrMCNDbCOefAM8/AuecWHY2ZmVWSPEn9BOAuSXdJugu4E/ivskZl7Ro9GvbaKw1IM39+0dGYmVmlyNP6/WZgU+D4bPl4RNxS7sCsfT//eaqKP/HEoiMxM7NKkaekDjAM+ASwNXCgpEPLF5Llsckm8I1vwBVXwN//XnQ0ZmZWCfJM6HIlqSX8jsD22ZLrgb2V1/e+Bx/7WOq/vnRp0dGYmVnReubYpwkYGnnnaLVu07s3/M//wBe/COedl5K7mZnVrzzV7zOAdcodiK2Y8ePTTG7f+57HhTczq3d5kvoAYKakWyRNal7KHZjlI8H558PixXDccUVHY2ZmRcpT/X5quYOwj2ajjdJELyedBH/5C4wdW3REZmZWhA5HlPtIJ5dGA+cCDcBvI+Inbey3H/BHYPuIaHe4uHofUa4tixfDdtvBG2/AzJnQp0/REZmZWVfo0hHlJH1a0kOS3pL0vqSlkhblOK4B+DWwOzAUOEjS0Fb26wMcBzyQJ2BrXa9ecOGF8Pzz8IMfFB2NmZkVIc8z9V8BBwHPAKsAX8m2dWQ4MCsiZkfE+8A1QGsVwz8CzgT+lStia9MOO8DRR8MvfuG+62Zm9SjX4DMRMQtoiIilEXEpMCLHYesBc0vW52Xb/k3StsDgiPjffOFaR37yExg0CL70JXj33aKjMTOz7pQnqb8jqRGYJulMSScAq+U4Tq1s+/cDfEk9gLOBb3Z4IukoSVMkTVmwYEGOr65fffvCxRfD00+naVrNzKx+5Enqh2T7HQO8DQwG9s1x3Lxs32aDgBdK1vsAW5Imi5kDfBqYJGm5xgARcWFENEVE08CBniCuI7vuCl/9Kpx9Ntx7b9HRmJlZd2m39XvW2O3yiJjQ6RNLPYGngS8AzwMPAV+MiMfb2P8u4Ftu/d413norzbve0ADTpsFqeepWzMys4nRZ6/eIWEqaT72xs0FExBJS6f4W4Ang2oh4XNIPJe3d2fNZ5/TuDZdeCrNmwXe/W3Q0ZmbWHfIMPjMHuC8bRe7t5o0R8fOODoyIycDkFtta7XAVESNyxGKdsMsuaZS5X/wC9twTRo4sOiIzMyunPM/UXwD+N9u3T8liVeCMM2DoUDj0UHAbQzOz2tZhST0iTuuOQKw8Vl0Vrr4ahg+Hww+HG25I48WbmVntydVP3arbVlvBz34GN94Iv/510dGYmVm5OKnXiWOOgTFj4FvfgunTi47GzMzKoc2kLumn2ev+3ReOlYuUWsP37w8HHQTvvFN0RGZm1tXaK6mPkdQLOKm7grHyWmstuPJKeOKJNDhNGSfoMzOzArSX1G8GXgG2krRI0pulr90Un3WxkSPTLG5XXAEXXVR0NGZm1pXaTOoR8e2I6AfcGBF9I6JP6Ws3xmhd7OSTYbfd4NhjwYPzmZnVjg4bykXEWElrS9ozWzz4epVraICrroJ11oH99oPXXis6IjMz6wodJvWsodyDwP7AAcCDkvYrd2BWXgMGwHXXwfz5MGECfPBB0RGZmdlHladL2/eB7SPisIg4FBgOnFzesKw7bL89nHsu3HRTes5uZmbVLc/Y7z0i4uWS9Vdx//aa8Z//CQ8/DD/+cRpO9otfLDoiMzNbUXmS+s2SbgGuztYPpMUkLVa9JPjVr+Cpp+CII2DTTVMJ3szMqk+ehnLfBi4AtgK2Bi6MiP8ud2DWfRob4U9/Sg3nxo6F558vOiIzM1sReUrqRMSfgT+XORYr0IABMGkSfOYzMG4c3H13mgzGzMyqh5+N27998pPwu9/B1Klw8MGwdGnREZmZWWc4qduH7L13ahE/cSIcd5yHkjUzqya5qt8lNQKbZatPRcTi8oVkRTv2WJg7N03XOngwnHhi0RGZmVkeHSZ1SSOAy4E5gIDBkg6LiHvKG5oV6Sc/gXnz4KSTYL314JBDio7IzMw6kqekfhYwKiKeApC0Gal727ByBmbF6tEjTdX64ovw5S+nGd52263oqMzMrD15nqn3ak7oABHxNNCrfCFZpVhpJbj+evjEJ2CffeAe182YmVW0PEl9iqSLJY3IlouAqeUOzCpDv35w662wwQaw557w0ENFR2RmZm3Jk9S/CjwOHAccD8wEji5nUFZZ1loLbrsN1lwTRo+GGTOKjsjMzFqjqLI+S01NTTHFk4AXYvZs2Gmn1H/9nntgs806PsbMzD4aSVMjoinPvm2W1CVdm70+Jml6y6WrgrXqsdFG8Ne/pmlaR4yAJ58sOiIzMyvVXuv347PXPbsjEKsOW2wBd90Fn/887LIL3HFHakhnZmbFa7OkHhHzs7dfi4jnShfga90TnlWioUNTYm9oSCX26a63MTOrCHkayo1sZdvuXR2IVZfNN0+Tvqy0Uiq1P/xw0RGZmVl7z9S/Kukx4OMtnqc/C7hsZmy6aUrsq62WSux33VV0RGZm9a29kvrvgb2ASdlr8zIsIiZ0Q2xWBTbeGO67DwYNSt3dJk4sOiIzs/rV3jP1hRExJyIOyp6jvwsE0FvS+t0WoVW8QYPg//4PttkG9t0XLr646IjMzOpTh8/UJe0l6RngWeBu0sQuN5U5Lqsya64Jt98OI0fCV74Cp5/uaVvNzLpbnoZypwOfBp6OiA2BLwD3lTUqq0qrrQaTJsGECXDyyXD44fD++0VHZWZWP/Ik9cUR8SrQQ1KPiLgT2KbMcVmVamyEK66AU0+Fyy+HUaPgtdeKjsrMrD7kSepvSOoN3AP8TtK5wJLyhmXVTIJTToGrroK//x122AGeeaboqMzMal+epD4WeAc4AbgZ+AepFXyHJI2W9JSkWZJObOXzb0iamXWVu13SBp0J3irbwQen5+yvvgrDh8PkyUVHZGZW2zpM6hHxdkR8EBFLIuJy4NfA6I6Ok9SQ7bs7MBQ4SNLQFrs9AjRFxFbAdcCZnb0Aq2w77pimax0yJE3d+qMfpbHjzcys67U3+ExfSSdJ+pWkUUqOAWYDB+Q493BgVkTMjoj3gWtIpf5/i4g7I+KdbPV+YNCKXYZVsg03TH3ZDz4YfvAD2GcfWLiw6KjMzGpPeyX1K4GPA48BXwFuBfYHxkbE2HaOa7YeMLdkfV62rS1H4K5yNWvVVVMDul/8IlXDb789PP540VGZmdWW9pL6RhHxpYi4ADgIaAL2jIhpOc+tVra12nNZ0oTs/D9r4/OjJE2RNGXBggU5v94qjQTHHptmdlu0KCX2iy5yf3Yzs67SXlJf3PwmIpYCz0bEm5049zxgcMn6IOCFljtJ2hX4HrB3RLzX2oki4sKIaIqIpoEDB3YiBKtEO+0E06al5+1HHQX77w+vv150VGZm1a+9pL61pEXZ8iawVfN7SYtynPshYFNJG0pqBMaTxpH/N0nbAheQEvrLK3oRVn3WWQduvhnOPBP+8hfYemu4996iozIzq27tjf3eEBF9s6VPRPQsed+3oxNHxBLgGOAW4Ang2oh4XNIPJe2d7fYzoDfwR0nTJE1q43RWg3r0gG9/G/72tzRozS67pP7tHoXOzGzFKKrsgWZTU1NMmTKl6DCsi70yXNbzAAAPmUlEQVT5JhxzTGpMt/XWcNllaYIYM7N6J2lqRDTl2TfP4DNmZdenTxpWduJEeOml1Iju1FNdajcz6wwndasoY8emrm7jx8Npp6WR6B55pOiozMyqg5O6VZw11oArr0wN6JpL7SeckLrBmZlZ25zUrWLtvTfMnAlHHgnnngtbbAHXXut+7WZmbXFSt4q2+upw/vlw//2pG9yBB8Juu8HTTxcdmZlZ5XFSt6owfDg8+CD88pfwwAOw5ZbwjW94rnYzs1JO6lY1GhpSt7ennoJDD4VzzoFNNkmvbiVvZuakblVonXXgt79NQ80OG5Ya0X3iE/DnP/t5u5nVNyd1q1pbbQW33go33gi9esG++6Zq+ptucnI3s/rkpG5VTYIxY2D6dLjkEliwIK3vuGOaDc7MrJ44qVtN6NkTDj88tYo//3x47jn4whfgc5+Dv/7VJXczqw9O6lZTGhvh6KNh1qzUgO7JJ2HkyFQt/6c/wQcfFB2hmVn5OKlbTVp5ZTj+eHj2WbjggjRf+377wdChqZr+X/8qOkIzs67npG41beWV4aijUje4a66BVVaBI46A9deHk0+G558vOkIzs67jpG51oaEhjUb38MNw222www7w4x/DkCFp8pi//c3P3c2s+jmpW12RYNdd02Qxs2bBccfBzTfDZz8LTU2pqn7hwqKjNDNbMU7qVrc22gjOOgvmzUst5t9/PzWyW3ddOOwwuPtul97NrLo4qVvd6907JfPp09O48oceChMnwogRsNlmcMYZMHdu0VGamXXMSd0sI6Wub7/5DcyfD5dfDh/7GHz3u6lh3U47wXnnpQFuzMwqkZO6WStWXTWV2O++Oz17/9GP0oxwX/96qp4fPTol/TfeKDpSM7NlFFX20LCpqSmmTJlSdBhWhyLgscfg6qtT97g5c9JIdiNGwLhxsPfeMHhw0VGaWa2RNDUimnLt66Ru1nkR6fn7xIlpeeqptH3YsJTgx45Nc75LxcZpZtXPSd2smz35ZOom95e/wP33p6S//vqw224walQah3711YuO0syqkZO6WYFefBFuuCH1f7/99tTvvUeP1Ahv1KiU6IcPT1X3ZmYdcVI3qxBLlsCDD8Itt6S53x98ME0q07t3GvBml11g551h++3TZDRmZi05qZtVqNdfT6X3O++Ee+6BGTPS9pVXTkPX7rxz6jq3/fbQt2+xsZpZZXBSN6sSr7wC996bus7dcw9Mm5ZK8hJssQV86lOpqv5Tn4JPftJV9mb1yEndrEotXJha1Zcur7ySPltlldS6fvhw2GabtGy+OfTqVWzMZlZenUnq/t1vVkH69UuN6UaNSusRqT98aZI/77xl88E3NsInPpES/NZbL3vt37+wSzCzArmkblZlliyBZ55JVfXNyyOPfHj42nXXTdX3Q4em1+Zl7bXdd96s2rj63azORKSudI8+mpYnnoCZM9PrW28t22/11Zcl+E02gY03Xrb061dc/GbWNle/m9UZKZXOm8elbxYBzz+fkntpor/hBnj55Q+fY801P5zkm5cNNkgT27iRnlnl83+mZjVMgkGD0jJy5Ic/W7QIZs+Gf/wjLc3v778f/vCH1Aq/WY8eKbEPHpxGymvtdcAAV+2bFc1J3axO9e27rBV9S4sXw3PPpST/z3+mZe7ctEydmsa7f++9Dx/T2AjrrJOWdddt+3XttT3Qjlm5OKmb2XJ69UrP3DfZpPXPI1LDvLlzlyX9+fPT8uKLqdT/t7+1Pff86qunkn1Hy5prptfVV0+1BWbWPid1M+s0CdZaKy3DhrW93+LF6dn9iy8uS/jz58NLL8Grr6Y++HPnLmu937L036xHj5TY+/dffunXr/1t/fqlYXkbGsrztzCrJGVN6pJGA+cCDcBvI+InLT5fCbgCGAa8ChwYEXPKGZOZdZ9evWC99dLSkQh4552U6JuX5sTfvCxcCG+8kZb585etv/12x+dfddWU3EuXPn2W39ba9lVXTYP/tLW4FsEqRdmSuqQG4NfASGAe8JCkSRExs2S3I4DXI2ITSeOBnwIHlismM6tcEqy2Wlo22KBzxy5enBJ8adJvXhYuhDffTF37mpfm9TfegHnzPrzt/fc7H3tjY/tJv3RZeWVYaaV0TGNj6+87+ryt9z17ph9S/pFRv8pZUh8OzIqI2QCSrgHGAqVJfSxwavb+OuBXkhTV1nnezArVq9ey5/Af1fvvp5J/c5J/8014990VX958Mz2CKN22eHF61PDee6mGoqtJKcG3XHr16tz2jj7r0SMtDQ3tv3bHPtKy1xV5/1GPL33f0JAeFxWhnEl9PWBuyfo84FNt7RMRSyQtBNYEXiljXGZmbWou/XbX/5SXLk3J/f3309La+44+b36/ZEnry+LFnfvsX//q+JilS1O3x45e61H//mlGxiKUM6m31mO15W/SPPsg6SjgKID111//o0dmZlYhGhrSM/tVVy06kvKI6Djx5/lx0NE+EWn54IOuef9Rji+yy2Y5k/o8YHDJ+iDghTb2mSepJ9APeK3liSLiQuBCSMPEliVaMzPrcs2PAqx7lLM5xUPAppI2lNQIjAcmtdhnEnBY9n4/4A4/TzczM1sxZfv9lD0jPwa4hdSl7ZKIeFzSD4EpETEJuBi4UtIsUgl9fLniMTMzq3VlrRSJiMnA5BbbflDy/l/A/uWMwczMrF64N6OZmVmNcFI3MzOrEU7qZmZmNcJJ3czMrEY4qZuZmdUIJ3UzM7MaoWob60XSAuC5LjrdAGpnnHlfS2XytVSmWrmWWrkO8LW0Z4OIGJhnx6pL6l1J0pSIaCo6jq7ga6lMvpbKVCvXUivXAb6WruLqdzMzsxrhpG5mZlYj6j2pX1h0AF3I11KZfC2VqVaupVauA3wtXaKun6mbmZnVknovqZuZmdWMuk3qkkZLekrSLEknFh1PZ0maI+kxSdMkTcm2rSHpNknPZK+rFx1nayRdIullSTNKtrUau5JfZPdpuqTtiot8eW1cy6mSns/uzTRJY0o+Oym7lqck7VZM1MuTNFjSnZKekPS4pOOz7VV3X9q5lmq8LytLelDSo9m1nJZt31DSA9l9+YOkxmz7Stn6rOzzIUXGX6qda7lM0rMl92WbbHvF/hsDkNQg6RFJ/5utV8Y9iYi6W0jzu/8D2AhoBB4FhhYdVyevYQ4woMW2M4ETs/cnAj8tOs42Yt8Z2A6Y0VHswBjgJkDAp4EHio4/x7WcCnyrlX2HZv/WVgI2zP4NNhR9DVls6wLbZe/7AE9n8VbdfWnnWqrxvgjonb3vBTyQ/b2vBcZn238DfDV7/zXgN9n78cAfir6GHNdyGbBfK/tX7L+xLL5vAL8H/jdbr4h7Uq8l9eHArIiYHRHvA9cAYwuOqSuMBS7P3l8OjCswljZFxD3Aay02txX7WOCKSO4H+ktat3si7Vgb19KWscA1EfFeRDwLzCL9WyxcRMyPiIez928CTwDrUYX3pZ1raUsl35eIiLey1V7ZEsDngeuy7S3vS/P9ug74giR1U7jtauda2lKx/8YkDQL2AH6brYsKuSf1mtTXA+aWrM+j/f/oK1EAt0qaKumobNvaETEf0v/YgLUKi67z2oq9Wu/VMVmV4SUlj0Gq4lqy6sFtSSWpqr4vLa4FqvC+ZNW804CXgdtINQlvRMSSbJfSeP99LdnnC4E1uzfitrW8lohovi8/zu7L2ZJWyrZV8n05B/gO8EG2viYVck/qNam39iup2roBfDYitgN2B74uaeeiAyqTarxX5wMbA9sA84Gzsu0Vfy2SegN/Av4rIha1t2sr2yr9WqryvkTE0ojYBhhEqkHYorXdstequhZJWwInAZsD2wNrAP+d7V6R1yJpT+DliJhaurmVXQu5J/Wa1OcBg0vWBwEvFBTLComIF7LXl4HrSf+xv9RcPZW9vlxchJ3WVuxVd68i4qXsf14fABexrCq3oq9FUi9SEvxdRPw521yV96W1a6nW+9IsIt4A7iI9X+4vqWf2UWm8/76W7PN+5H881G1KrmV09rgkIuI94FIq/758Fthb0hzSo9vPk0ruFXFP6jWpPwRsmrVWbCQ1XphUcEy5SVpNUp/m98AoYAbpGg7LdjsM+EsxEa6QtmKfBByatYT9NLCwuTq4UrV47rcP6d5AupbxWWvYDYFNgQe7O77WZM/4LgaeiIifl3xUdfelrWup0vsyUFL/7P0qwK6kNgJ3Avtlu7W8L833az/gjshaaBWtjWt5suRHo0jPoUvvS8X9G4uIkyJiUEQMIeWOOyLiYCrlnpSzFV4lL6SWlU+Tnk99r+h4Ohn7RqTWuo8CjzfHT3pOczvwTPa6RtGxthH/1aTqz8WkX7FHtBU7qerq19l9egxoKjr+HNdyZRbrdNJ/0OuW7P+97FqeAnYvOv6SuHYkVQlOB6Zly5hqvC/tXEs13petgEeymGcAP8i2b0T64TEL+COwUrZ95Wx9Vvb5RkVfQ45ruSO7LzOAq1jWQr5i/42VXNMIlrV+r4h74hHlzMzMakS9Vr+bmZnVHCd1MzOzGuGkbmZmViOc1M3MzGqEk7qZmVmNcFI3q0GSlpbMejVNHcxEKOloSYd2wffOkTTgo57HzFaMu7SZ1SBJb0VE7wK+dw6pP/Er3f3dZuaSulldyUrSP1Wa1/pBSZtk20+V9K3s/XGSZmYTbFyTbVtD0sRs2/2Stsq2rynp1mxe6QsoGeda0oTsO6ZJuiCbzKNBaf7sGZIek3RCAX8Gs5rlpG5Wm1ZpUf1+YMlniyJiOPAr0pjVLZ0IbBsRWwFHZ9tOAx7Jtn0XuCLbfgpwb0RsSxqlbX0ASVsAB5ImHtoGWAocTJpMZb2I2DIiPkka69vMukjPjncxsyr0bpZMW3N1yevZrXw+HfidpInAxGzbjsC+ABFxR1ZC7wfsDPxHtv1GSa9n+38BGAY8lE0dvQppMpgbgI0k/RK4Ebh1xS/RzFpySd2s/kQb75vtQRpzexgwNZtZqr3pI1s7h4DLI2KbbPl4RJwaEa8DW5Nm6Po68NsVvAYza4WTuln9ObDk9e+lH0jqAQyOiDuB7wD9gd7APaTqcySNAF6JNEd56fbdgdWzU90O7CdpreyzNSRtkLWM7xERfwJOBrYr10Wa1SNXv5vVplUkTStZvzkimru1rSTpAdKP+oNaHNcAXJVVrQs4OyLekHQqcKmk6cA7LJtK8jTgakkPA3cD/wSIiJmSvg/cmv1QWEwqmb+bnae5QHFS112ymblLm1kdcZczs9rm6nczM7Ma4ZK6mZlZjXBJ3czMrEY4qZuZmdUIJ3UzM7Ma4aRuZmZWI5zUzczMaoSTupmZWY34/zIcVytUGExAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting exploration schedule\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = np.array(range(1,episodes+1))\n",
    "y_series = epsilon_sequence[0:episodes]\n",
    "plt.plot(x_series, y_series, '-b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Ratio of random exploration')\n",
    "plt.title('Exploration schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 9831 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "WARNING:tensorflow:From C:\\Users\\Rzhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Rzhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Deploying instance of Double Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DDQN.\n",
      "Previous Experience Found: Loading into agent\n",
      "Memory pre-populated. Starting Training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06177887485e41769cbe7cb9235ae68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=400)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/400, Epsilon:1.0, Average reward: -50.27\n",
      "Prediction for [50,0,50,0] is: [[  5.391171    5.6116276  -8.684954   -4.685197    6.903873   -0.5748403\n",
      "    3.6277738 -12.080416 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 7.093569   4.2497635 -6.2271123 -5.710099   4.848765  -1.840569\n",
      "   0.7126622 -9.194014 ]]OK\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Ep_400_A_DDQN_State_Queues_Act_phases_Rew_Queues\\Single_Cross_Triple_BestAgent0_Memory.p\n",
      "WARNING:tensorflow:From C:\\Users\\Rzhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      " - 0s - loss: 380.7216\n",
      " - 0s - loss: 404.6912\n",
      " - 0s - loss: 403.5881\n",
      "Episode: 2/400, Epsilon:0.98, Average reward: -50.61\n",
      "Prediction for [50,0,50,0] is: [[  5.299347     5.7253146   -8.939668    -4.8765283    6.9152837\n",
      "   -0.81025094   3.5681603  -12.267413  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 7.00552     4.215682   -6.340553   -5.816359    4.8332534  -1.8857282\n",
      "   0.63617843 -9.276743  ]]OK\n",
      " - 0s - loss: 400.4482\n",
      " - 0s - loss: 437.0768\n",
      " - 0s - loss: 363.8368\n",
      "Episode: 3/400, Epsilon:0.97, Average reward: -47.31\n",
      "Prediction for [50,0,50,0] is: [[  5.1798735    5.7430763   -9.129328    -5.0178795    6.8824606\n",
      "   -0.97532886   3.4610803  -12.408101  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.931557   4.1573906 -6.4559193 -5.910936   4.7945776 -1.932791\n",
      "   0.5297402 -9.32702  ]]OK\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Ep_400_A_DDQN_State_Queues_Act_phases_Rew_Queues\\Single_Cross_Triple_BestAgent0_Memory.p\n",
      " - 0s - loss: 390.9320\n",
      " - 0s - loss: 395.1084\n",
      " - 0s - loss: 361.3747\n",
      "Episode: 4/400, Epsilon:0.95, Average reward: -49.85\n",
      "Prediction for [50,0,50,0] is: [[  5.066679    5.7216673  -9.295182   -5.120541    6.826623   -1.0995812\n",
      "    3.324576  -12.5275135]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.852184    4.094579   -6.585103   -6.009319    4.7383757  -1.975015\n",
      "   0.38815546 -9.345938  ]]OK\n",
      " - 0s - loss: 424.0476\n",
      " - 0s - loss: 401.8379\n",
      " - 0s - loss: 464.0860\n",
      "Episode: 5/400, Epsilon:0.93, Average reward: -47.31\n",
      "Prediction for [50,0,50,0] is: [[  4.952929    5.700372   -9.456606   -5.213011    6.7387133  -1.236432\n",
      "    3.1941144 -12.626921 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.7642555   4.033269   -6.6958294  -6.087148    4.670952   -2.014337\n",
      "   0.25277543 -9.347318  ]]OK\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Ep_400_A_DDQN_State_Queues_Act_phases_Rew_Queues\\Single_Cross_Triple_BestAgent0_Memory.p\n",
      " - 0s - loss: 420.1515\n",
      " - 0s - loss: 328.1358\n",
      " - 0s - loss: 432.9984\n",
      "Episode: 6/400, Epsilon:0.92, Average reward: -45.78\n",
      "Prediction for [50,0,50,0] is: [[  4.8243246   5.6290708  -9.588911   -5.2772713   6.623354   -1.3320202\n",
      "    3.0749123 -12.717314 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.675657   3.9701705 -6.7917614 -6.1412683  4.59227   -2.0483572\n",
      "   0.1359268 -9.360185 ]]OK\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Ep_400_A_DDQN_State_Queues_Act_phases_Rew_Queues\\Single_Cross_Triple_BestAgent0_Memory.p\n",
      " - 0s - loss: 306.1266\n",
      " - 0s - loss: 389.9162\n",
      " - 0s - loss: 391.0682\n",
      "Episode: 7/400, Epsilon:0.9, Average reward: -63.1\n",
      "Prediction for [50,0,50,0] is: [[  4.7115307   5.552468   -9.706443   -5.3028784   6.4681745  -1.4336759\n",
      "    2.9831564 -12.810123 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.5939445   3.9036741  -6.8801923  -6.18341     4.508485   -2.0808086\n",
      "   0.02500919 -9.373329  ]]OK\n",
      " - 0s - loss: 466.1469\n",
      " - 0s - loss: 457.3052\n",
      " - 0s - loss: 472.0131\n",
      "Episode: 8/400, Epsilon:0.89, Average reward: -53.06\n",
      "Prediction for [50,0,50,0] is: [[  4.569075    5.4703383  -9.839101   -5.3619547   6.3584647  -1.530652\n",
      "    2.885417  -12.931011 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.509168    3.8184962  -6.9658575  -6.231477    4.4447694  -2.1057494\n",
      "  -0.08872069 -9.387802  ]]OK\n",
      " - 0s - loss: 501.3385\n",
      " - 0s - loss: 569.9876\n",
      " - 0s - loss: 588.4521\n",
      "Episode: 9/400, Epsilon:0.87, Average reward: -62.6\n",
      "Prediction for [50,0,50,0] is: [[  4.4509625   5.384649   -9.982439   -5.4101377   6.213769   -1.6207424\n",
      "    2.8189251 -13.064793 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.4226747  3.719004  -7.0534    -6.270071   4.3495393 -2.1201608\n",
      "  -0.2021906 -9.380777 ]]OK\n",
      " - 0s - loss: 444.0911\n",
      " - 0s - loss: 509.1088\n",
      " - 0s - loss: 649.1139\n",
      "Episode: 10/400, Epsilon:0.86, Average reward: -53.61\n",
      "Prediction for [50,0,50,0] is: [[  4.3738046   5.2876015 -10.1045265  -5.415704    6.011022   -1.7198815\n",
      "    2.778773  -13.19016  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.3410873   3.6195476  -7.128365   -6.28901     4.241188   -2.1431468\n",
      "  -0.31778434 -9.36872   ]]OK\n",
      " - 0s - loss: 596.8918\n",
      " - 0s - loss: 580.3798\n",
      " - 0s - loss: 532.3343\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 11/400, Epsilon:0.84, Average reward: -59.82\n",
      "Prediction for [50,0,50,0] is: [[  4.2997274   5.140722  -10.184066   -5.381802    5.779695   -1.7895012\n",
      "    2.7361128 -13.290446 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.263976    3.5215797  -7.1928806  -6.290983    4.1309123  -2.1708412\n",
      "  -0.42909327 -9.36303   ]]OK\n",
      " - 0s - loss: 573.0627\n",
      " - 0s - loss: 549.4240\n",
      " - 0s - loss: 585.7609\n",
      "Episode: 12/400, Epsilon:0.83, Average reward: -61.58\n",
      "Prediction for [50,0,50,0] is: [[  4.2210383   5.0392838 -10.323137   -5.406351    5.598564   -1.885735\n",
      "    2.6929476 -13.447805 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.191587   3.437315  -7.2728143 -6.319705   4.0489073 -2.2007551\n",
      "  -0.5336121 -9.381126 ]]OK\n",
      " - 0s - loss: 578.9087\n",
      " - 0s - loss: 547.2712\n",
      " - 0s - loss: 694.6107\n",
      "Episode: 13/400, Epsilon:0.81, Average reward: -75.42\n",
      "Prediction for [50,0,50,0] is: [[  4.144038    4.9522777 -10.45436    -5.4360695   5.4238343  -1.9935628\n",
      "    2.6376543 -13.671204 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.125557    3.3660984  -7.3554993  -6.3558855   3.9679716  -2.2349758\n",
      "  -0.63893163 -9.412639  ]]OK\n",
      " - 0s - loss: 729.8102\n",
      " - 0s - loss: 784.9309\n",
      " - 0s - loss: 782.6223\n",
      "Episode: 14/400, Epsilon:0.8, Average reward: -62.43\n",
      "Prediction for [50,0,50,0] is: [[  4.09341     4.8455625 -10.561267   -5.431497    5.158874   -2.0796323\n",
      "    2.5617661 -13.816988 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 6.055981    3.289087   -7.4389315  -6.387613    3.863522   -2.2645218\n",
      "  -0.76257324 -9.412941  ]]OK\n",
      " - 0s - loss: 782.8759\n",
      " - 0s - loss: 745.5851\n",
      " - 0s - loss: 784.7460\n",
      "Episode: 15/400, Epsilon:0.78, Average reward: -64.4\n",
      "Prediction for [50,0,50,0] is: [[  4.0508046   4.72354   -10.638927   -5.391362    4.8357983  -2.1546743\n",
      "    2.465252  -13.906622 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 5.9848685  3.212979  -7.511564  -6.4063697  3.7337317 -2.2929554\n",
      "  -0.893145  -9.393311 ]]OK\n",
      " - 0s - loss: 628.8559\n",
      " - 0s - loss: 727.2300\n",
      " - 0s - loss: 746.1364\n",
      "Episode: 16/400, Epsilon:0.77, Average reward: -67.02\n",
      "Prediction for [50,0,50,0] is: [[  4.01944     4.5977116 -10.732078   -5.3379207   4.5689707  -2.2106316\n",
      "    2.4101005 -14.005941 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 5.8975215  3.1234365 -7.585544  -6.42551    3.6133819 -2.3076162\n",
      "  -1.0153097 -9.385642 ]]OK\n",
      " - 0s - loss: 639.6174\n",
      " - 0s - loss: 725.3331\n",
      " - 0s - loss: 768.8524\n",
      "Episode: 17/400, Epsilon:0.76, Average reward: -100.23\n",
      "Prediction for [50,0,50,0] is: [[  3.9749215   4.486966  -10.837414   -5.309051    4.3554635  -2.2648063\n",
      "    2.3628948 -14.1142845]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 5.8166356  3.0618382 -7.689882  -6.459828   3.5308225 -2.3394868\n",
      "  -1.1159695 -9.434031 ]]OK\n",
      " - 0s - loss: 1307.4695\n",
      " - 0s - loss: 1201.9230\n",
      " - 0s - loss: 1247.5919\n",
      "Episode: 18/400, Epsilon:0.75, Average reward: -103.46\n",
      "Prediction for [50,0,50,0] is: [[  3.8999286   4.347634  -10.9135     -5.2648463   4.1385283  -2.3135166\n",
      "    2.2972622 -14.195954 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 5.694825   2.9930959 -7.784434  -6.4897227  3.458723  -2.3823419\n",
      "  -1.2241117 -9.500603 ]]OK\n",
      " - 0s - loss: 1934.5214\n",
      " - 0s - loss: 1910.8600\n",
      " - 0s - loss: 1931.8271\n",
      "Episode: 19/400, Epsilon:0.73, Average reward: -63.9\n",
      "Prediction for [50,0,50,0] is: [[  3.8410046   4.142436  -10.946102   -5.1537304   3.8155024  -2.3672998\n",
      "    2.2592914 -14.22498  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 5.5555077  2.8904228 -7.8605695 -6.5145054  3.3697388 -2.4169326\n",
      "  -1.343381  -9.538447 ]]OK\n",
      " - 0s - loss: 1751.2345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 1709.3730\n",
      " - 0s - loss: 1785.1559\n",
      "Episode: 20/400, Epsilon:0.72, Average reward: -82.66\n",
      "Prediction for [50,0,50,0] is: [[  3.7913969   3.914636  -10.959812   -4.99843     3.4699051  -2.418485\n",
      "    2.193971  -14.248238 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[ 5.405736   2.7706604 -7.9235296 -6.519871   3.2545497 -2.446853\n",
      "  -1.4833997 -9.552233 ]]OK\n",
      " - 0s - loss: 1215.9208\n",
      " - 0s - loss: 1176.4658\n",
      " - 0s - loss: 1228.4132\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 21/400, Epsilon:0.71, Average reward: -75.45\n",
      "Prediction for [50,0,50,0] is: [[  3.7644517   3.6858292 -10.965355   -4.8156505   3.1096404  -2.4583688\n",
      "    2.1347027 -14.262146 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 5.2820497  2.6503587 -7.9853826 -6.497057   3.0999472 -2.4777274\n",
      "  -1.6323235 -9.535636 ]]OK\n",
      " - 0s - loss: 968.0533\n",
      " - 0s - loss: 950.1049\n",
      " - 0s - loss: 1012.2313\n",
      "Episode: 22/400, Epsilon:0.7, Average reward: -58.04\n",
      "Prediction for [50,0,50,0] is: [[  3.746824    3.4671526 -10.97172    -4.6436324   2.7748108  -2.4820218\n",
      "    2.043441  -14.256962 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 5.1834583  2.530611  -8.053158  -6.467831   2.94537   -2.5105803\n",
      "  -1.7963059 -9.513383 ]]OK\n",
      " - 0s - loss: 887.0449\n",
      " - 0s - loss: 868.4505\n",
      " - 0s - loss: 828.6639\n",
      "Episode: 23/400, Epsilon:0.68, Average reward: -79.01\n",
      "Prediction for [50,0,50,0] is: [[  3.7220237   3.26696   -10.981416   -4.4993057   2.4863124  -2.497902\n",
      "    1.9628094 -14.252414 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 5.0882864  2.4129434 -8.122835  -6.4459424  2.806701  -2.5381756\n",
      "  -1.9631332 -9.489387 ]]OK\n",
      " - 0s - loss: 1001.2292\n",
      " - 0s - loss: 1027.8517\n",
      " - 0s - loss: 937.9597\n",
      "Episode: 24/400, Epsilon:0.67, Average reward: -63.74\n",
      "Prediction for [50,0,50,0] is: [[  3.6888328   3.072494  -10.9784775  -4.3638315   2.2179666  -2.512828\n",
      "    1.8950144 -14.242961 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.991458   2.292042  -8.163415  -6.4153395  2.6853573 -2.5603242\n",
      "  -2.1166542 -9.454557 ]]OK\n",
      " - 0s - loss: 881.6654\n",
      " - 0s - loss: 862.0216\n",
      " - 0s - loss: 887.2474\n",
      "Episode: 25/400, Epsilon:0.66, Average reward: -67.11\n",
      "Prediction for [50,0,50,0] is: [[  3.6609905   2.896733  -10.963656   -4.2271814   1.9435325  -2.5396779\n",
      "    1.8536643 -14.225074 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.9105597  2.191581  -8.193201  -6.3798394  2.556872  -2.587435\n",
      "  -2.2600217 -9.412697 ]]OK\n",
      " - 0s - loss: 712.2143\n",
      " - 0s - loss: 735.5438\n",
      " - 0s - loss: 800.1406\n",
      "Episode: 26/400, Epsilon:0.65, Average reward: -66.0\n",
      "Prediction for [50,0,50,0] is: [[  3.6151364   2.7237775 -10.939248   -4.1081033   1.7053864  -2.5543406\n",
      "    1.8189534 -14.210668 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.8298755  2.097934  -8.215004  -6.353622   2.4436605 -2.611192\n",
      "  -2.3887432 -9.37272  ]]OK\n",
      " - 0s - loss: 836.6276\n",
      " - 0s - loss: 730.3287\n",
      " - 0s - loss: 720.8790\n",
      "Episode: 27/400, Epsilon:0.64, Average reward: -64.69\n",
      "Prediction for [50,0,50,0] is: [[  3.568167    2.5648105 -10.924512   -4.0065484   1.4954169  -2.5740068\n",
      "    1.8014019 -14.201517 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.7571564  2.0122333 -8.241036  -6.3319564  2.3294752 -2.6315815\n",
      "  -2.4985764 -9.333484 ]]OK\n",
      " - 0s - loss: 814.3305\n",
      " - 0s - loss: 811.7141\n",
      " - 0s - loss: 880.5246\n",
      "Episode: 28/400, Epsilon:0.63, Average reward: -59.26\n",
      "Prediction for [50,0,50,0] is: [[  3.5229244   2.410998  -10.9087925  -3.905467    1.2903142  -2.5895894\n",
      "    1.7651631 -14.179955 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.687419   1.9361651 -8.270251  -6.308055   2.2108223 -2.6501942\n",
      "  -2.6115541 -9.294297 ]]OK\n",
      " - 0s - loss: 708.1032\n",
      " - 0s - loss: 691.4990\n",
      " - 0s - loss: 692.7534\n",
      "Episode: 29/400, Epsilon:0.62, Average reward: -61.26\n",
      "Prediction for [50,0,50,0] is: [[  3.4818637   2.2712874 -10.903734   -3.809547    1.0945666  -2.6102922\n",
      "    1.722023  -14.158078 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.6240144  1.8653597 -8.300132  -6.285681   2.0986178 -2.6702669\n",
      "  -2.7183568 -9.259947 ]]OK\n",
      " - 0s - loss: 551.2355\n",
      " - 0s - loss: 567.6966\n",
      " - 0s - loss: 608.6774\n",
      "Episode: 30/400, Epsilon:0.61, Average reward: -85.48\n",
      "Prediction for [50,0,50,0] is: [[  3.4420118   2.1533694 -10.916085   -3.7221484   0.9171853  -2.6316266\n",
      "    1.6804262 -14.156048 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.5608945  1.8063915 -8.3376045 -6.26882    1.9941303 -2.688265\n",
      "  -2.8173447 -9.238782 ]]OK\n",
      " - 0s - loss: 967.9162\n",
      " - 0s - loss: 877.4542\n",
      " - 0s - loss: 851.4211\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 31/400, Epsilon:0.59, Average reward: -84.61\n",
      "Prediction for [50,0,50,0] is: [[  3.401258    2.03005   -10.92338    -3.6252494   0.7237424  -2.6413398\n",
      "    1.6263506 -14.132838 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.5026197  1.7485406 -8.373329  -6.2481575  1.8769468 -2.7053406\n",
      "  -2.9177206 -9.207775 ]]OK\n",
      " - 0s - loss: 1426.2893\n",
      " - 0s - loss: 1390.9408\n",
      " - 0s - loss: 1422.8478\n",
      "Episode: 32/400, Epsilon:0.58, Average reward: -111.16\n",
      "Prediction for [50,0,50,0] is: [[  3.334146     1.9146551  -10.961047    -3.5280812    0.52913153\n",
      "   -2.661745     1.588172   -14.135234  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.432646   1.6905016 -8.428253  -6.2384133  1.7542783 -2.7199302\n",
      "  -3.0256853 -9.185841 ]]OK\n",
      " - 0s - loss: 1531.0477\n",
      " - 0s - loss: 1374.8333\n",
      " - 0s - loss: 1496.2321\n",
      "Episode: 33/400, Epsilon:0.57, Average reward: -62.93\n",
      "Prediction for [50,0,50,0] is: [[  3.263515    1.7405    -10.991937   -3.4031568   0.2851442  -2.659901\n",
      "    1.55665   -14.138993 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.3528337  1.6242704 -8.487187  -6.2174816  1.6129442 -2.7327757\n",
      "  -3.1350954 -9.161049 ]]OK\n",
      " - 0s - loss: 1442.1523\n",
      " - 0s - loss: 1527.0724\n",
      " - 0s - loss: 1354.9247\n",
      "Episode: 34/400, Epsilon:0.56, Average reward: -85.68\n",
      "Prediction for [50,0,50,0] is: [[ 3.2048132e+00  1.5276086e+00 -1.1000411e+01 -3.2682464e+00\n",
      "   9.3612205e-03 -2.6413624e+00  1.5141796e+00 -1.4152741e+01]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.276369   1.567991  -8.558644  -6.1984854  1.455246  -2.7602773\n",
      "  -3.2582448 -9.15307  ]]OK\n",
      " - 0s - loss: 1312.5054\n",
      " - 0s - loss: 1350.2571\n",
      " - 0s - loss: 1517.5658\n",
      "Episode: 35/400, Epsilon:0.56, Average reward: -93.57\n",
      "Prediction for [50,0,50,0] is: [[  3.176307     1.3071147  -10.987677    -3.0984344   -0.31731358\n",
      "   -2.620138     1.4830431  -14.140796  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.21346    1.5121115 -8.624839  -6.1670856  1.277753  -2.7944\n",
      "  -3.3808918 -9.136276 ]]OK\n",
      " - 0s - loss: 1421.8192\n",
      " - 0s - loss: 1362.5851\n",
      " - 0s - loss: 1509.2111\n",
      "Episode: 36/400, Epsilon:0.55, Average reward: -109.15\n",
      "Prediction for [50,0,50,0] is: [[  3.1330657    1.0577033  -10.954312    -2.9031203   -0.65914536\n",
      "   -2.6095495    1.4587812  -14.174495  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.153517   1.4468992 -8.662873  -6.116478   1.116679  -2.8339927\n",
      "  -3.4786642 -9.134245 ]]OK\n",
      " - 0s - loss: 1618.9225\n",
      " - 0s - loss: 1523.4884\n",
      " - 0s - loss: 1838.7860\n",
      "Episode: 37/400, Epsilon:0.54, Average reward: -95.48\n",
      "Prediction for [50,0,50,0] is: [[  3.0630846    0.86347055 -10.981381    -2.689637    -1.0032337\n",
      "   -2.6705675    1.4463902  -14.232437  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.1022177  1.3891473 -8.695243  -6.0576615  0.9599465 -2.878038\n",
      "  -3.5515919 -9.149585 ]]OK\n",
      " - 0s - loss: 1603.7573\n",
      " - 0s - loss: 1614.0148\n",
      " - 0s - loss: 1670.7682\n",
      "Episode: 38/400, Epsilon:0.53, Average reward: -64.79\n",
      "Prediction for [50,0,50,0] is: [[  2.9645116    0.63807636 -11.003263    -2.4623265   -1.3549279\n",
      "   -2.7391872    1.4226385  -14.247712  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.0539026   1.315999   -8.710875   -5.9903865   0.80743027 -2.9205823\n",
      "  -3.618025   -9.152517  ]]OK\n",
      " - 0s - loss: 1344.1587\n",
      " - 0s - loss: 1626.8073\n",
      " - 0s - loss: 1535.5736\n",
      "Episode: 39/400, Epsilon:0.52, Average reward: -83.32\n",
      "Prediction for [50,0,50,0] is: [[  2.8643632    0.40013683 -11.013248    -2.2220728   -1.7164079\n",
      "   -2.798581     1.4135255  -14.26462   ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 4.008499   1.2410442 -8.721914  -5.912376   0.6410524 -2.9602787\n",
      "  -3.6728475 -9.156442 ]]OK\n",
      " - 0s - loss: 959.7451\n",
      " - 0s - loss: 1105.9663\n",
      " - 0s - loss: 1124.6799\n",
      "Episode: 40/400, Epsilon:0.51, Average reward: -64.01\n",
      "Prediction for [50,0,50,0] is: [[  2.7671719    0.19347869 -11.070066    -2.0653586   -2.0591056\n",
      "   -2.8783166    1.3619214  -14.345695  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 3.932879    1.1637853  -8.734019   -5.852774    0.49480313 -2.9864051\n",
      "  -3.735401   -9.158639  ]]OK\n",
      " - 0s - loss: 852.0728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 907.7599\n",
      " - 0s - loss: 814.9424\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 41/400, Epsilon:0.5, Average reward: -62.47\n",
      "Prediction for [50,0,50,0] is: [[ 2.6691990e+00  4.4717416e-03 -1.1120044e+01 -1.9415509e+00\n",
      "  -2.3438284e+00 -2.9409168e+00  1.3105365e+00 -1.4427327e+01]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 3.8333628   1.0861547  -8.761037   -5.8219953   0.36859992 -3.0059114\n",
      "  -3.80234    -9.175884  ]]OK\n",
      " - 0s - loss: 902.0003\n",
      " - 0s - loss: 788.6055\n",
      " - 0s - loss: 886.0867\n",
      "Episode: 42/400, Epsilon:0.49, Average reward: -107.7\n",
      "Prediction for [50,0,50,0] is: [[  2.572995    -0.16762546 -11.168213    -1.8380249   -2.5880053\n",
      "   -2.995719     1.2619479  -14.498312  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 3.7049959   1.0029854  -8.792707   -5.7879987   0.24677727 -3.0242884\n",
      "  -3.8764296  -9.188221  ]]OK\n",
      " - 0s - loss: 1242.2924\n",
      " - 0s - loss: 1421.1606\n",
      " - 0s - loss: 1176.6139\n",
      "Episode: 43/400, Epsilon:0.48, Average reward: -100.07\n",
      "Prediction for [50,0,50,0] is: [[  2.4740264   -0.33232543 -11.213578    -1.74356     -2.8225574\n",
      "   -3.04964      1.1977305  -14.5599985 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 3.5570095  0.9278852 -8.833485  -5.7481     0.1041497 -3.0450253\n",
      "  -3.9604301 -9.204503 ]]OK\n",
      " - 0s - loss: 1782.9983\n",
      " - 0s - loss: 1756.6079\n",
      " - 0s - loss: 1742.7386\n",
      "Episode: 44/400, Epsilon:0.47, Average reward: -130.73\n",
      "Prediction for [50,0,50,0] is: [[  2.355975   -0.5240368 -11.253536   -1.6448823  -3.0615683  -3.0941718\n",
      "    1.0963109 -14.620279 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 3.391051    0.83108443 -8.864546   -5.695194   -0.04040762 -3.0662591\n",
      "  -4.0675507  -9.217574  ]]OK\n",
      " - 0s - loss: 2527.8303\n",
      " - 0s - loss: 2524.0645\n",
      " - 0s - loss: 2455.0156\n",
      "Episode: 45/400, Epsilon:0.47, Average reward: -108.37\n",
      "Prediction for [50,0,50,0] is: [[  2.2398303  -0.7439402 -11.311027   -1.5230931  -3.3369715  -3.1763597\n",
      "    1.0256047 -14.725265 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 3.2356865   0.73328215 -8.903202   -5.6242642  -0.19616985 -3.1108363\n",
      "  -4.173831   -9.270904  ]]OK\n",
      " - 0s - loss: 2835.1023\n",
      " - 0s - loss: 2842.5752\n",
      " - 0s - loss: 3003.4177\n",
      "Episode: 46/400, Epsilon:0.46, Average reward: -145.58\n",
      "Prediction for [50,0,50,0] is: [[  2.0969381   -1.0008543  -11.372625    -1.3930713   -3.634207\n",
      "   -3.3150525    0.96349204 -14.867263  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 3.0909019   0.63186467 -8.938586   -5.5403247  -0.3366522  -3.176226\n",
      "  -4.2792387  -9.351024  ]]OK\n",
      " - 0s - loss: 3257.6194\n",
      " - 0s - loss: 3527.2605\n",
      " - 0s - loss: 3342.2039\n",
      "Episode: 47/400, Epsilon:0.45, Average reward: -100.33\n",
      "Prediction for [50,0,50,0] is: [[  1.9389305  -1.2286267 -11.456696   -1.2730796  -3.913585   -3.547983\n",
      "    0.8999165 -15.065357 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 2.9712238  0.5345497 -8.972531  -5.459757  -0.4556405 -3.2819853\n",
      "  -4.404304  -9.42847  ]]OK\n",
      " - 0s - loss: 2747.7554\n",
      " - 0s - loss: 2709.7581\n",
      " - 0s - loss: 2860.5791\n",
      "Episode: 48/400, Epsilon:0.44, Average reward: -111.82\n",
      "Prediction for [50,0,50,0] is: [[  1.7647786  -1.4265273 -11.587597   -1.2370076  -4.1195393  -3.8130193\n",
      "    0.8135232 -15.316505 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 2.8677213   0.45220608 -9.037764   -5.422617   -0.5405831  -3.4046676\n",
      "  -4.544843   -9.526168  ]]OK\n",
      " - 0s - loss: 2460.8689\n",
      " - 0s - loss: 2524.2109\n",
      " - 0s - loss: 2509.6648\n",
      "Episode: 49/400, Epsilon:0.44, Average reward: -96.54\n",
      "Prediction for [50,0,50,0] is: [[  1.6034209  -1.6033542 -11.75309    -1.2325945  -4.3048787  -4.0747104\n",
      "    0.689083  -15.571822 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 2.7801127  0.385069  -9.134552  -5.403969  -0.6311442 -3.5366833\n",
      "  -4.7037363 -9.631846 ]]OK\n",
      " - 0s - loss: 1618.5260\n",
      " - 0s - loss: 1574.4886\n",
      " - 0s - loss: 1491.2063\n",
      "Episode: 50/400, Epsilon:0.43, Average reward: -108.18\n",
      "Prediction for [50,0,50,0] is: [[  1.4599437  -1.7839757 -11.912717   -1.2287354  -4.490203   -4.29314\n",
      "    0.5645042 -15.796186 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 2.6810803   0.30333853 -9.2168455  -5.387838   -0.721227   -3.6414578\n",
      "  -4.8640213  -9.720207  ]]OK\n",
      " - 0s - loss: 2052.3530\n",
      " - 0s - loss: 1753.6703\n",
      " - 0s - loss: 1941.9501\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 51/400, Epsilon:0.42, Average reward: -135.23\n",
      "Prediction for [50,0,50,0] is: [[  1.3023858  -1.9741162 -12.059732   -1.2225543  -4.661222   -4.507153\n",
      "    0.439266  -16.019695 ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 2.5829177   0.20154503 -9.270615   -5.368682   -0.79170054 -3.7379644\n",
      "  -5.011945   -9.799409  ]]OK\n",
      " - 0s - loss: 2489.1772\n",
      " - 0s - loss: 2659.6702\n",
      " - 0s - loss: 2703.3879\n",
      "Episode: 52/400, Epsilon:0.41, Average reward: -84.71\n",
      "Prediction for [50,0,50,0] is: [[  1.1276618   -2.1683016  -12.193523    -1.2187556   -4.8240385\n",
      "   -4.729129     0.31139722 -16.272083  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 2.4851272   0.08612688 -9.299776   -5.3386383  -0.8537659  -3.8410966\n",
      "  -5.1631503  -9.876224  ]]OK\n",
      " - 0s - loss: 2157.2688\n",
      " - 0s - loss: 2182.3967\n",
      " - 0s - loss: 2210.5264\n",
      "Episode: 53/400, Epsilon:0.41, Average reward: -104.88\n",
      "Prediction for [50,0,50,0] is: [[  0.9636448   -2.342844   -12.332192    -1.2371328   -4.9792237\n",
      "   -4.924491     0.16101299 -16.511438  ]]NO\n",
      "Prediction for [0,50,0,50] is: [[ 2.390386   -0.03564272 -9.33532    -5.3162155  -0.90931344 -3.9408646\n",
      "  -5.321196   -9.961272  ]]OK\n",
      " - 0s - loss: 1826.6093\n",
      " - 0s - loss: 1990.6547\n",
      " - 0s - loss: 1989.5431\n",
      "Episode: 54/400, Epsilon:0.4, Average reward: -212.99\n",
      "Prediction for [50,0,50,0] is: [[  0.80285716  -2.520766   -12.46141     -1.2491088   -5.13646\n",
      "   -5.0912166    0.02467539 -16.73544   ]]NO\n",
      "Prediction for [0,50,0,50] is: [[  2.2888706   -0.16675833  -9.378925    -5.290313    -0.9785324\n",
      "   -4.0270753   -5.4700303  -10.049097  ]]OK\n",
      " - 0s - loss: 4692.6470\n",
      " - 0s - loss: 4701.5151\n",
      " - 0s - loss: 4334.7988\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                memory_population_length if mode == 'training' else simulation_length , timesteps_per_second,\\\n",
    "                                                                delete_results = delete_results, verbose = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or agent_type ==\"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or agent_type == \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION, POPULATION, DEBUG OR TEST ITERATION\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = True)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0\n",
    "        \n",
    "        # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"populate\":\n",
    "            if PER_activated:\n",
    "                memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                                vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                                seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                                demand_list, demand_change_timesteps, PER_activated)\n",
    "                print(\"PER memory prepopulated with {} entries\".format(memory_size))\n",
    "        \n",
    "        Vissim = None\n",
    "     \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\":\n",
    "        # Load previous memory if available, else create it\n",
    "        SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "        memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "        print('Memory pre-populated. Starting Training.\\n')\n",
    "        \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                # Run Network Parser and ensure agents are linked to their intersections\n",
    "                npa = NetworkParser(Vissim)\n",
    "                for index, agent in enumerate(Agents):\n",
    "                    agent.update_IDS(agent.signal_id, npa)\n",
    "                    agent.episode_reward = []\n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "                    \n",
    "            # Run Episode at maximum speed\n",
    "            SF.Select_Vissim_Mode(Vissim, mode)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "           \n",
    "             # Train agent with experience of episode and copy weights when necessary\n",
    "            # Update exploration rate\n",
    "            for agent in Agents: \n",
    "                for _ in range(3):\n",
    "                    agent.learn_batch(batch_size, episode)\n",
    "            # Copy weights \n",
    "                if (episode+1) % agent.copy_weights_frequency == 0 and episode != 0:\n",
    "                    agent.copy_weights()\n",
    "                agent.epsilon = epsilon_sequence[episode+1]\n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHORT PRETRAINED FROM MEMORY DEMO\n",
    "# Initialize storage\n",
    "reward_storage = []\n",
    "best_agent_weights = []\n",
    "best_agent_memory = []\n",
    "reward_plot = np.zeros([episodes,])\n",
    "loss_plot = np.zeros([episodes,])\n",
    "\n",
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n",
    "\n",
    "# Setting Random Seed\n",
    "Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "# Deploy Network Parser (crawl network)\n",
    "npa = NetworkParser(Vissim)\n",
    "print('NetworkParser has succesfully crawled the model network.')\n",
    "\n",
    "# Initialize agents\n",
    "if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "    Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                       gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                       DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                       Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "    agents_deployed = True\n",
    "else:\n",
    "    print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "    quit()\n",
    "if agents_deployed:\n",
    "    print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "\n",
    "#    memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "#                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "#                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "#                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "#print('Memory pre-populated. Starting Training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_heads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups[i].SetAttValue(\"SigState\", \"RED\")\n",
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SignalHeadsCollection and unpack the SignalHeads into a list by SignalController\n",
    "signal_heads = [[] for _ in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    print(SC)\n",
    "    for SG in range(signal_controllers[SC].SGs.Count):\n",
    "        print(SG)\n",
    "        signal_heads[SC].append(toList(signal_groups[SC][SG].SigHeads.GetAll())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanes = [[[] for b in range(len(signal_heads[a])) ] for a in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    for SH in range(len(signal_heads[SC])):\n",
    "        lanes[SC][SH].append(signal_heads[SC][SH].Lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser2(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(6000):\n",
    "    if i % 5 == 0:\n",
    "        Agents[0].copy_weights()\n",
    "    Agents[0].learn_batch(64, 0)\n",
    "    print(\"Epoch {}:\".format(i))\n",
    "    print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape([50,0,50,0], [1,4])))\\\n",
    "          + (\"OK\" if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1]  else \"NO\"))\n",
    "    true1 = True if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1] else False\n",
    "    print(\"Prediction for [0,50,0,50] is: {}\".format(Agents[0].model.predict(np.reshape([0,50,0,50], [1,4])))\\\n",
    "         + (\"OK\" if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1]  else \"NO\"))\n",
    "    true2 = True if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1] else False\n",
    "    if true1 and true2 == True and i>100:\n",
    "        print(\"FOUND CANDIDATE AT EPOCH {}. TERMINATING\".format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A TEST RUN\n",
    "SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "#timesteps_per_second = 10\n",
    "#Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(5).AttValue('QStops(Current,Last)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(Filename+\"bla\"):\n",
    "    Vissim.LoadNet(Filename+\"bla\")\n",
    "else:\n",
    "    raise Exception(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.SetAttValue('SimPeriod', sim_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
