{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Model Parameters\n",
    "Random_Seed = 42\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "program = 'DQN' # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'\n",
    "state_type  = 'Queues'\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "## Load trained model?\n",
    "Demo_Mode = False\n",
    "load_trained = False\n",
    "Quickmode = True\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling flags\n",
    "# Flag for restarting the COM Server\n",
    "reset_flag = True\n",
    "#cache_flag = False\n",
    "# If a fresh start is needed, all previous results from simulations are deleted\n",
    "Start_Fresh = True\n",
    "# Debug action\n",
    "debug_action = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 100\n",
    "partial_save_at = 10\n",
    "copy_weights_frequency = 5\n",
    "reset_frequency = 10\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "simulation_length = 36000*1\n",
    "memory_population_length = simulation_length*5\n",
    "## State-Action Parameters\n",
    "state_size = 4\n",
    "action_size = 5\n",
    "# Memory Size\n",
    "memory_size = 5000\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# Learning Rate\n",
    "alpha   = 0.0001\n",
    "# Discount Factor\n",
    "gamma   = 0.9\n",
    "# Exploration Schedule\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "epsilon_decay = (epsilon_end - epsilon_start)/(episodes-1)\n",
    "#epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes+1)) # Geometric decay\n",
    "# Demand Schedule\n",
    "demands = [100, 200, 400, 600, 800, 1000]\n",
    "# Session ID\n",
    "Session_ID = 'Episodes'+str(episodes)+'_Program'+program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Loading Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 18000.0 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Deploying instance of Standard Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DQN.\n",
      "Populating memory with Random Actions....\n",
      "Episode: 1/100, Epsilon:1, Average reward: -329.98\n",
      "Prediction for [500,0,500,0] is: [[ 77.13489   99.790115  52.991863 140.482     83.537155]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Episode: 2/100, Epsilon:0.99, Average reward: -113.45\n",
      "Prediction for [500,0,500,0] is: [[ 57.73544   84.18844   22.563284 112.394875  66.23149 ]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Episode: 3/100, Epsilon:0.98, Average reward: -272.16\n",
      "Prediction for [500,0,500,0] is: [[41.588863  66.38328    3.4648354 90.54008   53.948135 ]]\n",
      "Episode: 4/100, Epsilon:0.97, Average reward: -235.61\n",
      "Prediction for [500,0,500,0] is: [[ 29.706623  51.089294 -18.776096  65.86073   48.11267 ]]\n",
      "Episode: 5/100, Epsilon:0.96, Average reward: -230.87\n",
      "Prediction for [500,0,500,0] is: [[ 12.535153  37.242622 -40.537285  39.292274  42.205677]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 6/100, Epsilon:0.95, Average reward: -224.9\n",
      "Prediction for [500,0,500,0] is: [[  4.0013776  24.519882  -63.45606    18.106361   32.159157 ]]\n",
      "Episode: 7/100, Epsilon:0.94, Average reward: -194.84\n",
      "Prediction for [500,0,500,0] is: [[-14.306247   14.723595  -77.55182    -7.1291704  20.592062 ]]\n",
      "Episode: 8/100, Epsilon:0.93, Average reward: -156.26\n",
      "Prediction for [500,0,500,0] is: [[-26.558313   -2.9195263 -94.87971   -31.354563   13.096164 ]]\n",
      "Episode: 9/100, Epsilon:0.92, Average reward: -322.18\n",
      "Prediction for [500,0,500,0] is: [[ -42.474785   -23.72855   -109.358864   -49.718967     9.3347225]]\n",
      "Episode: 10/100, Epsilon:0.91, Average reward: -385.33\n",
      "Prediction for [500,0,500,0] is: [[ -54.952328   -43.923637  -131.42534    -66.6111       2.9428284]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 10.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 11/100, Epsilon:0.9, Average reward: -331.85\n",
      "Prediction for [500,0,500,0] is: [[ -69.33766    -75.70139   -162.16273    -90.33964     -1.3106347]]\n",
      "Episode: 12/100, Epsilon:0.89, Average reward: -325.88\n",
      "Prediction for [500,0,500,0] is: [[ -82.747505  -96.777054 -195.7918   -118.39718   -15.053358]]\n",
      "Episode: 13/100, Epsilon:0.88, Average reward: -276.64\n",
      "Prediction for [500,0,500,0] is: [[-101.11197  -121.30907  -230.67052  -145.29378   -28.857351]]\n",
      "Episode: 14/100, Epsilon:0.87, Average reward: -299.1\n",
      "Prediction for [500,0,500,0] is: [[-115.162384 -153.97368  -267.7795   -177.9181    -55.27258 ]]\n",
      "Episode: 15/100, Epsilon:0.86, Average reward: -378.93\n",
      "Prediction for [500,0,500,0] is: [[-137.97502 -196.43477 -304.93472 -217.61652  -73.86032]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 16/100, Epsilon:0.85, Average reward: -339.45\n",
      "Prediction for [500,0,500,0] is: [[-172.98448 -234.77832 -343.9321  -240.36342 -100.85057]]\n",
      "Episode: 17/100, Epsilon:0.84, Average reward: -264.33\n",
      "Prediction for [500,0,500,0] is: [[-205.45401 -278.2632  -405.81094 -267.1149  -140.5096 ]]\n",
      "Episode: 18/100, Epsilon:0.83, Average reward: -337.32\n",
      "Prediction for [500,0,500,0] is: [[-241.80254 -337.5194  -468.7506  -313.75308 -172.6897 ]]\n",
      "Episode: 19/100, Epsilon:0.82, Average reward: -261.48\n",
      "Prediction for [500,0,500,0] is: [[-275.77908 -383.49847 -548.831   -354.5564  -222.32999]]\n",
      "Episode: 20/100, Epsilon:0.81, Average reward: -364.82\n",
      "Prediction for [500,0,500,0] is: [[-328.5796  -430.17026 -631.4942  -399.38153 -284.74564]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 20.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 21/100, Epsilon:0.8, Average reward: -297.85\n",
      "Prediction for [500,0,500,0] is: [[-384.49243 -476.25018 -706.0821  -443.37738 -338.43185]]\n",
      "Episode: 22/100, Epsilon:0.79, Average reward: -371.19\n",
      "Prediction for [500,0,500,0] is: [[-451.39215 -558.57605 -825.3711  -517.2954  -417.2553 ]]\n",
      "Episode: 23/100, Epsilon:0.78, Average reward: -394.03\n",
      "Prediction for [500,0,500,0] is: [[-538.8095  -632.69586 -932.5514  -571.7715  -498.8857 ]]\n",
      "Episode: 24/100, Epsilon:0.77, Average reward: -201.15\n",
      "Prediction for [500,0,500,0] is: [[ -631.4726   -748.8856  -1049.376    -648.7656   -582.80646]]\n",
      "Episode: 25/100, Epsilon:0.76, Average reward: -341.91\n",
      "Prediction for [500,0,500,0] is: [[ -712.90314  -825.9014  -1153.6488   -729.71295  -677.18225]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 26/100, Epsilon:0.75, Average reward: -354.56\n",
      "Prediction for [500,0,500,0] is: [[ -783.53033  -884.92535 -1232.9625   -795.5131   -760.9307 ]]\n",
      "Episode: 27/100, Epsilon:0.74, Average reward: -359.03\n",
      "Prediction for [500,0,500,0] is: [[ -899.07916 -1003.8642  -1386.7855   -894.5136   -885.36865]]\n",
      "Episode: 28/100, Epsilon:0.73, Average reward: -278.61\n",
      "Prediction for [500,0,500,0] is: [[-1031.5009  -1150.2156  -1525.7897   -987.20184 -1019.0161 ]]\n",
      "Episode: 29/100, Epsilon:0.72, Average reward: -313.68\n",
      "Prediction for [500,0,500,0] is: [[-1149.8025 -1272.1901 -1651.5327 -1096.078  -1147.1559]]\n",
      "Episode: 30/100, Epsilon:0.71, Average reward: -361.48\n",
      "Prediction for [500,0,500,0] is: [[-1215.7064 -1365.4227 -1725.7107 -1207.7511 -1237.0715]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 30.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 31/100, Epsilon:0.7, Average reward: -172.12\n",
      "Prediction for [500,0,500,0] is: [[-1261.0867 -1420.0538 -1764.884  -1278.553  -1314.9548]]\n",
      "Episode: 32/100, Epsilon:0.69, Average reward: -395.36\n",
      "Prediction for [500,0,500,0] is: [[-1426.1732 -1580.0876 -1936.1434 -1406.305  -1475.8654]]\n",
      "Episode: 33/100, Epsilon:0.68, Average reward: -349.97\n",
      "Prediction for [500,0,500,0] is: [[-1510.5376 -1677.2549 -2028.3494 -1506.5972 -1588.9825]]\n",
      "Episode: 34/100, Epsilon:0.67, Average reward: -199.91\n",
      "Prediction for [500,0,500,0] is: [[-1585.8314 -1768.7057 -2078.569  -1625.2878 -1669.8231]]\n",
      "Episode: 35/100, Epsilon:0.66, Average reward: -151.89\n",
      "Prediction for [500,0,500,0] is: [[-1644.5646 -1825.9697 -2139.7615 -1707.7645 -1773.8949]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 36/100, Epsilon:0.65, Average reward: -91.73\n",
      "Prediction for [500,0,500,0] is: [[-1767.3335 -1886.3574 -2221.154  -1794.3955 -1881.5906]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Episode: 37/100, Epsilon:0.64, Average reward: -100.97\n",
      "Prediction for [500,0,500,0] is: [[-1899.3687 -1999.3627 -2350.163  -1933.6104 -2047.6833]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 38/100, Epsilon:0.63, Average reward: -164.12\n",
      "Prediction for [500,0,500,0] is: [[-2001.0485 -2115.4578 -2485.9634 -2072.7722 -2151.3806]]\n",
      "Episode: 39/100, Epsilon:0.62, Average reward: -105.37\n",
      "Prediction for [500,0,500,0] is: [[-2096.9246 -2164.5327 -2522.7778 -2133.4792 -2199.224 ]]\n",
      "Episode: 40/100, Epsilon:0.61, Average reward: -160.78\n",
      "Prediction for [500,0,500,0] is: [[-2123.9512 -2189.1257 -2513.4373 -2167.659  -2206.7417]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 40.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 41/100, Epsilon:0.6, Average reward: -155.84\n",
      "Prediction for [500,0,500,0] is: [[-2220.2158 -2248.7004 -2568.736  -2235.197  -2279.0308]]\n",
      "Episode: 42/100, Epsilon:0.59, Average reward: -368.66\n",
      "Prediction for [500,0,500,0] is: [[-2363.7483 -2397.4941 -2744.229  -2351.1758 -2465.344 ]]\n",
      "Episode: 43/100, Epsilon:0.58, Average reward: -161.58\n",
      "Prediction for [500,0,500,0] is: [[-2436.7866 -2477.0835 -2826.4368 -2438.3022 -2560.9746]]\n",
      "Episode: 44/100, Epsilon:0.57, Average reward: -161.94\n",
      "Prediction for [500,0,500,0] is: [[-2504.2837 -2585.2705 -2937.8948 -2513.1567 -2649.0813]]\n",
      "Episode: 45/100, Epsilon:0.56, Average reward: -140.87\n",
      "Prediction for [500,0,500,0] is: [[-2533.1467 -2609.104  -2964.5337 -2601.9438 -2717.2197]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 46/100, Epsilon:0.55, Average reward: -47.78\n",
      "Prediction for [500,0,500,0] is: [[-2589.412  -2656.8606 -2984.9138 -2631.3538 -2717.199 ]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Episode: 47/100, Epsilon:0.54, Average reward: -107.62\n",
      "Prediction for [500,0,500,0] is: [[-2722.51   -2749.2366 -3074.0854 -2729.834  -2811.7239]]\n",
      "Episode: 48/100, Epsilon:0.53, Average reward: -418.63\n",
      "Prediction for [500,0,500,0] is: [[-2856.0562 -2845.4285 -3172.0864 -2832.7512 -2932.9456]]\n",
      "Episode: 49/100, Epsilon:0.52, Average reward: -212.13\n",
      "Prediction for [500,0,500,0] is: [[-2911.31   -2860.8123 -3190.6504 -2864.919  -2947.7842]]\n",
      "Episode: 50/100, Epsilon:0.51, Average reward: -280.95\n",
      "Prediction for [500,0,500,0] is: [[-2954.128  -2936.4717 -3248.2537 -2937.9846 -3033.9463]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 50.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 51/100, Epsilon:0.5, Average reward: -180.06\n",
      "Prediction for [500,0,500,0] is: [[-2976.3982 -2951.5522 -3234.4    -2952.7644 -3030.581 ]]\n",
      "Episode: 52/100, Epsilon:0.49, Average reward: -162.76\n",
      "Prediction for [500,0,500,0] is: [[-3057.6511 -3052.1404 -3325.8267 -3065.135  -3101.1384]]\n",
      "Episode: 53/100, Epsilon:0.48, Average reward: -426.38\n",
      "Prediction for [500,0,500,0] is: [[-3126.59   -3124.3018 -3361.9958 -3109.0356 -3131.9714]]\n",
      "Episode: 54/100, Epsilon:0.47, Average reward: -403.03\n",
      "Prediction for [500,0,500,0] is: [[-3225.073  -3189.4038 -3437.8267 -3179.0872 -3170.811 ]]\n",
      "Episode: 55/100, Epsilon:0.46, Average reward: -337.81\n",
      "Prediction for [500,0,500,0] is: [[-3228.9841 -3174.1663 -3416.5122 -3189.2646 -3176.4324]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 56/100, Epsilon:0.45, Average reward: -422.27\n",
      "Prediction for [500,0,500,0] is: [[-3333.9392 -3270.6926 -3489.1055 -3297.6914 -3259.8955]]\n",
      "Episode: 57/100, Epsilon:0.44, Average reward: -419.61\n",
      "Prediction for [500,0,500,0] is: [[-3414.2236 -3411.2454 -3578.6267 -3389.0432 -3379.3733]]\n",
      "Episode: 58/100, Epsilon:0.43, Average reward: -419.43\n",
      "Prediction for [500,0,500,0] is: [[-3534.5906 -3496.5144 -3627.4805 -3448.8967 -3491.1008]]\n",
      "Episode: 59/100, Epsilon:0.42, Average reward: -373.16\n",
      "Prediction for [500,0,500,0] is: [[-3624.3538 -3550.7979 -3710.9165 -3552.3843 -3553.8447]]\n",
      "Episode: 60/100, Epsilon:0.41, Average reward: -414.06\n",
      "Prediction for [500,0,500,0] is: [[-3644.1902 -3596.4255 -3736.5903 -3594.8694 -3568.4727]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 60.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 61/100, Epsilon:0.4, Average reward: -437.38\n",
      "Prediction for [500,0,500,0] is: [[-3691.0276 -3640.4307 -3745.6077 -3644.1848 -3597.924 ]]\n",
      "Episode: 62/100, Epsilon:0.39, Average reward: -442.17\n",
      "Prediction for [500,0,500,0] is: [[-3802.5842 -3750.6453 -3838.783  -3794.2522 -3742.101 ]]\n",
      "Episode: 63/100, Epsilon:0.38, Average reward: -254.75\n",
      "Prediction for [500,0,500,0] is: [[-3886.644  -3828.2588 -3893.6282 -3942.5813 -3833.5234]]\n",
      "Episode: 64/100, Epsilon:0.37, Average reward: -263.57\n",
      "Prediction for [500,0,500,0] is: [[-3937.753  -3865.3198 -3934.3193 -3980.338  -3871.612 ]]\n",
      "Episode: 65/100, Epsilon:0.36, Average reward: -215.52\n",
      "Prediction for [500,0,500,0] is: [[-3969.5703 -3897.1116 -3958.6702 -4032.7612 -3907.0825]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 66/100, Epsilon:0.35, Average reward: -431.65\n",
      "Prediction for [500,0,500,0] is: [[-4003.5574 -3984.4478 -4017.3357 -4107.763  -3944.486 ]]\n",
      "Episode: 67/100, Epsilon:0.34, Average reward: -223.39\n",
      "Prediction for [500,0,500,0] is: [[-4062.48   -4028.5112 -4089.3623 -4129.747  -4029.3179]]\n",
      "Episode: 68/100, Epsilon:0.33, Average reward: -432.99\n",
      "Prediction for [500,0,500,0] is: [[-4120.0527 -4099.58   -4190.1978 -4253.7373 -4074.9958]]\n",
      "Episode: 69/100, Epsilon:0.32, Average reward: -385.29\n",
      "Prediction for [500,0,500,0] is: [[-4169.298  -4181.1733 -4243.136  -4286.127  -4146.624 ]]\n",
      "Episode: 70/100, Epsilon:0.31, Average reward: -356.07\n",
      "Prediction for [500,0,500,0] is: [[-4206.7695 -4256.453  -4280.856  -4303.891  -4192.0083]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 70.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 71/100, Epsilon:0.3, Average reward: -404.3\n",
      "Prediction for [500,0,500,0] is: [[-4183.1187 -4259.282  -4255.605  -4304.898  -4172.2036]]\n",
      "Episode: 72/100, Epsilon:0.29, Average reward: -460.77\n",
      "Prediction for [500,0,500,0] is: [[-4236.3823 -4338.16   -4283.037  -4354.977  -4156.234 ]]\n",
      "Episode: 73/100, Epsilon:0.28, Average reward: -443.17\n",
      "Prediction for [500,0,500,0] is: [[-4290.4663 -4453.689  -4356.315  -4401.8696 -4220.2417]]\n",
      "Episode: 74/100, Epsilon:0.27, Average reward: -363.34\n",
      "Prediction for [500,0,500,0] is: [[-4369.1304 -4516.7104 -4431.336  -4475.888  -4327.483 ]]\n",
      "Episode: 75/100, Epsilon:0.26, Average reward: -457.17\n",
      "Prediction for [500,0,500,0] is: [[-4404.123  -4541.513  -4446.62   -4504.8286 -4306.2246]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 76/100, Epsilon:0.25, Average reward: -452.91\n",
      "Prediction for [500,0,500,0] is: [[-4482.473  -4590.8003 -4467.9785 -4516.092  -4319.7065]]\n",
      "Episode: 77/100, Epsilon:0.24, Average reward: -444.35\n",
      "Prediction for [500,0,500,0] is: [[-4503.9097 -4654.863  -4516.088  -4567.195  -4377.285 ]]\n",
      "Episode: 78/100, Epsilon:0.23, Average reward: -446.1\n",
      "Prediction for [500,0,500,0] is: [[-4573.3047 -4709.5903 -4552.4185 -4576.021  -4474.533 ]]\n",
      "Episode: 79/100, Epsilon:0.22, Average reward: -413.59\n",
      "Prediction for [500,0,500,0] is: [[-4613.912  -4797.4204 -4629.076  -4653.934  -4598.3096]]\n",
      "Episode: 80/100, Epsilon:0.21, Average reward: -204.98\n",
      "Prediction for [500,0,500,0] is: [[-4622.221  -4807.1953 -4662.724  -4662.6616 -4650.995 ]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 80.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 81/100, Epsilon:0.2, Average reward: -149.13\n",
      "Prediction for [500,0,500,0] is: [[-4622.12   -4823.681  -4655.862  -4678.3896 -4650.9956]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 82/100, Epsilon:0.19, Average reward: -210.54\n",
      "Prediction for [500,0,500,0] is: [[-4688.8345 -4849.843  -4711.741  -4697.7046 -4700.755 ]]\n",
      "Episode: 83/100, Epsilon:0.18, Average reward: -160.65\n",
      "Prediction for [500,0,500,0] is: [[-4699.0757 -4902.631  -4730.676  -4715.536  -4754.789 ]]\n",
      "Episode: 84/100, Epsilon:0.17, Average reward: -180.37\n",
      "Prediction for [500,0,500,0] is: [[-4766.3516 -4951.102  -4736.941  -4789.875  -4792.949 ]]\n",
      "Episode: 85/100, Epsilon:0.16, Average reward: -239.51\n",
      "Prediction for [500,0,500,0] is: [[-4815.173  -4960.777  -4777.0215 -4818.2993 -4844.021 ]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 86/100, Epsilon:0.15, Average reward: -216.32\n",
      "Prediction for [500,0,500,0] is: [[-4892.2075 -5015.415  -4868.8384 -4929.1714 -4904.7163]]\n",
      "Episode: 87/100, Epsilon:0.14, Average reward: -152.44\n",
      "Prediction for [500,0,500,0] is: [[-4927.5425 -5094.548  -4957.374  -4954.5957 -4988.889 ]]\n",
      "Episode: 88/100, Epsilon:0.13, Average reward: -419.8\n",
      "Prediction for [500,0,500,0] is: [[-5028.24   -5213.3906 -5049.525  -5015.6523 -5123.3857]]\n",
      "Episode: 89/100, Epsilon:0.12, Average reward: -143.11\n",
      "Prediction for [500,0,500,0] is: [[-4945.158  -5179.9907 -5016.0293 -5015.832  -5122.666 ]]\n",
      "Episode: 90/100, Epsilon:0.11, Average reward: -144.57\n",
      "Prediction for [500,0,500,0] is: [[-4922.0015 -5181.427  -4975.755  -4976.8477 -5073.454 ]]\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 90.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 91/100, Epsilon:0.1, Average reward: -130.61\n",
      "Prediction for [500,0,500,0] is: [[-4903.577  -5134.453  -4975.4165 -4979.124  -5108.7324]]\n",
      "Episode: 92/100, Epsilon:0.09, Average reward: -134.23\n",
      "Prediction for [500,0,500,0] is: [[-4905.296  -5155.4834 -4961.7803 -5013.4673 -5102.6255]]\n",
      "Episode: 93/100, Epsilon:0.08, Average reward: -144.75\n",
      "Prediction for [500,0,500,0] is: [[-4944.23   -5152.1206 -5024.088  -5045.6353 -5111.016 ]]\n",
      "Episode: 94/100, Epsilon:0.07, Average reward: -143.03\n",
      "Prediction for [500,0,500,0] is: [[-4934.287  -5211.0366 -4978.4243 -5009.6826 -5077.5815]]\n",
      "Episode: 95/100, Epsilon:0.06, Average reward: -171.87\n",
      "Prediction for [500,0,500,0] is: [[-4920.7974 -5253.15   -5032.998  -5030.3896 -5062.149 ]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 96/100, Epsilon:0.05, Average reward: -160.65\n",
      "Prediction for [500,0,500,0] is: [[-4933.711  -5297.9487 -5081.6523 -5054.2764 -5071.9277]]\n",
      "Episode: 97/100, Epsilon:0.04, Average reward: -152.84\n",
      "Prediction for [500,0,500,0] is: [[-4937.7383 -5328.7886 -5090.0986 -5094.798  -5050.9414]]\n",
      "Episode: 98/100, Epsilon:0.03, Average reward: -161.95\n",
      "Prediction for [500,0,500,0] is: [[-4962.5156 -5331.2974 -5066.3677 -5120.1353 -5064.3154]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    \n",
    "    # If previous agents aren't to be loaded, proceed to memory population\n",
    "    if not load_trained:\n",
    "        # Initialize simulation\n",
    "        if 'Vissim' not in globals() or Vissim == None:\n",
    "            Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                        memory_population_length, Start_Fresh,\\\n",
    "                                                                        reset_flag = True, verbose = True)\n",
    "        else:\n",
    "            Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "            Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                  memory_population_length, Start_Fresh, reset_flag = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if program == \"DQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = False, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DuelingDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = False, Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = True, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DuelingDDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = True, Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    \n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), program))\n",
    "    \n",
    "    if Demo_Mode:\n",
    "        Agents = SF.load_agents(vissim_working_directory, model_name, Agents, Session_ID, best = True)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length, Demo_Mode)\n",
    "        Vissim = None\n",
    "    # Load previous trained data\n",
    "    elif load_trained:\n",
    "        Agents = SF.load_agents(vissim_working_directory, model_name, Agents, Session_ID, best = False)\n",
    "    # If previous data isn't to be loaded, have an initial longer random run to populate memory\n",
    "    else:\n",
    "        print('Populating memory with Random Actions....')\n",
    "        SF.Set_Quickmode(Vissim)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length, Demo_Mode)\n",
    "    \n",
    "    # Iterations of the simulation\n",
    "    for episode in range(episodes):\n",
    "        # Completely re-dispatch server every N iterations for performance\n",
    "        if episode % reset_frequency == 0 and episode !=0:\n",
    "            Vissim = None\n",
    "            Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                        simulation_length, Start_Fresh,\\\n",
    "                                                                        reset_flag = True, verbose = False)\n",
    "            print(\"Redispatched\")\n",
    "        else:\n",
    "            # If not the first episode, reset state at the start\n",
    "            Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                simulation_length, Start_Fresh, reset_flag = True)\n",
    "        npa = NetworkParser(Vissim) \n",
    "        for index, agent in enumerate(Agents):\n",
    "            agent.update_IDS(npa.signal_controllers_ids[index], npa)\n",
    "            agent.episode_reward = []\n",
    "        \n",
    "        # Change demand for every episode\n",
    "        if Random_Demand:\n",
    "            for vehicle_input in range(1,5):\n",
    "                Vissim.Net.VehicleInputs.ItemByKey(vehicle_input).SetAttValue('Volume(1)', demands[np.random.randint(0,len(demands)-1)])    \n",
    "        \n",
    "        # Run Episode at maximum speed\n",
    "\n",
    "        SF.Set_Quickmode(Vissim)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, Demo_Mode)\n",
    "        \n",
    "        # Calculate episode average reward\n",
    "        reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "        best_agent_weights = SF.best_agent(reward_storage, average_reward, best_agent_weights, vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "        # Train agent with experience of episode (indicated batch size)\n",
    "        for agent in Agents:\n",
    "            agent.replay(batch_size, episode)\n",
    "        # Security save for long trainings\n",
    "        if SaveResultsAgent:\n",
    "            if (episode+1)%partial_save_at == 0:\n",
    "                SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "    #Saving agents memory, weights and optimizer\n",
    "    if SaveResultsAgent:\n",
    "        SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "        print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved. Succesfully Terminated.\")\n",
    "    \n",
    "    # Plotting training progress\n",
    "    x_series = range(1,len(reward_storage)+1)\n",
    "    fit = np.polyfit(x_series,reward_storage,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average ageng reward in episode')\n",
    "    plt.title('Training evolution and trend')\n",
    "    plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Demo_Mode = True\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    \n",
    "    # If previous agents aren't to be loaded, proceed to memory population\n",
    "    if not load_trained:\n",
    "        # Initialize simulation\n",
    "        if 'Vissim' not in globals() or Vissim == None:\n",
    "            Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                        memory_population_length, Start_Fresh, reset_flag = True)\n",
    "        else:\n",
    "            Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "            Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                  memory_population_length, Start_Fresh, reset_flag = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if program == \"DQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, Vissim, DoubleDQN = False, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, Vissim, DoubleDQN = True, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DuelingDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = True, Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "      \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    \n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), program))\n",
    "    \n",
    "    if Demo_Mode:\n",
    "        for index, agent in enumerate(Agents):\n",
    "            Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'.h5')\n",
    "            agent.model = load_model(Filename)\n",
    "            Memory_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'_Memory'+'.p')\n",
    "            agent.memory = pickle.load(open(Memory_Filename, 'rb'))\n",
    "        print('Items successfully loaded.')        \n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length, Demo_Mode = True)\n",
    "        Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " # Plotting training progress\n",
    "    x_series = range(1,len(reward_storage)+1)\n",
    "    fit = np.polyfit(x_series,reward_storage,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average ageng reward in episode')\n",
    "    plt.title('Training evolution and trend')\n",
    "    plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].model.predict(Agents[0].state)[0][Agents[0].action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Agents[0].reward + 0.9 * np.max(Agents[0].target_model.predict(np.reshape(Agents[0].next_state,(1,4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
