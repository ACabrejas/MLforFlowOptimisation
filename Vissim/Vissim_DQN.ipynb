{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import math\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from Simulator_Functions import Select_Vissim_Mode\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PER\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 50\n",
    "partial_save_at =  5 # 100 \n",
    "copy_weights_frequency = 2 #10\n",
    "\n",
    "# Hyperparameters\n",
    "Surtrac = False\n",
    "PER_activated = True\n",
    "batch_size = 64\n",
    "memory_size = 1024\n",
    "alpha   = 0.000065\n",
    "gamma   = 0.95\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = batch_size*seconds_per_green*2.5 +1\n",
    "if PER_activated:\n",
    "    memory_population_length = int(memory_size*seconds_per_green*1.6) +1\n",
    "\n",
    "# Vissim autosave the result of the simulation    \n",
    "delete_results = True\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = 4\n",
    "action_size = 8\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP)\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":600, 'm':300, 'l':150}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep_50_A_DuelingDDQN_State_Queues_Act_phases_Rew_Queues\n"
     ]
    }
   ],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"populate\" = population of memory, generation of initial memory file\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "if mode == 'demo' :\n",
    "    simulation_length = 3600\n",
    "    demand_list = [[demand['l'], demand['l']]]\n",
    "    demand_change_timesteps = simulation_length\n",
    "    \n",
    "    \n",
    "if mode == 'test' : \n",
    "    simulation_length = 3600\n",
    "    demand_change_timesteps = 450\n",
    "    demand = {\"h\":800, 'm':400, 'l':200}\n",
    "    demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "                  [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "                  [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "                  [demand['l'], demand['h']], [demand['l'], demand['m']]]\n",
    "    delete_results = False\n",
    "\n",
    "model_name  = 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "# vissim_working_directory = 'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "agent_type = 'DuelingDDQN'        # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'   \n",
    "# 'Queues'          Sum of the queues for all lanes in intersection\n",
    "# 'QueuesDiff'      Difference in queue lengths in last timestep\n",
    "# 'QueuesDiffSC'    10000* QueuesDiff - Queues^2\n",
    "# 'TotalDelayDiff'\n",
    "\n",
    "state_type  = 'Queues'    # 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig'\n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Session ID\n",
    "#Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "\n",
    "# Adding the state type to the Session_ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_State_\"+state_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "print(Session_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting exploration schedule\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = np.array(range(1,episodes+1))\n",
    "y_series = epsilon_sequence[0:episodes]\n",
    "plt.plot(x_series, y_series, '-b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Ratio of random exploration')\n",
    "plt.title('Exploration schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 9831 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DuelingDDQN.\n",
      "After 0 timesteps, memory is 0.0 percent full\n",
      "After 1000 timesteps, memory is 8.89 percent full\n",
      "After 2000 timesteps, memory is 17.58 percent full\n",
      "After 3000 timesteps, memory is 26.17 percent full\n",
      "After 4000 timesteps, memory is 34.86 percent full\n",
      "After 5000 timesteps, memory is 43.36 percent full\n",
      "After 6000 timesteps, memory is 52.05 percent full\n",
      "After 7000 timesteps, memory is 60.84 percent full\n",
      "After 8000 timesteps, memory is 69.82 percent full\n",
      "After 9000 timesteps, memory is 78.32 percent full\n",
      "After 10000 timesteps, memory is 87.01 percent full\n",
      "After 11000 timesteps, memory is 95.51 percent full\n",
      "Memory filled. Saving as:C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Triple\\Agents_Results\\Ep_50_A_DuelingDDQN_State_Queues_Act_phases_Rew_Queues\\PERPre_1024.p\n",
      "Memory pre-populated. Starting Training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00d2a43c3b24d989f82657414a9a877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=50)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/50, Epsilon:1.0, Average reward: -70.55\n",
      "Prediction for [50,0,50,0] is: [[-17.565748  -13.793906   15.088413    4.8102765  -7.4265933   0.9015031\n",
      "  -24.846802   -6.3407097]]OK\n",
      "Prediction for [0,50,0,50] is: [[-11.513082  -12.503865  -19.168665  -11.542343    8.519331  -20.79184\n",
      "    1.4417057 -15.598379 ]]OK\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Triple\\Agents_Results\\Ep_50_A_DuelingDDQN_State_Queues_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      " - 0s - loss: 1261.4691\n",
      " - 0s - loss: 1610.7184\n",
      " - 0s - loss: 1302.5220\n",
      " - 0s - loss: 1391.5535\n",
      " - 0s - loss: 1425.6708\n",
      "Episode: 2/50, Epsilon:0.87, Average reward: -85.94\n",
      "Prediction for [50,0,50,0] is: [[-18.596981   -14.853233    13.925564     3.5950594   -8.547119\n",
      "   -0.16807747 -26.152765    -7.380363  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-12.717425   -13.373263   -20.18929    -12.616394     7.1899996\n",
      "  -21.735733     0.05960178 -16.595013  ]]OK\n",
      " - 0s - loss: 938.3148\n",
      " - 0s - loss: 968.1510\n",
      " - 0s - loss: 986.7678\n",
      " - 0s - loss: 960.6288\n",
      " - 0s - loss: 965.9792\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 3/50, Epsilon:0.75, Average reward: -98.97\n",
      "Prediction for [50,0,50,0] is: [[-19.59488   -15.98099    12.888472    2.663968   -9.631992   -1.4203348\n",
      "  -27.283081   -8.197846 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-13.797803  -14.3755    -21.103458  -13.872126    5.8131285 -22.762016\n",
      "   -1.0919809 -17.398132 ]]OK\n",
      " - 0s - loss: 1253.5186\n",
      " - 0s - loss: 1471.0038\n",
      " - 0s - loss: 1239.0476\n",
      " - 0s - loss: 1226.3282\n",
      " - 0s - loss: 1229.9858\n",
      "Episode: 4/50, Epsilon:0.66, Average reward: -134.36\n",
      "Prediction for [50,0,50,0] is: [[-20.587975  -16.980251   11.79437     1.7395153 -10.702854   -2.5422192\n",
      "  -28.279644   -8.926315 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-14.891056  -15.399628  -22.206974  -15.056757    4.395176  -23.781588\n",
      "   -2.3337011 -18.172829 ]]OK\n",
      " - 0s - loss: 2128.7864\n",
      " - 0s - loss: 2177.4905\n",
      " - 0s - loss: 2314.5461\n",
      " - 0s - loss: 2208.7007\n",
      " - 0s - loss: 2030.0229\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 5/50, Epsilon:0.57, Average reward: -198.41\n",
      "Prediction for [50,0,50,0] is: [[-21.529196  -18.074306   10.485848    0.8327732 -11.86862    -3.7242627\n",
      "  -29.217468   -9.7922125]]OK\n",
      "Prediction for [0,50,0,50] is: [[-15.998876  -16.550177  -23.4053    -16.262386    2.7920485 -24.897636\n",
      "   -3.4953394 -19.03186  ]]OK\n",
      " - 0s - loss: 4679.0020\n",
      " - 0s - loss: 4864.5293\n",
      " - 0s - loss: 4496.2979\n",
      " - 0s - loss: 4451.6514\n",
      " - 0s - loss: 4323.9614\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 5.\n",
      "Episode: 6/50, Epsilon:0.49, Average reward: -222.79\n",
      "Prediction for [50,0,50,0] is: [[-22.482456   -19.222042     8.822876    -0.12663078 -13.194177\n",
      "   -4.9504266  -30.232122   -10.716049  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-17.124836   -17.831823   -24.80888    -17.505148     0.99269867\n",
      "  -26.029026    -4.68701    -19.915003  ]]OK\n",
      " - 0s - loss: 7624.2100\n",
      " - 0s - loss: 7727.9131\n",
      " - 0s - loss: 6962.6782\n",
      " - 0s - loss: 7055.6675\n",
      " - 0s - loss: 7005.7598\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 7/50, Epsilon:0.43, Average reward: -187.05\n",
      "Prediction for [50,0,50,0] is: [[-23.494934  -20.391253    7.150648   -1.2535877 -14.799352   -6.1027074\n",
      "  -31.36951   -11.72501  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-18.319836  -19.116583  -26.27813   -18.925318   -1.0721397 -27.170284\n",
      "   -5.9670305 -20.934248 ]]OK\n",
      " - 0s - loss: 8603.8369\n",
      " - 0s - loss: 8551.9268\n",
      " - 0s - loss: 8302.5146\n",
      " - 0s - loss: 8302.6582\n",
      " - 0s - loss: 8270.8408\n",
      "Episode: 8/50, Epsilon:0.37, Average reward: -345.85\n",
      "Prediction for [50,0,50,0] is: [[-24.588808  -21.553242    5.483061   -2.5104084 -16.621788   -7.23481\n",
      "  -32.605026  -12.74453  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-19.559044  -20.457233  -27.811352  -20.452547   -3.2916298 -28.327824\n",
      "   -7.3440485 -22.015219 ]]OK\n",
      " - 0s - loss: 12518.0527\n",
      " - 0s - loss: 12622.8945\n",
      " - 0s - loss: 13011.6846\n",
      " - 0s - loss: 12120.6445\n",
      " - 0s - loss: 12778.6025\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 9/50, Epsilon:0.32, Average reward: -383.97\n",
      "Prediction for [50,0,50,0] is: [[-25.747463  -22.73055     3.6363153  -3.8858843 -18.55393    -8.417381\n",
      "  -33.955738  -13.809073 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-20.838623  -21.783693  -29.428421  -22.157154   -5.6497803 -29.519035\n",
      "   -8.812319  -23.127136 ]]OK\n",
      " - 0s - loss: 22515.8281\n",
      " - 0s - loss: 23655.9336\n",
      " - 0s - loss: 22404.2227\n",
      " - 0s - loss: 22416.5000\n",
      " - 0s - loss: 22605.0273\n",
      "Episode: 10/50, Epsilon:0.28, Average reward: -389.16\n",
      "Prediction for [50,0,50,0] is: [[-27.016617  -23.926476    1.6759892  -5.334543  -20.638878   -9.778025\n",
      "  -35.364677  -14.941095 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-22.260246 -23.147736 -31.043688 -23.899345  -8.196234 -30.906586\n",
      "  -10.345027 -24.28267 ]]OK\n",
      " - 0s - loss: 26850.4707\n",
      " - 0s - loss: 26600.1211\n",
      " - 0s - loss: 27205.6914\n",
      " - 0s - loss: 26549.0723\n",
      " - 0s - loss: 25879.4004\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 10.\n",
      "Episode: 11/50, Epsilon:0.24, Average reward: -363.34\n",
      "Prediction for [50,0,50,0] is: [[-28.398945   -25.163977    -0.20219421  -6.8498707  -22.903658\n",
      "  -11.363314   -36.82383    -16.097897  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-23.788849 -24.58976  -32.562008 -25.682161 -10.885768 -32.447323\n",
      "  -11.920557 -25.486473]]OK\n",
      " - 0s - loss: 26524.5469\n",
      " - 0s - loss: 28048.4180\n",
      " - 0s - loss: 27950.7676\n",
      " - 0s - loss: 27686.0938\n",
      " - 0s - loss: 26834.1191\n",
      "Episode: 12/50, Epsilon:0.21, Average reward: -423.5\n",
      "Prediction for [50,0,50,0] is: [[-29.804966  -26.44886    -1.9627056  -8.3899    -25.195513  -13.016946\n",
      "  -38.276222  -17.264248 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-25.332895 -26.1692   -34.010773 -27.49059  -13.64667  -33.974094\n",
      "  -13.52255  -26.715286]]OK\n",
      " - 0s - loss: 28351.6719\n",
      " - 0s - loss: 29061.6973\n",
      " - 0s - loss: 28419.9277\n",
      " - 0s - loss: 27480.7422\n",
      " - 0s - loss: 26920.7070\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 13/50, Epsilon:0.18, Average reward: -421.23\n",
      "Prediction for [50,0,50,0] is: [[-31.218811  -27.704693   -3.5617676  -9.932548  -27.470615  -14.691539\n",
      "  -39.71939   -18.461819 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-26.877586 -27.769249 -35.388298 -29.33443  -16.454311 -35.43929\n",
      "  -15.161714 -28.006224]]OK\n",
      " - 0s - loss: 31769.0234\n",
      " - 0s - loss: 31525.2793\n",
      " - 0s - loss: 30834.0391\n",
      " - 0s - loss: 30746.8242\n",
      " - 0s - loss: 30656.9395\n",
      "Episode: 14/50, Epsilon:0.16, Average reward: -468.36\n",
      "Prediction for [50,0,50,0] is: [[-32.652897  -28.973927   -5.2181015 -11.474868  -29.751705  -16.50438\n",
      "  -41.159115  -19.700708 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-28.38944  -29.32845  -36.736526 -31.1517   -19.190853 -36.879234\n",
      "  -16.76606  -29.299923]]OK\n",
      " - 0s - loss: 35179.5234\n",
      " - 0s - loss: 33538.5625\n",
      " - 0s - loss: 33787.4531\n",
      " - 0s - loss: 34787.5781\n",
      " - 0s - loss: 33456.6719\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 15/50, Epsilon:0.14, Average reward: -431.91\n",
      "Prediction for [50,0,50,0] is: [[-34.13716   -30.331356   -6.8073177 -13.092112  -32.202194  -18.337416\n",
      "  -42.670982  -20.944626 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-29.88227  -30.890095 -38.048843 -32.955162 -21.911499 -38.32418\n",
      "  -18.344553 -30.568447]]OK\n",
      " - 0s - loss: 36335.1875\n",
      " - 0s - loss: 37237.9922\n",
      " - 0s - loss: 36139.4062\n",
      " - 0s - loss: 36225.5234\n",
      " - 0s - loss: 35808.0000\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 15.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 16/50, Epsilon:0.12, Average reward: -308.37\n",
      "Prediction for [50,0,50,0] is: [[-35.65569  -31.751245  -8.431128 -14.749004 -34.755394 -20.135693\n",
      "  -44.24733  -22.193954]]OK\n",
      "Prediction for [0,50,0,50] is: [[-31.362509 -32.463406 -39.391422 -34.73241  -24.603247 -39.733505\n",
      "  -19.913332 -31.819824]]OK\n",
      " - 0s - loss: 23986.1074\n",
      " - 0s - loss: 24905.7910\n",
      " - 0s - loss: 23193.3984\n",
      " - 0s - loss: 24150.8125\n",
      " - 0s - loss: 24370.3281\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 17/50, Epsilon:0.1, Average reward: -465.05\n",
      "Prediction for [50,0,50,0] is: [[-37.024918 -33.088295  -9.925737 -16.245003 -37.04304  -21.767254\n",
      "  -45.78756  -23.358902]]OK\n",
      "Prediction for [0,50,0,50] is: [[-32.808285 -34.08972  -40.791252 -36.426468 -27.141811 -41.06888\n",
      "  -21.918844 -33.06623 ]]OK\n",
      " - 0s - loss: 26470.1836\n",
      " - 0s - loss: 25977.5332\n",
      " - 0s - loss: 25386.4609\n",
      " - 0s - loss: 26578.9492\n",
      " - 0s - loss: 25544.5039\n",
      "Episode: 18/50, Epsilon:0.09, Average reward: -506.73\n",
      "Prediction for [50,0,50,0] is: [[-38.19405  -34.254913 -11.437988 -17.489822 -38.909492 -23.18737\n",
      "  -47.22931  -24.374159]]OK\n",
      "Prediction for [0,50,0,50] is: [[-34.218647 -35.72521  -42.3598   -37.99417  -29.462208 -42.357418\n",
      "  -24.23485  -34.275814]]OK\n",
      " - 0s - loss: 39660.4688\n",
      " - 0s - loss: 38502.0625\n",
      " - 0s - loss: 39066.6562\n",
      " - 0s - loss: 38821.6289\n",
      " - 0s - loss: 39671.8828\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 19/50, Epsilon:0.08, Average reward: -364.7\n",
      "Prediction for [50,0,50,0] is: [[-39.41116  -35.41073  -13.223728 -18.823887 -40.85989  -24.693531\n",
      "  -48.684692 -25.434513]]OK\n",
      "Prediction for [0,50,0,50] is: [[-35.621647 -37.274307 -44.117985 -39.586952 -31.785336 -43.684696\n",
      "  -26.3673   -35.46953 ]]OK\n",
      " - 0s - loss: 32838.1250\n",
      " - 0s - loss: 34636.8477\n",
      " - 0s - loss: 32507.8105\n",
      " - 0s - loss: 34218.5781\n",
      " - 0s - loss: 33624.3555\n",
      "Episode: 20/50, Epsilon:0.07, Average reward: -380.81\n",
      "Prediction for [50,0,50,0] is: [[-40.69546  -36.62352  -15.001844 -20.237087 -42.94665  -26.220442\n",
      "  -50.240547 -26.528713]]OK\n",
      "Prediction for [0,50,0,50] is: [[-37.066753 -38.869545 -45.871574 -41.21937  -34.153603 -45.022533\n",
      "  -28.607143 -36.691547]]OK\n",
      " - 0s - loss: 21651.3164\n",
      " - 0s - loss: 22249.5430\n",
      " - 0s - loss: 21125.8555\n",
      " - 0s - loss: 21885.9844\n",
      " - 0s - loss: 21900.0762\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 20.\n",
      "Episode: 21/50, Epsilon:0.06, Average reward: -329.56\n",
      "Prediction for [50,0,50,0] is: [[-41.838867 -37.77356  -16.767319 -21.462715 -44.7081   -27.632416\n",
      "  -51.74472  -27.690159]]OK\n",
      "Prediction for [0,50,0,50] is: [[-38.41738  -40.43307  -47.56074  -42.651875 -36.220264 -46.254974\n",
      "  -30.95264  -37.925922]]OK\n",
      " - 0s - loss: 20146.8828\n",
      " - 0s - loss: 21114.2344\n",
      " - 0s - loss: 20161.5020\n",
      " - 0s - loss: 20729.2930\n",
      " - 0s - loss: 19495.3867\n",
      "Episode: 22/50, Epsilon:0.05, Average reward: -346.75\n",
      "Prediction for [50,0,50,0] is: [[-42.910763 -38.830467 -18.754139 -22.561306 -46.144638 -29.185047\n",
      "  -53.101143 -29.32234 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-39.51081  -41.671524 -49.16292  -43.767937 -37.755566 -47.3377\n",
      "  -32.814713 -39.11591 ]]OK\n",
      " - 0s - loss: 18946.1289\n",
      " - 0s - loss: 18722.7852\n",
      " - 0s - loss: 18366.1914\n",
      " - 0s - loss: 18631.9102\n",
      " - 0s - loss: 18700.9453\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 23/50, Epsilon:0.04, Average reward: -449.25\n",
      "Prediction for [50,0,50,0] is: [[-43.965427 -39.842674 -20.882332 -23.592102 -47.41221  -30.918442\n",
      "  -54.331387 -31.280918]]OK\n",
      "Prediction for [0,50,0,50] is: [[-40.35994  -42.60718  -50.6863   -44.624077 -38.871696 -48.306656\n",
      "  -34.1307   -40.22806 ]]OK\n",
      " - 0s - loss: 26798.3438\n",
      " - 0s - loss: 27418.3105\n",
      " - 0s - loss: 25934.1758\n",
      " - 0s - loss: 25757.8086\n",
      " - 0s - loss: 26146.9453\n",
      "Episode: 24/50, Epsilon:0.04, Average reward: -396.14\n",
      "Prediction for [50,0,50,0] is: [[-45.00291  -40.86839  -23.260658 -24.593853 -48.592396 -32.68997\n",
      "  -55.51918  -33.152023]]OK\n",
      "Prediction for [0,50,0,50] is: [[-41.1313   -43.45618  -52.46068  -45.416126 -39.830532 -49.294006\n",
      "  -35.229866 -41.329594]]OK\n",
      " - 0s - loss: 30250.1914\n",
      " - 0s - loss: 28988.4023\n",
      " - 0s - loss: 27941.0977\n",
      " - 0s - loss: 28605.6719\n",
      " - 0s - loss: 28372.0801\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 25/50, Epsilon:0.03, Average reward: -419.6\n",
      "Prediction for [50,0,50,0] is: [[-46.032303 -41.937653 -25.896044 -25.650883 -49.75676  -34.36273\n",
      "  -56.716057 -34.747242]]OK\n",
      "Prediction for [0,50,0,50] is: [[-41.93406  -44.35689  -54.61029  -46.28537  -40.799736 -50.35745\n",
      "  -36.31437  -42.38837 ]]OK\n",
      " - 0s - loss: 27757.1465\n",
      " - 0s - loss: 27037.3125\n",
      " - 0s - loss: 26960.9961\n",
      " - 0s - loss: 27117.9102\n",
      " - 0s - loss: 27078.3145\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 25.\n",
      "Episode: 26/50, Epsilon:0.03, Average reward: -421.83\n",
      "Prediction for [50,0,50,0] is: [[-47.037743 -43.057247 -28.590384 -26.841599 -50.927124 -35.842297\n",
      "  -57.920013 -36.151333]]OK\n",
      "Prediction for [0,50,0,50] is: [[-42.74999  -45.31579  -56.87119  -47.250553 -41.78628  -51.42386\n",
      "  -37.392017 -43.414062]]OK\n",
      " - 0s - loss: 27572.2578\n",
      " - 0s - loss: 28922.0215\n",
      " - 0s - loss: 28034.8906\n",
      " - 0s - loss: 27541.1445\n",
      " - 0s - loss: 27359.7500\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 27/50, Epsilon:0.03, Average reward: -403.95\n",
      "Prediction for [50,0,50,0] is: [[-47.9989   -44.226715 -31.066225 -28.003973 -52.054375 -37.117523\n",
      "  -59.215965 -37.37131 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-43.731094 -46.55311  -59.123764 -48.320766 -42.963192 -52.551598\n",
      "  -39.019012 -44.51048 ]]OK\n",
      " - 0s - loss: 28833.2891\n",
      " - 0s - loss: 29361.5273\n",
      " - 0s - loss: 28738.4707\n",
      " - 0s - loss: 29396.9844\n",
      " - 0s - loss: 28304.8828\n",
      "Episode: 28/50, Epsilon:0.02, Average reward: -366.31\n",
      "Prediction for [50,0,50,0] is: [[-48.88038  -45.32261  -32.98526  -28.999378 -53.07     -38.12729\n",
      "  -60.625336 -38.35553 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-44.990353 -48.14516  -61.24849  -49.542923 -44.462135 -53.7599\n",
      "  -41.539703 -45.719234]]OK\n",
      " - 0s - loss: 24134.2070\n",
      " - 0s - loss: 23697.2070\n",
      " - 0s - loss: 23492.9023\n",
      " - 0s - loss: 23956.8203\n",
      " - 0s - loss: 23355.9629\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 29/50, Epsilon:0.02, Average reward: -366.51\n",
      "Prediction for [50,0,50,0] is: [[-49.779804 -46.43544  -34.532673 -30.34771  -54.143723 -39.00503\n",
      "  -62.057747 -39.313564]]OK\n",
      "Prediction for [0,50,0,50] is: [[-46.28505  -49.75558  -63.080063 -50.847668 -46.00203  -54.902172\n",
      "  -44.19884  -46.89378 ]]OK\n",
      " - 0s - loss: 20589.4258\n",
      " - 0s - loss: 20951.2949\n",
      " - 0s - loss: 21168.1074\n",
      " - 0s - loss: 20463.1367\n",
      " - 0s - loss: 21227.8262\n",
      "Episode: 30/50, Epsilon:0.02, Average reward: -351.26\n",
      "Prediction for [50,0,50,0] is: [[-50.86483  -47.71535  -36.119003 -32.42851  -55.486034 -39.961494\n",
      "  -63.498817 -40.46692 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-47.347317 -51.067318 -64.499886 -52.161835 -47.289722 -55.80948\n",
      "  -46.23742  -47.8673  ]]OK\n",
      " - 0s - loss: 19600.5996\n",
      " - 0s - loss: 19254.0156\n",
      " - 0s - loss: 19348.6758\n",
      " - 0s - loss: 19279.5625\n",
      " - 0s - loss: 18888.2031\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 30.\n",
      "Episode: 31/50, Epsilon:0.01, Average reward: -380.53\n",
      "Prediction for [50,0,50,0] is: [[-52.06344  -49.135174 -37.716156 -34.943016 -57.00014  -40.97896\n",
      "  -64.90221  -41.746635]]OK\n",
      "Prediction for [0,50,0,50] is: [[-48.167934 -52.11937  -65.56955  -53.428    -48.32377  -56.501186\n",
      "  -47.65235  -48.652115]]OK\n",
      " - 0s - loss: 21181.2344\n",
      " - 0s - loss: 21023.9434\n",
      " - 0s - loss: 20267.6055\n",
      " - 0s - loss: 20843.0977\n",
      " - 0s - loss: 19985.6445\n",
      "Episode: 32/50, Epsilon:0.01, Average reward: -420.27\n",
      "Prediction for [50,0,50,0] is: [[-53.32159  -50.619335 -39.32647  -37.572414 -58.569515 -42.128746\n",
      "  -66.2652   -43.07955 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-48.85413  -53.022255 -66.4398   -54.617023 -49.201378 -57.122158\n",
      "  -48.699875 -49.331406]]OK\n",
      " - 0s - loss: 24545.1348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 23908.9219\n",
      " - 0s - loss: 24382.5078\n",
      " - 0s - loss: 23686.9961\n",
      " - 0s - loss: 24270.4473\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 33/50, Epsilon:0.01, Average reward: -449.74\n",
      "Prediction for [50,0,50,0] is: [[-54.672905 -52.065556 -41.05974  -40.04325  -60.125107 -43.79386\n",
      "  -67.58565  -44.461178]]OK\n",
      "Prediction for [0,50,0,50] is: [[-49.51733  -53.829605 -67.26645  -55.6946   -49.99957  -57.92867\n",
      "  -49.553185 -49.99825 ]]OK\n",
      " - 0s - loss: 29669.4277\n",
      " - 0s - loss: 29180.3828\n",
      " - 0s - loss: 29861.1289\n",
      " - 0s - loss: 29806.5039\n",
      " - 0s - loss: 29974.6719\n",
      "Episode: 34/50, Epsilon:0.01, Average reward: -407.65\n",
      "Prediction for [50,0,50,0] is: [[-56.183422 -53.496563 -42.934685 -42.169033 -61.64839  -46.210907\n",
      "  -68.91438  -45.940575]]OK\n",
      "Prediction for [0,50,0,50] is: [[-50.25335  -54.62148  -68.15132  -56.649494 -50.784195 -59.118893\n",
      "  -50.341545 -50.74836 ]]OK\n",
      " - 0s - loss: 28222.9258\n",
      " - 0s - loss: 28017.5312\n",
      " - 0s - loss: 27751.6680\n",
      " - 0s - loss: 28131.9102\n",
      " - 0s - loss: 28313.2852\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 35/50, Epsilon:0.01, Average reward: -394.41\n",
      "Prediction for [50,0,50,0] is: [[-57.8417   -54.955124 -44.977467 -44.092365 -63.194874 -49.10713\n",
      "  -70.31675  -47.622017]]OK\n",
      "Prediction for [0,50,0,50] is: [[-51.048187 -55.403416 -69.08976  -57.51013  -51.56117  -60.54367\n",
      "  -51.10134  -51.59285 ]]OK\n",
      " - 0s - loss: 24954.0547\n",
      " - 0s - loss: 24539.0000\n",
      " - 0s - loss: 24736.9531\n",
      " - 0s - loss: 24516.2734\n",
      " - 0s - loss: 24011.8203\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 35.\n",
      "Episode: 36/50, Epsilon:0.01, Average reward: -408.4\n",
      "Prediction for [50,0,50,0] is: [[-59.551773 -56.47003  -47.135868 -46.12938  -64.82274  -52.005\n",
      "  -71.813065 -49.76383 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-51.795074 -56.12764  -69.97623  -58.29039  -52.280956 -61.882072\n",
      "  -51.79232  -52.492104]]OK\n",
      " - 0s - loss: 24942.3555\n",
      " - 0s - loss: 24749.3984\n",
      " - 0s - loss: 24332.7695\n",
      " - 0s - loss: 24086.9102\n",
      " - 0s - loss: 24847.1680\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 37/50, Epsilon:0.01, Average reward: -371.22\n",
      "Prediction for [50,0,50,0] is: [[-61.21355  -58.003975 -49.28901  -48.397675 -66.51073  -54.533497\n",
      "  -73.36506  -52.427357]]OK\n",
      "Prediction for [0,50,0,50] is: [[-52.453934 -56.791634 -70.77252  -59.04006  -52.942753 -62.97388\n",
      "  -52.418594 -53.522034]]OK\n",
      " - 0s - loss: 25371.7734\n",
      " - 0s - loss: 24382.8770\n",
      " - 0s - loss: 24754.4336\n",
      " - 0s - loss: 24594.2129\n",
      " - 0s - loss: 24146.9160\n",
      "Episode: 38/50, Epsilon:0.01, Average reward: -353.94\n",
      "Prediction for [50,0,50,0] is: [[-62.786556 -59.56042  -51.382576 -50.773422 -68.198    -56.700935\n",
      "  -74.92416  -55.318035]]OK\n",
      "Prediction for [0,50,0,50] is: [[-53.052197 -57.438766 -71.51918  -59.801685 -53.580246 -63.87517\n",
      "  -53.018837 -54.685684]]OK\n",
      " - 0s - loss: 20496.1211\n",
      " - 0s - loss: 20126.5391\n",
      " - 0s - loss: 19839.9648\n",
      " - 0s - loss: 18974.6699\n",
      " - 0s - loss: 19733.6953\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 39/50, Epsilon:0.0, Average reward: -340.02\n",
      "Prediction for [50,0,50,0] is: [[-64.251114 -61.070755 -53.336327 -53.156654 -69.83307  -58.56263\n",
      "  -76.41261  -58.10346 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-53.596638 -58.056393 -72.210396 -60.56065  -54.185966 -64.62785\n",
      "  -53.57943  -55.805717]]OK\n",
      " - 0s - loss: 17995.0234\n",
      " - 0s - loss: 18559.0918\n",
      " - 0s - loss: 17656.9102\n",
      " - 0s - loss: 18241.6367\n",
      " - 0s - loss: 17585.8047\n",
      "Episode: 40/50, Epsilon:0.0, Average reward: -364.51\n",
      "Prediction for [50,0,50,0] is: [[-65.64312  -62.505535 -55.125107 -55.560635 -71.4252   -60.218033\n",
      "  -77.81886  -60.66731 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-54.084282 -58.610832 -72.818436 -61.286556 -54.739826 -65.25526\n",
      "  -54.077606 -56.768406]]OK\n",
      " - 0s - loss: 18863.1914\n",
      " - 0s - loss: 18549.3496\n",
      " - 0s - loss: 18390.6680\n",
      " - 0s - loss: 18077.6484\n",
      " - 0s - loss: 17687.5020\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 40.\n",
      "Episode: 41/50, Epsilon:0.0, Average reward: -376.5\n",
      "Prediction for [50,0,50,0] is: [[-67.0213   -63.895107 -56.829746 -57.889435 -72.96724  -61.919754\n",
      "  -79.1542   -62.93199 ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-54.55186  -59.130844 -73.38257  -61.972095 -55.260067 -65.86408\n",
      "  -54.535957 -57.576088]]OK\n",
      " - 0s - loss: 20511.3301\n",
      " - 0s - loss: 20525.7188\n",
      " - 0s - loss: 20752.8438\n",
      " - 0s - loss: 20114.8359\n",
      " - 0s - loss: 20459.7070\n",
      "Episode: 42/50, Epsilon:0.0, Average reward: -357.25\n",
      "Prediction for [50,0,50,0] is: [[-68.436195 -65.26363  -58.500443 -60.02238  -74.44616  -63.91988\n",
      "  -80.42762  -64.8657  ]]OK\n",
      "Prediction for [0,50,0,50] is: [[-55.053326 -59.658566 -73.95592  -62.6244   -55.78099  -66.59025\n",
      "  -54.992256 -58.27975 ]]OK\n",
      " - 0s - loss: 20497.0078\n",
      " - 0s - loss: 19949.2344\n",
      " - 0s - loss: 19725.1953\n",
      " - 0s - loss: 18563.6055\n",
      " - 0s - loss: 19249.1484\n",
      "Weights succesfully copied to Target model.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                memory_population_length if mode == 'training' else simulation_length , timesteps_per_second,\\\n",
    "                                                                delete_results = delete_results, verbose = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or agent_type ==\"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or agent_type == \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION, POPULATION, DEBUG OR TEST ITERATION\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents, reward_storage = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = True)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0 #1\n",
    "        \n",
    "        # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"populate\":\n",
    "            if PER_activated:\n",
    "                memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                                vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                                seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                                demand_list, demand_change_timesteps, PER_activated)\n",
    "                print(\"PER memory prepopulated with {} entries\".format(memory_size))\n",
    "        \n",
    "        Vissim = None\n",
    "     \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\":\n",
    "        # Load previous memory if available, else create it\n",
    "        SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "        memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "        print('Memory pre-populated. Starting Training.\\n')\n",
    "        \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                # Run Network Parser and ensure agents are linked to their intersections\n",
    "                npa = NetworkParser(Vissim)\n",
    "                for index, agent in enumerate(Agents):\n",
    "                    agent.update_IDS(agent.signal_id, npa)\n",
    "                    agent.episode_reward = []\n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "                    \n",
    "            # Run Episode at maximum speed\n",
    "            SF.Select_Vissim_Mode(Vissim, mode)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "           \n",
    "             # Train agent with experience of episode and copy weights when necessary\n",
    "            # Update exploration rate\n",
    "            for agent in Agents: \n",
    "                for _ in range(5):\n",
    "                    agent.learn_batch(batch_size, episode)\n",
    "            # Copy weights \n",
    "                if (episode+1) % agent.copy_weights_frequency == 0 and episode != 0:\n",
    "                    agent.copy_weights()\n",
    "                agent.epsilon = epsilon_sequence[episode+1]\n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHORT PRETRAINED FROM MEMORY DEMO\n",
    "# Initialize storage\n",
    "reward_storage = []\n",
    "best_agent_weights = []\n",
    "best_agent_memory = []\n",
    "reward_plot = np.zeros([episodes,])\n",
    "loss_plot = np.zeros([episodes,])\n",
    "\n",
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n",
    "\n",
    "# Setting Random Seed\n",
    "Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "# Deploy Network Parser (crawl network)\n",
    "npa = NetworkParser(Vissim)\n",
    "print('NetworkParser has succesfully crawled the model network.')\n",
    "\n",
    "# Initialize agents\n",
    "if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "    Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                       gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                       DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                       Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "    agents_deployed = True\n",
    "else:\n",
    "    print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "    quit()\n",
    "if agents_deployed:\n",
    "    print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "\n",
    "#    memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "#                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "#                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "#                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "#print('Memory pre-populated. Starting Training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_heads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups[i].SetAttValue(\"SigState\", \"RED\")\n",
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SignalHeadsCollection and unpack the SignalHeads into a list by SignalController\n",
    "signal_heads = [[] for _ in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    print(SC)\n",
    "    for SG in range(signal_controllers[SC].SGs.Count):\n",
    "        print(SG)\n",
    "        signal_heads[SC].append(toList(signal_groups[SC][SG].SigHeads.GetAll())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanes = [[[] for b in range(len(signal_heads[a])) ] for a in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    for SH in range(len(signal_heads[SC])):\n",
    "        lanes[SC][SH].append(signal_heads[SC][SH].Lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser2(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(6000):\n",
    "    if i % 5 == 0:\n",
    "        Agents[0].copy_weights()\n",
    "    Agents[0].learn_batch(64, 0)\n",
    "    print(\"Epoch {}:\".format(i))\n",
    "    print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape([50,0,50,0], [1,4])))\\\n",
    "          + (\"OK\" if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1]  else \"NO\"))\n",
    "    true1 = True if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1] else False\n",
    "    print(\"Prediction for [0,50,0,50] is: {}\".format(Agents[0].model.predict(np.reshape([0,50,0,50], [1,4])))\\\n",
    "         + (\"OK\" if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1]  else \"NO\"))\n",
    "    true2 = True if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1] else False\n",
    "    if true1 and true2 == True and i>100:\n",
    "        print(\"FOUND CANDIDATE AT EPOCH {}. TERMINATING\".format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A TEST RUN\n",
    "SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "#timesteps_per_second = 10\n",
    "#Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(5).AttValue('QStops(Current,Last)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(Filename+\"bla\"):\n",
    "    Vissim.LoadNet(Filename+\"bla\")\n",
    "else:\n",
    "    raise Exception(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.SetAttValue('SimPeriod', sim_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
