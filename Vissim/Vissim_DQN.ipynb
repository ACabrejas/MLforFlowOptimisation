{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import Simulator_Functions as SF\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 500\n",
    "partial_save_at = 100\n",
    "copy_weights_frequency = 5\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = (simulation_length-1)*200+1\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = 4\n",
    "action_size = 8\n",
    "\n",
    "# Hyperparameters\n",
    "PER_activated = True\n",
    "batch_size = 256\n",
    "memory_size = 100000\n",
    "alpha   = 0.001\n",
    "gamma   = 0.95\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP)\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":800, 'm':400, 'l':200}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"debug\"\n",
    "# \"populate\" = population of memory, generation of initial memory file\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "## Network Model Parameters\n",
    "\n",
    "model_name  = 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "agent_type = 'DuelingDDQN'         # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'   \n",
    "# 'Queues'          Sum of the queues for all lanes in intersection\n",
    "# 'QueuesDiff'      Difference in queue lengths in last timestep\n",
    "# 'QueuesDiffSC'    10000* QueuesDiff - Queues^2\n",
    "# 'TotalDelayDiff'\n",
    "state_type  = 'Queues'    \n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Session ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_phases = [[1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                  [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                  [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                  [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                  [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                  [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                  [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                  [0,0,0,0,1,1,0,0,0,0,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting exploration schedule\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = np.array(range(1,episodes+1))\n",
    "y_series = epsilon_sequence[0:episodes]\n",
    "plt.plot(x_series, y_series, '-b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Ratio of random exploration')\n",
    "plt.title('Exploration schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 720001 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Deploying instance of Double Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DuelingDDQN.\n",
      "Update at time 1\n",
      "Update at time 7\n",
      "Update at time 10\n",
      "Update at time 16\n",
      "Update at time 19\n",
      "Update at time 25\n",
      "Update at time 28\n",
      "Update at time 34\n",
      "Update at time 37\n",
      "Update at time 43\n",
      "Update at time 49\n",
      "Update at time 52\n",
      "Update at time 58\n",
      "Update at time 61\n",
      "Update at time 67\n",
      "Update at time 70\n",
      "Update at time 76\n",
      "Update at time 79\n",
      "Update at time 85\n",
      "Update at time 88\n",
      "Update at time 94\n",
      "Update at time 97\n",
      "Update at time 103\n",
      "Update at time 106\n",
      "Update at time 112\n",
      "Update at time 115\n",
      "Update at time 121\n",
      "Update at time 127\n",
      "Update at time 130\n",
      "Update at time 136\n",
      "Update at time 139\n",
      "Update at time 145\n",
      "Update at time 148\n",
      "Update at time 154\n",
      "Update at time 157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-db27b5aecdb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m             SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n\u001b[0;32m     56\u001b[0m                                       \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_green\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_yellow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                                       demand_list, demand_change_timesteps, mode, PER_activated)\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"test\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mrun_simulation_episode\u001b[1;34m(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second, seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode, PER_activated)\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;31m# Advance the game to the next second (proportionally to the simulator resolution).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                         \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                memory_population_length, timesteps_per_second,\\\n",
    "                                                                delete_results = True, verbose = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION, POPULATION, DEBUG OR TEST ITERATION\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = True)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0\n",
    "        \n",
    "        # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"populate\":\n",
    "            if PER_activated:\n",
    "                memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                                vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                                seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                                demand_list, demand_change_timesteps, PER_activated)\n",
    "                print(\"PER memory prepopulated with {} entries\".format(memory_size))\n",
    "        \n",
    "        Vissim = None\n",
    "     \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\":\n",
    "        # Load previous memory if available, else create it\n",
    "        SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "        memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "        print('Memory pre-populated. Starting Training.\\n')\n",
    "        \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                # Run Network Parser and ensure agents are linked to their intersections\n",
    "                npa = NetworkParser(Vissim)\n",
    "                for index, agent in enumerate(Agents):\n",
    "                    agent.update_IDS(agent.signal_id, npa)\n",
    "                    agent.episode_reward = []\n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "                    \n",
    "            # Run Episode at maximum speed\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "            # Train agent with experience of episode and copy weights when necessary\n",
    "            # Update exploration rate\n",
    "            for agent in Agents:\n",
    "                agent.learn_batch(batch_size, episode)\n",
    "                agent.epsilon = epsilon_sequence[episode+1]\n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHORT PRETRAINED FROM MEMORY DEMO\n",
    "# Initialize storage\n",
    "reward_storage = []\n",
    "best_agent_weights = []\n",
    "best_agent_memory = []\n",
    "reward_plot = np.zeros([episodes,])\n",
    "loss_plot = np.zeros([episodes,])\n",
    "\n",
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n",
    "\n",
    "# Setting Random Seed\n",
    "Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "# Deploy Network Parser (crawl network)\n",
    "npa = NetworkParser(Vissim)\n",
    "print('NetworkParser has succesfully crawled the model network.')\n",
    "\n",
    "# Initialize agents\n",
    "if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "    Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                       gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                       DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                       Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "    agents_deployed = True\n",
    "else:\n",
    "    print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "    quit()\n",
    "if agents_deployed:\n",
    "    print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "\n",
    "#    memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "#                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "#                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "#                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "#print('Memory pre-populated. Starting Training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_heads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups[i].SetAttValue(\"SigState\", \"RED\")\n",
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SignalHeadsCollection and unpack the SignalHeads into a list by SignalController\n",
    "signal_heads = [[] for _ in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    print(SC)\n",
    "    for SG in range(signal_controllers[SC].SGs.Count):\n",
    "        print(SG)\n",
    "        signal_heads[SC].append(toList(signal_groups[SC][SG].SigHeads.GetAll())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanes = [[[] for b in range(len(signal_heads[a])) ] for a in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    for SH in range(len(signal_heads[SC])):\n",
    "        lanes[SC][SH].append(signal_heads[SC][SH].Lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser2(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(6000):\n",
    "    if i % 5 == 0:\n",
    "        Agents[0].copy_weights()\n",
    "    Agents[0].learn_batch(64, 0)\n",
    "    print(\"Epoch {}:\".format(i))\n",
    "    print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape([50,0,50,0], [1,4])))\\\n",
    "          + (\"OK\" if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1]  else \"NO\"))\n",
    "    true1 = True if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1] else False\n",
    "    print(\"Prediction for [0,50,0,50] is: {}\".format(Agents[0].model.predict(np.reshape([0,50,0,50], [1,4])))\\\n",
    "         + (\"OK\" if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1]  else \"NO\"))\n",
    "    true2 = True if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1] else False\n",
    "    if true1 and true2 == True and i>100:\n",
    "        print(\"FOUND CANDIDATE AT EPOCH {}. TERMINATING\".format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A TEST RUN\n",
    "SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "#timesteps_per_second = 10\n",
    "#Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(5).AttValue('QStops(Current,Last)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(Filename+\"bla\"):\n",
    "    Vissim.LoadNet(Filename+\"bla\")\n",
    "else:\n",
    "    raise Exception(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
