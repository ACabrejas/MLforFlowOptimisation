{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import Simulator_Functions as SF\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Model Parameters\n",
    "Random_Seed = 42\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "program = 'DQN' # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'\n",
    "state_type  = 'Queues'\n",
    "PER_activated = True\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "## Load trained model?\n",
    "Demo_Mode = False\n",
    "load_trained = False\n",
    "Quickmode = True\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling flags\n",
    "# Flag for restarting the COM Server\n",
    "reset_flag = True\n",
    "#cache_flag = False\n",
    "# If a fresh start is needed, all previous results from simulations are deleted\n",
    "Start_Fresh = True\n",
    "# Debug action\n",
    "debug_action = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 400\n",
    "partial_save_at = 100\n",
    "copy_weights_frequency = 5\n",
    "reset_frequency = 100\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = simulation_length*5\n",
    "\n",
    "## State-Action Parameters\n",
    "state_size = 4\n",
    "action_size = 5\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "memory_size = 1000\n",
    "alpha   = 0.0001\n",
    "#alpha   = 0.001\n",
    "\n",
    "gamma   = 0.95\n",
    "\n",
    "# Exploration Schedule\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "#epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes+1)) # Geometric decay\n",
    "\n",
    "# Demand Schedule\n",
    "demands = [100, 200, 400, 600, 800, 1000]\n",
    "# Session ID\n",
    "Session_ID = 'Episodes'+str(episodes)+'_Program'+program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Loading Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 1800.5 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Deploying instance of Standard Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DQN.\n",
      "Pre-Populating memory with Random Actions....\n",
      "Previous Experience Found\n",
      "Memory pre-populated. Starting Training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3af8eed77d2440ba999d4972aaec02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=400)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/400, Epsilon:1, Average reward: -188.45\n",
      "Prediction for [500,0,500,0] is: [[-162.48917  -196.98933    31.672508   28.150572   39.737335]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes400_ProgramDQN_BestAgent0_Memory.p\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Episode: 2/400, Epsilon:1.0, Average reward: -347.66\n",
      "Prediction for [500,0,500,0] is: [[-162.7689   -197.47704    30.634598   26.498503   38.63601 ]]\n",
      "Episode: 3/400, Epsilon:0.99, Average reward: -207.93\n",
      "Prediction for [500,0,500,0] is: [[-163.58284  -198.01201    29.66532    24.856731   37.60223 ]]\n",
      "Episode: 4/400, Epsilon:0.99, Average reward: -306.5\n",
      "Prediction for [500,0,500,0] is: [[-164.41486  -198.33835    28.695993   23.533579   36.3581  ]]\n",
      "Episode: 5/400, Epsilon:0.99, Average reward: -264.65\n",
      "Prediction for [500,0,500,0] is: [[-165.26828  -198.73251    27.706747   22.222958   35.13552 ]]\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 6/400, Epsilon:0.98, Average reward: -42.5\n",
      "Prediction for [500,0,500,0] is: [[-166.2006   -199.17833    26.605629   20.84406    34.165184]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes400_ProgramDQN_BestAgent0_Memory.p\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147023170, 'The remote procedure call failed.', None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-aede2fe3029b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;31m# If not the first episode, reset state at the start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n\u001b[1;32m--> 111\u001b[1;33m                                                 simulation_length, Start_Fresh, reset_flag = True)\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# Run Network Parser and ensure agents are linked to their intersections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\COMServer.py\u001b[0m in \u001b[0;36mCOMServerReload\u001b[1;34m(Vissim, model_name, vissim_working_directory, simulation_length, Start_Fresh, reset_flag)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mFilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.inpx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m## Setting Simulation End\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mLoadNet\u001b[1;34m(self, NetPath, Additive)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36m_ApplyTypes_\u001b[1;34m(self, dispid, wFlags, retType, argTypes, user, resultCLSID, *args)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_ApplyTypes_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwFlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margTypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultCLSID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvokeTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLCID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwFlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margTypes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_good_object_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultCLSID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147023170, 'The remote procedure call failed.', None, None)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # If previous agents aren't to be loaded, proceed to memory population\n",
    "    if not load_trained:\n",
    "        # Initialize simulation\n",
    "        if 'Vissim' not in globals() or Vissim == None:\n",
    "            Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                        memory_population_length, Start_Fresh,\\\n",
    "                                                                        reset_flag = True, verbose = True)\n",
    "        else:\n",
    "            Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "            Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                  memory_population_length, Start_Fresh, reset_flag = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "    Vissim.Simulation.SetAttValue('SimRes', 1)\n",
    "\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if program == \"DQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, PER_activated, DoubleDQN = False,\\\n",
    "                           Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DuelingDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, PER_activated, DoubleDQN = False,\\\n",
    "                           Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, PER_activated, DoubleDQN = True,\\\n",
    "                           Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DuelingDDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, PER_activated, DoubleDQN = True,\\\n",
    "                           Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    \n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), program))\n",
    "    \n",
    "    if Demo_Mode:\n",
    "        Agents = SF.load_agents(vissim_working_directory, model_name, Agents, Session_ID, best = True)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length, Demo_Mode, PER_activated)\n",
    "        Vissim = None\n",
    "    # Load previous trained data\n",
    "    elif load_trained:\n",
    "        Agents = SF.load_agents(vissim_working_directory, model_name, Agents, Session_ID, best = False)\n",
    "    # If previous data isn't to be loaded, have an initial longer random run to populate memory\n",
    "    else:\n",
    "        print('Pre-Populating memory with Random Actions....')\n",
    "        SF.Set_Quickmode(Vissim)\n",
    "        if PER_activated:\n",
    "            memory = SF.PER_prepopulate_memory(Agents, Vissim, state_type, state_size, memory_size, vissim_working_directory, model_name)\n",
    "        else:\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length, Demo_Mode, PER_activated)\n",
    "        print('Memory pre-populated. Starting Training.\\n')\n",
    "        \n",
    "    # Iterations of the simulation\n",
    "    for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "#        # Starting Plotting Backend\n",
    "#        if episode == 0:\n",
    "#            # Start reward plot\n",
    "#            rew,axr = plt.subplots(1,1)\n",
    "#            axr.set_xlabel('Episodes')\n",
    "#            axr.set_ylabel('Average agent reward in episode')\n",
    "#            axr.set_xlim(0,episodes)\n",
    "#            axr.set_ylim(-500,0)\n",
    "#            # Start loss plot\n",
    "#            loss,axl = plt.subplots(1,1)\n",
    "#            axl.set_xlabel('Episodes')\n",
    "#            axl.set_ylabel('Average agent loss')\n",
    "#            axl.set_xlim(0,episodes)\n",
    "#            axl.set_ylim(0,2000000)\n",
    "#            pltlive(reward_plot, episodes, rew, axr, ['r'])\n",
    "#            pltlive(loss_plot, episodes, loss, axl, ['b'])\n",
    "#        else:\n",
    "#            reward_plot[episode] = reward_storage[-1]\n",
    "#            loss_plot[episode] = Agents[0].loss[-1][0]\n",
    "#\n",
    "#            pltlive(reward_plot, episodes, rew, axr, ['r'])\n",
    "#            pltlive(loss_plot, episodes, loss, axl, ['b'])\n",
    "        \n",
    "        # Completely re-dispatch server every N iterations for performance\n",
    "        if episode % reset_frequency == 0 and episode !=0:\n",
    "            Vissim = None\n",
    "            Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                        simulation_length, Start_Fresh,\\\n",
    "                                                                        reset_flag = True, verbose = False)\n",
    "            print(\"Redispatched\")\n",
    "        else:\n",
    "            # If not the first episode, reset state at the start\n",
    "            Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                simulation_length, Start_Fresh, reset_flag = True)\n",
    "        \n",
    "        # Run Network Parser and ensure agents are linked to their intersections\n",
    "        npa = NetworkParser(Vissim) \n",
    "        for index, agent in enumerate(Agents):\n",
    "            agent.update_IDS(npa.signal_controllers_ids[index], npa)\n",
    "            agent.episode_reward = []\n",
    "        \n",
    "        # Change demand for every episode\n",
    "        if Random_Demand:\n",
    "            for vehicle_input in range(1,5):\n",
    "                Vissim.Net.VehicleInputs.ItemByKey(vehicle_input).SetAttValue('Volume(1)', demands[np.random.randint(0,len(demands)-1)])    \n",
    "        \n",
    "        # Change the random seed\n",
    "        Random_Seed += 1\n",
    "        Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "        \n",
    "        # Run Episode at maximum speed\n",
    "        SF.Set_Quickmode(Vissim)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, Demo_Mode, PER_activated)\n",
    "        \n",
    "        # Calculate episode average reward\n",
    "        reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "        best_agent_weights = SF.best_agent(reward_storage, average_reward, best_agent_weights, vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "        # Train agent with experience of episode (indicated batch size)\n",
    "        for agent in Agents:\n",
    "            agent.replay_batch(batch_size, episode)\n",
    "            #break\n",
    "        # Security save for long trainings\n",
    "        if SaveResultsAgent:\n",
    "            if (episode+1)%partial_save_at == 0:\n",
    "                SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "    #Saving agents memory, weights and optimizer\n",
    "    if SaveResultsAgent:\n",
    "        SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "        print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved. Succesfully Terminated.\")\n",
    "    \n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1,episodes,episodes).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Start reward plot\n",
    "rew,axr = plt.subplots(1,1)\n",
    "axr.set_xlabel('Episodes')\n",
    "axr.set_ylabel('Average agent reward in episode')\n",
    "axr.set_xlim(0,400)\n",
    "axr.set_ylim(-500,0)\n",
    "pltlive(reward_storage, 400,rew, axr, ['r'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axr = plt.subplots(1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot[1] = Agents[0].loss[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].loss[-1][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
