{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: GPU DEVICE NOT FOUND.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import math\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from Actor_Critic_Agents import ACAgent\n",
    "\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PER\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 200\n",
    "partial_save_at =  50 # 100 \n",
    "copy_weights_frequency = 10 #10\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "Surtrac = False\n",
    "PER_activated = True\n",
    "batch_size = 64\n",
    "memory_size = 1024\n",
    "alpha   = 0.000065\n",
    "gamma   = 0.95\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 4\n",
    "simulation_length = 1800*1 + 1\n",
    "Random_Seed = 42\n",
    "memory_population_length = batch_size*seconds_per_green*2 +1\n",
    "if PER_activated:\n",
    "    memory_population_length = int(memory_size*seconds_per_green*2) +1\n",
    "\n",
    "# Vissim autosave the result of the simulation    \n",
    "delete_results = True\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = 33 #4  17 33 49 97\n",
    "action_size = 2 # 8 \n",
    "\n",
    "\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP)  # worked with 600 and demand = {\"h\":600, 'm':300, 'l':150}\n",
    "demand_change_timesteps = 225\n",
    "demand = {\"h\":400, 'm':200, 'l':100}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep_200_A_DuelingDDQN_State_CellsSpeedOccSig_Act_phases_Rew_Queues\n"
     ]
    }
   ],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"populate\" = population of memory, generation of initial memory file\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "if mode == 'demo' :\n",
    "    simulation_length = 3600\n",
    "    demand_list = [[demand['l'], demand['l']]]\n",
    "    demand_change_timesteps = simulation_length\n",
    "    \n",
    "    \n",
    "if mode == 'test' : \n",
    "    simulation_length = 3600\n",
    "    demand_change_timesteps = 450\n",
    "    demand = {\"h\":800, 'm':400, 'l':200}\n",
    "    demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "                  [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "                  [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "                  [demand['l'], demand['h']], [demand['l'], demand['m']]]\n",
    "    delete_results = False\n",
    "\n",
    "model_name  = 'Single_Cross_Mod2'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Mod'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'  #'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "agent_type = 'DuelingDDQN'        # DQN, DuelingDQN, DDQN, DuelingDDQN AC\n",
    "reward_type = 'Queues'   \n",
    "# 'Queues'          Sum of the queues for all lanes in intersection\n",
    "# 'QueuesDiff'      Difference in queue lengths in last timestep\n",
    "# 'QueuesDiffSC'    10000* QueuesDiff - Queues^2\n",
    "# 'TotalDelayDiff'\n",
    "state_type  = 'CellsSpeedOccSig'\n",
    "#CellsSpeedOccSig'    # 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig' CellsSpeedSig \n",
    "#CellsSpeedOccSig 'CellsOccSig' 'CellsT'\n",
    "\n",
    "\n",
    "#hyper parameters of the AC agent\n",
    "if agent_type == 'AC':\n",
    "    alpha   = 0.000065\n",
    "    gamma   = 0.99\n",
    "    n_step_size = 12 # number of step in the n step learning # 32\n",
    "    PER_activated = False\n",
    "\n",
    "\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Session ID\n",
    "# Adding the state type to the Session_ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_State_\"+state_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "print(Session_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting exploration schedule\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = np.array(range(1,episodes+1))\n",
    "y_series = epsilon_sequence[0:episodes]\n",
    "plt.plot(x_series, y_series, '-b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Ratio of random exploration')\n",
    "plt.title('Exploration schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Mod2.inpx ...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Mod2.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 12289 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DuelingDDQN.\n",
      "After 0 timesteps, memory is 0.0 percent full\n",
      "After 1000 timesteps, memory is 10.84 percent full\n",
      "After 2000 timesteps, memory is 21.78 percent full\n",
      "After 3000 timesteps, memory is 32.03 percent full\n",
      "After 4000 timesteps, memory is 42.97 percent full\n",
      "After 5000 timesteps, memory is 53.52 percent full\n",
      "After 6000 timesteps, memory is 64.26 percent full\n",
      "After 7000 timesteps, memory is 75.29 percent full\n",
      "After 8000 timesteps, memory is 86.33 percent full\n",
      "After 9000 timesteps, memory is 97.36 percent full\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_200_A_DuelingDDQN_State_CellsSpeedOccSig_Act_phases_Rew_Queues\\PERPre_1024.p\n",
      "Memory pre-populated. Starting Training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1d0bbe071a474d83255d881c1a6776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/200, Epsilon:1.0, Average reward: -19.25\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_200_A_DuelingDDQN_State_CellsSpeedOccSig_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 896.4058\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1015.0174\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 872.4275\n",
      "Episode: 2/200, Epsilon:0.97, Average reward: -15.37\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_200_A_DuelingDDQN_State_CellsSpeedOccSig_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 553.7772\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 768.5810\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 689.5944\n",
      "Episode: 3/200, Epsilon:0.93, Average reward: -21.93\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 465.1435\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 337.4583\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 373.9350\n",
      "Episode: 4/200, Epsilon:0.9, Average reward: -16.75\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 308.9619\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 437.3648\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 465.4312\n",
      "Episode: 5/200, Epsilon:0.87, Average reward: -22.36\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 299.5917\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 351.8441\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 458.1626\n",
      "Episode: 6/200, Epsilon:0.84, Average reward: -21.64\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 472.8529\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 527.8293\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 521.9928\n",
      "Episode: 7/200, Epsilon:0.81, Average reward: -24.4\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 608.8944\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 416.3454\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 503.3360\n",
      "Episode: 8/200, Epsilon:0.78, Average reward: -29.53\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 418.4678\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 659.7575\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 431.4606\n",
      "Episode: 9/200, Epsilon:0.76, Average reward: -31.76\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1028.0714\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 967.5128\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 598.0342\n",
      "Episode: 10/200, Epsilon:0.73, Average reward: -28.15\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 913.6962\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 806.8885\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 744.9506\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 11/200, Epsilon:0.71, Average reward: -19.78\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 926.0551\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1017.7238\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 880.4524\n",
      "Episode: 12/200, Epsilon:0.68, Average reward: -24.67\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 646.2998\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 575.0091\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 941.8276\n",
      "Episode: 13/200, Epsilon:0.66, Average reward: -24.02\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 630.9771\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 707.8686\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 474.3000\n",
      "Episode: 14/200, Epsilon:0.64, Average reward: -19.33\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 553.5449\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 503.4326\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 571.5378\n",
      "Episode: 15/200, Epsilon:0.62, Average reward: -18.87\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 390.7853\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 594.4929\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 420.0549\n",
      "Episode: 16/200, Epsilon:0.59, Average reward: -17.06\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 347.5827\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 400.3851\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 459.9311\n",
      "Episode: 17/200, Epsilon:0.57, Average reward: -28.26\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 775.0385\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 574.4644\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 470.3544\n",
      "Episode: 18/200, Epsilon:0.55, Average reward: -20.2\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 481.3550\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 656.9746\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 313.3001\n",
      "Episode: 19/200, Epsilon:0.54, Average reward: -32.1\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 988.6285\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 679.2272\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 410.3983\n",
      "Episode: 20/200, Epsilon:0.52, Average reward: -25.59\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 764.6043\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 824.6354\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 631.7787\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 21/200, Epsilon:0.5, Average reward: -20.04\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 606.8408\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 918.5817\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 768.3658\n",
      "Episode: 22/200, Epsilon:0.48, Average reward: -30.64\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 957.6219\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1005.4328\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1186.6135\n",
      "Episode: 23/200, Epsilon:0.47, Average reward: -24.11\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 815.1196\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 609.8177\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 537.8895\n",
      "Episode: 24/200, Epsilon:0.45, Average reward: -29.2\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 530.2708\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 635.5590\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 602.2183\n",
      "Episode: 25/200, Epsilon:0.43, Average reward: -19.01\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 568.0295\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 531.9049\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 995.5681\n",
      "Episode: 26/200, Epsilon:0.42, Average reward: -62.17\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1661.6730\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1534.9476\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1692.8937\n",
      "Episode: 27/200, Epsilon:0.41, Average reward: -25.23\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1905.9363\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1453.7096\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1286.2229\n",
      "Episode: 28/200, Epsilon:0.39, Average reward: -41.12\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1858.2502\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2181.2803\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1927.1704\n",
      "Episode: 29/200, Epsilon:0.38, Average reward: -17.99\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1502.5698\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1761.5652\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1903.4899\n",
      "Episode: 30/200, Epsilon:0.37, Average reward: -21.58\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1115.7651\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 791.5306\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 793.5051\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 31/200, Epsilon:0.35, Average reward: -35.04\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1261.7134\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 781.5139\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 822.1723\n",
      "Episode: 32/200, Epsilon:0.34, Average reward: -24.63\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1201.2139\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1320.1895\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 901.2043\n",
      "Episode: 33/200, Epsilon:0.33, Average reward: -16.37\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 676.9802\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 490.8328\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 735.5977\n",
      "Episode: 34/200, Epsilon:0.32, Average reward: -22.17\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 617.3882\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 724.6389\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 751.9211\n",
      "Episode: 35/200, Epsilon:0.31, Average reward: -28.94\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 553.0918\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 680.1845\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 740.8200\n",
      "Episode: 36/200, Epsilon:0.3, Average reward: -22.22\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 434.6578\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 503.2054\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 530.9536\n",
      "Episode: 37/200, Epsilon:0.29, Average reward: -19.37\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 523.3653\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 485.5764\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 363.8354\n",
      "Episode: 38/200, Epsilon:0.28, Average reward: -15.63\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 501.6386\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 579.4336\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 328.1256\n",
      "Episode: 39/200, Epsilon:0.27, Average reward: -23.37\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 452.8691\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 503.9107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 532.7555\n",
      "Episode: 40/200, Epsilon:0.26, Average reward: -20.74\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 497.2642\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 265.5108\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 361.3880\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 41/200, Epsilon:0.25, Average reward: -19.75\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 423.4166\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 547.8025\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 444.7483\n",
      "Episode: 42/200, Epsilon:0.24, Average reward: -29.97\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 709.0507\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 578.7167\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 566.8744\n",
      "Episode: 43/200, Epsilon:0.23, Average reward: -17.97\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 584.2830\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 609.8697\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 727.6392\n",
      "Episode: 44/200, Epsilon:0.22, Average reward: -19.97\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 657.4014\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 678.5173\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 578.4875\n",
      "Episode: 45/200, Epsilon:0.22, Average reward: -20.02\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 691.4341\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 530.3135\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 922.1097\n",
      "Episode: 46/200, Epsilon:0.21, Average reward: -12.75\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_200_A_DuelingDDQN_State_CellsSpeedOccSig_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 579.6299\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 611.1572\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 554.6434\n",
      "Episode: 47/200, Epsilon:0.2, Average reward: -15.06\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 363.9455\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 466.7980\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 208.9045\n",
      "Episode: 48/200, Epsilon:0.2, Average reward: -21.4\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 333.0073\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 305.1378\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 242.8012\n",
      "Episode: 49/200, Epsilon:0.19, Average reward: -24.28\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 530.5463\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 471.1953\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 330.9247\n",
      "Episode: 50/200, Epsilon:0.18, Average reward: -27.49\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 567.7756\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 557.7570\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 731.2620\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 50.\n",
      "Episode: 51/200, Epsilon:0.18, Average reward: -22.14\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 618.7515\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 699.2707\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 698.9043\n",
      "Episode: 52/200, Epsilon:0.17, Average reward: -17.01\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 760.5947\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 632.2486\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 672.8641\n",
      "Episode: 53/200, Epsilon:0.16, Average reward: -44.29\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1519.2858\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1475.2349\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1521.7155\n",
      "Episode: 54/200, Epsilon:0.16, Average reward: -23.83\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1047.2794\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1389.2148\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1073.7021\n",
      "Episode: 55/200, Epsilon:0.15, Average reward: -22.7\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1382.9865\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1025.7269\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 952.4099\n",
      "Episode: 56/200, Epsilon:0.15, Average reward: -16.36\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1369.7727\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1168.5596\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1101.6177\n",
      "Episode: 57/200, Epsilon:0.14, Average reward: -17.71\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 484.1643\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 706.8922\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 366.6967\n",
      "Episode: 58/200, Epsilon:0.14, Average reward: -18.87\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 242.1104\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 363.5800\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 290.6801\n",
      "Episode: 59/200, Epsilon:0.13, Average reward: -33.42\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 491.0061\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 566.4891\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 441.5212\n",
      "Episode: 60/200, Epsilon:0.13, Average reward: -13.15\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 437.3005\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 454.4586\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 350.1597\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 61/200, Epsilon:0.12, Average reward: -20.05\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 498.1561\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 564.5952\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 649.1112\n",
      "Episode: 62/200, Epsilon:0.12, Average reward: -17.66\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 601.4912\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 621.1961\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 555.0481\n",
      "Episode: 63/200, Epsilon:0.12, Average reward: -15.78\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 437.7459\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 482.9806\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 429.4659\n",
      "Episode: 64/200, Epsilon:0.11, Average reward: -16.26\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 395.0529\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 293.6190\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 251.2062\n",
      "Episode: 65/200, Epsilon:0.11, Average reward: -15.69\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 368.4201\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 277.0674\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 254.0343\n",
      "Episode: 66/200, Epsilon:0.1, Average reward: -20.78\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 379.5795\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 414.5043\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 269.2007\n",
      "Episode: 67/200, Epsilon:0.1, Average reward: -14.38\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 247.2078\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 381.4814\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 236.1837\n",
      "Episode: 68/200, Epsilon:0.1, Average reward: -14.31\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 377.3382\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 436.4868\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 338.9738\n",
      "Episode: 69/200, Epsilon:0.09, Average reward: -16.43\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 301.5381\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 410.6583\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 250.9603\n",
      "Episode: 70/200, Epsilon:0.09, Average reward: -16.23\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 286.8377\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 247.5050\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 300.7717\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 71/200, Epsilon:0.09, Average reward: -19.41\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 319.6269\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 387.3260\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 338.4760\n",
      "Episode: 72/200, Epsilon:0.09, Average reward: -18.86\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 312.0534\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 470.4276\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 484.3875\n",
      "Episode: 73/200, Epsilon:0.08, Average reward: -22.4\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 386.1755\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 288.9152\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 641.1240\n",
      "Episode: 74/200, Epsilon:0.08, Average reward: -16.77\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 591.3020\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 435.5295\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 416.1518\n",
      "Episode: 75/200, Epsilon:0.08, Average reward: -19.53\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 471.5220\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 586.6683\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 317.8129\n",
      "Episode: 76/200, Epsilon:0.07, Average reward: -17.44\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 402.6306\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 245.3988\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 650.2036\n",
      "Episode: 77/200, Epsilon:0.07, Average reward: -19.65\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 378.4791\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 351.4486\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 442.5504\n",
      "Episode: 78/200, Epsilon:0.07, Average reward: -14.4\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 344.9821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 400.5085\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 498.2178\n",
      "Episode: 79/200, Epsilon:0.07, Average reward: -18.1\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 644.0018\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 441.1169\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 465.8049\n",
      "Episode: 80/200, Epsilon:0.06, Average reward: -21.07\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 464.3833\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 514.5818\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 427.6930\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 81/200, Epsilon:0.06, Average reward: -17.21\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 602.9637\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 428.9730\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 463.4122\n",
      "Episode: 82/200, Epsilon:0.06, Average reward: -18.21\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 485.5234\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 598.4662\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 371.7489\n",
      "Episode: 83/200, Epsilon:0.06, Average reward: -25.25\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 637.2059\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 534.0106\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 778.9514\n",
      "Episode: 84/200, Epsilon:0.06, Average reward: -22.59\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 661.9107\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 672.0025\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 724.9069\n",
      "Episode: 85/200, Epsilon:0.05, Average reward: -16.87\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 600.4940\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 669.4595\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 526.8651\n",
      "Episode: 86/200, Epsilon:0.05, Average reward: -20.67\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 499.9693\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 765.8076\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 737.9340\n",
      "Episode: 87/200, Epsilon:0.05, Average reward: -11.48\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_200_A_DuelingDDQN_State_CellsSpeedOccSig_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 490.2209\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 681.0240\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 484.1576\n",
      "Episode: 88/200, Epsilon:0.05, Average reward: -16.02\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 361.6824\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 302.7800\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 361.5058\n",
      "Episode: 89/200, Epsilon:0.05, Average reward: -25.4\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 516.3049\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 342.0612\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 548.1569\n",
      "Episode: 90/200, Epsilon:0.05, Average reward: -17.7\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 367.8637\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 452.2859\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 437.9461\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 91/200, Epsilon:0.04, Average reward: -12.99\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 574.2576\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 572.7120\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 622.1622\n",
      "Episode: 92/200, Epsilon:0.04, Average reward: -21.25\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 593.6705\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 584.6439\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 517.6890\n",
      "Episode: 93/200, Epsilon:0.04, Average reward: -16.13\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 424.7337\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 720.2018\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 593.5203\n",
      "Episode: 94/200, Epsilon:0.04, Average reward: -22.43\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 417.9993\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 391.4880\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 448.9694\n",
      "Episode: 95/200, Epsilon:0.04, Average reward: -16.14\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 476.4628\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 570.6965\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 555.7585\n",
      "Episode: 96/200, Epsilon:0.04, Average reward: -23.12\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 585.9255\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 471.0484\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 452.4938\n",
      "Episode: 97/200, Epsilon:0.04, Average reward: -17.0\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 463.4537\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 753.9960\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 700.4413\n",
      "Episode: 98/200, Epsilon:0.03, Average reward: -21.64\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 560.3585\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 445.9590\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 446.7706\n",
      "Episode: 99/200, Epsilon:0.03, Average reward: -16.54\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 598.0063\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 617.6102\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 656.2448\n",
      "Episode: 100/200, Epsilon:0.03, Average reward: -16.83\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 291.4234\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 472.8852\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 416.8804\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 100.\n",
      "Episode: 101/200, Epsilon:0.03, Average reward: -18.63\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 572.5595\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 555.8553\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 643.7640\n",
      "Episode: 102/200, Epsilon:0.03, Average reward: -15.56\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 460.7923\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 476.9472\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 815.0280\n",
      "Episode: 103/200, Epsilon:0.03, Average reward: -25.8\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 770.7604\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1134.6693\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 611.2421\n",
      "Episode: 104/200, Epsilon:0.03, Average reward: -21.05\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 770.0385\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 590.6746\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 788.3823\n",
      "Episode: 105/200, Epsilon:0.03, Average reward: -18.01\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 540.8839\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 462.2820\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 386.3207\n",
      "Episode: 106/200, Epsilon:0.03, Average reward: -17.96\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 594.4440\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 710.4205\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 674.9464\n",
      "Episode: 107/200, Epsilon:0.03, Average reward: -24.56\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 981.9986\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 598.4157\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 554.8459\n",
      "Episode: 108/200, Epsilon:0.02, Average reward: -21.87\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 615.6877\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 632.7448\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 483.1027\n",
      "Episode: 109/200, Epsilon:0.02, Average reward: -18.58\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 722.2479\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 658.7316\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 703.9577\n",
      "Episode: 110/200, Epsilon:0.02, Average reward: -17.34\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 633.7316\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 669.0245\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 568.6530\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 111/200, Epsilon:0.02, Average reward: -21.33\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 938.8937\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 809.3937\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 882.5213\n",
      "Episode: 112/200, Epsilon:0.02, Average reward: -22.9\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 831.8314\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 769.5151\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 791.3085\n",
      "Episode: 113/200, Epsilon:0.02, Average reward: -22.19\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 814.5500\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 715.0312\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 897.0665\n",
      "Episode: 114/200, Epsilon:0.02, Average reward: -14.6\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 544.7001\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 677.2778\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 903.1195\n",
      "Episode: 115/200, Epsilon:0.02, Average reward: -18.56\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 582.3845\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 778.3782\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1018.7227\n",
      "Episode: 116/200, Epsilon:0.02, Average reward: -15.53\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 722.6175\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 497.2013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 539.8972\n",
      "Episode: 117/200, Epsilon:0.02, Average reward: -17.24\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 683.1888\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 715.3083\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 448.5906\n",
      "Episode: 118/200, Epsilon:0.02, Average reward: -20.18\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 545.5105\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 573.4406\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 638.2686\n",
      "Episode: 119/200, Epsilon:0.02, Average reward: -22.74\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 689.1669\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 569.3640\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 619.4958\n",
      "Episode: 120/200, Epsilon:0.02, Average reward: -16.2\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 485.4431\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 592.8677\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 774.0702\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 121/200, Epsilon:0.02, Average reward: -19.99\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 993.3126\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 992.7565\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 871.9218\n",
      "Episode: 122/200, Epsilon:0.01, Average reward: -22.0\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1315.2599\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 701.3358\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 919.0803\n",
      "Episode: 123/200, Epsilon:0.01, Average reward: -22.29\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 799.9117\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 733.6121\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 674.4642\n",
      "Episode: 124/200, Epsilon:0.01, Average reward: -20.5\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 503.0435\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 817.7173\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1026.0916\n",
      "Episode: 125/200, Epsilon:0.01, Average reward: -15.64\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 618.2669\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 679.0406\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1023.9313\n",
      "Episode: 126/200, Epsilon:0.01, Average reward: -36.77\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1388.7599\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1335.5996\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1344.0338\n",
      "Episode: 127/200, Epsilon:0.01, Average reward: -15.79\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1321.1943\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 703.6856\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 850.2914\n",
      "Episode: 128/200, Epsilon:0.01, Average reward: -26.07\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1366.9524\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1245.2552\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1077.6893\n",
      "Episode: 129/200, Epsilon:0.01, Average reward: -23.16\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1084.3463\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1235.4958\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1043.4700\n",
      "Episode: 130/200, Epsilon:0.01, Average reward: -19.43\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 967.5707\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1676.7081\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2010.9457\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 131/200, Epsilon:0.01, Average reward: -38.47\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1864.7489\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1698.0139\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1780.5531\n",
      "Episode: 132/200, Epsilon:0.01, Average reward: -17.14\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1212.9219\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1434.5947\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 979.1285\n",
      "Episode: 133/200, Epsilon:0.01, Average reward: -19.98\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1247.9850\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 975.7901\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1446.2340\n",
      "Episode: 134/200, Epsilon:0.01, Average reward: -19.51\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1657.7341\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1406.0933\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1579.1398\n",
      "Episode: 135/200, Epsilon:0.01, Average reward: -18.99\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1576.8740\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1047.4102\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1175.1427\n",
      "Episode: 136/200, Epsilon:0.01, Average reward: -29.61\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1408.3179\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 800.6010\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1314.4348\n",
      "Episode: 137/200, Epsilon:0.01, Average reward: -23.6\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 798.0623\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1017.8218\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1048.8264\n",
      "Episode: 138/200, Epsilon:0.01, Average reward: -23.08\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1714.3273\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1044.4436\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1135.2114\n",
      "Episode: 139/200, Epsilon:0.01, Average reward: -14.41\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1175.2804\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1150.6246\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1102.0388\n",
      "Episode: 140/200, Epsilon:0.01, Average reward: -38.13\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1322.4142\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1421.7817\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 918.5378\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 141/200, Epsilon:0.01, Average reward: -16.66\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1315.3425\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1712.7583\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1832.9961\n",
      "Episode: 142/200, Epsilon:0.01, Average reward: -16.84\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1390.1327\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1758.3064\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 977.6750\n",
      "Episode: 143/200, Epsilon:0.01, Average reward: -16.5\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1384.7283\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 869.9996\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1193.6688\n",
      "Episode: 144/200, Epsilon:0.01, Average reward: -17.31\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1367.5470\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1040.3893\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1544.5779\n",
      "Episode: 145/200, Epsilon:0.01, Average reward: -40.23\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1377.2413\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2066.9917\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1490.1927\n",
      "Episode: 146/200, Epsilon:0.01, Average reward: -24.44\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1479.7889\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1303.8514\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1320.7695\n",
      "Episode: 147/200, Epsilon:0.01, Average reward: -88.02\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 4403.6494\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3302.5396\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3354.4653\n",
      "Episode: 148/200, Epsilon:0.01, Average reward: -35.96\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3418.6765\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3679.7742\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3568.4761\n",
      "Episode: 149/200, Epsilon:0.01, Average reward: -27.32\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3555.5232\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3294.0095\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3499.4231\n",
      "Episode: 150/200, Epsilon:0.01, Average reward: -25.05\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3086.3823\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3174.7632\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3808.1567\n",
      "Weights succesfully copied to Target model.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 150.\n",
      "Episode: 151/200, Epsilon:0.01, Average reward: -25.05\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1580.1222\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2236.5352\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1639.6200\n",
      "Episode: 152/200, Epsilon:0.01, Average reward: -19.34\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2069.3538\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1771.5264\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1808.6710\n",
      "Episode: 153/200, Epsilon:0.01, Average reward: -20.21\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1758.1956\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1630.6549\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1463.3082\n",
      "Episode: 154/200, Epsilon:0.0, Average reward: -22.23\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1337.7793\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1398.9756\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1362.0778\n",
      "Episode: 155/200, Epsilon:0.0, Average reward: -22.94\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1229.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1841.2188\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1406.9608\n",
      "Episode: 156/200, Epsilon:0.0, Average reward: -21.19\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1383.7780\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1775.2460\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1217.7772\n",
      "Episode: 157/200, Epsilon:0.0, Average reward: -19.62\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1551.3898\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1443.9872\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1543.8844\n",
      "Episode: 158/200, Epsilon:0.0, Average reward: -24.25\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1489.2968\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1701.6819\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1335.3025\n",
      "Episode: 159/200, Epsilon:0.0, Average reward: -26.24\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1512.9697\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1757.7792\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1557.5071\n",
      "Episode: 160/200, Epsilon:0.0, Average reward: -25.6\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1297.0581\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1458.8673\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1085.9969\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 161/200, Epsilon:0.0, Average reward: -40.3\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1827.2115\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1822.2773\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1759.9857\n",
      "Episode: 162/200, Epsilon:0.0, Average reward: -23.88\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1896.3812\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2122.1562\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1461.2904\n",
      "Episode: 163/200, Epsilon:0.0, Average reward: -25.45\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1539.3762\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2521.8545\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1692.7052\n",
      "Episode: 164/200, Epsilon:0.0, Average reward: -87.68\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3646.5811\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3864.5991\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3410.0032\n",
      "Episode: 165/200, Epsilon:0.0, Average reward: -23.19\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 4730.2222\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 4094.1189\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2672.3494\n",
      "Episode: 166/200, Epsilon:0.0, Average reward: -18.65\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3900.8025\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3646.3855\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3843.8406\n",
      "Episode: 167/200, Epsilon:0.0, Average reward: -26.64\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3388.0754\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 4524.1284\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3272.1614\n",
      "Episode: 168/200, Epsilon:0.0, Average reward: -23.13\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3135.1108\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3931.0635\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2668.4958\n",
      "Episode: 169/200, Epsilon:0.0, Average reward: -18.43\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1849.0908\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2186.2405\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 1974.3793\n",
      "Episode: 170/200, Epsilon:0.0, Average reward: -50.06\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2306.8794\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2396.0303\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3222.3794\n",
      "Weights succesfully copied to Target model.\n",
      "Episode: 171/200, Epsilon:0.0, Average reward: -152.92\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 7104.4434\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 6905.2212\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 6859.9351\n",
      "Episode: 172/200, Epsilon:0.0, Average reward: -36.33\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 5922.1421\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 7882.2168\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 8019.2842\n",
      "Episode: 173/200, Epsilon:0.0, Average reward: -20.23\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 8432.2109\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 6893.0566\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 7897.2856\n",
      "Episode: 174/200, Epsilon:0.0, Average reward: -25.79\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 6379.0732\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 7841.4663\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 6305.6641\n",
      "Episode: 175/200, Epsilon:0.0, Average reward: -46.4\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3357.0190\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 2813.3425\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 3700.9033\n",
      "Episode: 176/200, Epsilon:0.0, Average reward: -134.13\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 7328.6523\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 5462.7720\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 7842.9648\n",
      "Episode: 177/200, Epsilon:0.0, Average reward: -165.22\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 11921.1621\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 13081.7490\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 11593.8672\n",
      "Episode: 178/200, Epsilon:0.0, Average reward: -90.01\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 13883.7695\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 15025.8330\n",
      "Train on 64 samples\n",
      "64/64 - 0s - loss: 12072.5625\n"
     ]
    }
   ],
   "source": [
    "# Have to find a way to reduce entropy over time entropy = exploration\n",
    "\n",
    "## Converging network\n",
    "# - reward queue, state queue\n",
    "# converging with updates every steps and entropy = 0.00001 and 1 core layer of 42\n",
    "# converging well with updates every steps and entropy = 0.00001 and 1 core layer of 128\n",
    "\n",
    "# - reward queue state queues + sig\n",
    "# converging well with updates every steps and entropy = 0.00001 and 1 core layer of 128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                memory_population_length if mode == 'training'\\\n",
    "                                                                and PER_activated else simulation_length ,\\\n",
    "                                                                timesteps_per_second,\\\n",
    "                                                                delete_results = delete_results, verbose = True)\n",
    "    \n",
    "    SF.Select_Vissim_Mode(Vissim,mode)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or agent_type ==\"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or agent_type == \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif agent_type in ['AC'] :\n",
    "        Agents = [ACAgent(state_size, action_size, ID, state_type, npa, n_step_size, gamma, alpha, Vissim) for ID in npa.signal_controllers_ids] \n",
    "        for agent in Agents:\n",
    "            # to initialise the computational graph ot the model (I am sure there is a better way to to this)\n",
    "            agent.test()\n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION, POPULATION, DEBUG OR TEST ITERATION\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents, reward_storage = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = True)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0 #1\n",
    "        \n",
    "        # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"populate\":\n",
    "            if PER_activated:\n",
    "                memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                                vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                                seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                                demand_list, demand_change_timesteps, PER_activated)\n",
    "                print(\"PER memory prepopulated with {} entries\".format(memory_size))\n",
    "        \n",
    "        Vissim = None\n",
    "     \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\":\n",
    "        \n",
    "        if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "            \n",
    "            # Load previous memory if available, else create it\n",
    "            SF.Select_Vissim_Mode(Vissim, mode)\n",
    "            memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                            vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                            seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                            demand_list, demand_change_timesteps, PER_activated)\n",
    "            print('Memory pre-populated. Starting Training.\\n')\n",
    "        \n",
    "        # Iterations of the simulation\n",
    "        runflag = True\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                # Run Network Parser and ensure agents are linked to their intersections\n",
    "                npa = NetworkParser(Vissim)\n",
    "                for index, agent in enumerate(Agents):\n",
    "                    agent.update_IDS(agent.signal_id, npa)\n",
    "                    agent.episode_reward = []\n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "                    \n",
    "            # Run Episode at maximum speed\n",
    "            SF.Select_Vissim_Mode(Vissim, mode)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "           \n",
    "             # Train agent with experience of episode and copy weights when necessary\n",
    "            # Update exploration rate\n",
    "            for agent in Agents: \n",
    "                if agent.type == 'DQN':\n",
    "                    for _ in range(3):\n",
    "                        agent.learn_batch(batch_size, episode)\n",
    "                # Copy weights \n",
    "                    if (episode+1) % agent.copy_weights_frequency == 0 and episode != 0:\n",
    "                        agent.copy_weights()\n",
    "                    agent.epsilon = epsilon_sequence[episode+1]\n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].DoubleDQN\n",
    "# average reward 17.69--25 full random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHORT PRETRAINED FROM MEMORY DEMO\n",
    "# Initialize storage\n",
    "reward_storage = []\n",
    "best_agent_weights = []\n",
    "best_agent_memory = []\n",
    "reward_plot = np.zeros([episodes,])\n",
    "loss_plot = np.zeros([episodes,])\n",
    "\n",
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n",
    "\n",
    "# Setting Random Seed\n",
    "Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "# Deploy Network Parser (crawl network)\n",
    "npa = NetworkParser(Vissim)\n",
    "print('NetworkParser has succesfully crawled the model network.')\n",
    "\n",
    "# Initialize agents\n",
    "if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "    Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                       gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                       DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                       Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "    agents_deployed = True\n",
    "else:\n",
    "    print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "    quit()\n",
    "if agents_deployed:\n",
    "    print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "\n",
    "#    memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "#                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "#                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "#                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "#print('Memory pre-populated. Starting Training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_heads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups[i].SetAttValue(\"SigState\", \"RED\")\n",
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SignalHeadsCollection and unpack the SignalHeads into a list by SignalController\n",
    "signal_heads = [[] for _ in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    print(SC)\n",
    "    for SG in range(signal_controllers[SC].SGs.Count):\n",
    "        print(SG)\n",
    "        signal_heads[SC].append(toList(signal_groups[SC][SG].SigHeads.GetAll())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanes = [[[] for b in range(len(signal_heads[a])) ] for a in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    for SH in range(len(signal_heads[SC])):\n",
    "        lanes[SC][SH].append(signal_heads[SC][SH].Lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser2(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(6000):\n",
    "    if i % 5 == 0:\n",
    "        Agents[0].copy_weights()\n",
    "    Agents[0].learn_batch(64, 0)\n",
    "    print(\"Epoch {}:\".format(i))\n",
    "    print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape([50,0,50,0], [1,4])))\\\n",
    "          + (\"OK\" if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1]  else \"NO\"))\n",
    "    true1 = True if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1] else False\n",
    "    print(\"Prediction for [0,50,0,50] is: {}\".format(Agents[0].model.predict(np.reshape([0,50,0,50], [1,4])))\\\n",
    "         + (\"OK\" if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1]  else \"NO\"))\n",
    "    true2 = True if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1] else False\n",
    "    if true1 and true2 == True and i>100:\n",
    "        print(\"FOUND CANDIDATE AT EPOCH {}. TERMINATING\".format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A TEST RUN\n",
    "SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "#timesteps_per_second = 10\n",
    "#Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(5).AttValue('QStops(Current,Last)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(Filename+\"bla\"):\n",
    "    Vissim.LoadNet(Filename+\"bla\")\n",
    "else:\n",
    "    raise Exception(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.SetAttValue('SimPeriod', sim_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
