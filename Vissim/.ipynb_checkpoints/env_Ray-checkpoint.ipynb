{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_critic_class import ACAgent\n",
    "from MasterAC_Agent import MasterAC_Agent\n",
    "from MasterDQN_Agent import MasterDQN_Agent\n",
    "\n",
    "# Network Specific Libraries\n",
    "from Balance_Functions import balance_dictionary\n",
    "\n",
    "# General Libraries\n",
    "import numpy as np \n",
    "import pylab as plt\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = \"E:\\Backup - Onedrive\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\"\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "agent_type = 'AC'\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Agent hyperparameters\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "timesteps_per_second = 1\n",
    "\n",
    "\n",
    "# for the monitoring only for AC\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiAc_Agents.train(1000)\n",
    "\n",
    "Balance_MultiAc_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n",
    "\n",
    "Balance_MultiAc_Agents.load(best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Agents = []\n",
    "for idx, info in Balance_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        \n",
    "        \n",
    "        # Only for AC\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance RL DQN Partial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.log10(1000)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance_int3_default_actions_500_9000_DDQN\n"
     ]
    }
   ],
   "source": [
    "intersection = 3\n",
    "map_name  = 'Balance_int'+str(intersection)\n",
    "model_name = map_name\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "vissim_working_directory = \"E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "Session_ID = map_name + \"_\" + actions + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type\n",
    "print(\"Current simulation: {}\".format(Session_ID))\n",
    "\n",
    "print(Session_ID)\n",
    "## Simulation Parameters\n",
    "Random_Seed = 42\n",
    "sim_length = 9001\n",
    "timesteps_per_second = 1\n",
    "agent_type = \"DDQN\"\n",
    "actions = 'default_actions'     # 'default_actions' or 'all_actions'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 500\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 5000\n",
    "batch_size = 128\n",
    "learning_iterations_per_episode = 10\n",
    "\n",
    "alpha = 0.00005\n",
    "gamma = 0.95\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "if intersection == \"1_2_4\":\n",
    "    intersection = 1\n",
    "elif intersection == \"11_12\":\n",
    "    intersection = 11\n",
    "partial_dictionary = {\"junctions\": { (intersection-1) : Balance_dictionary[\"junctions\"][intersection-1]},\\\n",
    "                      \"demand\": Balance_dictionary[\"demand\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 2: SETTING UP AGENT\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 14)]              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 48)                720       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 4)                 196       \n",
      "=================================================================\n",
      "Total params: 5,620\n",
      "Trainable params: 5,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, partial_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, learning_iterations, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience file not found. Generating now...\n",
      "Working Directory set to: E:\\OneDrive - University of Warwick\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int3.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 9001 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.1 seconds.\n",
      "\n",
      "After 0 actions taken by the Agents,  Agent 0 memory is 0.0 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 0 memory is 20.0 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 0 memory is 40.0 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 0 memory is 60.0 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 0 memory is 80.0 percent full\n",
      "Memory filled. Saving as:E:\\OneDrive - University of Warwick\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Balance_int3\\Agents_Results\\DQN\\Agent0_PERPre_5000.p\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: E:\\OneDrive - University of Warwick\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int3.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 9001 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.1 seconds.\n",
      "\n",
      "Episode 1: Finished running.\n",
      "Agent 0, Average Reward: -916.8\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 245221.5938\n",
      "256/256 - 0s - loss: 249663.8281\n",
      "256/256 - 0s - loss: 251484.8438\n",
      "256/256 - 0s - loss: 246123.2812\n",
      "256/256 - 0s - loss: 253311.5312\n",
      "Reducing exploration for all agents to 0.9863\n",
      "\n",
      "Episode 2: Starting computation.\n",
      "Episode 2: Finished running.\n",
      "Agent 0, Average Reward: -875.62\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 247008.4375\n",
      "256/256 - 0s - loss: 246638.4688\n",
      "256/256 - 0s - loss: 246542.7500\n",
      "256/256 - 0s - loss: 242941.3750\n",
      "256/256 - 0s - loss: 249870.8750\n",
      "Reducing exploration for all agents to 0.9727\n",
      "\n",
      "Episode 3: Starting computation.\n",
      "Episode 3: Finished running.\n",
      "Agent 0, Average Reward: -943.26\n",
      "256/256 - 0s - loss: 246252.8750\n",
      "256/256 - 0s - loss: 242396.1250\n",
      "256/256 - 0s - loss: 242216.3281\n",
      "256/256 - 0s - loss: 250574.2812\n",
      "256/256 - 0s - loss: 240537.5000\n",
      "Reducing exploration for all agents to 0.9593\n",
      "\n",
      "Episode 4: Starting computation.\n",
      "Episode 4: Finished running.\n",
      "Agent 0, Average Reward: -878.23\n",
      "256/256 - 0s - loss: 240515.9688\n",
      "256/256 - 0s - loss: 248675.5000\n",
      "256/256 - 0s - loss: 239581.1875\n",
      "256/256 - 0s - loss: 243279.4062\n",
      "256/256 - 0s - loss: 238153.6562\n",
      "Reducing exploration for all agents to 0.9461\n",
      "\n",
      "Episode 5: Starting computation.\n",
      "Episode 5: Finished running.\n",
      "Agent 0, Average Reward: -832.38\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 231124.9062\n",
      "256/256 - 0s - loss: 218967.5625\n",
      "256/256 - 0s - loss: 231296.5312\n",
      "256/256 - 0s - loss: 232753.6094\n",
      "256/256 - 0s - loss: 231538.4688\n",
      "Reducing exploration for all agents to 0.9331\n",
      "\n",
      "Episode 6: Starting computation.\n",
      "Episode 6: Finished running.\n",
      "Agent 0, Average Reward: -826.08\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 213647.1562\n",
      "256/256 - 0s - loss: 217691.7812\n",
      "256/256 - 0s - loss: 217577.2188\n",
      "256/256 - 0s - loss: 223766.5469\n",
      "256/256 - 0s - loss: 218101.5781\n",
      "Reducing exploration for all agents to 0.9203\n",
      "\n",
      "Episode 7: Starting computation.\n",
      "Episode 7: Finished running.\n",
      "Agent 0, Average Reward: -843.84\n",
      "256/256 - 0s - loss: 220746.7656\n",
      "256/256 - 0s - loss: 225764.8438\n",
      "256/256 - 0s - loss: 222468.0312\n",
      "256/256 - 0s - loss: 222676.5312\n",
      "256/256 - 0s - loss: 220815.5625\n",
      "Reducing exploration for all agents to 0.9076\n",
      "\n",
      "Episode 8: Starting computation.\n",
      "Episode 8: Finished running.\n",
      "Agent 0, Average Reward: -892.56\n",
      "256/256 - 0s - loss: 219362.7656\n",
      "256/256 - 0s - loss: 219109.0781\n",
      "256/256 - 0s - loss: 220469.4375\n",
      "256/256 - 0s - loss: 213985.0938\n",
      "256/256 - 0s - loss: 223778.1094\n",
      "Reducing exploration for all agents to 0.8952\n",
      "\n",
      "Episode 9: Starting computation.\n",
      "Episode 9: Finished running.\n",
      "Agent 0, Average Reward: -797.83\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 215079.5000\n",
      "256/256 - 0s - loss: 219573.4531\n",
      "256/256 - 0s - loss: 213368.8438\n",
      "256/256 - 0s - loss: 210711.6250\n",
      "256/256 - 0s - loss: 206844.9688\n",
      "Reducing exploration for all agents to 0.8829\n",
      "\n",
      "Episode 10: Starting computation.\n",
      "Episode 10: Finished running.\n",
      "Agent 0, Average Reward: -728.86\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 202244.8438\n",
      "256/256 - 0s - loss: 197332.6562\n",
      "256/256 - 0s - loss: 198911.8281\n",
      "256/256 - 0s - loss: 201826.7812\n",
      "256/256 - 0s - loss: 197857.6875\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.8707\n",
      "\n",
      "Episode 11: Starting computation.\n",
      "Episode 11: Finished running.\n",
      "Agent 0, Average Reward: -759.16\n",
      "256/256 - 0s - loss: 202647.4844\n",
      "256/256 - 0s - loss: 198468.6562\n",
      "256/256 - 0s - loss: 200839.9375\n",
      "256/256 - 0s - loss: 208362.2500\n",
      "256/256 - 0s - loss: 196639.4531\n",
      "Reducing exploration for all agents to 0.8588\n",
      "\n",
      "Episode 12: Starting computation.\n",
      "Episode 12: Finished running.\n",
      "Agent 0, Average Reward: -669.71\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 176273.8750\n",
      "256/256 - 0s - loss: 177897.4375\n",
      "256/256 - 0s - loss: 185070.7188\n",
      "256/256 - 0s - loss: 186180.9375\n",
      "256/256 - 0s - loss: 182649.6562\n",
      "Reducing exploration for all agents to 0.8469\n",
      "\n",
      "Episode 13: Starting computation.\n",
      "Episode 13: Finished running.\n",
      "Agent 0, Average Reward: -683.44\n",
      "256/256 - 0s - loss: 161650.0000\n",
      "256/256 - 0s - loss: 167781.2656\n",
      "256/256 - 0s - loss: 162687.8125\n",
      "256/256 - 0s - loss: 160646.2188\n",
      "256/256 - 0s - loss: 162045.9844\n",
      "Reducing exploration for all agents to 0.8353\n",
      "\n",
      "Episode 14: Starting computation.\n",
      "Episode 14: Finished running.\n",
      "Agent 0, Average Reward: -704.95\n",
      "256/256 - 0s - loss: 153976.0938\n",
      "256/256 - 0s - loss: 159910.7500\n",
      "256/256 - 0s - loss: 153573.0000\n",
      "256/256 - 0s - loss: 153067.9688\n",
      "256/256 - 0s - loss: 157874.8750\n",
      "Reducing exploration for all agents to 0.8238\n",
      "\n",
      "Episode 15: Starting computation.\n",
      "Episode 15: Finished running.\n",
      "Agent 0, Average Reward: -632.95\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 149330.5000\n",
      "256/256 - 0s - loss: 153168.6875\n",
      "256/256 - 0s - loss: 148217.5938\n",
      "256/256 - 0s - loss: 148127.6875\n",
      "256/256 - 0s - loss: 147054.1094\n",
      "Reducing exploration for all agents to 0.8125\n",
      "\n",
      "Episode 16: Starting computation.\n",
      "Episode 16: Finished running.\n",
      "Agent 0, Average Reward: -595.84\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 130092.6484\n",
      "256/256 - 0s - loss: 131867.7188\n",
      "256/256 - 0s - loss: 129978.1484\n",
      "256/256 - 0s - loss: 125718.8750\n",
      "256/256 - 0s - loss: 128690.6641\n",
      "Reducing exploration for all agents to 0.8013\n",
      "\n",
      "Episode 17: Starting computation.\n",
      "Episode 17: Finished running.\n",
      "Agent 0, Average Reward: -538.68\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 121745.3359\n",
      "256/256 - 0s - loss: 119660.5781\n",
      "256/256 - 0s - loss: 117637.8047\n",
      "256/256 - 0s - loss: 124390.9062\n",
      "256/256 - 0s - loss: 121209.2266\n",
      "Reducing exploration for all agents to 0.7903\n",
      "\n",
      "Episode 18: Starting computation.\n",
      "Episode 18: Finished running.\n",
      "Agent 0, Average Reward: -581.58\n",
      "256/256 - 0s - loss: 115172.4688\n",
      "256/256 - 0s - loss: 111621.2422\n",
      "256/256 - 0s - loss: 114449.1250\n",
      "256/256 - 0s - loss: 116795.1953\n",
      "256/256 - 0s - loss: 114651.7891\n",
      "Reducing exploration for all agents to 0.7794\n",
      "\n",
      "Episode 19: Starting computation.\n",
      "Episode 19: Finished running.\n",
      "Agent 0, Average Reward: -628.74\n",
      "256/256 - 0s - loss: 107024.4141\n",
      "256/256 - 0s - loss: 108738.4609\n",
      "256/256 - 0s - loss: 113340.8828\n",
      "256/256 - 0s - loss: 108318.1484\n",
      "256/256 - 0s - loss: 107828.5078\n",
      "Reducing exploration for all agents to 0.7687\n",
      "\n",
      "Episode 20: Starting computation.\n",
      "Episode 20: Finished running.\n",
      "Agent 0, Average Reward: -579.58\n",
      "256/256 - 0s - loss: 104351.9141\n",
      "256/256 - 0s - loss: 106058.3984\n",
      "256/256 - 0s - loss: 107322.1094\n",
      "256/256 - 0s - loss: 106216.3828\n",
      "256/256 - 0s - loss: 106636.5625\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.7582\n",
      "\n",
      "Episode 21: Starting computation.\n",
      "Episode 21: Finished running.\n",
      "Agent 0, Average Reward: -606.79\n",
      "256/256 - 0s - loss: 121010.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 115293.1562\n",
      "256/256 - 0s - loss: 115392.1797\n",
      "256/256 - 0s - loss: 117160.3516\n",
      "256/256 - 0s - loss: 115598.0078\n",
      "Reducing exploration for all agents to 0.7477\n",
      "\n",
      "Episode 22: Starting computation.\n",
      "Episode 22: Finished running.\n",
      "Agent 0, Average Reward: -539.51\n",
      "256/256 - 0s - loss: 114583.3047\n",
      "256/256 - 0s - loss: 114689.4297\n",
      "256/256 - 0s - loss: 111666.0938\n",
      "256/256 - 0s - loss: 113663.7656\n",
      "256/256 - 0s - loss: 110542.3125\n",
      "Reducing exploration for all agents to 0.7375\n",
      "\n",
      "Episode 23: Starting computation.\n",
      "Episode 23: Finished running.\n",
      "Agent 0, Average Reward: -604.26\n",
      "256/256 - 0s - loss: 116283.1172\n",
      "256/256 - 0s - loss: 123271.3672\n",
      "256/256 - 0s - loss: 117820.6328\n",
      "256/256 - 0s - loss: 120641.8359\n",
      "256/256 - 0s - loss: 114863.2578\n",
      "Reducing exploration for all agents to 0.7273\n",
      "\n",
      "Episode 24: Starting computation.\n",
      "Episode 24: Finished running.\n",
      "Agent 0, Average Reward: -827.35\n",
      "256/256 - 0s - loss: 146113.1250\n",
      "256/256 - 0s - loss: 147149.6094\n",
      "256/256 - 0s - loss: 150306.7812\n",
      "256/256 - 0s - loss: 149739.6406\n",
      "256/256 - 0s - loss: 146438.6094\n",
      "Reducing exploration for all agents to 0.7173\n",
      "\n",
      "Episode 25: Starting computation.\n",
      "Episode 25: Finished running.\n",
      "Agent 0, Average Reward: -538.72\n",
      "256/256 - 0s - loss: 147746.2969\n",
      "256/256 - 0s - loss: 145599.8125\n",
      "256/256 - 0s - loss: 138038.6250\n",
      "256/256 - 0s - loss: 143853.2812\n",
      "256/256 - 0s - loss: 145300.9688\n",
      "Reducing exploration for all agents to 0.7075\n",
      "\n",
      "Episode 26: Starting computation.\n",
      "Episode 26: Finished running.\n",
      "Agent 0, Average Reward: -503.96\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 139457.5625\n",
      "256/256 - 0s - loss: 141873.9375\n",
      "256/256 - 0s - loss: 133092.0156\n",
      "256/256 - 0s - loss: 140675.2500\n",
      "256/256 - 0s - loss: 132406.5469\n",
      "Reducing exploration for all agents to 0.6977\n",
      "\n",
      "Episode 27: Starting computation.\n",
      "Episode 27: Finished running.\n",
      "Agent 0, Average Reward: -659.16\n",
      "256/256 - 0s - loss: 156010.7188\n",
      "256/256 - 0s - loss: 143962.7656\n",
      "256/256 - 0s - loss: 151438.8750\n",
      "256/256 - 0s - loss: 149836.3438\n",
      "256/256 - 0s - loss: 152625.2188\n",
      "Reducing exploration for all agents to 0.6881\n",
      "\n",
      "Episode 28: Starting computation.\n",
      "Episode 28: Finished running.\n",
      "Agent 0, Average Reward: -577.54\n",
      "256/256 - 0s - loss: 149584.9062\n",
      "256/256 - 0s - loss: 141733.3125\n",
      "256/256 - 0s - loss: 137892.9375\n",
      "256/256 - 0s - loss: 136751.5938\n",
      "256/256 - 0s - loss: 141787.6719\n",
      "Reducing exploration for all agents to 0.6787\n",
      "\n",
      "Episode 29: Starting computation.\n",
      "Episode 29: Finished running.\n",
      "Agent 0, Average Reward: -561.31\n",
      "256/256 - 0s - loss: 117175.6953\n",
      "256/256 - 0s - loss: 111032.8516\n",
      "256/256 - 0s - loss: 115204.2734\n",
      "256/256 - 0s - loss: 110373.0859\n",
      "256/256 - 0s - loss: 109990.1172\n",
      "Reducing exploration for all agents to 0.6693\n",
      "\n",
      "Episode 30: Starting computation.\n",
      "Episode 30: Finished running.\n",
      "Agent 0, Average Reward: -518.81\n",
      "256/256 - 0s - loss: 105993.3203\n",
      "256/256 - 0s - loss: 105861.3750\n",
      "256/256 - 0s - loss: 116484.0859\n",
      "256/256 - 0s - loss: 111480.1797\n",
      "256/256 - 0s - loss: 113052.3984\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.6601\n",
      "\n",
      "Episode 31: Starting computation.\n",
      "Episode 31: Finished running.\n",
      "Agent 0, Average Reward: -435.24\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 115884.4297\n",
      "256/256 - 0s - loss: 117168.1484\n",
      "256/256 - 0s - loss: 116171.6016\n",
      "256/256 - 0s - loss: 116649.1719\n",
      "256/256 - 0s - loss: 116627.0312\n",
      "Reducing exploration for all agents to 0.6511\n",
      "\n",
      "Episode 32: Starting computation.\n",
      "Episode 32: Finished running.\n",
      "Agent 0, Average Reward: -512.78\n",
      "256/256 - 0s - loss: 100331.2734\n",
      "256/256 - 0s - loss: 102296.4297\n",
      "256/256 - 0s - loss: 102478.2656\n",
      "256/256 - 0s - loss: 100874.7031\n",
      "256/256 - 0s - loss: 100961.6719\n",
      "Reducing exploration for all agents to 0.6421\n",
      "\n",
      "Episode 33: Starting computation.\n",
      "Episode 33: Finished running.\n",
      "Agent 0, Average Reward: -716.93\n",
      "256/256 - 0s - loss: 114165.6328\n",
      "256/256 - 0s - loss: 115544.7422\n",
      "256/256 - 0s - loss: 110356.7812\n",
      "256/256 - 0s - loss: 120248.1406\n",
      "256/256 - 0s - loss: 115529.4219\n",
      "Reducing exploration for all agents to 0.6333\n",
      "\n",
      "Episode 34: Starting computation.\n",
      "Episode 34: Finished running.\n",
      "Agent 0, Average Reward: -562.48\n",
      "256/256 - 0s - loss: 117548.2188\n",
      "256/256 - 0s - loss: 114434.4688\n",
      "256/256 - 0s - loss: 114804.9531\n",
      "256/256 - 0s - loss: 121649.1328\n",
      "256/256 - 0s - loss: 120595.9844\n",
      "Reducing exploration for all agents to 0.6246\n",
      "\n",
      "Episode 35: Starting computation.\n",
      "Episode 35: Finished running.\n",
      "Agent 0, Average Reward: -534.84\n",
      "256/256 - 0s - loss: 114902.3359\n",
      "256/256 - 0s - loss: 122545.0625\n",
      "256/256 - 0s - loss: 120361.8047\n",
      "256/256 - 0s - loss: 118664.5859\n",
      "256/256 - 0s - loss: 121935.8828\n",
      "Reducing exploration for all agents to 0.616\n",
      "\n",
      "Episode 36: Starting computation.\n",
      "Episode 36: Finished running.\n",
      "Agent 0, Average Reward: -545.93\n",
      "256/256 - 0s - loss: 131023.9375\n",
      "256/256 - 0s - loss: 127109.1250\n",
      "256/256 - 0s - loss: 128688.2812\n",
      "256/256 - 0s - loss: 130282.6641\n",
      "256/256 - 0s - loss: 124153.2188\n",
      "Reducing exploration for all agents to 0.6075\n",
      "\n",
      "Episode 37: Starting computation.\n",
      "Episode 37: Finished running.\n",
      "Agent 0, Average Reward: -517.96\n",
      "256/256 - 0s - loss: 123171.5000\n",
      "256/256 - 0s - loss: 123760.0781\n",
      "256/256 - 0s - loss: 119808.0469\n",
      "256/256 - 0s - loss: 123806.2344\n",
      "256/256 - 0s - loss: 111023.5000\n",
      "Reducing exploration for all agents to 0.5992\n",
      "\n",
      "Episode 38: Starting computation.\n",
      "Episode 38: Finished running.\n",
      "Agent 0, Average Reward: -582.38\n",
      "256/256 - 0s - loss: 106781.8359\n",
      "256/256 - 0s - loss: 107672.0000\n",
      "256/256 - 0s - loss: 110193.8125\n",
      "256/256 - 0s - loss: 107692.4844\n",
      "256/256 - 0s - loss: 109341.6172\n",
      "Reducing exploration for all agents to 0.5909\n",
      "\n",
      "Episode 39: Starting computation.\n",
      "Episode 39: Finished running.\n",
      "Agent 0, Average Reward: -813.06\n",
      "256/256 - 0s - loss: 130781.8672\n",
      "256/256 - 0s - loss: 135249.0000\n",
      "256/256 - 0s - loss: 131898.0000\n",
      "256/256 - 0s - loss: 130397.2812\n",
      "256/256 - 0s - loss: 130902.4219\n",
      "Reducing exploration for all agents to 0.5828\n",
      "\n",
      "Episode 40: Starting computation.\n",
      "Episode 40: Finished running.\n",
      "Agent 0, Average Reward: -937.51\n",
      "256/256 - 0s - loss: 167814.6094\n",
      "256/256 - 0s - loss: 165585.4375\n",
      "256/256 - 0s - loss: 170730.8750\n",
      "256/256 - 0s - loss: 167012.1875\n",
      "256/256 - 0s - loss: 174223.8750\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.5748\n",
      "\n",
      "Episode 41: Starting computation.\n",
      "Episode 41: Finished running.\n",
      "Agent 0, Average Reward: -711.36\n",
      "256/256 - 0s - loss: 211239.5000\n",
      "256/256 - 0s - loss: 206072.1562\n",
      "256/256 - 0s - loss: 208103.3750\n",
      "256/256 - 0s - loss: 207096.4688\n",
      "256/256 - 0s - loss: 204608.8594\n",
      "Reducing exploration for all agents to 0.5669\n",
      "\n",
      "Episode 42: Starting computation.\n",
      "Episode 42: Finished running.\n",
      "Agent 0, Average Reward: -827.05\n",
      "256/256 - 0s - loss: 236607.1562\n",
      "256/256 - 0s - loss: 230209.3750\n",
      "256/256 - 0s - loss: 224193.7188\n",
      "256/256 - 0s - loss: 225708.3125\n",
      "256/256 - 0s - loss: 227804.4062\n",
      "Reducing exploration for all agents to 0.5591\n",
      "\n",
      "Episode 43: Starting computation.\n",
      "Episode 43: Finished running.\n",
      "Agent 0, Average Reward: -902.57\n",
      "256/256 - 0s - loss: 248687.0938\n",
      "256/256 - 0s - loss: 243742.9375\n",
      "256/256 - 0s - loss: 243577.2188\n",
      "256/256 - 0s - loss: 246349.9062\n",
      "256/256 - 0s - loss: 242084.1875\n",
      "Reducing exploration for all agents to 0.5514\n",
      "\n",
      "Episode 44: Starting computation.\n",
      "Episode 44: Finished running.\n",
      "Agent 0, Average Reward: -971.63\n",
      "256/256 - 0s - loss: 241681.0312\n",
      "256/256 - 0s - loss: 243392.7812\n",
      "256/256 - 0s - loss: 243299.8125\n",
      "256/256 - 0s - loss: 246916.8281\n",
      "256/256 - 0s - loss: 243380.0938\n",
      "Reducing exploration for all agents to 0.5438\n",
      "\n",
      "Episode 45: Starting computation.\n",
      "Episode 45: Finished running.\n",
      "Agent 0, Average Reward: -1067.33\n",
      "256/256 - 0s - loss: 240325.7656\n",
      "256/256 - 0s - loss: 244122.4219\n",
      "256/256 - 0s - loss: 247708.1250\n",
      "256/256 - 0s - loss: 247016.8125\n",
      "256/256 - 0s - loss: 238083.8594\n",
      "Reducing exploration for all agents to 0.5364\n",
      "\n",
      "Episode 46: Starting computation.\n",
      "Episode 46: Finished running.\n",
      "Agent 0, Average Reward: -1039.2\n",
      "256/256 - 0s - loss: 261457.8125\n",
      "256/256 - 0s - loss: 260314.1875\n",
      "256/256 - 0s - loss: 263377.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 252335.8594\n",
      "256/256 - 0s - loss: 251849.8594\n",
      "Reducing exploration for all agents to 0.529\n",
      "\n",
      "Episode 47: Starting computation.\n",
      "Episode 47: Finished running.\n",
      "Agent 0, Average Reward: -1299.87\n",
      "256/256 - 0s - loss: 304539.1250\n",
      "256/256 - 0s - loss: 293525.8125\n",
      "256/256 - 0s - loss: 292480.8750\n",
      "256/256 - 0s - loss: 285945.3750\n",
      "256/256 - 0s - loss: 287547.9375\n",
      "Reducing exploration for all agents to 0.5217\n",
      "\n",
      "Episode 48: Starting computation.\n",
      "Episode 48: Finished running.\n",
      "Agent 0, Average Reward: -1212.63\n",
      "256/256 - 0s - loss: 324922.5625\n",
      "256/256 - 0s - loss: 328045.3125\n",
      "256/256 - 0s - loss: 321949.0312\n",
      "256/256 - 0s - loss: 324221.2500\n",
      "256/256 - 0s - loss: 318160.6250\n",
      "Reducing exploration for all agents to 0.5145\n",
      "\n",
      "Episode 49: Starting computation.\n",
      "Episode 49: Finished running.\n",
      "Agent 0, Average Reward: -1122.3\n",
      "256/256 - 0s - loss: 333052.3438\n",
      "256/256 - 0s - loss: 329712.2500\n",
      "256/256 - 0s - loss: 329352.1875\n",
      "256/256 - 0s - loss: 326209.5000\n",
      "256/256 - 0s - loss: 320259.8438\n",
      "Reducing exploration for all agents to 0.5075\n",
      "\n",
      "Episode 50: Starting computation.\n",
      "Episode 50: Finished running.\n",
      "Agent 0, Average Reward: -1257.49\n",
      "256/256 - 0s - loss: 340745.6562\n",
      "256/256 - 0s - loss: 333538.1250\n",
      "256/256 - 0s - loss: 334256.3750\n",
      "256/256 - 0s - loss: 335545.3438\n",
      "256/256 - 0s - loss: 321548.5625\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.5005\n",
      "\n",
      "Episode 51: Starting computation.\n",
      "Episode 51: Finished running.\n",
      "Agent 0, Average Reward: -1182.7\n",
      "256/256 - 0s - loss: 495331.6250\n",
      "256/256 - 0s - loss: 490782.3750\n",
      "256/256 - 0s - loss: 483591.6562\n",
      "256/256 - 0s - loss: 475728.2812\n",
      "256/256 - 0s - loss: 470930.6875\n",
      "Reducing exploration for all agents to 0.4936\n",
      "\n",
      "Episode 52: Starting computation.\n",
      "Episode 52: Finished running.\n",
      "Agent 0, Average Reward: -1324.48\n",
      "256/256 - 0s - loss: 458415.1875\n",
      "256/256 - 0s - loss: 461796.6875\n",
      "256/256 - 0s - loss: 453960.3750\n",
      "256/256 - 0s - loss: 443514.1875\n",
      "256/256 - 0s - loss: 438410.6250\n",
      "Reducing exploration for all agents to 0.4868\n",
      "\n",
      "Episode 53: Starting computation.\n",
      "Episode 53: Finished running.\n",
      "Agent 0, Average Reward: -1291.71\n",
      "256/256 - 0s - loss: 433333.9062\n",
      "256/256 - 0s - loss: 421707.2500\n",
      "256/256 - 0s - loss: 413214.5625\n",
      "256/256 - 0s - loss: 412717.0625\n",
      "256/256 - 0s - loss: 421784.3125\n",
      "Reducing exploration for all agents to 0.4801\n",
      "\n",
      "Episode 54: Starting computation.\n",
      "Episode 54: Finished running.\n",
      "Agent 0, Average Reward: -1538.57\n",
      "256/256 - 0s - loss: 446143.6250\n",
      "256/256 - 0s - loss: 441150.0000\n",
      "256/256 - 0s - loss: 446048.5625\n",
      "256/256 - 0s - loss: 435419.6562\n",
      "256/256 - 0s - loss: 424670.1875\n",
      "Reducing exploration for all agents to 0.4735\n",
      "\n",
      "Episode 55: Starting computation.\n",
      "Episode 55: Finished running.\n",
      "Agent 0, Average Reward: -982.29\n",
      "256/256 - 0s - loss: 391280.4375\n",
      "256/256 - 0s - loss: 374831.6875\n",
      "256/256 - 0s - loss: 384964.6562\n",
      "256/256 - 0s - loss: 365388.1250\n",
      "256/256 - 0s - loss: 367294.8750\n",
      "Reducing exploration for all agents to 0.467\n",
      "\n",
      "Episode 56: Starting computation.\n",
      "Episode 56: Finished running.\n",
      "Agent 0, Average Reward: -1552.59\n",
      "256/256 - 0s - loss: 387076.0000\n",
      "256/256 - 0s - loss: 388878.2500\n",
      "256/256 - 0s - loss: 373223.8125\n",
      "256/256 - 0s - loss: 367478.2500\n",
      "256/256 - 0s - loss: 364510.5000\n",
      "Reducing exploration for all agents to 0.4606\n",
      "\n",
      "Episode 57: Starting computation.\n",
      "Episode 57: Finished running.\n",
      "Agent 0, Average Reward: -1357.5\n",
      "256/256 - 0s - loss: 369746.1562\n",
      "256/256 - 0s - loss: 367771.2812\n",
      "256/256 - 0s - loss: 352854.4062\n",
      "256/256 - 0s - loss: 354706.0312\n",
      "256/256 - 0s - loss: 340102.3438\n",
      "Reducing exploration for all agents to 0.4543\n",
      "\n",
      "Episode 58: Starting computation.\n",
      "Episode 58: Finished running.\n",
      "Agent 0, Average Reward: -1456.05\n",
      "256/256 - 0s - loss: 341994.5625\n",
      "256/256 - 0s - loss: 338416.2500\n",
      "256/256 - 0s - loss: 337316.0625\n",
      "256/256 - 0s - loss: 319936.4688\n",
      "256/256 - 0s - loss: 311052.5625\n",
      "Reducing exploration for all agents to 0.448\n",
      "\n",
      "Episode 59: Starting computation.\n",
      "Episode 59: Finished running.\n",
      "Agent 0, Average Reward: -1680.58\n",
      "256/256 - 0s - loss: 311292.1875\n",
      "256/256 - 0s - loss: 312698.3125\n",
      "256/256 - 0s - loss: 298851.6562\n",
      "256/256 - 0s - loss: 302290.2500\n",
      "256/256 - 0s - loss: 284014.5312\n",
      "Reducing exploration for all agents to 0.4419\n",
      "\n",
      "Episode 60: Starting computation.\n",
      "Episode 60: Finished running.\n",
      "Agent 0, Average Reward: -1396.5\n",
      "256/256 - 0s - loss: 300357.2500\n",
      "256/256 - 0s - loss: 282734.0000\n",
      "256/256 - 0s - loss: 273994.5625\n",
      "256/256 - 0s - loss: 278630.0625\n",
      "256/256 - 0s - loss: 268299.0000\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.4358\n",
      "\n",
      "Episode 61: Starting computation.\n",
      "Episode 61: Finished running.\n",
      "Agent 0, Average Reward: -1545.61\n",
      "256/256 - 0s - loss: 636947.1250\n",
      "256/256 - 0s - loss: 619576.0625\n",
      "256/256 - 0s - loss: 606970.3750\n",
      "256/256 - 0s - loss: 595191.2500\n",
      "256/256 - 0s - loss: 569710.3125\n",
      "Reducing exploration for all agents to 0.4298\n",
      "\n",
      "Episode 62: Starting computation.\n",
      "Episode 62: Finished running.\n",
      "Agent 0, Average Reward: -2304.09\n",
      "256/256 - 0s - loss: 753707.9375\n",
      "256/256 - 0s - loss: 736317.4375\n",
      "256/256 - 0s - loss: 722977.8125\n",
      "256/256 - 0s - loss: 694760.5625\n",
      "256/256 - 0s - loss: 670665.4375\n",
      "Reducing exploration for all agents to 0.4239\n",
      "\n",
      "Episode 63: Starting computation.\n",
      "Episode 63: Finished running.\n",
      "Agent 0, Average Reward: -1727.76\n",
      "256/256 - 0s - loss: 730782.1250\n",
      "256/256 - 0s - loss: 708951.7500\n",
      "256/256 - 0s - loss: 698344.9375\n",
      "256/256 - 0s - loss: 656683.3125\n",
      "256/256 - 0s - loss: 646852.5625\n",
      "Reducing exploration for all agents to 0.4181\n",
      "\n",
      "Episode 64: Starting computation.\n",
      "Episode 64: Finished running.\n",
      "Agent 0, Average Reward: -2415.92\n",
      "256/256 - 0s - loss: 774113.9375\n",
      "256/256 - 0s - loss: 762154.5625\n",
      "256/256 - 0s - loss: 727932.5625\n",
      "256/256 - 0s - loss: 696974.3750\n",
      "256/256 - 0s - loss: 671603.5625\n",
      "Reducing exploration for all agents to 0.4123\n",
      "\n",
      "Episode 65: Starting computation.\n",
      "Episode 65: Finished running.\n",
      "Agent 0, Average Reward: -1427.26\n",
      "256/256 - 0s - loss: 673781.5625\n",
      "256/256 - 0s - loss: 668062.9375\n",
      "256/256 - 0s - loss: 621475.6875\n",
      "256/256 - 0s - loss: 610336.6250\n",
      "256/256 - 0s - loss: 596511.4375\n",
      "Reducing exploration for all agents to 0.4066\n",
      "\n",
      "Episode 66: Starting computation.\n",
      "Episode 66: Finished running.\n",
      "Agent 0, Average Reward: -1193.57\n",
      "256/256 - 0s - loss: 541338.8125\n",
      "256/256 - 0s - loss: 521086.0938\n",
      "256/256 - 0s - loss: 517331.8125\n",
      "256/256 - 0s - loss: 476734.0625\n",
      "256/256 - 0s - loss: 466086.7188\n",
      "Reducing exploration for all agents to 0.4011\n",
      "\n",
      "Episode 67: Starting computation.\n",
      "Episode 67: Finished running.\n",
      "Agent 0, Average Reward: -1043.14\n",
      "256/256 - 0s - loss: 345105.0625\n",
      "256/256 - 0s - loss: 336115.0000\n",
      "256/256 - 0s - loss: 315701.8750\n",
      "256/256 - 0s - loss: 301017.0000\n",
      "256/256 - 0s - loss: 293984.7500\n",
      "Reducing exploration for all agents to 0.3955\n",
      "\n",
      "Episode 68: Starting computation.\n",
      "Episode 68: Finished running.\n",
      "Agent 0, Average Reward: -1488.06\n",
      "256/256 - 0s - loss: 246344.8906\n",
      "256/256 - 0s - loss: 230718.8438\n",
      "256/256 - 0s - loss: 230260.6719\n",
      "256/256 - 0s - loss: 217353.3594\n",
      "256/256 - 0s - loss: 206263.8750\n",
      "Reducing exploration for all agents to 0.3901\n",
      "\n",
      "Episode 69: Starting computation.\n",
      "Episode 69: Finished running.\n",
      "Agent 0, Average Reward: -879.16\n",
      "256/256 - 0s - loss: 112042.3828\n",
      "256/256 - 0s - loss: 108165.8438\n",
      "256/256 - 0s - loss: 104180.9922\n",
      "256/256 - 0s - loss: 100436.5234\n",
      "256/256 - 0s - loss: 95843.6406\n",
      "Reducing exploration for all agents to 0.3847\n",
      "\n",
      "Episode 70: Starting computation.\n",
      "Episode 70: Finished running.\n",
      "Agent 0, Average Reward: -755.28\n",
      "256/256 - 0s - loss: 78127.3516\n",
      "256/256 - 0s - loss: 73465.1406\n",
      "256/256 - 0s - loss: 68481.8047\n",
      "256/256 - 0s - loss: 72017.4219\n",
      "256/256 - 0s - loss: 64414.0234\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.3795\n",
      "\n",
      "Episode 71: Starting computation.\n",
      "Episode 71: Finished running.\n",
      "Agent 0, Average Reward: -237.53\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 276589.9062\n",
      "256/256 - 0s - loss: 278156.8750\n",
      "256/256 - 0s - loss: 263142.1250\n",
      "256/256 - 0s - loss: 265096.3125\n",
      "256/256 - 0s - loss: 250090.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.3742\n",
      "\n",
      "Episode 72: Starting computation.\n",
      "Episode 72: Finished running.\n",
      "Agent 0, Average Reward: -209.31\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 189105.4531\n",
      "256/256 - 0s - loss: 186002.4688\n",
      "256/256 - 0s - loss: 178086.4688\n",
      "256/256 - 0s - loss: 178953.4062\n",
      "256/256 - 0s - loss: 174054.1406\n",
      "Reducing exploration for all agents to 0.3691\n",
      "\n",
      "Episode 73: Starting computation.\n",
      "Episode 73: Finished running.\n",
      "Agent 0, Average Reward: -236.15\n",
      "256/256 - 0s - loss: 76362.7578\n",
      "256/256 - 0s - loss: 79234.7109\n",
      "256/256 - 0s - loss: 75437.1719\n",
      "256/256 - 0s - loss: 66042.2734\n",
      "256/256 - 0s - loss: 75754.4062\n",
      "Reducing exploration for all agents to 0.364\n",
      "\n",
      "Episode 74: Starting computation.\n",
      "Episode 74: Finished running.\n",
      "Agent 0, Average Reward: -299.02\n",
      "256/256 - 0s - loss: 48252.0547\n",
      "256/256 - 0s - loss: 45626.2852\n",
      "256/256 - 0s - loss: 47154.0938\n",
      "256/256 - 0s - loss: 45644.2539\n",
      "256/256 - 0s - loss: 41475.8281\n",
      "Reducing exploration for all agents to 0.359\n",
      "\n",
      "Episode 75: Starting computation.\n",
      "Episode 75: Finished running.\n",
      "Agent 0, Average Reward: -1760.36\n",
      "256/256 - 0s - loss: 156235.0781\n",
      "256/256 - 0s - loss: 150539.9375\n",
      "256/256 - 0s - loss: 156076.8750\n",
      "256/256 - 0s - loss: 158605.1094\n",
      "256/256 - 0s - loss: 154243.6719\n",
      "Reducing exploration for all agents to 0.3541\n",
      "\n",
      "Episode 76: Starting computation.\n",
      "Episode 76: Finished running.\n",
      "Agent 0, Average Reward: -1664.9\n",
      "256/256 - 0s - loss: 266868.6250\n",
      "256/256 - 0s - loss: 263476.6250\n",
      "256/256 - 0s - loss: 258858.6094\n",
      "256/256 - 0s - loss: 254540.5469\n",
      "256/256 - 0s - loss: 262275.2500\n",
      "Reducing exploration for all agents to 0.3492\n",
      "\n",
      "Episode 77: Starting computation.\n",
      "Episode 77: Finished running.\n",
      "Agent 0, Average Reward: -641.64\n",
      "256/256 - 0s - loss: 277449.9375\n",
      "256/256 - 0s - loss: 268144.0625\n",
      "256/256 - 0s - loss: 267552.0625\n",
      "256/256 - 0s - loss: 255820.4375\n",
      "256/256 - 0s - loss: 249692.4219\n",
      "Reducing exploration for all agents to 0.3444\n",
      "\n",
      "Episode 78: Starting computation.\n",
      "Episode 78: Finished running.\n",
      "Agent 0, Average Reward: -398.52\n",
      "256/256 - 0s - loss: 245465.4375\n",
      "256/256 - 0s - loss: 237683.5312\n",
      "256/256 - 0s - loss: 245159.8906\n",
      "256/256 - 0s - loss: 225917.2812\n",
      "256/256 - 0s - loss: 216663.5312\n",
      "Reducing exploration for all agents to 0.3397\n",
      "\n",
      "Episode 79: Starting computation.\n",
      "Episode 79: Finished running.\n",
      "Agent 0, Average Reward: -343.93\n",
      "256/256 - 0s - loss: 228135.2500\n",
      "256/256 - 0s - loss: 208595.5625\n",
      "256/256 - 0s - loss: 210022.8906\n",
      "256/256 - 0s - loss: 193533.4062\n",
      "256/256 - 0s - loss: 202692.0156\n",
      "Reducing exploration for all agents to 0.335\n",
      "\n",
      "Episode 80: Starting computation.\n",
      "Episode 80: Finished running.\n",
      "Agent 0, Average Reward: -262.85\n",
      "256/256 - 0s - loss: 112504.8047\n",
      "256/256 - 0s - loss: 100439.8359\n",
      "256/256 - 0s - loss: 104905.9141\n",
      "256/256 - 0s - loss: 100797.8516\n",
      "256/256 - 0s - loss: 97944.4375\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.3304\n",
      "\n",
      "Episode 81: Starting computation.\n",
      "Episode 81: Finished running.\n",
      "Agent 0, Average Reward: -332.7\n",
      "256/256 - 0s - loss: 71456.5859\n",
      "256/256 - 0s - loss: 64901.3711\n",
      "256/256 - 0s - loss: 71793.2031\n",
      "256/256 - 0s - loss: 66997.4141\n",
      "256/256 - 0s - loss: 67909.1875\n",
      "Reducing exploration for all agents to 0.3259\n",
      "\n",
      "Episode 82: Starting computation.\n",
      "Episode 82: Finished running.\n",
      "Agent 0, Average Reward: -348.5\n",
      "256/256 - 0s - loss: 50715.3750\n",
      "256/256 - 0s - loss: 47876.8242\n",
      "256/256 - 0s - loss: 48442.3164\n",
      "256/256 - 0s - loss: 43717.2266\n",
      "256/256 - 0s - loss: 47370.5273\n",
      "Reducing exploration for all agents to 0.3214\n",
      "\n",
      "Episode 83: Starting computation.\n",
      "Episode 83: Finished running.\n",
      "Agent 0, Average Reward: -341.62\n",
      "256/256 - 0s - loss: 42468.8906\n",
      "256/256 - 0s - loss: 42484.9570\n",
      "256/256 - 0s - loss: 41406.5117\n",
      "256/256 - 0s - loss: 41325.9453\n",
      "256/256 - 0s - loss: 41552.4570\n",
      "Reducing exploration for all agents to 0.317\n",
      "\n",
      "Episode 84: Starting computation.\n",
      "Episode 84: Finished running.\n",
      "Agent 0, Average Reward: -294.71\n",
      "256/256 - 0s - loss: 39054.6992\n",
      "256/256 - 0s - loss: 35335.9883\n",
      "256/256 - 0s - loss: 36140.1211\n",
      "256/256 - 0s - loss: 38019.7930\n",
      "256/256 - 0s - loss: 37259.1523\n",
      "Reducing exploration for all agents to 0.3126\n",
      "\n",
      "Episode 85: Starting computation.\n",
      "Episode 85: Finished running.\n",
      "Agent 0, Average Reward: -328.39\n",
      "256/256 - 0s - loss: 36795.4531\n",
      "256/256 - 0s - loss: 42858.0352\n",
      "256/256 - 0s - loss: 37815.3203\n",
      "256/256 - 0s - loss: 38003.9883\n",
      "256/256 - 0s - loss: 37566.8867\n",
      "Reducing exploration for all agents to 0.3083\n",
      "\n",
      "Episode 86: Starting computation.\n",
      "Episode 86: Finished running.\n",
      "Agent 0, Average Reward: -405.79\n",
      "256/256 - 0s - loss: 39550.3008\n",
      "256/256 - 0s - loss: 42501.6250\n",
      "256/256 - 0s - loss: 39111.0195\n",
      "256/256 - 0s - loss: 39866.7188\n",
      "256/256 - 0s - loss: 35529.0859\n",
      "Reducing exploration for all agents to 0.3041\n",
      "\n",
      "Episode 87: Starting computation.\n",
      "Episode 87: Finished running.\n",
      "Agent 0, Average Reward: -618.99\n",
      "256/256 - 0s - loss: 46043.3320\n",
      "256/256 - 0s - loss: 47516.6055\n",
      "256/256 - 0s - loss: 49442.4648\n",
      "256/256 - 0s - loss: 47735.9258\n",
      "256/256 - 0s - loss: 50607.2148\n",
      "Reducing exploration for all agents to 0.2999\n",
      "\n",
      "Episode 88: Starting computation.\n",
      "Episode 88: Finished running.\n",
      "Agent 0, Average Reward: -726.79\n",
      "256/256 - 0s - loss: 60511.5195\n",
      "256/256 - 0s - loss: 61546.9727\n",
      "256/256 - 0s - loss: 60861.6562\n",
      "256/256 - 0s - loss: 62688.4062\n",
      "256/256 - 0s - loss: 59981.3594\n",
      "Reducing exploration for all agents to 0.2958\n",
      "\n",
      "Episode 89: Starting computation.\n",
      "Episode 89: Finished running.\n",
      "Agent 0, Average Reward: -2073.13\n",
      "256/256 - 0s - loss: 275317.5000\n",
      "256/256 - 0s - loss: 273777.2500\n",
      "256/256 - 0s - loss: 263533.3125\n",
      "256/256 - 0s - loss: 254545.7344\n",
      "256/256 - 0s - loss: 251072.6406\n",
      "Reducing exploration for all agents to 0.2917\n",
      "\n",
      "Episode 90: Starting computation.\n",
      "Episode 90: Finished running.\n",
      "Agent 0, Average Reward: -637.98\n",
      "256/256 - 0s - loss: 267374.4062\n",
      "256/256 - 0s - loss: 253466.6875\n",
      "256/256 - 0s - loss: 245074.3594\n",
      "256/256 - 0s - loss: 236264.5156\n",
      "256/256 - 0s - loss: 240408.4688\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.2877\n",
      "\n",
      "Episode 91: Starting computation.\n",
      "Episode 91: Finished running.\n",
      "Agent 0, Average Reward: -327.58\n",
      "256/256 - 0s - loss: 343376.0938\n",
      "256/256 - 0s - loss: 340565.5938\n",
      "256/256 - 0s - loss: 335930.2500\n",
      "256/256 - 0s - loss: 308537.1250\n",
      "256/256 - 0s - loss: 301985.1875\n",
      "Reducing exploration for all agents to 0.2837\n",
      "\n",
      "Episode 92: Starting computation.\n",
      "Episode 92: Finished running.\n",
      "Agent 0, Average Reward: -233.05\n",
      "256/256 - 0s - loss: 288160.8125\n",
      "256/256 - 0s - loss: 276445.3438\n",
      "256/256 - 0s - loss: 265628.5625\n",
      "256/256 - 0s - loss: 261424.3750\n",
      "256/256 - 0s - loss: 246091.0156\n",
      "Reducing exploration for all agents to 0.2798\n",
      "\n",
      "Episode 93: Starting computation.\n",
      "Episode 93: Finished running.\n",
      "Agent 0, Average Reward: -274.27\n",
      "256/256 - 0s - loss: 216987.9531\n",
      "256/256 - 0s - loss: 214638.0469\n",
      "256/256 - 0s - loss: 208902.3906\n",
      "256/256 - 0s - loss: 202874.0938\n",
      "256/256 - 0s - loss: 198280.8594\n",
      "Reducing exploration for all agents to 0.276\n",
      "\n",
      "Episode 94: Starting computation.\n",
      "Episode 94: Finished running.\n",
      "Agent 0, Average Reward: -299.37\n",
      "256/256 - 0s - loss: 25812.3066\n",
      "256/256 - 0s - loss: 26552.6113\n",
      "256/256 - 0s - loss: 26486.5176\n",
      "256/256 - 0s - loss: 28554.7402\n",
      "256/256 - 0s - loss: 27758.3203\n",
      "Reducing exploration for all agents to 0.2722\n",
      "\n",
      "Episode 95: Starting computation.\n",
      "Episode 95: Finished running.\n",
      "Agent 0, Average Reward: -1508.84\n",
      "256/256 - 0s - loss: 88889.7656\n",
      "256/256 - 0s - loss: 89642.3594\n",
      "256/256 - 0s - loss: 87126.9062\n",
      "256/256 - 0s - loss: 87584.1250\n",
      "256/256 - 0s - loss: 85233.2969\n",
      "Reducing exploration for all agents to 0.2684\n",
      "\n",
      "Episode 96: Starting computation.\n",
      "Episode 96: Finished running.\n",
      "Agent 0, Average Reward: -1654.2\n",
      "256/256 - 0s - loss: 155843.1094\n",
      "256/256 - 0s - loss: 155280.2969\n",
      "256/256 - 0s - loss: 144214.7656\n",
      "256/256 - 0s - loss: 141236.7656\n",
      "256/256 - 0s - loss: 128962.4531\n",
      "Reducing exploration for all agents to 0.2648\n",
      "\n",
      "Episode 97: Starting computation.\n",
      "Episode 97: Finished running.\n",
      "Agent 0, Average Reward: -265.15\n",
      "256/256 - 0s - loss: 127147.0234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 117518.7734\n",
      "256/256 - 0s - loss: 111547.5625\n",
      "256/256 - 0s - loss: 109136.0547\n",
      "256/256 - 0s - loss: 99446.2578\n",
      "Reducing exploration for all agents to 0.2611\n",
      "\n",
      "Episode 98: Starting computation.\n",
      "Episode 98: Finished running.\n",
      "Agent 0, Average Reward: -257.41\n",
      "256/256 - 0s - loss: 90493.0938\n",
      "256/256 - 0s - loss: 85738.3203\n",
      "256/256 - 0s - loss: 81190.3359\n",
      "256/256 - 0s - loss: 78058.4062\n",
      "256/256 - 0s - loss: 71090.3203\n",
      "Reducing exploration for all agents to 0.2575\n",
      "\n",
      "Episode 99: Starting computation.\n",
      "Episode 99: Finished running.\n",
      "Agent 0, Average Reward: -185.32\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 66884.0156\n",
      "256/256 - 0s - loss: 63075.0234\n",
      "256/256 - 0s - loss: 57046.7656\n",
      "256/256 - 0s - loss: 50224.3047\n",
      "256/256 - 0s - loss: 46360.8750\n",
      "Reducing exploration for all agents to 0.254\n",
      "\n",
      "Episode 100: Starting computation.\n",
      "Episode 100: Finished running.\n",
      "Agent 0, Average Reward: -339.95\n",
      "256/256 - 0s - loss: 28224.7695\n",
      "256/256 - 0s - loss: 27471.1055\n",
      "256/256 - 0s - loss: 25012.2559\n",
      "256/256 - 0s - loss: 22784.4082\n",
      "256/256 - 0s - loss: 21664.0332\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.2505\n",
      "\n",
      "Episode 101: Starting computation.\n",
      "Episode 101: Finished running.\n",
      "Agent 0, Average Reward: -736.8\n",
      "256/256 - 0s - loss: 51410.8906\n",
      "256/256 - 0s - loss: 50173.7734\n",
      "256/256 - 0s - loss: 44268.5273\n",
      "256/256 - 0s - loss: 41931.5273\n",
      "256/256 - 0s - loss: 44603.0078\n",
      "Reducing exploration for all agents to 0.2471\n",
      "\n",
      "Episode 102: Starting computation.\n",
      "Episode 102: Finished running.\n",
      "Agent 0, Average Reward: -1678.63\n",
      "256/256 - 0s - loss: 189429.4531\n",
      "256/256 - 0s - loss: 181272.6719\n",
      "256/256 - 0s - loss: 184589.1875\n",
      "256/256 - 0s - loss: 182606.3750\n",
      "256/256 - 0s - loss: 179619.3906\n",
      "Reducing exploration for all agents to 0.2437\n",
      "\n",
      "Episode 103: Starting computation.\n",
      "Episode 103: Finished running.\n",
      "Agent 0, Average Reward: -1779.57\n",
      "256/256 - 0s - loss: 328277.6250\n",
      "256/256 - 0s - loss: 319056.2812\n",
      "256/256 - 0s - loss: 304655.2812\n",
      "256/256 - 0s - loss: 299201.4062\n",
      "256/256 - 0s - loss: 290272.3438\n",
      "Reducing exploration for all agents to 0.2403\n",
      "\n",
      "Episode 104: Starting computation.\n",
      "Episode 104: Finished running.\n",
      "Agent 0, Average Reward: -1605.97\n",
      "256/256 - 0s - loss: 385407.5938\n",
      "256/256 - 0s - loss: 377588.4062\n",
      "256/256 - 0s - loss: 362365.2188\n",
      "256/256 - 0s - loss: 365611.4688\n",
      "256/256 - 0s - loss: 350654.5625\n",
      "Reducing exploration for all agents to 0.237\n",
      "\n",
      "Episode 105: Starting computation.\n",
      "Episode 105: Finished running.\n",
      "Agent 0, Average Reward: -2770.37\n",
      "256/256 - 0s - loss: 609375.3125\n",
      "256/256 - 0s - loss: 571312.4375\n",
      "256/256 - 0s - loss: 552217.3750\n",
      "256/256 - 0s - loss: 524097.3438\n",
      "256/256 - 0s - loss: 502973.3125\n",
      "Reducing exploration for all agents to 0.2337\n",
      "\n",
      "Episode 106: Starting computation.\n",
      "Episode 106: Finished running.\n",
      "Agent 0, Average Reward: -2785.74\n",
      "256/256 - 0s - loss: 644483.6875\n",
      "256/256 - 0s - loss: 596914.6875\n",
      "256/256 - 0s - loss: 559125.1875\n",
      "256/256 - 0s - loss: 515062.9688\n",
      "256/256 - 0s - loss: 463275.7188\n",
      "Reducing exploration for all agents to 0.2305\n",
      "\n",
      "Episode 107: Starting computation.\n",
      "Episode 107: Finished running.\n",
      "Agent 0, Average Reward: -1882.38\n",
      "256/256 - 0s - loss: 430563.9688\n",
      "256/256 - 0s - loss: 373938.1562\n",
      "256/256 - 0s - loss: 342365.5938\n",
      "256/256 - 0s - loss: 298420.2812\n",
      "256/256 - 0s - loss: 256402.5938\n",
      "Reducing exploration for all agents to 0.2274\n",
      "\n",
      "Episode 108: Starting computation.\n",
      "Episode 108: Finished running.\n",
      "Agent 0, Average Reward: -1781.42\n",
      "256/256 - 0s - loss: 225469.7656\n",
      "256/256 - 0s - loss: 181683.4062\n",
      "256/256 - 0s - loss: 156016.5469\n",
      "256/256 - 0s - loss: 128499.1016\n",
      "256/256 - 0s - loss: 105320.0859\n",
      "Reducing exploration for all agents to 0.2242\n",
      "\n",
      "Episode 109: Starting computation.\n",
      "Episode 109: Finished running.\n",
      "Agent 0, Average Reward: -1778.22\n",
      "256/256 - 0s - loss: 73069.0391\n",
      "256/256 - 0s - loss: 54899.5195\n",
      "256/256 - 0s - loss: 38916.4648\n",
      "256/256 - 0s - loss: 29202.1230\n",
      "256/256 - 0s - loss: 21016.1523\n",
      "Reducing exploration for all agents to 0.2212\n",
      "\n",
      "Episode 110: Starting computation.\n",
      "Episode 110: Finished running.\n",
      "Agent 0, Average Reward: -1758.19\n",
      "256/256 - 0s - loss: 10162.2969\n",
      "256/256 - 0s - loss: 6355.5981\n",
      "256/256 - 0s - loss: 5968.6245\n",
      "256/256 - 0s - loss: 6505.3018\n",
      "256/256 - 0s - loss: 4775.6636\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.2181\n",
      "\n",
      "Episode 111: Starting computation.\n",
      "Episode 111: Finished running.\n",
      "Agent 0, Average Reward: -1745.7\n",
      "256/256 - 0s - loss: 608440.1875\n",
      "256/256 - 0s - loss: 574705.6250\n",
      "256/256 - 0s - loss: 533801.3750\n",
      "256/256 - 0s - loss: 472164.4688\n",
      "256/256 - 0s - loss: 430771.0938\n",
      "Reducing exploration for all agents to 0.2151\n",
      "\n",
      "Episode 112: Starting computation.\n",
      "Episode 112: Finished running.\n",
      "Agent 0, Average Reward: -1736.98\n",
      "256/256 - 0s - loss: 379739.2188\n",
      "256/256 - 0s - loss: 382081.5938\n",
      "256/256 - 0s - loss: 326746.0625\n",
      "256/256 - 0s - loss: 281029.3125\n",
      "256/256 - 0s - loss: 271558.5938\n",
      "Reducing exploration for all agents to 0.2122\n",
      "\n",
      "Episode 113: Starting computation.\n",
      "Episode 113: Finished running.\n",
      "Agent 0, Average Reward: -2228.94\n",
      "256/256 - 0s - loss: 342445.9062\n",
      "256/256 - 0s - loss: 296253.6562\n",
      "256/256 - 0s - loss: 262877.8438\n",
      "256/256 - 0s - loss: 233925.5312\n",
      "256/256 - 0s - loss: 205064.4375\n",
      "Reducing exploration for all agents to 0.2092\n",
      "\n",
      "Episode 114: Starting computation.\n",
      "Episode 114: Finished running.\n",
      "Agent 0, Average Reward: -3016.38\n",
      "256/256 - 0s - loss: 313514.4062\n",
      "256/256 - 0s - loss: 284389.9375\n",
      "256/256 - 0s - loss: 258416.8906\n",
      "256/256 - 0s - loss: 238854.0000\n",
      "256/256 - 0s - loss: 201353.5000\n",
      "Reducing exploration for all agents to 0.2064\n",
      "\n",
      "Episode 115: Starting computation.\n",
      "Episode 115: Finished running.\n",
      "Agent 0, Average Reward: -3003.56\n",
      "256/256 - 0s - loss: 253340.0000\n",
      "256/256 - 0s - loss: 216899.2188\n",
      "256/256 - 0s - loss: 188075.7188\n",
      "256/256 - 0s - loss: 169942.6406\n",
      "256/256 - 0s - loss: 143275.3125\n",
      "Reducing exploration for all agents to 0.2035\n",
      "\n",
      "Episode 116: Starting computation.\n",
      "Episode 116: Finished running.\n",
      "Agent 0, Average Reward: -3049.17\n",
      "256/256 - 0s - loss: 143885.0625\n",
      "256/256 - 0s - loss: 119549.8828\n",
      "256/256 - 0s - loss: 100028.0391\n",
      "256/256 - 0s - loss: 79990.7344\n",
      "256/256 - 0s - loss: 70390.6094\n",
      "Reducing exploration for all agents to 0.2007\n",
      "\n",
      "Episode 117: Starting computation.\n",
      "Episode 117: Finished running.\n",
      "Agent 0, Average Reward: -1683.88\n",
      "256/256 - 0s - loss: 92540.6016\n",
      "256/256 - 0s - loss: 76392.3438\n",
      "256/256 - 0s - loss: 66855.4688\n",
      "256/256 - 0s - loss: 59232.8398\n",
      "256/256 - 0s - loss: 53785.9883\n",
      "Reducing exploration for all agents to 0.198\n",
      "\n",
      "Episode 118: Starting computation.\n",
      "Episode 118: Finished running.\n",
      "Agent 0, Average Reward: -509.19\n",
      "256/256 - 0s - loss: 45221.1016\n",
      "256/256 - 0s - loss: 49185.2188\n",
      "256/256 - 0s - loss: 44224.1836\n",
      "256/256 - 0s - loss: 51059.0312\n",
      "256/256 - 0s - loss: 60909.7734\n",
      "Reducing exploration for all agents to 0.1952\n",
      "\n",
      "Episode 119: Starting computation.\n",
      "Episode 119: Finished running.\n",
      "Agent 0, Average Reward: -396.71\n",
      "256/256 - 0s - loss: 50903.6680\n",
      "256/256 - 0s - loss: 48591.2266\n",
      "256/256 - 0s - loss: 60112.5508\n",
      "256/256 - 0s - loss: 44445.7852\n",
      "256/256 - 0s - loss: 51181.8867\n",
      "Reducing exploration for all agents to 0.1926\n",
      "\n",
      "Episode 120: Starting computation.\n",
      "Episode 120: Finished running.\n",
      "Agent 0, Average Reward: -413.36\n",
      "256/256 - 0s - loss: 41485.9219\n",
      "256/256 - 0s - loss: 41214.5508\n",
      "256/256 - 0s - loss: 35956.4805\n",
      "256/256 - 0s - loss: 40056.6562\n",
      "256/256 - 0s - loss: 38674.3398\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.1899\n",
      "\n",
      "Episode 121: Starting computation.\n",
      "Episode 121: Finished running.\n",
      "Agent 0, Average Reward: -899.92\n",
      "256/256 - 0s - loss: 191877.6562\n",
      "256/256 - 0s - loss: 189892.5312\n",
      "256/256 - 0s - loss: 187181.8125\n",
      "256/256 - 0s - loss: 181719.2188\n",
      "256/256 - 0s - loss: 184809.9688\n",
      "Reducing exploration for all agents to 0.1873\n",
      "\n",
      "Episode 122: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 122: Finished running.\n",
      "Agent 0, Average Reward: -3119.09\n",
      "256/256 - 0s - loss: 419751.7812\n",
      "256/256 - 0s - loss: 421357.5625\n",
      "256/256 - 0s - loss: 406524.4062\n",
      "256/256 - 0s - loss: 395756.4375\n",
      "256/256 - 0s - loss: 384949.2500\n",
      "Reducing exploration for all agents to 0.1847\n",
      "\n",
      "Episode 123: Starting computation.\n",
      "Episode 123: Finished running.\n",
      "Agent 0, Average Reward: -3119.48\n",
      "256/256 - 0s - loss: 636579.3750\n",
      "256/256 - 0s - loss: 597464.6875\n",
      "256/256 - 0s - loss: 580613.0000\n",
      "256/256 - 0s - loss: 538509.8750\n",
      "256/256 - 0s - loss: 484103.7188\n",
      "Reducing exploration for all agents to 0.1822\n",
      "\n",
      "Episode 124: Starting computation.\n",
      "Episode 124: Finished running.\n",
      "Agent 0, Average Reward: -326.71\n",
      "256/256 - 0s - loss: 431001.7812\n",
      "256/256 - 0s - loss: 374842.7812\n",
      "256/256 - 0s - loss: 325940.5312\n",
      "256/256 - 0s - loss: 277613.3750\n",
      "256/256 - 0s - loss: 205816.5781\n",
      "Reducing exploration for all agents to 0.1797\n",
      "\n",
      "Episode 125: Starting computation.\n",
      "Episode 125: Finished running.\n",
      "Agent 0, Average Reward: -1672.75\n",
      "256/256 - 0s - loss: 195232.1562\n",
      "256/256 - 0s - loss: 156437.4531\n",
      "256/256 - 0s - loss: 123091.2344\n",
      "256/256 - 0s - loss: 99018.3516\n",
      "256/256 - 0s - loss: 80457.6562\n",
      "Reducing exploration for all agents to 0.1772\n",
      "\n",
      "Episode 126: Starting computation.\n",
      "Episode 126: Finished running.\n",
      "Agent 0, Average Reward: -1862.25\n",
      "256/256 - 0s - loss: 52355.0312\n",
      "256/256 - 0s - loss: 40267.6875\n",
      "256/256 - 0s - loss: 30981.0801\n",
      "256/256 - 0s - loss: 21206.3906\n",
      "256/256 - 0s - loss: 18595.6562\n",
      "Reducing exploration for all agents to 0.1748\n",
      "\n",
      "Episode 127: Starting computation.\n",
      "Episode 127: Finished running.\n",
      "Agent 0, Average Reward: -1841.19\n",
      "256/256 - 0s - loss: 15948.6123\n",
      "256/256 - 0s - loss: 14513.3008\n",
      "256/256 - 0s - loss: 16580.2344\n",
      "256/256 - 0s - loss: 13385.1855\n",
      "256/256 - 0s - loss: 15329.5586\n",
      "Reducing exploration for all agents to 0.1724\n",
      "\n",
      "Episode 128: Starting computation.\n",
      "Episode 128: Finished running.\n",
      "Agent 0, Average Reward: -1909.57\n",
      "256/256 - 0s - loss: 8648.6836\n",
      "256/256 - 0s - loss: 9107.5127\n",
      "256/256 - 0s - loss: 12613.6699\n",
      "256/256 - 0s - loss: 14762.3779\n",
      "256/256 - 0s - loss: 12820.1699\n",
      "Reducing exploration for all agents to 0.17\n",
      "\n",
      "Episode 129: Starting computation.\n",
      "Episode 129: Finished running.\n",
      "Agent 0, Average Reward: -1801.06\n",
      "256/256 - 0s - loss: 13108.3311\n",
      "256/256 - 0s - loss: 24340.4297\n",
      "256/256 - 0s - loss: 17386.1016\n",
      "256/256 - 0s - loss: 20268.2207\n",
      "256/256 - 0s - loss: 20380.4102\n",
      "Reducing exploration for all agents to 0.1677\n",
      "\n",
      "Episode 130: Starting computation.\n",
      "Episode 130: Finished running.\n",
      "Agent 0, Average Reward: -1893.98\n",
      "256/256 - 0s - loss: 16370.8398\n",
      "256/256 - 0s - loss: 18469.9785\n",
      "256/256 - 0s - loss: 21015.5039\n",
      "256/256 - 0s - loss: 15842.3721\n",
      "256/256 - 0s - loss: 22091.3867\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.1654\n",
      "\n",
      "Episode 131: Starting computation.\n",
      "Episode 131: Finished running.\n",
      "Agent 0, Average Reward: -1860.27\n",
      "256/256 - 0s - loss: 492116.1250\n",
      "256/256 - 0s - loss: 463999.4375\n",
      "256/256 - 0s - loss: 460711.1562\n",
      "256/256 - 0s - loss: 418639.5625\n",
      "256/256 - 0s - loss: 364354.5938\n",
      "Reducing exploration for all agents to 0.1631\n",
      "\n",
      "Episode 132: Starting computation.\n",
      "Episode 132: Finished running.\n",
      "Agent 0, Average Reward: -338.37\n",
      "256/256 - 0s - loss: 271013.4688\n",
      "256/256 - 0s - loss: 225696.5625\n",
      "256/256 - 0s - loss: 205407.6875\n",
      "256/256 - 0s - loss: 231162.3125\n",
      "256/256 - 0s - loss: 196616.4688\n",
      "Reducing exploration for all agents to 0.1608\n",
      "\n",
      "Episode 133: Starting computation.\n",
      "Episode 133: Finished running.\n",
      "Agent 0, Average Reward: -174.83\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 131433.2188\n",
      "256/256 - 0s - loss: 108521.4766\n",
      "256/256 - 0s - loss: 82126.5938\n",
      "256/256 - 0s - loss: 68539.2891\n",
      "256/256 - 0s - loss: 54106.5508\n",
      "Reducing exploration for all agents to 0.1586\n",
      "\n",
      "Episode 134: Starting computation.\n",
      "Episode 134: Finished running.\n",
      "Agent 0, Average Reward: -341.68\n",
      "256/256 - 0s - loss: 39610.1641\n",
      "256/256 - 0s - loss: 36654.9102\n",
      "256/256 - 0s - loss: 31321.9941\n",
      "256/256 - 0s - loss: 22412.9238\n",
      "256/256 - 0s - loss: 20160.2793\n",
      "Reducing exploration for all agents to 0.1565\n",
      "\n",
      "Episode 135: Starting computation.\n",
      "Episode 135: Finished running.\n",
      "Agent 0, Average Reward: -1757.0\n",
      "256/256 - 0s - loss: 110670.8906\n",
      "256/256 - 0s - loss: 107522.2500\n",
      "256/256 - 0s - loss: 102020.4688\n",
      "256/256 - 0s - loss: 107320.2109\n",
      "256/256 - 0s - loss: 103005.7734\n",
      "Reducing exploration for all agents to 0.1543\n",
      "\n",
      "Episode 136: Starting computation.\n",
      "Episode 136: Finished running.\n",
      "Agent 0, Average Reward: -1878.96\n",
      "256/256 - 0s - loss: 184422.0938\n",
      "256/256 - 0s - loss: 192371.3594\n",
      "256/256 - 0s - loss: 176912.6250\n",
      "256/256 - 0s - loss: 160408.4375\n",
      "256/256 - 0s - loss: 153550.9844\n",
      "Reducing exploration for all agents to 0.1522\n",
      "\n",
      "Episode 137: Starting computation.\n",
      "Episode 137: Finished running.\n",
      "Agent 0, Average Reward: -1934.5\n",
      "256/256 - 0s - loss: 211516.8125\n",
      "256/256 - 0s - loss: 201127.1719\n",
      "256/256 - 0s - loss: 189612.5625\n",
      "256/256 - 0s - loss: 177284.7812\n",
      "256/256 - 0s - loss: 164198.9531\n",
      "Reducing exploration for all agents to 0.1501\n",
      "\n",
      "Episode 138: Starting computation.\n",
      "Episode 138: Finished running.\n",
      "Agent 0, Average Reward: -2361.93\n",
      "256/256 - 0s - loss: 182668.3125\n",
      "256/256 - 0s - loss: 172312.8750\n",
      "256/256 - 0s - loss: 148266.6250\n",
      "256/256 - 0s - loss: 126610.1719\n",
      "256/256 - 0s - loss: 114369.6094\n",
      "Reducing exploration for all agents to 0.148\n",
      "\n",
      "Episode 139: Starting computation.\n",
      "Episode 139: Finished running.\n",
      "Agent 0, Average Reward: -3051.8\n",
      "256/256 - 0s - loss: 109647.9453\n",
      "256/256 - 0s - loss: 88226.7969\n",
      "256/256 - 0s - loss: 66132.9219\n",
      "256/256 - 0s - loss: 57543.3008\n",
      "256/256 - 0s - loss: 46150.7344\n",
      "Reducing exploration for all agents to 0.146\n",
      "\n",
      "Episode 140: Starting computation.\n",
      "Episode 140: Finished running.\n",
      "Agent 0, Average Reward: -2979.34\n",
      "256/256 - 0s - loss: 35594.8164\n",
      "256/256 - 0s - loss: 31310.2227\n",
      "256/256 - 0s - loss: 29324.5137\n",
      "256/256 - 0s - loss: 22712.0391\n",
      "256/256 - 0s - loss: 20928.4434\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.144\n",
      "\n",
      "Episode 141: Starting computation.\n",
      "Episode 141: Finished running.\n",
      "Agent 0, Average Reward: -3060.77\n",
      "256/256 - 0s - loss: 1081991.7500\n",
      "256/256 - 0s - loss: 986778.8750\n",
      "256/256 - 0s - loss: 870178.2500\n",
      "256/256 - 0s - loss: 750652.5000\n",
      "256/256 - 0s - loss: 674390.2500\n",
      "Reducing exploration for all agents to 0.142\n",
      "\n",
      "Episode 142: Starting computation.\n",
      "Episode 142: Finished running.\n",
      "Agent 0, Average Reward: -2078.91\n",
      "256/256 - 0s - loss: 565133.8125\n",
      "256/256 - 0s - loss: 422797.4688\n",
      "256/256 - 0s - loss: 321525.5000\n",
      "256/256 - 0s - loss: 219566.8125\n",
      "256/256 - 0s - loss: 136107.2969\n",
      "Reducing exploration for all agents to 0.1401\n",
      "\n",
      "Episode 143: Starting computation.\n",
      "Episode 143: Finished running.\n",
      "Agent 0, Average Reward: -1980.31\n",
      "256/256 - 0s - loss: 79547.4297\n",
      "256/256 - 0s - loss: 45135.3398\n",
      "256/256 - 0s - loss: 27151.5547\n",
      "256/256 - 0s - loss: 29632.5801\n",
      "256/256 - 0s - loss: 27602.8047\n",
      "Reducing exploration for all agents to 0.1381\n",
      "\n",
      "Episode 144: Starting computation.\n",
      "Episode 144: Finished running.\n",
      "Agent 0, Average Reward: -1942.84\n",
      "256/256 - 0s - loss: 21245.3359\n",
      "256/256 - 0s - loss: 39231.5547\n",
      "256/256 - 0s - loss: 47368.7422\n",
      "256/256 - 0s - loss: 53591.4297\n",
      "256/256 - 0s - loss: 65195.6328\n",
      "Reducing exploration for all agents to 0.1362\n",
      "\n",
      "Episode 145: Starting computation.\n",
      "Episode 145: Finished running.\n",
      "Agent 0, Average Reward: -1890.37\n",
      "256/256 - 0s - loss: 47080.4023\n",
      "256/256 - 0s - loss: 62359.0195\n",
      "256/256 - 0s - loss: 45671.2617\n",
      "256/256 - 0s - loss: 54098.5469\n",
      "256/256 - 0s - loss: 60481.9141\n",
      "Reducing exploration for all agents to 0.1344\n",
      "\n",
      "Episode 146: Starting computation.\n",
      "Episode 146: Finished running.\n",
      "Agent 0, Average Reward: -1979.82\n",
      "256/256 - 0s - loss: 33517.0312\n",
      "256/256 - 0s - loss: 24582.8242\n",
      "256/256 - 0s - loss: 27609.2617\n",
      "256/256 - 0s - loss: 31489.1758\n",
      "256/256 - 0s - loss: 32956.8516\n",
      "Reducing exploration for all agents to 0.1325\n",
      "\n",
      "Episode 147: Starting computation.\n",
      "Episode 147: Finished running.\n",
      "Agent 0, Average Reward: -1900.58\n",
      "256/256 - 0s - loss: 26036.2129\n",
      "256/256 - 0s - loss: 20565.0977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 18493.2559\n",
      "256/256 - 0s - loss: 16621.8281\n",
      "256/256 - 0s - loss: 18079.4551\n",
      "Reducing exploration for all agents to 0.1307\n",
      "\n",
      "Episode 148: Starting computation.\n",
      "Episode 148: Finished running.\n",
      "Agent 0, Average Reward: -1955.98\n",
      "256/256 - 0s - loss: 20523.4766\n",
      "256/256 - 0s - loss: 18108.1914\n",
      "256/256 - 0s - loss: 19062.7344\n",
      "256/256 - 0s - loss: 14447.8516\n",
      "256/256 - 0s - loss: 18145.1895\n",
      "Reducing exploration for all agents to 0.1289\n",
      "\n",
      "Episode 149: Starting computation.\n",
      "Episode 149: Finished running.\n",
      "Agent 0, Average Reward: -1950.24\n",
      "256/256 - 0s - loss: 23031.0039\n",
      "256/256 - 0s - loss: 26197.9590\n",
      "256/256 - 0s - loss: 15834.0156\n",
      "256/256 - 0s - loss: 16088.4111\n",
      "256/256 - 0s - loss: 15029.7520\n",
      "Reducing exploration for all agents to 0.1271\n",
      "\n",
      "Episode 150: Starting computation.\n",
      "Episode 150: Finished running.\n",
      "Agent 0, Average Reward: -1974.9\n",
      "256/256 - 0s - loss: 16466.0312\n",
      "256/256 - 0s - loss: 17844.7441\n",
      "256/256 - 0s - loss: 24649.9883\n",
      "256/256 - 0s - loss: 22215.2109\n",
      "256/256 - 0s - loss: 22489.4258\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.1254\n",
      "\n",
      "Episode 151: Starting computation.\n",
      "Episode 151: Finished running.\n",
      "Agent 0, Average Reward: -1955.37\n",
      "256/256 - 0s - loss: 376894.3438\n",
      "256/256 - 0s - loss: 381698.1562\n",
      "256/256 - 0s - loss: 341431.6250\n",
      "256/256 - 0s - loss: 313857.1562\n",
      "256/256 - 0s - loss: 278126.8438\n",
      "Reducing exploration for all agents to 0.1236\n",
      "\n",
      "Episode 152: Starting computation.\n",
      "Episode 152: Finished running.\n",
      "Agent 0, Average Reward: -1933.84\n",
      "256/256 - 0s - loss: 227218.5469\n",
      "256/256 - 0s - loss: 187747.3125\n",
      "256/256 - 0s - loss: 145083.1719\n",
      "256/256 - 0s - loss: 109460.9922\n",
      "256/256 - 0s - loss: 80284.9609\n",
      "Reducing exploration for all agents to 0.1219\n",
      "\n",
      "Episode 153: Starting computation.\n",
      "Episode 153: Finished running.\n",
      "Agent 0, Average Reward: -281.88\n",
      "256/256 - 0s - loss: 48776.3555\n",
      "256/256 - 0s - loss: 107861.4297\n",
      "256/256 - 0s - loss: 77801.8359\n",
      "256/256 - 0s - loss: 57844.6797\n",
      "256/256 - 0s - loss: 46684.9453\n",
      "Reducing exploration for all agents to 0.1203\n",
      "\n",
      "Episode 154: Starting computation.\n",
      "Episode 154: Finished running.\n",
      "Agent 0, Average Reward: -180.24\n",
      "256/256 - 0s - loss: 25177.6953\n",
      "256/256 - 0s - loss: 17340.9648\n",
      "256/256 - 0s - loss: 14089.3955\n",
      "256/256 - 0s - loss: 8837.7568\n",
      "256/256 - 0s - loss: 11704.6113\n",
      "Reducing exploration for all agents to 0.1186\n",
      "\n",
      "Episode 155: Starting computation.\n",
      "Episode 155: Finished running.\n",
      "Agent 0, Average Reward: -377.18\n",
      "256/256 - 0s - loss: 30065.6309\n",
      "256/256 - 0s - loss: 31030.4023\n",
      "256/256 - 0s - loss: 32854.3203\n",
      "256/256 - 0s - loss: 30953.5312\n",
      "256/256 - 0s - loss: 32740.1562\n",
      "Reducing exploration for all agents to 0.117\n",
      "\n",
      "Episode 156: Starting computation.\n",
      "Episode 156: Finished running.\n",
      "Agent 0, Average Reward: -1746.54\n",
      "256/256 - 0s - loss: 92023.8125\n",
      "256/256 - 0s - loss: 104214.6797\n",
      "256/256 - 0s - loss: 95400.8672\n",
      "256/256 - 0s - loss: 92287.1641\n",
      "256/256 - 0s - loss: 99619.2734\n",
      "Reducing exploration for all agents to 0.1154\n",
      "\n",
      "Episode 157: Starting computation.\n",
      "Episode 157: Finished running.\n",
      "Agent 0, Average Reward: -738.88\n",
      "256/256 - 0s - loss: 97065.1328\n",
      "256/256 - 0s - loss: 110353.8047\n",
      "256/256 - 0s - loss: 96062.7578\n",
      "256/256 - 0s - loss: 93174.1172\n",
      "256/256 - 0s - loss: 92701.0547\n",
      "Reducing exploration for all agents to 0.1138\n",
      "\n",
      "Episode 158: Starting computation.\n",
      "Episode 158: Finished running.\n",
      "Agent 0, Average Reward: -3169.11\n",
      "256/256 - 0s - loss: 128218.9141\n",
      "256/256 - 0s - loss: 116342.8672\n",
      "256/256 - 0s - loss: 128424.9297\n",
      "256/256 - 0s - loss: 103011.1328\n",
      "256/256 - 0s - loss: 88626.4453\n",
      "Reducing exploration for all agents to 0.1122\n",
      "\n",
      "Episode 159: Starting computation.\n",
      "Episode 159: Finished running.\n",
      "Agent 0, Average Reward: -3156.22\n",
      "256/256 - 0s - loss: 91875.7109\n",
      "256/256 - 0s - loss: 86598.5703\n",
      "256/256 - 0s - loss: 71631.7109\n",
      "256/256 - 0s - loss: 59611.2969\n",
      "256/256 - 0s - loss: 56021.3398\n",
      "Reducing exploration for all agents to 0.1107\n",
      "\n",
      "Episode 160: Starting computation.\n",
      "Episode 160: Finished running.\n",
      "Agent 0, Average Reward: -3175.74\n",
      "256/256 - 0s - loss: 39627.6562\n",
      "256/256 - 0s - loss: 47435.4414\n",
      "256/256 - 0s - loss: 31277.0234\n",
      "256/256 - 0s - loss: 29259.4238\n",
      "256/256 - 0s - loss: 26244.2949\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.1092\n",
      "\n",
      "Episode 161: Starting computation.\n",
      "Episode 161: Finished running.\n",
      "Agent 0, Average Reward: -3184.61\n",
      "256/256 - 0s - loss: 988284.0000\n",
      "256/256 - 0s - loss: 892456.5000\n",
      "256/256 - 0s - loss: 754418.7500\n",
      "256/256 - 0s - loss: 602468.8125\n",
      "256/256 - 0s - loss: 575626.2500\n",
      "Reducing exploration for all agents to 0.1077\n",
      "\n",
      "Episode 162: Starting computation.\n",
      "Episode 162: Finished running.\n",
      "Agent 0, Average Reward: -2023.41\n",
      "256/256 - 0s - loss: 387634.1562\n",
      "256/256 - 0s - loss: 284120.2188\n",
      "256/256 - 0s - loss: 186733.1719\n",
      "256/256 - 0s - loss: 108623.4609\n",
      "256/256 - 0s - loss: 62042.9531\n",
      "Reducing exploration for all agents to 0.1062\n",
      "\n",
      "Episode 163: Starting computation.\n",
      "Episode 163: Finished running.\n",
      "Agent 0, Average Reward: -1944.17\n",
      "256/256 - 0s - loss: 41228.4336\n",
      "256/256 - 0s - loss: 23403.8398\n",
      "256/256 - 0s - loss: 22055.9609\n",
      "256/256 - 0s - loss: 42340.7461\n",
      "256/256 - 0s - loss: 60032.4570\n",
      "Reducing exploration for all agents to 0.1047\n",
      "\n",
      "Episode 164: Starting computation.\n",
      "Episode 164: Finished running.\n",
      "Agent 0, Average Reward: -1972.18\n",
      "256/256 - 0s - loss: 68133.9297\n",
      "256/256 - 0s - loss: 78857.7656\n",
      "256/256 - 0s - loss: 99580.5625\n",
      "256/256 - 0s - loss: 90167.3750\n",
      "256/256 - 0s - loss: 115278.3281\n",
      "Reducing exploration for all agents to 0.1033\n",
      "\n",
      "Episode 165: Starting computation.\n",
      "Episode 165: Finished running.\n",
      "Agent 0, Average Reward: -1942.12\n",
      "256/256 - 0s - loss: 51148.1523\n",
      "256/256 - 0s - loss: 56234.3242\n",
      "256/256 - 0s - loss: 61658.2148\n",
      "256/256 - 0s - loss: 40556.6367\n",
      "256/256 - 0s - loss: 65633.3125\n",
      "Reducing exploration for all agents to 0.1019\n",
      "\n",
      "Episode 166: Starting computation.\n",
      "Episode 166: Finished running.\n",
      "Agent 0, Average Reward: -2015.94\n",
      "256/256 - 0s - loss: 23456.1621\n",
      "256/256 - 0s - loss: 19249.1836\n",
      "256/256 - 0s - loss: 21794.3340\n",
      "256/256 - 0s - loss: 13610.3271\n",
      "256/256 - 0s - loss: 14824.0479\n",
      "Reducing exploration for all agents to 0.1005\n",
      "\n",
      "Episode 167: Starting computation.\n",
      "Episode 167: Finished running.\n",
      "Agent 0, Average Reward: -1957.85\n",
      "256/256 - 0s - loss: 13568.8213\n",
      "256/256 - 0s - loss: 12015.3857\n",
      "256/256 - 0s - loss: 13590.5332\n",
      "256/256 - 0s - loss: 14532.3867\n",
      "256/256 - 0s - loss: 17715.3789\n",
      "Reducing exploration for all agents to 0.0991\n",
      "\n",
      "Episode 168: Starting computation.\n",
      "Episode 168: Finished running.\n",
      "Agent 0, Average Reward: -1991.29\n",
      "256/256 - 0s - loss: 11914.8770\n",
      "256/256 - 0s - loss: 14127.0420\n",
      "256/256 - 0s - loss: 11230.4912\n",
      "256/256 - 0s - loss: 9762.2559\n",
      "256/256 - 0s - loss: 15864.5830\n",
      "Reducing exploration for all agents to 0.0977\n",
      "\n",
      "Episode 169: Starting computation.\n",
      "Episode 169: Finished running.\n",
      "Agent 0, Average Reward: -1983.59\n",
      "256/256 - 0s - loss: 12875.7373\n",
      "256/256 - 0s - loss: 10878.2217\n",
      "256/256 - 0s - loss: 13425.6621\n",
      "256/256 - 0s - loss: 15921.8320\n",
      "256/256 - 0s - loss: 30840.4824\n",
      "Reducing exploration for all agents to 0.0964\n",
      "\n",
      "Episode 170: Starting computation.\n",
      "Episode 170: Finished running.\n",
      "Agent 0, Average Reward: -2008.95\n",
      "256/256 - 0s - loss: 9000.0215\n",
      "256/256 - 0s - loss: 22770.9609\n",
      "256/256 - 0s - loss: 11617.3467\n",
      "256/256 - 0s - loss: 23984.9570\n",
      "256/256 - 0s - loss: 10909.0020\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0951\n",
      "\n",
      "Episode 171: Starting computation.\n",
      "Episode 171: Finished running.\n",
      "Agent 0, Average Reward: -1967.19\n",
      "256/256 - 0s - loss: 305727.4375\n",
      "256/256 - 0s - loss: 286380.9375\n",
      "256/256 - 0s - loss: 253936.1562\n",
      "256/256 - 0s - loss: 222243.9219\n",
      "256/256 - 0s - loss: 187734.6406\n",
      "Reducing exploration for all agents to 0.0937\n",
      "\n",
      "Episode 172: Starting computation.\n",
      "Episode 172: Finished running.\n",
      "Agent 0, Average Reward: -1926.69\n",
      "256/256 - 0s - loss: 156543.7656\n",
      "256/256 - 0s - loss: 127109.4219\n",
      "256/256 - 0s - loss: 96509.2500\n",
      "256/256 - 0s - loss: 116915.5781\n",
      "256/256 - 0s - loss: 103579.8125\n",
      "Reducing exploration for all agents to 0.0925\n",
      "\n",
      "Episode 173: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 173: Finished running.\n",
      "Agent 0, Average Reward: -2338.12\n",
      "256/256 - 0s - loss: 155183.1719\n",
      "256/256 - 0s - loss: 130322.0078\n",
      "256/256 - 0s - loss: 117482.4531\n",
      "256/256 - 0s - loss: 102207.4766\n",
      "256/256 - 0s - loss: 96141.6719\n",
      "Reducing exploration for all agents to 0.0912\n",
      "\n",
      "Episode 174: Starting computation.\n",
      "Episode 174: Finished running.\n",
      "Agent 0, Average Reward: -2424.02\n",
      "256/256 - 0s - loss: 156885.4531\n",
      "256/256 - 0s - loss: 130866.3594\n",
      "256/256 - 0s - loss: 151425.7344\n",
      "256/256 - 0s - loss: 146804.9531\n",
      "256/256 - 0s - loss: 122493.7969\n",
      "Reducing exploration for all agents to 0.0899\n",
      "\n",
      "Episode 175: Starting computation.\n",
      "Episode 175: Finished running.\n",
      "Agent 0, Average Reward: -503.56\n",
      "256/256 - 0s - loss: 120677.7656\n",
      "256/256 - 0s - loss: 99699.6484\n",
      "256/256 - 0s - loss: 88635.1250\n",
      "256/256 - 0s - loss: 84498.4375\n",
      "256/256 - 0s - loss: 66655.0000\n",
      "Reducing exploration for all agents to 0.0887\n",
      "\n",
      "Episode 176: Starting computation.\n",
      "Episode 176: Finished running.\n",
      "Agent 0, Average Reward: -422.17\n",
      "256/256 - 0s - loss: 81970.3984\n",
      "256/256 - 0s - loss: 72179.7500\n",
      "256/256 - 0s - loss: 69214.3281\n",
      "256/256 - 0s - loss: 72120.4219\n",
      "256/256 - 0s - loss: 59121.2109\n",
      "Reducing exploration for all agents to 0.0875\n",
      "\n",
      "Episode 177: Starting computation.\n",
      "Episode 177: Finished running.\n",
      "Agent 0, Average Reward: -460.02\n",
      "256/256 - 0s - loss: 71786.4297\n",
      "256/256 - 0s - loss: 82017.8906\n",
      "256/256 - 0s - loss: 74331.2344\n",
      "256/256 - 0s - loss: 86300.7656\n",
      "256/256 - 0s - loss: 77373.1875\n",
      "Reducing exploration for all agents to 0.0863\n",
      "\n",
      "Episode 178: Starting computation.\n",
      "Episode 178: Finished running.\n",
      "Agent 0, Average Reward: -470.43\n",
      "256/256 - 0s - loss: 90156.5312\n",
      "256/256 - 0s - loss: 84406.5312\n",
      "256/256 - 0s - loss: 142750.3281\n",
      "256/256 - 0s - loss: 96304.1719\n",
      "256/256 - 0s - loss: 133415.7969\n",
      "Reducing exploration for all agents to 0.0851\n",
      "\n",
      "Episode 179: Starting computation.\n",
      "Episode 179: Finished running.\n",
      "Agent 0, Average Reward: -447.67\n",
      "256/256 - 0s - loss: 86726.0703\n",
      "256/256 - 0s - loss: 119945.0469\n",
      "256/256 - 0s - loss: 88095.8359\n",
      "256/256 - 0s - loss: 144500.7031\n",
      "256/256 - 0s - loss: 137224.9531\n",
      "Reducing exploration for all agents to 0.0839\n",
      "\n",
      "Episode 180: Starting computation.\n",
      "Episode 180: Finished running.\n",
      "Agent 0, Average Reward: -489.49\n",
      "256/256 - 0s - loss: 131049.1719\n",
      "256/256 - 0s - loss: 99253.8750\n",
      "256/256 - 0s - loss: 158559.2969\n",
      "256/256 - 0s - loss: 105248.2500\n",
      "256/256 - 0s - loss: 114366.9453\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0828\n",
      "\n",
      "Episode 181: Starting computation.\n",
      "Episode 181: Finished running.\n",
      "Agent 0, Average Reward: -466.78\n",
      "256/256 - 0s - loss: 118714.2891\n",
      "256/256 - 0s - loss: 132297.6406\n",
      "256/256 - 0s - loss: 129830.7344\n",
      "256/256 - 0s - loss: 123027.7422\n",
      "256/256 - 0s - loss: 137023.5625\n",
      "Reducing exploration for all agents to 0.0816\n",
      "\n",
      "Episode 182: Starting computation.\n",
      "Episode 182: Finished running.\n",
      "Agent 0, Average Reward: -457.93\n",
      "256/256 - 0s - loss: 150520.8281\n",
      "256/256 - 0s - loss: 121994.3750\n",
      "256/256 - 0s - loss: 156600.1094\n",
      "256/256 - 0s - loss: 164377.4688\n",
      "256/256 - 0s - loss: 115565.9062\n",
      "Reducing exploration for all agents to 0.0805\n",
      "\n",
      "Episode 183: Starting computation.\n",
      "Episode 183: Finished running.\n",
      "Agent 0, Average Reward: -616.9\n",
      "256/256 - 0s - loss: 137363.1094\n",
      "256/256 - 0s - loss: 139276.7031\n",
      "256/256 - 0s - loss: 121105.9453\n",
      "256/256 - 0s - loss: 152542.7969\n",
      "256/256 - 0s - loss: 165254.7031\n",
      "Reducing exploration for all agents to 0.0794\n",
      "\n",
      "Episode 184: Starting computation.\n",
      "Episode 184: Finished running.\n",
      "Agent 0, Average Reward: -1167.91\n",
      "256/256 - 0s - loss: 186671.5469\n",
      "256/256 - 0s - loss: 146560.3281\n",
      "256/256 - 0s - loss: 169208.1094\n",
      "256/256 - 0s - loss: 191944.4375\n",
      "256/256 - 0s - loss: 211799.1250\n",
      "Reducing exploration for all agents to 0.0783\n",
      "\n",
      "Episode 185: Starting computation.\n",
      "Episode 185: Finished running.\n",
      "Agent 0, Average Reward: -3115.83\n",
      "256/256 - 0s - loss: 282459.5312\n",
      "256/256 - 0s - loss: 324674.4688\n",
      "256/256 - 0s - loss: 317598.7812\n",
      "256/256 - 0s - loss: 255116.1250\n",
      "256/256 - 0s - loss: 227156.9844\n",
      "Reducing exploration for all agents to 0.0772\n",
      "\n",
      "Episode 186: Starting computation.\n",
      "Episode 186: Finished running.\n",
      "Agent 0, Average Reward: -794.11\n",
      "256/256 - 0s - loss: 302983.0938\n",
      "256/256 - 0s - loss: 204163.5625\n",
      "256/256 - 0s - loss: 274481.0000\n",
      "256/256 - 0s - loss: 245089.7031\n",
      "256/256 - 0s - loss: 224062.3281\n",
      "Reducing exploration for all agents to 0.0762\n",
      "\n",
      "Episode 187: Starting computation.\n",
      "Episode 187: Finished running.\n",
      "Agent 0, Average Reward: -254.74\n",
      "256/256 - 0s - loss: 169299.6406\n",
      "256/256 - 0s - loss: 148855.4531\n",
      "256/256 - 0s - loss: 141577.7031\n",
      "256/256 - 0s - loss: 162706.5469\n",
      "256/256 - 0s - loss: 168259.8906\n",
      "Reducing exploration for all agents to 0.0751\n",
      "\n",
      "Episode 188: Starting computation.\n",
      "Episode 188: Finished running.\n",
      "Agent 0, Average Reward: -200.64\n",
      "256/256 - 0s - loss: 80282.6016\n",
      "256/256 - 0s - loss: 111607.7812\n",
      "256/256 - 0s - loss: 101243.7812\n",
      "256/256 - 0s - loss: 86991.9844\n",
      "256/256 - 0s - loss: 92083.7188\n",
      "Reducing exploration for all agents to 0.0741\n",
      "\n",
      "Episode 189: Starting computation.\n",
      "Episode 189: Finished running.\n",
      "Agent 0, Average Reward: -198.21\n",
      "256/256 - 0s - loss: 83224.6562\n",
      "256/256 - 0s - loss: 47354.8047\n",
      "256/256 - 0s - loss: 125906.7500\n",
      "256/256 - 0s - loss: 90203.7344\n",
      "256/256 - 0s - loss: 70239.1875\n",
      "Reducing exploration for all agents to 0.0731\n",
      "\n",
      "Episode 190: Starting computation.\n",
      "Episode 190: Finished running.\n",
      "Agent 0, Average Reward: -200.81\n",
      "256/256 - 0s - loss: 41348.7344\n",
      "256/256 - 0s - loss: 44639.9141\n",
      "256/256 - 0s - loss: 78273.5625\n",
      "256/256 - 0s - loss: 65141.4297\n",
      "256/256 - 0s - loss: 100195.9062\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0721\n",
      "\n",
      "Episode 191: Starting computation.\n",
      "Episode 191: Finished running.\n",
      "Agent 0, Average Reward: -187.8\n",
      "256/256 - 0s - loss: 25974.7246\n",
      "256/256 - 0s - loss: 27284.6055\n",
      "256/256 - 0s - loss: 23946.1250\n",
      "256/256 - 0s - loss: 23490.9883\n",
      "256/256 - 0s - loss: 33108.7852\n",
      "Reducing exploration for all agents to 0.0711\n",
      "\n",
      "Episode 192: Starting computation.\n",
      "Episode 192: Finished running.\n",
      "Agent 0, Average Reward: -199.75\n",
      "256/256 - 0s - loss: 20775.2734\n",
      "256/256 - 0s - loss: 18418.9375\n",
      "256/256 - 0s - loss: 19454.7383\n",
      "256/256 - 0s - loss: 18511.9414\n",
      "256/256 - 0s - loss: 19516.1602\n",
      "Reducing exploration for all agents to 0.0701\n",
      "\n",
      "Episode 193: Starting computation.\n",
      "Episode 193: Finished running.\n",
      "Agent 0, Average Reward: -198.68\n",
      "256/256 - 0s - loss: 32852.7188\n",
      "256/256 - 0s - loss: 21237.3828\n",
      "256/256 - 0s - loss: 20449.8047\n",
      "256/256 - 0s - loss: 20984.1797\n",
      "256/256 - 0s - loss: 22802.2402\n",
      "Reducing exploration for all agents to 0.0691\n",
      "\n",
      "Episode 194: Starting computation.\n",
      "Episode 194: Finished running.\n",
      "Agent 0, Average Reward: -197.98\n",
      "256/256 - 0s - loss: 18015.0469\n",
      "256/256 - 0s - loss: 18726.7090\n",
      "256/256 - 0s - loss: 17703.7402\n",
      "256/256 - 0s - loss: 18673.2559\n",
      "256/256 - 0s - loss: 16718.6113\n",
      "Reducing exploration for all agents to 0.0682\n",
      "\n",
      "Episode 195: Starting computation.\n",
      "Episode 195: Finished running.\n",
      "Agent 0, Average Reward: -201.71\n",
      "256/256 - 0s - loss: 18572.0215\n",
      "256/256 - 0s - loss: 17051.7324\n",
      "256/256 - 0s - loss: 17342.7383\n",
      "256/256 - 0s - loss: 16150.3105\n",
      "256/256 - 0s - loss: 18226.4336\n",
      "Reducing exploration for all agents to 0.0672\n",
      "\n",
      "Episode 196: Starting computation.\n",
      "Episode 196: Finished running.\n",
      "Agent 0, Average Reward: -199.86\n",
      "256/256 - 0s - loss: 20791.2500\n",
      "256/256 - 0s - loss: 22716.0469\n",
      "256/256 - 0s - loss: 16473.4473\n",
      "256/256 - 0s - loss: 16117.5332\n",
      "256/256 - 0s - loss: 16005.1270\n",
      "Reducing exploration for all agents to 0.0663\n",
      "\n",
      "Episode 197: Starting computation.\n",
      "Episode 197: Finished running.\n",
      "Agent 0, Average Reward: -207.16\n",
      "256/256 - 0s - loss: 19339.0039\n",
      "256/256 - 0s - loss: 25340.0117\n",
      "256/256 - 0s - loss: 20318.6250\n",
      "256/256 - 0s - loss: 19566.4023\n",
      "256/256 - 0s - loss: 16279.1143\n",
      "Reducing exploration for all agents to 0.0654\n",
      "\n",
      "Episode 198: Starting computation.\n",
      "Episode 198: Finished running.\n",
      "Agent 0, Average Reward: -201.52\n",
      "256/256 - 0s - loss: 19144.7969\n",
      "256/256 - 0s - loss: 18470.6914\n",
      "256/256 - 0s - loss: 18864.0215\n",
      "256/256 - 0s - loss: 17688.0664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 17201.2988\n",
      "Reducing exploration for all agents to 0.0645\n",
      "\n",
      "Episode 199: Starting computation.\n",
      "Episode 199: Finished running.\n",
      "Agent 0, Average Reward: -208.71\n",
      "256/256 - 0s - loss: 20702.8145\n",
      "256/256 - 0s - loss: 22242.9395\n",
      "256/256 - 0s - loss: 24347.3320\n",
      "256/256 - 0s - loss: 21589.5938\n",
      "256/256 - 0s - loss: 16987.6953\n",
      "Reducing exploration for all agents to 0.0636\n",
      "\n",
      "Episode 200: Starting computation.\n",
      "Episode 200: Finished running.\n",
      "Agent 0, Average Reward: -215.11\n",
      "256/256 - 0s - loss: 25115.4219\n",
      "256/256 - 0s - loss: 21614.3613\n",
      "256/256 - 0s - loss: 21191.3594\n",
      "256/256 - 0s - loss: 21850.1895\n",
      "256/256 - 0s - loss: 24339.4824\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0627\n",
      "\n",
      "Episode 201: Starting computation.\n",
      "Episode 201: Finished running.\n",
      "Agent 0, Average Reward: -214.3\n",
      "256/256 - 0s - loss: 21625.2188\n",
      "256/256 - 0s - loss: 20688.1602\n",
      "256/256 - 0s - loss: 24071.0020\n",
      "256/256 - 0s - loss: 21250.4844\n",
      "256/256 - 0s - loss: 24032.0137\n",
      "Reducing exploration for all agents to 0.0619\n",
      "\n",
      "Episode 202: Starting computation.\n",
      "Episode 202: Finished running.\n",
      "Agent 0, Average Reward: -214.1\n",
      "256/256 - 0s - loss: 27037.5840\n",
      "256/256 - 0s - loss: 22287.9727\n",
      "256/256 - 0s - loss: 21280.6641\n",
      "256/256 - 0s - loss: 26746.8613\n",
      "256/256 - 0s - loss: 23159.8496\n",
      "Reducing exploration for all agents to 0.061\n",
      "\n",
      "Episode 203: Starting computation.\n",
      "Episode 203: Finished running.\n",
      "Agent 0, Average Reward: -210.88\n",
      "256/256 - 0s - loss: 24024.4590\n",
      "256/256 - 0s - loss: 25767.1816\n",
      "256/256 - 0s - loss: 23659.4141\n",
      "256/256 - 0s - loss: 21167.5547\n",
      "256/256 - 0s - loss: 24253.6289\n",
      "Reducing exploration for all agents to 0.0602\n",
      "\n",
      "Episode 204: Starting computation.\n",
      "Episode 204: Finished running.\n",
      "Agent 0, Average Reward: -219.23\n",
      "256/256 - 0s - loss: 24129.4062\n",
      "256/256 - 0s - loss: 21982.2266\n",
      "256/256 - 0s - loss: 18429.0117\n",
      "256/256 - 0s - loss: 21276.6934\n",
      "256/256 - 0s - loss: 19513.9844\n",
      "Reducing exploration for all agents to 0.0594\n",
      "\n",
      "Episode 205: Starting computation.\n",
      "Episode 205: Finished running.\n",
      "Agent 0, Average Reward: -204.3\n",
      "256/256 - 0s - loss: 27527.0605\n",
      "256/256 - 0s - loss: 24610.7480\n",
      "256/256 - 0s - loss: 26453.9648\n",
      "256/256 - 0s - loss: 22397.5664\n",
      "256/256 - 0s - loss: 18855.4062\n",
      "Reducing exploration for all agents to 0.0586\n",
      "\n",
      "Episode 206: Starting computation.\n",
      "Episode 206: Finished running.\n",
      "Agent 0, Average Reward: -222.32\n",
      "256/256 - 0s - loss: 18943.0664\n",
      "256/256 - 0s - loss: 27211.2461\n",
      "256/256 - 0s - loss: 24372.8086\n",
      "256/256 - 0s - loss: 24404.5488\n",
      "256/256 - 0s - loss: 19767.9648\n",
      "Reducing exploration for all agents to 0.0577\n",
      "\n",
      "Episode 207: Starting computation.\n",
      "Episode 207: Finished running.\n",
      "Agent 0, Average Reward: -215.53\n",
      "256/256 - 0s - loss: 21523.8008\n",
      "256/256 - 0s - loss: 23298.3867\n",
      "256/256 - 0s - loss: 19846.5684\n",
      "256/256 - 0s - loss: 22240.5430\n",
      "256/256 - 0s - loss: 25048.6914\n",
      "Reducing exploration for all agents to 0.057\n",
      "\n",
      "Episode 208: Starting computation.\n",
      "Episode 208: Finished running.\n",
      "Agent 0, Average Reward: -218.79\n",
      "256/256 - 0s - loss: 19696.7090\n",
      "256/256 - 0s - loss: 18942.6211\n",
      "256/256 - 0s - loss: 19609.1406\n",
      "256/256 - 0s - loss: 22729.6152\n",
      "256/256 - 0s - loss: 26975.0215\n",
      "Reducing exploration for all agents to 0.0562\n",
      "\n",
      "Episode 209: Starting computation.\n",
      "Episode 209: Finished running.\n",
      "Agent 0, Average Reward: -210.69\n",
      "256/256 - 0s - loss: 22579.3848\n",
      "256/256 - 0s - loss: 17946.0703\n",
      "256/256 - 0s - loss: 20396.8242\n",
      "256/256 - 0s - loss: 21302.1934\n",
      "256/256 - 0s - loss: 18581.3359\n",
      "Reducing exploration for all agents to 0.0554\n",
      "\n",
      "Episode 210: Starting computation.\n",
      "Episode 210: Finished running.\n",
      "Agent 0, Average Reward: -216.8\n",
      "256/256 - 0s - loss: 21424.9824\n",
      "256/256 - 0s - loss: 17968.5078\n",
      "256/256 - 0s - loss: 19566.7168\n",
      "256/256 - 0s - loss: 24031.1191\n",
      "256/256 - 0s - loss: 22585.8594\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0546\n",
      "\n",
      "Episode 211: Starting computation.\n",
      "Episode 211: Finished running.\n",
      "Agent 0, Average Reward: -208.8\n",
      "256/256 - 0s - loss: 23505.2852\n",
      "256/256 - 0s - loss: 22700.7949\n",
      "256/256 - 0s - loss: 21326.1914\n",
      "256/256 - 0s - loss: 23987.4180\n",
      "256/256 - 0s - loss: 25396.0547\n",
      "Reducing exploration for all agents to 0.0539\n",
      "\n",
      "Episode 212: Starting computation.\n",
      "Episode 212: Finished running.\n",
      "Agent 0, Average Reward: -224.69\n",
      "256/256 - 0s - loss: 23830.4648\n",
      "256/256 - 0s - loss: 23255.1758\n",
      "256/256 - 0s - loss: 19804.6523\n",
      "256/256 - 0s - loss: 24851.1211\n",
      "256/256 - 0s - loss: 19788.6016\n",
      "Reducing exploration for all agents to 0.0531\n",
      "\n",
      "Episode 213: Starting computation.\n",
      "Episode 213: Finished running.\n",
      "Agent 0, Average Reward: -225.78\n",
      "256/256 - 0s - loss: 21162.2266\n",
      "256/256 - 0s - loss: 23750.6484\n",
      "256/256 - 0s - loss: 19962.1758\n",
      "256/256 - 0s - loss: 20192.9609\n",
      "256/256 - 0s - loss: 20479.0859\n",
      "Reducing exploration for all agents to 0.0524\n",
      "\n",
      "Episode 214: Starting computation.\n",
      "Episode 214: Finished running.\n",
      "Agent 0, Average Reward: -230.74\n",
      "256/256 - 0s - loss: 27725.9375\n",
      "256/256 - 0s - loss: 19138.5195\n",
      "256/256 - 0s - loss: 22003.1758\n",
      "256/256 - 0s - loss: 18490.5391\n",
      "256/256 - 0s - loss: 23362.3633\n",
      "Reducing exploration for all agents to 0.0517\n",
      "\n",
      "Episode 215: Starting computation.\n",
      "Episode 215: Finished running.\n",
      "Agent 0, Average Reward: -223.87\n",
      "256/256 - 0s - loss: 18661.9648\n",
      "256/256 - 0s - loss: 20641.0312\n",
      "256/256 - 0s - loss: 22408.8438\n",
      "256/256 - 0s - loss: 23264.3164\n",
      "256/256 - 0s - loss: 23891.8242\n",
      "Reducing exploration for all agents to 0.051\n",
      "\n",
      "Episode 216: Starting computation.\n",
      "Episode 216: Finished running.\n",
      "Agent 0, Average Reward: -231.72\n",
      "256/256 - 0s - loss: 20915.9336\n",
      "256/256 - 0s - loss: 21791.8477\n",
      "256/256 - 0s - loss: 18018.3242\n",
      "256/256 - 0s - loss: 20584.9805\n",
      "256/256 - 0s - loss: 21017.3828\n",
      "Reducing exploration for all agents to 0.0503\n",
      "\n",
      "Episode 217: Starting computation.\n",
      "Episode 217: Finished running.\n",
      "Agent 0, Average Reward: -227.81\n",
      "256/256 - 0s - loss: 22110.1406\n",
      "256/256 - 0s - loss: 17897.0137\n",
      "256/256 - 0s - loss: 24185.9668\n",
      "256/256 - 0s - loss: 18892.3164\n",
      "256/256 - 0s - loss: 21723.9609\n",
      "Reducing exploration for all agents to 0.0496\n",
      "\n",
      "Episode 218: Starting computation.\n",
      "Episode 218: Finished running.\n",
      "Agent 0, Average Reward: -233.16\n",
      "256/256 - 0s - loss: 17859.1855\n",
      "256/256 - 0s - loss: 18894.1660\n",
      "256/256 - 0s - loss: 19931.9844\n",
      "256/256 - 0s - loss: 21691.2383\n",
      "256/256 - 0s - loss: 16279.2598\n",
      "Reducing exploration for all agents to 0.0489\n",
      "\n",
      "Episode 219: Starting computation.\n",
      "Episode 219: Finished running.\n",
      "Agent 0, Average Reward: -227.79\n",
      "256/256 - 0s - loss: 14511.1543\n",
      "256/256 - 0s - loss: 20915.6641\n",
      "256/256 - 0s - loss: 17203.7656\n",
      "256/256 - 0s - loss: 23853.8926\n",
      "256/256 - 0s - loss: 18615.0039\n",
      "Reducing exploration for all agents to 0.0482\n",
      "\n",
      "Episode 220: Starting computation.\n",
      "Episode 220: Finished running.\n",
      "Agent 0, Average Reward: -213.41\n",
      "256/256 - 0s - loss: 17207.5703\n",
      "256/256 - 0s - loss: 19856.7793\n",
      "256/256 - 0s - loss: 23830.9531\n",
      "256/256 - 0s - loss: 18864.2422\n",
      "256/256 - 0s - loss: 17431.2070\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0476\n",
      "\n",
      "Episode 221: Starting computation.\n",
      "Episode 221: Finished running.\n",
      "Agent 0, Average Reward: -214.12\n",
      "256/256 - 0s - loss: 21257.9375\n",
      "256/256 - 0s - loss: 19353.7930\n",
      "256/256 - 0s - loss: 16895.3750\n",
      "256/256 - 0s - loss: 18960.4570\n",
      "256/256 - 0s - loss: 19035.6133\n",
      "Reducing exploration for all agents to 0.0469\n",
      "\n",
      "Episode 222: Starting computation.\n",
      "Episode 222: Finished running.\n",
      "Agent 0, Average Reward: -221.85\n",
      "256/256 - 0s - loss: 19850.1641\n",
      "256/256 - 0s - loss: 22133.5508\n",
      "256/256 - 0s - loss: 23893.6816\n",
      "256/256 - 0s - loss: 21566.2812\n",
      "256/256 - 0s - loss: 23482.4258\n",
      "Reducing exploration for all agents to 0.0463\n",
      "\n",
      "Episode 223: Starting computation.\n",
      "Episode 223: Finished running.\n",
      "Agent 0, Average Reward: -205.9\n",
      "256/256 - 0s - loss: 19778.7383\n",
      "256/256 - 0s - loss: 17207.0566\n",
      "256/256 - 0s - loss: 18287.8008\n",
      "256/256 - 0s - loss: 19938.9062\n",
      "256/256 - 0s - loss: 18027.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0456\n",
      "\n",
      "Episode 224: Starting computation.\n",
      "Episode 224: Finished running.\n",
      "Agent 0, Average Reward: -192.98\n",
      "256/256 - 0s - loss: 18611.7070\n",
      "256/256 - 0s - loss: 18039.9844\n",
      "256/256 - 0s - loss: 16881.5723\n",
      "256/256 - 0s - loss: 19697.2715\n",
      "256/256 - 0s - loss: 18759.5312\n",
      "Reducing exploration for all agents to 0.045\n",
      "\n",
      "Episode 225: Starting computation.\n",
      "Episode 225: Finished running.\n",
      "Agent 0, Average Reward: -199.81\n",
      "256/256 - 0s - loss: 14770.2949\n",
      "256/256 - 0s - loss: 18492.5469\n",
      "256/256 - 0s - loss: 18322.2891\n",
      "256/256 - 0s - loss: 20153.1758\n",
      "256/256 - 0s - loss: 17362.8301\n",
      "Reducing exploration for all agents to 0.0444\n",
      "\n",
      "Episode 226: Starting computation.\n",
      "Episode 226: Finished running.\n",
      "Agent 0, Average Reward: -214.21\n",
      "256/256 - 0s - loss: 16947.2266\n",
      "256/256 - 0s - loss: 21263.6113\n",
      "256/256 - 0s - loss: 19883.2695\n",
      "256/256 - 0s - loss: 17000.5547\n",
      "256/256 - 0s - loss: 19375.8516\n",
      "Reducing exploration for all agents to 0.0438\n",
      "\n",
      "Episode 227: Starting computation.\n",
      "Episode 227: Finished running.\n",
      "Agent 0, Average Reward: -201.37\n",
      "256/256 - 0s - loss: 15310.6592\n",
      "256/256 - 0s - loss: 17018.1035\n",
      "256/256 - 0s - loss: 17682.0859\n",
      "256/256 - 0s - loss: 16637.7129\n",
      "256/256 - 0s - loss: 18450.8672\n",
      "Reducing exploration for all agents to 0.0432\n",
      "\n",
      "Episode 228: Starting computation.\n",
      "Episode 228: Finished running.\n",
      "Agent 0, Average Reward: -209.12\n",
      "256/256 - 0s - loss: 18664.9551\n",
      "256/256 - 0s - loss: 14863.0508\n",
      "256/256 - 0s - loss: 17826.4141\n",
      "256/256 - 0s - loss: 15896.7207\n",
      "256/256 - 0s - loss: 15472.9424\n",
      "Reducing exploration for all agents to 0.0426\n",
      "\n",
      "Episode 229: Starting computation.\n",
      "Episode 229: Finished running.\n",
      "Agent 0, Average Reward: -207.44\n",
      "256/256 - 0s - loss: 15263.6152\n",
      "256/256 - 0s - loss: 14317.9668\n",
      "256/256 - 0s - loss: 14668.6494\n",
      "256/256 - 0s - loss: 14287.1426\n",
      "256/256 - 0s - loss: 13261.0449\n",
      "Reducing exploration for all agents to 0.042\n",
      "\n",
      "Episode 230: Starting computation.\n",
      "Episode 230: Finished running.\n",
      "Agent 0, Average Reward: -198.86\n",
      "256/256 - 0s - loss: 15522.7432\n",
      "256/256 - 0s - loss: 15239.4219\n",
      "256/256 - 0s - loss: 16251.6191\n",
      "256/256 - 0s - loss: 15968.3125\n",
      "256/256 - 0s - loss: 14800.3301\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0414\n",
      "\n",
      "Episode 231: Starting computation.\n",
      "Episode 231: Finished running.\n",
      "Agent 0, Average Reward: -193.3\n",
      "256/256 - 0s - loss: 13449.8496\n",
      "256/256 - 0s - loss: 13152.3203\n",
      "256/256 - 0s - loss: 20500.9688\n",
      "256/256 - 0s - loss: 13489.7715\n",
      "256/256 - 0s - loss: 14037.5234\n",
      "Reducing exploration for all agents to 0.0409\n",
      "\n",
      "Episode 232: Starting computation.\n",
      "Episode 232: Finished running.\n",
      "Agent 0, Average Reward: -182.02\n",
      "256/256 - 0s - loss: 13550.9863\n",
      "256/256 - 0s - loss: 16336.5576\n",
      "256/256 - 0s - loss: 17627.6562\n",
      "256/256 - 0s - loss: 15102.8730\n",
      "256/256 - 0s - loss: 16808.0039\n",
      "Reducing exploration for all agents to 0.0403\n",
      "\n",
      "Episode 233: Starting computation.\n",
      "Episode 233: Finished running.\n",
      "Agent 0, Average Reward: -180.46\n",
      "256/256 - 0s - loss: 17552.2266\n",
      "256/256 - 0s - loss: 13474.8652\n",
      "256/256 - 0s - loss: 17329.3906\n",
      "256/256 - 0s - loss: 15187.4668\n",
      "256/256 - 0s - loss: 21029.4883\n",
      "Reducing exploration for all agents to 0.0397\n",
      "\n",
      "Episode 234: Starting computation.\n",
      "Episode 234: Finished running.\n",
      "Agent 0, Average Reward: -171.96\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 15273.5537\n",
      "256/256 - 0s - loss: 12421.4844\n",
      "256/256 - 0s - loss: 11653.7949\n",
      "256/256 - 0s - loss: 17639.6543\n",
      "256/256 - 0s - loss: 15999.8027\n",
      "Reducing exploration for all agents to 0.0392\n",
      "\n",
      "Episode 235: Starting computation.\n",
      "Episode 235: Finished running.\n",
      "Agent 0, Average Reward: -175.93\n",
      "256/256 - 0s - loss: 21970.4277\n",
      "256/256 - 0s - loss: 12705.7002\n",
      "256/256 - 0s - loss: 13149.6885\n",
      "256/256 - 0s - loss: 14040.5098\n",
      "256/256 - 0s - loss: 15239.3672\n",
      "Reducing exploration for all agents to 0.0387\n",
      "\n",
      "Episode 236: Starting computation.\n",
      "Episode 236: Finished running.\n",
      "Agent 0, Average Reward: -179.05\n",
      "256/256 - 0s - loss: 13477.3252\n",
      "256/256 - 0s - loss: 14922.0879\n",
      "256/256 - 0s - loss: 11811.6582\n",
      "256/256 - 0s - loss: 13400.4258\n",
      "256/256 - 0s - loss: 11942.6045\n",
      "Reducing exploration for all agents to 0.0381\n",
      "\n",
      "Episode 237: Starting computation.\n",
      "Episode 237: Finished running.\n",
      "Agent 0, Average Reward: -179.42\n",
      "256/256 - 0s - loss: 12402.5596\n",
      "256/256 - 0s - loss: 12673.6543\n",
      "256/256 - 0s - loss: 13217.8975\n",
      "256/256 - 0s - loss: 16483.5586\n",
      "256/256 - 0s - loss: 11436.7432\n",
      "Reducing exploration for all agents to 0.0376\n",
      "\n",
      "Episode 238: Starting computation.\n",
      "Episode 238: Finished running.\n",
      "Agent 0, Average Reward: -172.84\n",
      "256/256 - 0s - loss: 12029.5879\n",
      "256/256 - 0s - loss: 11260.2578\n",
      "256/256 - 0s - loss: 9698.2949\n",
      "256/256 - 0s - loss: 13510.5000\n",
      "256/256 - 0s - loss: 14680.4834\n",
      "Reducing exploration for all agents to 0.0371\n",
      "\n",
      "Episode 239: Starting computation.\n",
      "Episode 239: Finished running.\n",
      "Agent 0, Average Reward: -178.5\n",
      "256/256 - 0s - loss: 14001.9971\n",
      "256/256 - 0s - loss: 10047.6084\n",
      "256/256 - 0s - loss: 10550.8643\n",
      "256/256 - 0s - loss: 8983.5049\n",
      "256/256 - 0s - loss: 13468.5869\n",
      "Reducing exploration for all agents to 0.0366\n",
      "\n",
      "Episode 240: Starting computation.\n",
      "Episode 240: Finished running.\n",
      "Agent 0, Average Reward: -192.52\n",
      "256/256 - 0s - loss: 13567.4111\n",
      "256/256 - 0s - loss: 13382.4092\n",
      "256/256 - 0s - loss: 10218.5654\n",
      "256/256 - 0s - loss: 11780.1660\n",
      "256/256 - 0s - loss: 11825.5664\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0361\n",
      "\n",
      "Episode 241: Starting computation.\n",
      "Episode 241: Finished running.\n",
      "Agent 0, Average Reward: -199.26\n",
      "256/256 - 0s - loss: 14921.1484\n",
      "256/256 - 0s - loss: 11930.8594\n",
      "256/256 - 0s - loss: 15017.0723\n",
      "256/256 - 0s - loss: 12083.2490\n",
      "256/256 - 0s - loss: 16343.6465\n",
      "Reducing exploration for all agents to 0.0356\n",
      "\n",
      "Episode 242: Starting computation.\n",
      "Episode 242: Finished running.\n",
      "Agent 0, Average Reward: -194.78\n",
      "256/256 - 0s - loss: 14454.8154\n",
      "256/256 - 0s - loss: 14713.8281\n",
      "256/256 - 0s - loss: 18199.2109\n",
      "256/256 - 0s - loss: 16938.7539\n",
      "256/256 - 0s - loss: 11972.3105\n",
      "Reducing exploration for all agents to 0.0351\n",
      "\n",
      "Episode 243: Starting computation.\n",
      "Episode 243: Finished running.\n",
      "Agent 0, Average Reward: -190.5\n",
      "256/256 - 0s - loss: 13683.8770\n",
      "256/256 - 0s - loss: 18523.4707\n",
      "256/256 - 0s - loss: 14568.3525\n",
      "256/256 - 0s - loss: 17040.9883\n",
      "256/256 - 0s - loss: 17003.5371\n",
      "Reducing exploration for all agents to 0.0346\n",
      "\n",
      "Episode 244: Starting computation.\n",
      "Episode 244: Finished running.\n",
      "Agent 0, Average Reward: -226.22\n",
      "256/256 - 0s - loss: 15518.3057\n",
      "256/256 - 0s - loss: 20214.9395\n",
      "256/256 - 0s - loss: 16732.8320\n",
      "256/256 - 0s - loss: 17569.9102\n",
      "256/256 - 0s - loss: 19912.8281\n",
      "Reducing exploration for all agents to 0.0341\n",
      "\n",
      "Episode 245: Starting computation.\n",
      "Episode 245: Finished running.\n",
      "Agent 0, Average Reward: -214.04\n",
      "256/256 - 0s - loss: 17349.7344\n",
      "256/256 - 0s - loss: 15419.6758\n",
      "256/256 - 0s - loss: 15733.3086\n",
      "256/256 - 0s - loss: 16200.1191\n",
      "256/256 - 0s - loss: 16502.6133\n",
      "Reducing exploration for all agents to 0.0337\n",
      "\n",
      "Episode 246: Starting computation.\n",
      "Episode 246: Finished running.\n",
      "Agent 0, Average Reward: -213.85\n",
      "256/256 - 0s - loss: 21867.8867\n",
      "256/256 - 0s - loss: 14698.7344\n",
      "256/256 - 0s - loss: 17194.1328\n",
      "256/256 - 0s - loss: 16372.8711\n",
      "256/256 - 0s - loss: 15610.5664\n",
      "Reducing exploration for all agents to 0.0332\n",
      "\n",
      "Episode 247: Starting computation.\n",
      "Episode 247: Finished running.\n",
      "Agent 0, Average Reward: -210.49\n",
      "256/256 - 0s - loss: 12990.6211\n",
      "256/256 - 0s - loss: 14877.4219\n",
      "256/256 - 0s - loss: 15129.5498\n",
      "256/256 - 0s - loss: 14066.2891\n",
      "256/256 - 0s - loss: 17421.3945\n",
      "Reducing exploration for all agents to 0.0327\n",
      "\n",
      "Episode 248: Starting computation.\n",
      "Episode 248: Finished running.\n",
      "Agent 0, Average Reward: -205.45\n",
      "256/256 - 0s - loss: 16003.1484\n",
      "256/256 - 0s - loss: 17959.4512\n",
      "256/256 - 0s - loss: 15735.1816\n",
      "256/256 - 0s - loss: 16198.6406\n",
      "256/256 - 0s - loss: 16490.4336\n",
      "Reducing exploration for all agents to 0.0323\n",
      "\n",
      "Episode 249: Starting computation.\n",
      "Episode 249: Finished running.\n",
      "Agent 0, Average Reward: -206.6\n",
      "256/256 - 0s - loss: 9268.3242\n",
      "256/256 - 0s - loss: 16319.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 13580.8652\n",
      "256/256 - 0s - loss: 12978.8711\n",
      "256/256 - 0s - loss: 15418.8369\n",
      "Reducing exploration for all agents to 0.0318\n",
      "\n",
      "Episode 250: Starting computation.\n",
      "Episode 250: Finished running.\n",
      "Agent 0, Average Reward: -203.3\n",
      "256/256 - 0s - loss: 13927.7266\n",
      "256/256 - 0s - loss: 16059.1855\n",
      "256/256 - 0s - loss: 13835.8711\n",
      "256/256 - 0s - loss: 13385.6260\n",
      "256/256 - 0s - loss: 14885.2275\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0314\n",
      "\n",
      "Episode 251: Starting computation.\n",
      "Episode 251: Finished running.\n",
      "Agent 0, Average Reward: -197.75\n",
      "256/256 - 0s - loss: 14561.8730\n",
      "256/256 - 0s - loss: 14870.8818\n",
      "256/256 - 0s - loss: 12313.2754\n",
      "256/256 - 0s - loss: 15104.2803\n",
      "256/256 - 0s - loss: 11105.4590\n",
      "Reducing exploration for all agents to 0.031\n",
      "\n",
      "Episode 252: Starting computation.\n",
      "Episode 252: Finished running.\n",
      "Agent 0, Average Reward: -192.81\n",
      "256/256 - 0s - loss: 13600.0371\n",
      "256/256 - 0s - loss: 15426.1738\n",
      "256/256 - 0s - loss: 15503.2705\n",
      "256/256 - 0s - loss: 17189.5137\n",
      "256/256 - 0s - loss: 17047.0430\n",
      "Reducing exploration for all agents to 0.0305\n",
      "\n",
      "Episode 253: Starting computation.\n",
      "Episode 253: Finished running.\n",
      "Agent 0, Average Reward: -186.69\n",
      "256/256 - 0s - loss: 13586.5596\n",
      "256/256 - 0s - loss: 13104.3340\n",
      "256/256 - 0s - loss: 15934.1309\n",
      "256/256 - 0s - loss: 16833.6621\n",
      "256/256 - 0s - loss: 12519.7598\n",
      "Reducing exploration for all agents to 0.0301\n",
      "\n",
      "Episode 254: Starting computation.\n",
      "Episode 254: Finished running.\n",
      "Agent 0, Average Reward: -177.21\n",
      "256/256 - 0s - loss: 13815.5732\n",
      "256/256 - 0s - loss: 10785.7207\n",
      "256/256 - 0s - loss: 20204.2598\n",
      "256/256 - 0s - loss: 10371.7539\n",
      "256/256 - 0s - loss: 12932.6641\n",
      "Reducing exploration for all agents to 0.0297\n",
      "\n",
      "Episode 255: Starting computation.\n",
      "Episode 255: Finished running.\n",
      "Agent 0, Average Reward: -171.37\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 13252.4961\n",
      "256/256 - 0s - loss: 13097.7793\n",
      "256/256 - 0s - loss: 11332.9219\n",
      "256/256 - 0s - loss: 9962.0820\n",
      "256/256 - 0s - loss: 12106.0889\n",
      "Reducing exploration for all agents to 0.0293\n",
      "\n",
      "Episode 256: Starting computation.\n",
      "Episode 256: Finished running.\n",
      "Agent 0, Average Reward: -172.93\n",
      "256/256 - 0s - loss: 10136.7090\n",
      "256/256 - 0s - loss: 11552.9834\n",
      "256/256 - 0s - loss: 12161.9277\n",
      "256/256 - 0s - loss: 9026.3555\n",
      "256/256 - 0s - loss: 9638.3428\n",
      "Reducing exploration for all agents to 0.0289\n",
      "\n",
      "Episode 257: Starting computation.\n",
      "Episode 257: Finished running.\n",
      "Agent 0, Average Reward: -170.73\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 10271.7461\n",
      "256/256 - 0s - loss: 6841.6147\n",
      "256/256 - 0s - loss: 11141.7363\n",
      "256/256 - 0s - loss: 12297.4219\n",
      "256/256 - 0s - loss: 10586.1826\n",
      "Reducing exploration for all agents to 0.0285\n",
      "\n",
      "Episode 258: Starting computation.\n",
      "Episode 258: Finished running.\n",
      "Agent 0, Average Reward: -172.94\n",
      "256/256 - 0s - loss: 8484.5371\n",
      "256/256 - 0s - loss: 10096.6914\n",
      "256/256 - 0s - loss: 11067.8877\n",
      "256/256 - 0s - loss: 8911.3340\n",
      "256/256 - 0s - loss: 11063.7852\n",
      "Reducing exploration for all agents to 0.0281\n",
      "\n",
      "Episode 259: Starting computation.\n",
      "Episode 259: Finished running.\n",
      "Agent 0, Average Reward: -174.73\n",
      "256/256 - 0s - loss: 8617.8105\n",
      "256/256 - 0s - loss: 11687.3340\n",
      "256/256 - 0s - loss: 12502.1641\n",
      "256/256 - 0s - loss: 11103.6162\n",
      "256/256 - 0s - loss: 12175.6602\n",
      "Reducing exploration for all agents to 0.0277\n",
      "\n",
      "Episode 260: Starting computation.\n",
      "Episode 260: Finished running.\n",
      "Agent 0, Average Reward: -183.47\n",
      "256/256 - 0s - loss: 10443.1094\n",
      "256/256 - 0s - loss: 12679.3545\n",
      "256/256 - 0s - loss: 10806.3203\n",
      "256/256 - 0s - loss: 11154.3535\n",
      "256/256 - 0s - loss: 11988.4805\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0273\n",
      "\n",
      "Episode 261: Starting computation.\n",
      "Episode 261: Finished running.\n",
      "Agent 0, Average Reward: -194.9\n",
      "256/256 - 0s - loss: 13763.3125\n",
      "256/256 - 0s - loss: 13761.1201\n",
      "256/256 - 0s - loss: 12133.3525\n",
      "256/256 - 0s - loss: 15040.2617\n",
      "256/256 - 0s - loss: 11109.3037\n",
      "Reducing exploration for all agents to 0.027\n",
      "\n",
      "Episode 262: Starting computation.\n",
      "Episode 262: Finished running.\n",
      "Agent 0, Average Reward: -221.47\n",
      "256/256 - 0s - loss: 13042.8672\n",
      "256/256 - 0s - loss: 14499.6289\n",
      "256/256 - 0s - loss: 13993.9609\n",
      "256/256 - 0s - loss: 14554.1943\n",
      "256/256 - 0s - loss: 15410.8857\n",
      "Reducing exploration for all agents to 0.0266\n",
      "\n",
      "Episode 263: Starting computation.\n",
      "Episode 263: Finished running.\n",
      "Agent 0, Average Reward: -226.83\n",
      "256/256 - 0s - loss: 16755.8086\n",
      "256/256 - 0s - loss: 11726.5264\n",
      "256/256 - 0s - loss: 15102.0039\n",
      "256/256 - 0s - loss: 20485.0781\n",
      "256/256 - 0s - loss: 14978.4141\n",
      "Reducing exploration for all agents to 0.0262\n",
      "\n",
      "Episode 264: Starting computation.\n",
      "Episode 264: Finished running.\n",
      "Agent 0, Average Reward: -252.63\n",
      "256/256 - 0s - loss: 15164.4531\n",
      "256/256 - 0s - loss: 14118.8730\n",
      "256/256 - 0s - loss: 16876.0117\n",
      "256/256 - 0s - loss: 18498.4336\n",
      "256/256 - 0s - loss: 17316.6406\n",
      "Reducing exploration for all agents to 0.0259\n",
      "\n",
      "Episode 265: Starting computation.\n",
      "Episode 265: Finished running.\n",
      "Agent 0, Average Reward: -245.01\n",
      "256/256 - 0s - loss: 16243.1377\n",
      "256/256 - 0s - loss: 16215.5947\n",
      "256/256 - 0s - loss: 15570.4531\n",
      "256/256 - 0s - loss: 18357.7227\n",
      "256/256 - 0s - loss: 17149.4902\n",
      "Reducing exploration for all agents to 0.0255\n",
      "\n",
      "Episode 266: Starting computation.\n",
      "Episode 266: Finished running.\n",
      "Agent 0, Average Reward: -239.41\n",
      "256/256 - 0s - loss: 21218.9863\n",
      "256/256 - 0s - loss: 18897.8691\n",
      "256/256 - 0s - loss: 17660.0820\n",
      "256/256 - 0s - loss: 19121.9688\n",
      "256/256 - 0s - loss: 16031.2559\n",
      "Reducing exploration for all agents to 0.0252\n",
      "\n",
      "Episode 267: Starting computation.\n",
      "Episode 267: Finished running.\n",
      "Agent 0, Average Reward: -239.24\n",
      "256/256 - 0s - loss: 20733.0059\n",
      "256/256 - 0s - loss: 22541.2305\n",
      "256/256 - 0s - loss: 18254.6836\n",
      "256/256 - 0s - loss: 17485.3359\n",
      "256/256 - 0s - loss: 19488.1309\n",
      "Reducing exploration for all agents to 0.0248\n",
      "\n",
      "Episode 268: Starting computation.\n",
      "Episode 268: Finished running.\n",
      "Agent 0, Average Reward: -240.28\n",
      "256/256 - 0s - loss: 17201.2637\n",
      "256/256 - 0s - loss: 15661.8408\n",
      "256/256 - 0s - loss: 15122.5537\n",
      "256/256 - 0s - loss: 14593.0645\n",
      "256/256 - 0s - loss: 20905.3750\n",
      "Reducing exploration for all agents to 0.0245\n",
      "\n",
      "Episode 269: Starting computation.\n",
      "Episode 269: Finished running.\n",
      "Agent 0, Average Reward: -236.63\n",
      "256/256 - 0s - loss: 14686.9863\n",
      "256/256 - 0s - loss: 18377.8945\n",
      "256/256 - 0s - loss: 15872.7930\n",
      "256/256 - 0s - loss: 18690.0781\n",
      "256/256 - 0s - loss: 17647.0332\n",
      "Reducing exploration for all agents to 0.0241\n",
      "\n",
      "Episode 270: Starting computation.\n",
      "Episode 270: Finished running.\n",
      "Agent 0, Average Reward: -246.91\n",
      "256/256 - 0s - loss: 19998.1094\n",
      "256/256 - 0s - loss: 19374.8828\n",
      "256/256 - 0s - loss: 15689.8174\n",
      "256/256 - 0s - loss: 16836.1973\n",
      "256/256 - 0s - loss: 16752.3320\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0238\n",
      "\n",
      "Episode 271: Starting computation.\n",
      "Episode 271: Finished running.\n",
      "Agent 0, Average Reward: -250.49\n",
      "256/256 - 0s - loss: 18547.0781\n",
      "256/256 - 0s - loss: 23529.5859\n",
      "256/256 - 0s - loss: 23065.8145\n",
      "256/256 - 0s - loss: 16732.9824\n",
      "256/256 - 0s - loss: 17330.0898\n",
      "Reducing exploration for all agents to 0.0235\n",
      "\n",
      "Episode 272: Starting computation.\n",
      "Episode 272: Finished running.\n",
      "Agent 0, Average Reward: -257.31\n",
      "256/256 - 0s - loss: 18421.0312\n",
      "256/256 - 0s - loss: 18324.7715\n",
      "256/256 - 0s - loss: 16907.6973\n",
      "256/256 - 0s - loss: 17472.8633\n",
      "256/256 - 0s - loss: 17971.9258\n",
      "Reducing exploration for all agents to 0.0232\n",
      "\n",
      "Episode 273: Starting computation.\n",
      "Episode 273: Finished running.\n",
      "Agent 0, Average Reward: -258.97\n",
      "256/256 - 0s - loss: 19414.2012\n",
      "256/256 - 0s - loss: 14812.2100\n",
      "256/256 - 0s - loss: 17425.5273\n",
      "256/256 - 0s - loss: 17575.0625\n",
      "256/256 - 0s - loss: 16734.3711\n",
      "Reducing exploration for all agents to 0.0228\n",
      "\n",
      "Episode 274: Starting computation.\n",
      "Episode 274: Finished running.\n",
      "Agent 0, Average Reward: -264.2\n",
      "256/256 - 0s - loss: 18664.6523\n",
      "256/256 - 0s - loss: 21270.2871\n",
      "256/256 - 0s - loss: 18479.3301\n",
      "256/256 - 0s - loss: 16523.4766\n",
      "256/256 - 0s - loss: 18869.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0225\n",
      "\n",
      "Episode 275: Starting computation.\n",
      "Episode 275: Finished running.\n",
      "Agent 0, Average Reward: -264.44\n",
      "256/256 - 0s - loss: 15594.8799\n",
      "256/256 - 0s - loss: 19512.9961\n",
      "256/256 - 0s - loss: 18645.0117\n",
      "256/256 - 0s - loss: 19096.7324\n",
      "256/256 - 0s - loss: 14891.2666\n",
      "Reducing exploration for all agents to 0.0222\n",
      "\n",
      "Episode 276: Starting computation.\n",
      "Episode 276: Finished running.\n",
      "Agent 0, Average Reward: -253.98\n",
      "256/256 - 0s - loss: 14890.9932\n",
      "256/256 - 0s - loss: 16006.8271\n",
      "256/256 - 0s - loss: 17599.7969\n",
      "256/256 - 0s - loss: 16611.3613\n",
      "256/256 - 0s - loss: 17623.4199\n",
      "Reducing exploration for all agents to 0.0219\n",
      "\n",
      "Episode 277: Starting computation.\n",
      "Episode 277: Finished running.\n",
      "Agent 0, Average Reward: -253.64\n",
      "256/256 - 0s - loss: 21656.1641\n",
      "256/256 - 0s - loss: 19978.8125\n",
      "256/256 - 0s - loss: 20461.6582\n",
      "256/256 - 0s - loss: 18343.7305\n",
      "256/256 - 0s - loss: 15146.9561\n",
      "Reducing exploration for all agents to 0.0216\n",
      "\n",
      "Episode 278: Starting computation.\n",
      "Episode 278: Finished running.\n",
      "Agent 0, Average Reward: -259.98\n",
      "256/256 - 0s - loss: 16932.2227\n",
      "256/256 - 0s - loss: 15392.1445\n",
      "256/256 - 0s - loss: 14216.8408\n",
      "256/256 - 0s - loss: 15377.3076\n",
      "256/256 - 0s - loss: 16528.9941\n",
      "Reducing exploration for all agents to 0.0213\n",
      "\n",
      "Episode 279: Starting computation.\n",
      "Episode 279: Finished running.\n",
      "Agent 0, Average Reward: -242.8\n",
      "256/256 - 0s - loss: 17979.0332\n",
      "256/256 - 0s - loss: 16556.0391\n",
      "256/256 - 0s - loss: 17232.1055\n",
      "256/256 - 0s - loss: 16327.9619\n",
      "256/256 - 0s - loss: 16878.2461\n",
      "Reducing exploration for all agents to 0.021\n",
      "\n",
      "Episode 280: Starting computation.\n",
      "Episode 280: Finished running.\n",
      "Agent 0, Average Reward: -251.58\n",
      "256/256 - 0s - loss: 16302.9502\n",
      "256/256 - 0s - loss: 14337.1338\n",
      "256/256 - 0s - loss: 16611.1641\n",
      "256/256 - 0s - loss: 15910.6211\n",
      "256/256 - 0s - loss: 17237.4961\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0207\n",
      "\n",
      "Episode 281: Starting computation.\n",
      "Episode 281: Finished running.\n",
      "Agent 0, Average Reward: -249.87\n",
      "256/256 - 0s - loss: 15886.2197\n",
      "256/256 - 0s - loss: 20237.7246\n",
      "256/256 - 0s - loss: 16255.6182\n",
      "256/256 - 0s - loss: 19071.6875\n",
      "256/256 - 0s - loss: 19664.9883\n",
      "Reducing exploration for all agents to 0.0204\n",
      "\n",
      "Episode 282: Starting computation.\n",
      "Episode 282: Finished running.\n",
      "Agent 0, Average Reward: -227.69\n",
      "256/256 - 0s - loss: 15624.9639\n",
      "256/256 - 0s - loss: 21811.6094\n",
      "256/256 - 0s - loss: 16822.7070\n",
      "256/256 - 0s - loss: 17319.5898\n",
      "256/256 - 0s - loss: 18947.1992\n",
      "Reducing exploration for all agents to 0.0202\n",
      "\n",
      "Episode 283: Starting computation.\n",
      "Episode 283: Finished running.\n",
      "Agent 0, Average Reward: -239.98\n",
      "256/256 - 0s - loss: 17621.8926\n",
      "256/256 - 0s - loss: 15841.4678\n",
      "256/256 - 0s - loss: 18040.2031\n",
      "256/256 - 0s - loss: 25501.0000\n",
      "256/256 - 0s - loss: 20110.1660\n",
      "Reducing exploration for all agents to 0.0199\n",
      "\n",
      "Episode 284: Starting computation.\n",
      "Episode 284: Finished running.\n",
      "Agent 0, Average Reward: -218.33\n",
      "256/256 - 0s - loss: 21951.6406\n",
      "256/256 - 0s - loss: 20698.8477\n",
      "256/256 - 0s - loss: 18067.2969\n",
      "256/256 - 0s - loss: 21674.9688\n",
      "256/256 - 0s - loss: 17245.9648\n",
      "Reducing exploration for all agents to 0.0196\n",
      "\n",
      "Episode 285: Starting computation.\n",
      "Episode 285: Finished running.\n",
      "Agent 0, Average Reward: -255.2\n",
      "256/256 - 0s - loss: 23784.5684\n",
      "256/256 - 0s - loss: 14823.0791\n",
      "256/256 - 0s - loss: 18048.0000\n",
      "256/256 - 0s - loss: 20041.8301\n",
      "256/256 - 0s - loss: 22709.4629\n",
      "Reducing exploration for all agents to 0.0193\n",
      "\n",
      "Episode 286: Starting computation.\n",
      "Episode 286: Finished running.\n",
      "Agent 0, Average Reward: -319.24\n",
      "256/256 - 0s - loss: 27652.8809\n",
      "256/256 - 0s - loss: 28060.0547\n",
      "256/256 - 0s - loss: 29725.9316\n",
      "256/256 - 0s - loss: 30859.5156\n",
      "256/256 - 0s - loss: 24567.7383\n",
      "Reducing exploration for all agents to 0.0191\n",
      "\n",
      "Episode 287: Starting computation.\n",
      "Episode 287: Finished running.\n",
      "Agent 0, Average Reward: -358.16\n",
      "256/256 - 0s - loss: 34376.3398\n",
      "256/256 - 0s - loss: 40527.6797\n",
      "256/256 - 0s - loss: 40374.5938\n",
      "256/256 - 0s - loss: 37881.8281\n",
      "256/256 - 0s - loss: 35167.4336\n",
      "Reducing exploration for all agents to 0.0188\n",
      "\n",
      "Episode 288: Starting computation.\n",
      "Episode 288: Finished running.\n",
      "Agent 0, Average Reward: -353.29\n",
      "256/256 - 0s - loss: 46598.5664\n",
      "256/256 - 0s - loss: 46444.8516\n",
      "256/256 - 0s - loss: 49669.1367\n",
      "256/256 - 0s - loss: 40928.8125\n",
      "256/256 - 0s - loss: 45336.5117\n",
      "Reducing exploration for all agents to 0.0186\n",
      "\n",
      "Episode 289: Starting computation.\n",
      "Episode 289: Finished running.\n",
      "Agent 0, Average Reward: -291.75\n",
      "256/256 - 0s - loss: 47132.9062\n",
      "256/256 - 0s - loss: 79132.2031\n",
      "256/256 - 0s - loss: 49045.2617\n",
      "256/256 - 0s - loss: 51769.8359\n",
      "256/256 - 0s - loss: 40759.9805\n",
      "Reducing exploration for all agents to 0.0183\n",
      "\n",
      "Episode 290: Starting computation.\n",
      "Episode 290: Finished running.\n",
      "Agent 0, Average Reward: -255.77\n",
      "256/256 - 0s - loss: 53993.2109\n",
      "256/256 - 0s - loss: 46791.5938\n",
      "256/256 - 0s - loss: 35684.0234\n",
      "256/256 - 0s - loss: 43981.5312\n",
      "256/256 - 0s - loss: 38063.3516\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0181\n",
      "\n",
      "Episode 291: Starting computation.\n",
      "Episode 291: Finished running.\n",
      "Agent 0, Average Reward: -235.0\n",
      "256/256 - 0s - loss: 41466.6562\n",
      "256/256 - 0s - loss: 31541.1406\n",
      "256/256 - 0s - loss: 43875.0352\n",
      "256/256 - 0s - loss: 40961.0195\n",
      "256/256 - 0s - loss: 42050.8789\n",
      "Reducing exploration for all agents to 0.0178\n",
      "\n",
      "Episode 292: Starting computation.\n",
      "Episode 292: Finished running.\n",
      "Agent 0, Average Reward: -225.76\n",
      "256/256 - 0s - loss: 32206.8633\n",
      "256/256 - 0s - loss: 33123.6836\n",
      "256/256 - 0s - loss: 33555.6797\n",
      "256/256 - 0s - loss: 25495.5801\n",
      "256/256 - 0s - loss: 28094.7207\n",
      "Reducing exploration for all agents to 0.0176\n",
      "\n",
      "Episode 293: Starting computation.\n",
      "Episode 293: Finished running.\n",
      "Agent 0, Average Reward: -221.61\n",
      "256/256 - 0s - loss: 20496.2539\n",
      "256/256 - 0s - loss: 21786.5469\n",
      "256/256 - 0s - loss: 23070.6289\n",
      "256/256 - 0s - loss: 18599.2051\n",
      "256/256 - 0s - loss: 19594.1680\n",
      "Reducing exploration for all agents to 0.0173\n",
      "\n",
      "Episode 294: Starting computation.\n",
      "Episode 294: Finished running.\n",
      "Agent 0, Average Reward: -215.06\n",
      "256/256 - 0s - loss: 16451.9004\n",
      "256/256 - 0s - loss: 18072.3809\n",
      "256/256 - 0s - loss: 16566.7402\n",
      "256/256 - 0s - loss: 16345.6455\n",
      "256/256 - 0s - loss: 15479.5674\n",
      "Reducing exploration for all agents to 0.0171\n",
      "\n",
      "Episode 295: Starting computation.\n",
      "Episode 295: Finished running.\n",
      "Agent 0, Average Reward: -211.05\n",
      "256/256 - 0s - loss: 15881.6455\n",
      "256/256 - 0s - loss: 14305.4912\n",
      "256/256 - 0s - loss: 18500.5020\n",
      "256/256 - 0s - loss: 20241.7734\n",
      "256/256 - 0s - loss: 16709.9180\n",
      "Reducing exploration for all agents to 0.0168\n",
      "\n",
      "Episode 296: Starting computation.\n",
      "Episode 296: Finished running.\n",
      "Agent 0, Average Reward: -218.67\n",
      "256/256 - 0s - loss: 13010.3770\n",
      "256/256 - 0s - loss: 15488.2451\n",
      "256/256 - 0s - loss: 17480.6621\n",
      "256/256 - 0s - loss: 16439.1426\n",
      "256/256 - 0s - loss: 16386.4316\n",
      "Reducing exploration for all agents to 0.0166\n",
      "\n",
      "Episode 297: Starting computation.\n",
      "Episode 297: Finished running.\n",
      "Agent 0, Average Reward: -209.92\n",
      "256/256 - 0s - loss: 15757.8193\n",
      "256/256 - 0s - loss: 14968.0596\n",
      "256/256 - 0s - loss: 17070.5820\n",
      "256/256 - 0s - loss: 14316.1494\n",
      "256/256 - 0s - loss: 19352.1270\n",
      "Reducing exploration for all agents to 0.0164\n",
      "\n",
      "Episode 298: Starting computation.\n",
      "Episode 298: Finished running.\n",
      "Agent 0, Average Reward: -214.47\n",
      "256/256 - 0s - loss: 13163.2334\n",
      "256/256 - 0s - loss: 17318.6270\n",
      "256/256 - 0s - loss: 17948.0527\n",
      "256/256 - 0s - loss: 17819.8691\n",
      "256/256 - 0s - loss: 18716.2402\n",
      "Reducing exploration for all agents to 0.0162\n",
      "\n",
      "Episode 299: Starting computation.\n",
      "Episode 299: Finished running.\n",
      "Agent 0, Average Reward: -209.09\n",
      "256/256 - 0s - loss: 19827.4062\n",
      "256/256 - 0s - loss: 16448.3262\n",
      "256/256 - 0s - loss: 18172.7188\n",
      "256/256 - 0s - loss: 16482.0508\n",
      "256/256 - 0s - loss: 16690.0449\n",
      "Reducing exploration for all agents to 0.0159\n",
      "\n",
      "Episode 300: Starting computation.\n",
      "Episode 300: Finished running.\n",
      "Agent 0, Average Reward: -202.29\n",
      "256/256 - 0s - loss: 13962.1230\n",
      "256/256 - 0s - loss: 16844.1094\n",
      "256/256 - 0s - loss: 14498.6846\n",
      "256/256 - 0s - loss: 15994.9229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 14836.0283\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0157\n",
      "\n",
      "Episode 301: Starting computation.\n",
      "Episode 301: Finished running.\n",
      "Agent 0, Average Reward: -201.83\n",
      "256/256 - 0s - loss: 15215.7852\n",
      "256/256 - 0s - loss: 16762.4043\n",
      "256/256 - 0s - loss: 17761.7129\n",
      "256/256 - 0s - loss: 16814.4434\n",
      "256/256 - 0s - loss: 16657.1973\n",
      "Reducing exploration for all agents to 0.0155\n",
      "\n",
      "Episode 302: Starting computation.\n",
      "Episode 302: Finished running.\n",
      "Agent 0, Average Reward: -200.12\n",
      "256/256 - 0s - loss: 17094.4941\n",
      "256/256 - 0s - loss: 20973.9355\n",
      "256/256 - 0s - loss: 15311.0977\n",
      "256/256 - 0s - loss: 15671.2402\n",
      "256/256 - 0s - loss: 14809.3965\n",
      "Reducing exploration for all agents to 0.0153\n",
      "\n",
      "Episode 303: Starting computation.\n",
      "Episode 303: Finished running.\n",
      "Agent 0, Average Reward: -195.12\n",
      "256/256 - 0s - loss: 12903.3242\n",
      "256/256 - 0s - loss: 15946.8311\n",
      "256/256 - 0s - loss: 19348.2695\n",
      "256/256 - 0s - loss: 17138.0098\n",
      "256/256 - 0s - loss: 14579.4482\n",
      "Reducing exploration for all agents to 0.0151\n",
      "\n",
      "Episode 304: Starting computation.\n",
      "Episode 304: Finished running.\n",
      "Agent 0, Average Reward: -190.03\n",
      "256/256 - 0s - loss: 14413.6582\n",
      "256/256 - 0s - loss: 15762.7246\n",
      "256/256 - 0s - loss: 23809.7988\n",
      "256/256 - 0s - loss: 15859.5527\n",
      "256/256 - 0s - loss: 19901.8398\n",
      "Reducing exploration for all agents to 0.0149\n",
      "\n",
      "Episode 305: Starting computation.\n",
      "Episode 305: Finished running.\n",
      "Agent 0, Average Reward: -193.87\n",
      "256/256 - 0s - loss: 12090.4443\n",
      "256/256 - 0s - loss: 11116.7490\n",
      "256/256 - 0s - loss: 12295.3867\n",
      "256/256 - 0s - loss: 12643.2021\n",
      "256/256 - 0s - loss: 14468.0049\n",
      "Reducing exploration for all agents to 0.0147\n",
      "\n",
      "Episode 306: Starting computation.\n",
      "Episode 306: Finished running.\n",
      "Agent 0, Average Reward: -195.98\n",
      "256/256 - 0s - loss: 14990.5596\n",
      "256/256 - 0s - loss: 16454.3145\n",
      "256/256 - 0s - loss: 13813.0752\n",
      "256/256 - 0s - loss: 13276.6045\n",
      "256/256 - 0s - loss: 11401.6807\n",
      "Reducing exploration for all agents to 0.0145\n",
      "\n",
      "Episode 307: Starting computation.\n",
      "Episode 307: Finished running.\n",
      "Agent 0, Average Reward: -189.79\n",
      "256/256 - 0s - loss: 14021.4033\n",
      "256/256 - 0s - loss: 10302.3291\n",
      "256/256 - 0s - loss: 15215.7178\n",
      "256/256 - 0s - loss: 13299.3906\n",
      "256/256 - 0s - loss: 16159.1562\n",
      "Reducing exploration for all agents to 0.0143\n",
      "\n",
      "Episode 308: Starting computation.\n",
      "Episode 308: Finished running.\n",
      "Agent 0, Average Reward: -187.18\n",
      "256/256 - 0s - loss: 16946.3281\n",
      "256/256 - 0s - loss: 11684.2275\n",
      "256/256 - 0s - loss: 12703.6943\n",
      "256/256 - 0s - loss: 17500.8730\n",
      "256/256 - 0s - loss: 10511.6689\n",
      "Reducing exploration for all agents to 0.0141\n",
      "\n",
      "Episode 309: Starting computation.\n",
      "Episode 309: Finished running.\n",
      "Agent 0, Average Reward: -193.64\n",
      "256/256 - 0s - loss: 13747.5283\n",
      "256/256 - 0s - loss: 14906.7832\n",
      "256/256 - 0s - loss: 10593.6807\n",
      "256/256 - 0s - loss: 11939.7842\n",
      "256/256 - 0s - loss: 12516.7285\n",
      "Reducing exploration for all agents to 0.0139\n",
      "\n",
      "Episode 310: Starting computation.\n",
      "Episode 310: Finished running.\n",
      "Agent 0, Average Reward: -181.91\n",
      "256/256 - 0s - loss: 13567.5928\n",
      "256/256 - 0s - loss: 11991.4619\n",
      "256/256 - 0s - loss: 8780.4131\n",
      "256/256 - 0s - loss: 12843.6963\n",
      "256/256 - 0s - loss: 11778.0186\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0137\n",
      "\n",
      "Episode 311: Starting computation.\n",
      "Episode 311: Finished running.\n",
      "Agent 0, Average Reward: -179.36\n",
      "256/256 - 0s - loss: 12240.5293\n",
      "256/256 - 0s - loss: 11563.3809\n",
      "256/256 - 0s - loss: 13288.3369\n",
      "256/256 - 0s - loss: 11447.6855\n",
      "256/256 - 0s - loss: 13569.2197\n",
      "Reducing exploration for all agents to 0.0135\n",
      "\n",
      "Episode 312: Starting computation.\n",
      "Episode 312: Finished running.\n",
      "Agent 0, Average Reward: -180.02\n",
      "256/256 - 0s - loss: 10699.3018\n",
      "256/256 - 0s - loss: 9913.1582\n",
      "256/256 - 0s - loss: 13111.4502\n",
      "256/256 - 0s - loss: 11672.3867\n",
      "256/256 - 0s - loss: 16643.2637\n",
      "Reducing exploration for all agents to 0.0133\n",
      "\n",
      "Episode 313: Starting computation.\n",
      "Episode 313: Finished running.\n",
      "Agent 0, Average Reward: -202.32\n",
      "256/256 - 0s - loss: 11570.1182\n",
      "256/256 - 0s - loss: 13485.0908\n",
      "256/256 - 0s - loss: 14976.9287\n",
      "256/256 - 0s - loss: 15779.9004\n",
      "256/256 - 0s - loss: 12572.9561\n",
      "Reducing exploration for all agents to 0.0131\n",
      "\n",
      "Episode 314: Starting computation.\n",
      "Episode 314: Finished running.\n",
      "Agent 0, Average Reward: -257.03\n",
      "256/256 - 0s - loss: 18665.8301\n",
      "256/256 - 0s - loss: 16883.9336\n",
      "256/256 - 0s - loss: 16605.5312\n",
      "256/256 - 0s - loss: 19583.6738\n",
      "256/256 - 0s - loss: 14707.3516\n",
      "Reducing exploration for all agents to 0.0129\n",
      "\n",
      "Episode 315: Starting computation.\n",
      "Episode 315: Finished running.\n",
      "Agent 0, Average Reward: -368.39\n",
      "256/256 - 0s - loss: 30276.6816\n",
      "256/256 - 0s - loss: 33032.5391\n",
      "256/256 - 0s - loss: 30543.3809\n",
      "256/256 - 0s - loss: 31690.8887\n",
      "256/256 - 0s - loss: 26843.2559\n",
      "Reducing exploration for all agents to 0.0128\n",
      "\n",
      "Episode 316: Starting computation.\n",
      "Episode 316: Finished running.\n",
      "Agent 0, Average Reward: -426.32\n",
      "256/256 - 0s - loss: 47374.4883\n",
      "256/256 - 0s - loss: 54303.0781\n",
      "256/256 - 0s - loss: 42447.8984\n",
      "256/256 - 0s - loss: 56613.4062\n",
      "256/256 - 0s - loss: 47510.2500\n",
      "Reducing exploration for all agents to 0.0126\n",
      "\n",
      "Episode 317: Starting computation.\n",
      "Episode 317: Finished running.\n",
      "Agent 0, Average Reward: -405.65\n",
      "256/256 - 0s - loss: 66333.0156\n",
      "256/256 - 0s - loss: 61080.9922\n",
      "256/256 - 0s - loss: 61802.0195\n",
      "256/256 - 0s - loss: 63777.3320\n",
      "256/256 - 0s - loss: 71135.6719\n",
      "Reducing exploration for all agents to 0.0124\n",
      "\n",
      "Episode 318: Starting computation.\n",
      "Episode 318: Finished running.\n",
      "Agent 0, Average Reward: -283.89\n",
      "256/256 - 0s - loss: 70904.1250\n",
      "256/256 - 0s - loss: 71631.5234\n",
      "256/256 - 0s - loss: 64498.5508\n",
      "256/256 - 0s - loss: 57853.9570\n",
      "256/256 - 0s - loss: 62620.9453\n",
      "Reducing exploration for all agents to 0.0123\n",
      "\n",
      "Episode 319: Starting computation.\n",
      "Episode 319: Finished running.\n",
      "Agent 0, Average Reward: -283.4\n",
      "256/256 - 0s - loss: 42549.0664\n",
      "256/256 - 0s - loss: 58131.0000\n",
      "256/256 - 0s - loss: 52857.0156\n",
      "256/256 - 0s - loss: 66369.4375\n",
      "256/256 - 0s - loss: 52303.6484\n",
      "Reducing exploration for all agents to 0.0121\n",
      "\n",
      "Episode 320: Starting computation.\n",
      "Episode 320: Finished running.\n",
      "Agent 0, Average Reward: -278.85\n",
      "256/256 - 0s - loss: 44208.7812\n",
      "256/256 - 0s - loss: 45061.1641\n",
      "256/256 - 0s - loss: 37703.2656\n",
      "256/256 - 0s - loss: 41244.5547\n",
      "256/256 - 0s - loss: 47431.7422\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0119\n",
      "\n",
      "Episode 321: Starting computation.\n",
      "Episode 321: Finished running.\n",
      "Agent 0, Average Reward: -279.16\n",
      "256/256 - 0s - loss: 29531.3164\n",
      "256/256 - 0s - loss: 32214.7500\n",
      "256/256 - 0s - loss: 36959.1289\n",
      "256/256 - 0s - loss: 27224.9941\n",
      "256/256 - 0s - loss: 27838.8574\n",
      "Reducing exploration for all agents to 0.0118\n",
      "\n",
      "Episode 322: Starting computation.\n",
      "Episode 322: Finished running.\n",
      "Agent 0, Average Reward: -265.81\n",
      "256/256 - 0s - loss: 24569.5098\n",
      "256/256 - 0s - loss: 24224.9121\n",
      "256/256 - 0s - loss: 22171.4570\n",
      "256/256 - 0s - loss: 20621.2578\n",
      "256/256 - 0s - loss: 20813.8965\n",
      "Reducing exploration for all agents to 0.0116\n",
      "\n",
      "Episode 323: Starting computation.\n",
      "Episode 323: Finished running.\n",
      "Agent 0, Average Reward: -253.25\n",
      "256/256 - 0s - loss: 22219.1953\n",
      "256/256 - 0s - loss: 22271.1133\n",
      "256/256 - 0s - loss: 17107.3887\n",
      "256/256 - 0s - loss: 21179.2051\n",
      "256/256 - 0s - loss: 23162.6777\n",
      "Reducing exploration for all agents to 0.0114\n",
      "\n",
      "Episode 324: Starting computation.\n",
      "Episode 324: Finished running.\n",
      "Agent 0, Average Reward: -229.57\n",
      "256/256 - 0s - loss: 20365.8926\n",
      "256/256 - 0s - loss: 19274.8730\n",
      "256/256 - 0s - loss: 21262.6035\n",
      "256/256 - 0s - loss: 19922.9668\n",
      "256/256 - 0s - loss: 21676.8105\n",
      "Reducing exploration for all agents to 0.0113\n",
      "\n",
      "Episode 325: Starting computation.\n",
      "Episode 325: Finished running.\n",
      "Agent 0, Average Reward: -226.06\n",
      "256/256 - 0s - loss: 18628.7129\n",
      "256/256 - 0s - loss: 20347.8652\n",
      "256/256 - 0s - loss: 15824.7227\n",
      "256/256 - 0s - loss: 17238.9043\n",
      "256/256 - 0s - loss: 18678.5273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0111\n",
      "\n",
      "Episode 326: Starting computation.\n",
      "Episode 326: Finished running.\n",
      "Agent 0, Average Reward: -211.84\n",
      "256/256 - 0s - loss: 15384.3340\n",
      "256/256 - 0s - loss: 18030.3691\n",
      "256/256 - 0s - loss: 18177.8223\n",
      "256/256 - 0s - loss: 18655.6230\n",
      "256/256 - 0s - loss: 17506.6973\n",
      "Reducing exploration for all agents to 0.011\n",
      "\n",
      "Episode 327: Starting computation.\n",
      "Episode 327: Finished running.\n",
      "Agent 0, Average Reward: -200.46\n",
      "256/256 - 0s - loss: 17283.2363\n",
      "256/256 - 0s - loss: 14133.2539\n",
      "256/256 - 0s - loss: 20323.0918\n",
      "256/256 - 0s - loss: 17567.5117\n",
      "256/256 - 0s - loss: 18231.4883\n",
      "Reducing exploration for all agents to 0.0108\n",
      "\n",
      "Episode 328: Starting computation.\n",
      "Episode 328: Finished running.\n",
      "Agent 0, Average Reward: -194.5\n",
      "256/256 - 0s - loss: 16569.7949\n",
      "256/256 - 0s - loss: 18810.7148\n",
      "256/256 - 0s - loss: 15435.9316\n",
      "256/256 - 0s - loss: 18022.0742\n",
      "256/256 - 0s - loss: 15748.5830\n",
      "Reducing exploration for all agents to 0.0107\n",
      "\n",
      "Episode 329: Starting computation.\n",
      "Episode 329: Finished running.\n",
      "Agent 0, Average Reward: -188.2\n",
      "256/256 - 0s - loss: 14896.5762\n",
      "256/256 - 0s - loss: 12417.3057\n",
      "256/256 - 0s - loss: 17416.2637\n",
      "256/256 - 0s - loss: 17908.0332\n",
      "256/256 - 0s - loss: 15189.0977\n",
      "Reducing exploration for all agents to 0.0105\n",
      "\n",
      "Episode 330: Starting computation.\n",
      "Episode 330: Finished running.\n",
      "Agent 0, Average Reward: -176.29\n",
      "256/256 - 0s - loss: 12762.3652\n",
      "256/256 - 0s - loss: 9404.6445\n",
      "256/256 - 0s - loss: 10492.6123\n",
      "256/256 - 0s - loss: 13029.3379\n",
      "256/256 - 0s - loss: 13170.8438\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0104\n",
      "\n",
      "Episode 331: Starting computation.\n",
      "Episode 331: Finished running.\n",
      "Agent 0, Average Reward: -175.65\n",
      "256/256 - 0s - loss: 12201.6807\n",
      "256/256 - 0s - loss: 15105.5605\n",
      "256/256 - 0s - loss: 12525.9023\n",
      "256/256 - 0s - loss: 13404.9912\n",
      "256/256 - 0s - loss: 12801.9805\n",
      "Reducing exploration for all agents to 0.0102\n",
      "\n",
      "Episode 332: Starting computation.\n",
      "Episode 332: Finished running.\n",
      "Agent 0, Average Reward: -176.49\n",
      "256/256 - 0s - loss: 10794.3398\n",
      "256/256 - 0s - loss: 12192.5537\n",
      "256/256 - 0s - loss: 11819.0879\n",
      "256/256 - 0s - loss: 13370.8477\n",
      "256/256 - 0s - loss: 13601.5820\n",
      "Reducing exploration for all agents to 0.0101\n",
      "\n",
      "Episode 333: Starting computation.\n",
      "Episode 333: Finished running.\n",
      "Agent 0, Average Reward: -175.88\n",
      "256/256 - 0s - loss: 12173.8672\n",
      "256/256 - 0s - loss: 14526.3379\n",
      "256/256 - 0s - loss: 13006.3115\n",
      "256/256 - 0s - loss: 11244.6973\n",
      "256/256 - 0s - loss: 12029.2822\n",
      "Reducing exploration for all agents to 0.01\n",
      "\n",
      "Episode 334: Starting computation.\n",
      "Episode 334: Finished running.\n",
      "Agent 0, Average Reward: -176.85\n",
      "256/256 - 0s - loss: 10352.2969\n",
      "256/256 - 0s - loss: 11752.5498\n",
      "256/256 - 0s - loss: 12630.9570\n",
      "256/256 - 0s - loss: 12202.4863\n",
      "256/256 - 0s - loss: 15674.7178\n",
      "Reducing exploration for all agents to 0.0098\n",
      "\n",
      "Episode 335: Starting computation.\n",
      "Episode 335: Finished running.\n",
      "Agent 0, Average Reward: -185.12\n",
      "256/256 - 0s - loss: 9983.2910\n",
      "256/256 - 0s - loss: 11445.9473\n",
      "256/256 - 0s - loss: 11826.5605\n",
      "256/256 - 0s - loss: 10721.7500\n",
      "256/256 - 0s - loss: 13553.0332\n",
      "Reducing exploration for all agents to 0.0097\n",
      "\n",
      "Episode 336: Starting computation.\n",
      "Episode 336: Finished running.\n",
      "Agent 0, Average Reward: -219.2\n",
      "256/256 - 0s - loss: 12570.7217\n",
      "256/256 - 0s - loss: 10364.6914\n",
      "256/256 - 0s - loss: 15255.0449\n",
      "256/256 - 0s - loss: 16829.5098\n",
      "256/256 - 0s - loss: 13547.3320\n",
      "Reducing exploration for all agents to 0.0095\n",
      "\n",
      "Episode 337: Starting computation.\n",
      "Episode 337: Finished running.\n",
      "Agent 0, Average Reward: -244.83\n",
      "256/256 - 0s - loss: 16160.0430\n",
      "256/256 - 0s - loss: 13386.7783\n",
      "256/256 - 0s - loss: 15199.0000\n",
      "256/256 - 0s - loss: 14572.6270\n",
      "256/256 - 0s - loss: 14237.1797\n",
      "Reducing exploration for all agents to 0.0094\n",
      "\n",
      "Episode 338: Starting computation.\n",
      "Episode 338: Finished running.\n",
      "Agent 0, Average Reward: -268.79\n",
      "256/256 - 0s - loss: 17141.0234\n",
      "256/256 - 0s - loss: 15493.3857\n",
      "256/256 - 0s - loss: 17895.2246\n",
      "256/256 - 0s - loss: 16051.0410\n",
      "256/256 - 0s - loss: 21049.2520\n",
      "Reducing exploration for all agents to 0.0093\n",
      "\n",
      "Episode 339: Starting computation.\n",
      "Episode 339: Finished running.\n",
      "Agent 0, Average Reward: -283.31\n",
      "256/256 - 0s - loss: 19404.0820\n",
      "256/256 - 0s - loss: 21416.8047\n",
      "256/256 - 0s - loss: 17728.6680\n",
      "256/256 - 0s - loss: 21165.5430\n",
      "256/256 - 0s - loss: 19697.5137\n",
      "Reducing exploration for all agents to 0.0092\n",
      "\n",
      "Episode 340: Starting computation.\n",
      "Episode 340: Finished running.\n",
      "Agent 0, Average Reward: -289.19\n",
      "256/256 - 0s - loss: 21319.1074\n",
      "256/256 - 0s - loss: 19805.7129\n",
      "256/256 - 0s - loss: 20433.3262\n",
      "256/256 - 0s - loss: 17140.4199\n",
      "256/256 - 0s - loss: 19106.9863\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.009\n",
      "\n",
      "Episode 341: Starting computation.\n",
      "Episode 341: Finished running.\n",
      "Agent 0, Average Reward: -288.46\n",
      "256/256 - 0s - loss: 22067.5859\n",
      "256/256 - 0s - loss: 22891.4629\n",
      "256/256 - 0s - loss: 22840.6797\n",
      "256/256 - 0s - loss: 20176.2148\n",
      "256/256 - 0s - loss: 22772.8438\n",
      "Reducing exploration for all agents to 0.0089\n",
      "\n",
      "Episode 342: Starting computation.\n",
      "Episode 342: Finished running.\n",
      "Agent 0, Average Reward: -290.15\n",
      "256/256 - 0s - loss: 23560.5430\n",
      "256/256 - 0s - loss: 22715.0723\n",
      "256/256 - 0s - loss: 24187.5605\n",
      "256/256 - 0s - loss: 22867.7695\n",
      "256/256 - 0s - loss: 21727.8633\n",
      "Reducing exploration for all agents to 0.0088\n",
      "\n",
      "Episode 343: Starting computation.\n",
      "Episode 343: Finished running.\n",
      "Agent 0, Average Reward: -275.13\n",
      "256/256 - 0s - loss: 22637.0078\n",
      "256/256 - 0s - loss: 20595.6465\n",
      "256/256 - 0s - loss: 22011.4805\n",
      "256/256 - 0s - loss: 21438.2852\n",
      "256/256 - 0s - loss: 25586.3691\n",
      "Reducing exploration for all agents to 0.0087\n",
      "\n",
      "Episode 344: Starting computation.\n",
      "Episode 344: Finished running.\n",
      "Agent 0, Average Reward: -244.95\n",
      "256/256 - 0s - loss: 21958.4609\n",
      "256/256 - 0s - loss: 20893.4121\n",
      "256/256 - 0s - loss: 22067.4551\n",
      "256/256 - 0s - loss: 22227.2695\n",
      "256/256 - 0s - loss: 20378.6543\n",
      "Reducing exploration for all agents to 0.0085\n",
      "\n",
      "Episode 345: Starting computation.\n",
      "Episode 345: Finished running.\n",
      "Agent 0, Average Reward: -227.06\n",
      "256/256 - 0s - loss: 19811.6387\n",
      "256/256 - 0s - loss: 21501.7051\n",
      "256/256 - 0s - loss: 17303.7559\n",
      "256/256 - 0s - loss: 19041.5645\n",
      "256/256 - 0s - loss: 22814.4316\n",
      "Reducing exploration for all agents to 0.0084\n",
      "\n",
      "Episode 346: Starting computation.\n",
      "Episode 346: Finished running.\n",
      "Agent 0, Average Reward: -214.77\n",
      "256/256 - 0s - loss: 18119.0996\n",
      "256/256 - 0s - loss: 22086.4277\n",
      "256/256 - 0s - loss: 21241.2871\n",
      "256/256 - 0s - loss: 16409.8145\n",
      "256/256 - 0s - loss: 16761.3105\n",
      "Reducing exploration for all agents to 0.0083\n",
      "\n",
      "Episode 347: Starting computation.\n",
      "Episode 347: Finished running.\n",
      "Agent 0, Average Reward: -202.54\n",
      "256/256 - 0s - loss: 18286.8340\n",
      "256/256 - 0s - loss: 18235.8184\n",
      "256/256 - 0s - loss: 16334.6279\n",
      "256/256 - 0s - loss: 16156.9980\n",
      "256/256 - 0s - loss: 18815.9902\n",
      "Reducing exploration for all agents to 0.0082\n",
      "\n",
      "Episode 348: Starting computation.\n",
      "Episode 348: Finished running.\n",
      "Agent 0, Average Reward: -190.38\n",
      "256/256 - 0s - loss: 13094.1152\n",
      "256/256 - 0s - loss: 13703.9629\n",
      "256/256 - 0s - loss: 17811.2656\n",
      "256/256 - 0s - loss: 17692.4980\n",
      "256/256 - 0s - loss: 18705.7031\n",
      "Reducing exploration for all agents to 0.0081\n",
      "\n",
      "Episode 349: Starting computation.\n",
      "Episode 349: Finished running.\n",
      "Agent 0, Average Reward: -183.36\n",
      "256/256 - 0s - loss: 17565.3496\n",
      "256/256 - 0s - loss: 18501.7988\n",
      "256/256 - 0s - loss: 15131.7881\n",
      "256/256 - 0s - loss: 17265.0430\n",
      "256/256 - 0s - loss: 14265.1846\n",
      "Reducing exploration for all agents to 0.008\n",
      "\n",
      "Episode 350: Starting computation.\n",
      "Episode 350: Finished running.\n",
      "Agent 0, Average Reward: -180.78\n",
      "256/256 - 0s - loss: 13856.3379\n",
      "256/256 - 0s - loss: 15323.3516\n",
      "256/256 - 0s - loss: 14352.5938\n",
      "256/256 - 0s - loss: 13572.0596\n",
      "256/256 - 0s - loss: 13166.1064\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0079\n",
      "\n",
      "Episode 351: Starting computation.\n",
      "Episode 351: Finished running.\n",
      "Agent 0, Average Reward: -174.4\n",
      "256/256 - 0s - loss: 11901.6895\n",
      "256/256 - 0s - loss: 13303.1816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 11682.8477\n",
      "256/256 - 0s - loss: 12253.6650\n",
      "256/256 - 0s - loss: 13130.6436\n",
      "Reducing exploration for all agents to 0.0078\n",
      "\n",
      "Episode 352: Starting computation.\n",
      "Episode 352: Finished running.\n",
      "Agent 0, Average Reward: -177.09\n",
      "256/256 - 0s - loss: 10452.2188\n",
      "256/256 - 0s - loss: 14716.9766\n",
      "256/256 - 0s - loss: 16170.6816\n",
      "256/256 - 0s - loss: 12588.3613\n",
      "256/256 - 0s - loss: 14884.7275\n",
      "Reducing exploration for all agents to 0.0077\n",
      "\n",
      "Episode 353: Starting computation.\n",
      "Episode 353: Finished running.\n",
      "Agent 0, Average Reward: -174.62\n",
      "256/256 - 0s - loss: 13117.7510\n",
      "256/256 - 0s - loss: 13427.3867\n",
      "256/256 - 0s - loss: 14338.4375\n",
      "256/256 - 0s - loss: 13401.1982\n",
      "256/256 - 0s - loss: 11037.1084\n",
      "Reducing exploration for all agents to 0.0075\n",
      "\n",
      "Episode 354: Starting computation.\n",
      "Episode 354: Finished running.\n",
      "Agent 0, Average Reward: -229.61\n",
      "256/256 - 0s - loss: 19867.1641\n",
      "256/256 - 0s - loss: 18629.5605\n",
      "256/256 - 0s - loss: 18980.2832\n",
      "256/256 - 0s - loss: 18592.7539\n",
      "256/256 - 0s - loss: 22916.2012\n",
      "Reducing exploration for all agents to 0.0074\n",
      "\n",
      "Episode 355: Starting computation.\n",
      "Episode 355: Finished running.\n",
      "Agent 0, Average Reward: -378.54\n",
      "256/256 - 0s - loss: 47883.3438\n",
      "256/256 - 0s - loss: 29743.3945\n",
      "256/256 - 0s - loss: 29850.3027\n",
      "256/256 - 0s - loss: 34917.9492\n",
      "256/256 - 0s - loss: 33835.7188\n",
      "Reducing exploration for all agents to 0.0073\n",
      "\n",
      "Episode 356: Starting computation.\n",
      "Episode 356: Finished running.\n",
      "Agent 0, Average Reward: -461.98\n",
      "256/256 - 0s - loss: 61507.0898\n",
      "256/256 - 0s - loss: 57598.1445\n",
      "256/256 - 0s - loss: 57937.9375\n",
      "256/256 - 0s - loss: 68392.4844\n",
      "256/256 - 0s - loss: 47200.5859\n",
      "Reducing exploration for all agents to 0.0072\n",
      "\n",
      "Episode 357: Starting computation.\n",
      "Episode 357: Finished running.\n",
      "Agent 0, Average Reward: -344.91\n",
      "256/256 - 0s - loss: 61948.2539\n",
      "256/256 - 0s - loss: 61531.1367\n",
      "256/256 - 0s - loss: 77489.2031\n",
      "256/256 - 0s - loss: 58185.2539\n",
      "256/256 - 0s - loss: 56635.2266\n",
      "Reducing exploration for all agents to 0.0071\n",
      "\n",
      "Episode 358: Starting computation.\n",
      "Episode 358: Finished running.\n",
      "Agent 0, Average Reward: -279.29\n",
      "256/256 - 0s - loss: 84522.6328\n",
      "256/256 - 0s - loss: 64415.8047\n",
      "256/256 - 0s - loss: 81732.6250\n",
      "256/256 - 0s - loss: 77573.3281\n",
      "256/256 - 0s - loss: 70127.3750\n",
      "Reducing exploration for all agents to 0.007\n",
      "\n",
      "Episode 359: Starting computation.\n",
      "Episode 359: Finished running.\n",
      "Agent 0, Average Reward: -268.53\n",
      "256/256 - 0s - loss: 55650.7812\n",
      "256/256 - 0s - loss: 56407.9375\n",
      "256/256 - 0s - loss: 50235.1680\n",
      "256/256 - 0s - loss: 61559.3906\n",
      "256/256 - 0s - loss: 61875.1016\n",
      "Reducing exploration for all agents to 0.0069\n",
      "\n",
      "Episode 360: Starting computation.\n",
      "Episode 360: Finished running.\n",
      "Agent 0, Average Reward: -234.86\n",
      "256/256 - 0s - loss: 44685.7070\n",
      "256/256 - 0s - loss: 34807.7852\n",
      "256/256 - 0s - loss: 34551.8945\n",
      "256/256 - 0s - loss: 45902.3164\n",
      "256/256 - 0s - loss: 33747.1562\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0068\n",
      "\n",
      "Episode 361: Starting computation.\n",
      "Episode 361: Finished running.\n",
      "Agent 0, Average Reward: -210.3\n",
      "256/256 - 0s - loss: 28168.0977\n",
      "256/256 - 0s - loss: 27717.9941\n",
      "256/256 - 0s - loss: 25197.5918\n",
      "256/256 - 0s - loss: 30983.0625\n",
      "256/256 - 0s - loss: 28271.9043\n",
      "Reducing exploration for all agents to 0.0068\n",
      "\n",
      "Episode 362: Starting computation.\n",
      "Episode 362: Finished running.\n",
      "Agent 0, Average Reward: -189.47\n",
      "256/256 - 0s - loss: 17387.7266\n",
      "256/256 - 0s - loss: 20403.3145\n",
      "256/256 - 0s - loss: 20781.1621\n",
      "256/256 - 0s - loss: 21106.8301\n",
      "256/256 - 0s - loss: 19417.9805\n",
      "Reducing exploration for all agents to 0.0067\n",
      "\n",
      "Episode 363: Starting computation.\n",
      "Episode 363: Finished running.\n",
      "Agent 0, Average Reward: -185.05\n",
      "256/256 - 0s - loss: 18819.3828\n",
      "256/256 - 0s - loss: 17737.0391\n",
      "256/256 - 0s - loss: 15696.2393\n",
      "256/256 - 0s - loss: 20072.3789\n",
      "256/256 - 0s - loss: 17714.7812\n",
      "Reducing exploration for all agents to 0.0066\n",
      "\n",
      "Episode 364: Starting computation.\n",
      "Episode 364: Finished running.\n",
      "Agent 0, Average Reward: -189.66\n",
      "256/256 - 0s - loss: 15704.2217\n",
      "256/256 - 0s - loss: 19310.8535\n",
      "256/256 - 0s - loss: 18639.7852\n",
      "256/256 - 0s - loss: 16427.3496\n",
      "256/256 - 0s - loss: 15249.8525\n",
      "Reducing exploration for all agents to 0.0065\n",
      "\n",
      "Episode 365: Starting computation.\n",
      "Episode 365: Finished running.\n",
      "Agent 0, Average Reward: -187.46\n",
      "256/256 - 0s - loss: 11617.6768\n",
      "256/256 - 0s - loss: 17078.5273\n",
      "256/256 - 0s - loss: 17234.1855\n",
      "256/256 - 0s - loss: 15908.7568\n",
      "256/256 - 0s - loss: 15907.3691\n",
      "Reducing exploration for all agents to 0.0064\n",
      "\n",
      "Episode 366: Starting computation.\n",
      "Episode 366: Finished running.\n",
      "Agent 0, Average Reward: -188.33\n",
      "256/256 - 0s - loss: 12609.1846\n",
      "256/256 - 0s - loss: 13361.8193\n",
      "256/256 - 0s - loss: 14710.6709\n",
      "256/256 - 0s - loss: 12839.6729\n",
      "256/256 - 0s - loss: 13533.3311\n",
      "Reducing exploration for all agents to 0.0063\n",
      "\n",
      "Episode 367: Starting computation.\n",
      "Episode 367: Finished running.\n",
      "Agent 0, Average Reward: -225.21\n",
      "256/256 - 0s - loss: 12284.4629\n",
      "256/256 - 0s - loss: 13672.2070\n",
      "256/256 - 0s - loss: 17789.8672\n",
      "256/256 - 0s - loss: 15164.2979\n",
      "256/256 - 0s - loss: 13840.8604\n",
      "Reducing exploration for all agents to 0.0062\n",
      "\n",
      "Episode 368: Starting computation.\n",
      "Episode 368: Finished running.\n",
      "Agent 0, Average Reward: -244.84\n",
      "256/256 - 0s - loss: 16198.3877\n",
      "256/256 - 0s - loss: 15134.5908\n",
      "256/256 - 0s - loss: 11843.4092\n",
      "256/256 - 0s - loss: 21470.3906\n",
      "256/256 - 0s - loss: 16168.8350\n",
      "Reducing exploration for all agents to 0.0061\n",
      "\n",
      "Episode 369: Starting computation.\n",
      "Episode 369: Finished running.\n",
      "Agent 0, Average Reward: -264.3\n",
      "256/256 - 0s - loss: 16323.0459\n",
      "256/256 - 0s - loss: 14600.5479\n",
      "256/256 - 0s - loss: 17116.1309\n",
      "256/256 - 0s - loss: 18696.5801\n",
      "256/256 - 0s - loss: 17855.0723\n",
      "Reducing exploration for all agents to 0.006\n",
      "\n",
      "Episode 370: Starting computation.\n",
      "Episode 370: Finished running.\n",
      "Agent 0, Average Reward: -274.49\n",
      "256/256 - 0s - loss: 20435.1562\n",
      "256/256 - 0s - loss: 18292.7598\n",
      "256/256 - 0s - loss: 18965.1621\n",
      "256/256 - 0s - loss: 20582.1387\n",
      "256/256 - 0s - loss: 20490.0059\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.006\n",
      "\n",
      "Episode 371: Starting computation.\n",
      "Episode 371: Finished running.\n",
      "Agent 0, Average Reward: -283.33\n",
      "256/256 - 0s - loss: 24961.2207\n",
      "256/256 - 0s - loss: 23408.1504\n",
      "256/256 - 0s - loss: 25825.1777\n",
      "256/256 - 0s - loss: 21002.0195\n",
      "256/256 - 0s - loss: 21532.7676\n",
      "Reducing exploration for all agents to 0.0059\n",
      "\n",
      "Episode 372: Starting computation.\n",
      "Episode 372: Finished running.\n",
      "Agent 0, Average Reward: -290.74\n",
      "256/256 - 0s - loss: 26386.5645\n",
      "256/256 - 0s - loss: 21593.1465\n",
      "256/256 - 0s - loss: 21395.1816\n",
      "256/256 - 0s - loss: 19496.4941\n",
      "256/256 - 0s - loss: 22610.5059\n",
      "Reducing exploration for all agents to 0.0058\n",
      "\n",
      "Episode 373: Starting computation.\n",
      "Episode 373: Finished running.\n",
      "Agent 0, Average Reward: -283.88\n",
      "256/256 - 0s - loss: 20493.4082\n",
      "256/256 - 0s - loss: 21878.4199\n",
      "256/256 - 0s - loss: 23254.3555\n",
      "256/256 - 0s - loss: 20589.8281\n",
      "256/256 - 0s - loss: 19707.8848\n",
      "Reducing exploration for all agents to 0.0057\n",
      "\n",
      "Episode 374: Starting computation.\n",
      "Episode 374: Finished running.\n",
      "Agent 0, Average Reward: -271.95\n",
      "256/256 - 0s - loss: 25228.1484\n",
      "256/256 - 0s - loss: 22127.6191\n",
      "256/256 - 0s - loss: 22298.0391\n",
      "256/256 - 0s - loss: 22196.6836\n",
      "256/256 - 0s - loss: 23813.9395\n",
      "Reducing exploration for all agents to 0.0056\n",
      "\n",
      "Episode 375: Starting computation.\n",
      "Episode 375: Finished running.\n",
      "Agent 0, Average Reward: -253.01\n",
      "256/256 - 0s - loss: 22962.5469\n",
      "256/256 - 0s - loss: 18954.2695\n",
      "256/256 - 0s - loss: 17350.2207\n",
      "256/256 - 0s - loss: 20868.4922\n",
      "256/256 - 0s - loss: 24781.6250\n",
      "Reducing exploration for all agents to 0.0056\n",
      "\n",
      "Episode 376: Starting computation.\n",
      "Episode 376: Finished running.\n",
      "Agent 0, Average Reward: -226.73\n",
      "256/256 - 0s - loss: 17702.5684\n",
      "256/256 - 0s - loss: 18013.3730\n",
      "256/256 - 0s - loss: 20452.4219\n",
      "256/256 - 0s - loss: 17216.3750\n",
      "256/256 - 0s - loss: 20236.2461\n",
      "Reducing exploration for all agents to 0.0055\n",
      "\n",
      "Episode 377: Starting computation.\n",
      "Episode 377: Finished running.\n",
      "Agent 0, Average Reward: -222.76\n",
      "256/256 - 0s - loss: 18688.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 18829.9609\n",
      "256/256 - 0s - loss: 20544.3594\n",
      "256/256 - 0s - loss: 18602.4727\n",
      "256/256 - 0s - loss: 20546.5000\n",
      "Reducing exploration for all agents to 0.0054\n",
      "\n",
      "Episode 378: Starting computation.\n",
      "Episode 378: Finished running.\n",
      "Agent 0, Average Reward: -224.95\n",
      "256/256 - 0s - loss: 17253.8262\n",
      "256/256 - 0s - loss: 15611.5615\n",
      "256/256 - 0s - loss: 14779.6885\n",
      "256/256 - 0s - loss: 21198.0898\n",
      "256/256 - 0s - loss: 16594.5605\n",
      "Reducing exploration for all agents to 0.0053\n",
      "\n",
      "Episode 379: Starting computation.\n",
      "Episode 379: Finished running.\n",
      "Agent 0, Average Reward: -214.73\n",
      "256/256 - 0s - loss: 18176.0781\n",
      "256/256 - 0s - loss: 15270.0508\n",
      "256/256 - 0s - loss: 19695.3086\n",
      "256/256 - 0s - loss: 18047.0371\n",
      "256/256 - 0s - loss: 15373.4775\n",
      "Reducing exploration for all agents to 0.0053\n",
      "\n",
      "Episode 380: Starting computation.\n",
      "Episode 380: Finished running.\n",
      "Agent 0, Average Reward: -211.45\n",
      "256/256 - 0s - loss: 22275.5508\n",
      "256/256 - 0s - loss: 16587.5859\n",
      "256/256 - 0s - loss: 18813.4102\n",
      "256/256 - 0s - loss: 17864.3418\n",
      "256/256 - 0s - loss: 16921.4941\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0052\n",
      "\n",
      "Episode 381: Starting computation.\n",
      "Episode 381: Finished running.\n",
      "Agent 0, Average Reward: -212.98\n",
      "256/256 - 0s - loss: 16983.9609\n",
      "256/256 - 0s - loss: 18075.9609\n",
      "256/256 - 0s - loss: 20315.8984\n",
      "256/256 - 0s - loss: 19604.6328\n",
      "256/256 - 0s - loss: 18388.9414\n",
      "Reducing exploration for all agents to 0.0051\n",
      "\n",
      "Episode 382: Starting computation.\n",
      "Episode 382: Finished running.\n",
      "Agent 0, Average Reward: -203.98\n",
      "256/256 - 0s - loss: 18452.1484\n",
      "256/256 - 0s - loss: 16856.9434\n",
      "256/256 - 0s - loss: 17613.8633\n",
      "256/256 - 0s - loss: 20807.5977\n",
      "256/256 - 0s - loss: 20820.0195\n",
      "Reducing exploration for all agents to 0.0051\n",
      "\n",
      "Episode 383: Starting computation.\n",
      "Episode 383: Finished running.\n",
      "Agent 0, Average Reward: -199.46\n",
      "256/256 - 0s - loss: 18115.4336\n",
      "256/256 - 0s - loss: 15575.7402\n",
      "256/256 - 0s - loss: 17995.4785\n",
      "256/256 - 0s - loss: 18200.2930\n",
      "256/256 - 0s - loss: 17288.9395\n",
      "Reducing exploration for all agents to 0.005\n",
      "\n",
      "Episode 384: Starting computation.\n",
      "Episode 384: Finished running.\n",
      "Agent 0, Average Reward: -193.2\n",
      "256/256 - 0s - loss: 18286.6719\n",
      "256/256 - 0s - loss: 17265.2734\n",
      "256/256 - 0s - loss: 15662.5898\n",
      "256/256 - 0s - loss: 18710.2012\n",
      "256/256 - 0s - loss: 17916.4570\n",
      "Reducing exploration for all agents to 0.0049\n",
      "\n",
      "Episode 385: Starting computation.\n",
      "Episode 385: Finished running.\n",
      "Agent 0, Average Reward: -180.52\n",
      "256/256 - 0s - loss: 13350.8643\n",
      "256/256 - 0s - loss: 14114.2002\n",
      "256/256 - 0s - loss: 19957.4375\n",
      "256/256 - 0s - loss: 16286.6143\n",
      "256/256 - 0s - loss: 12875.0967\n",
      "Reducing exploration for all agents to 0.0048\n",
      "\n",
      "Episode 386: Starting computation.\n",
      "Episode 386: Finished running.\n",
      "Agent 0, Average Reward: -189.82\n",
      "256/256 - 0s - loss: 15657.1494\n",
      "256/256 - 0s - loss: 12972.7627\n",
      "256/256 - 0s - loss: 11977.1553\n",
      "256/256 - 0s - loss: 17716.0625\n",
      "256/256 - 0s - loss: 16771.5859\n",
      "Reducing exploration for all agents to 0.0048\n",
      "\n",
      "Episode 387: Starting computation.\n",
      "Episode 387: Finished running.\n",
      "Agent 0, Average Reward: -191.0\n",
      "256/256 - 0s - loss: 11301.8037\n",
      "256/256 - 0s - loss: 14304.1279\n",
      "256/256 - 0s - loss: 9110.9297\n",
      "256/256 - 0s - loss: 16880.0801\n",
      "256/256 - 0s - loss: 11444.6885\n",
      "Reducing exploration for all agents to 0.0047\n",
      "\n",
      "Episode 388: Starting computation.\n",
      "Episode 388: Finished running.\n",
      "Agent 0, Average Reward: -176.41\n",
      "256/256 - 0s - loss: 10664.6396\n",
      "256/256 - 0s - loss: 12692.0771\n",
      "256/256 - 0s - loss: 9621.3643\n",
      "256/256 - 0s - loss: 13870.8398\n",
      "256/256 - 0s - loss: 15099.0781\n",
      "Reducing exploration for all agents to 0.0046\n",
      "\n",
      "Episode 389: Starting computation.\n",
      "Episode 389: Finished running.\n",
      "Agent 0, Average Reward: -171.4\n",
      "256/256 - 0s - loss: 11992.7158\n",
      "256/256 - 0s - loss: 11951.8643\n",
      "256/256 - 0s - loss: 11728.1348\n",
      "256/256 - 0s - loss: 14040.2012\n",
      "256/256 - 0s - loss: 12460.1162\n",
      "Reducing exploration for all agents to 0.0046\n",
      "\n",
      "Episode 390: Starting computation.\n",
      "Episode 390: Finished running.\n",
      "Agent 0, Average Reward: -201.65\n",
      "256/256 - 0s - loss: 13257.2021\n",
      "256/256 - 0s - loss: 16254.7686\n",
      "256/256 - 0s - loss: 10873.8584\n",
      "256/256 - 0s - loss: 16042.4209\n",
      "256/256 - 0s - loss: 15722.2432\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0045\n",
      "\n",
      "Episode 391: Starting computation.\n",
      "Episode 391: Finished running.\n",
      "Agent 0, Average Reward: -232.08\n",
      "256/256 - 0s - loss: 12880.9014\n",
      "256/256 - 0s - loss: 19185.1504\n",
      "256/256 - 0s - loss: 15392.9541\n",
      "256/256 - 0s - loss: 15701.1572\n",
      "256/256 - 0s - loss: 16029.4990\n",
      "Reducing exploration for all agents to 0.0045\n",
      "\n",
      "Episode 392: Starting computation.\n",
      "Episode 392: Finished running.\n",
      "Agent 0, Average Reward: -260.05\n",
      "256/256 - 0s - loss: 17947.7109\n",
      "256/256 - 0s - loss: 18276.3203\n",
      "256/256 - 0s - loss: 16122.3477\n",
      "256/256 - 0s - loss: 15649.6826\n",
      "256/256 - 0s - loss: 15799.4951\n",
      "Reducing exploration for all agents to 0.0044\n",
      "\n",
      "Episode 393: Starting computation.\n",
      "Episode 393: Finished running.\n",
      "Agent 0, Average Reward: -284.03\n",
      "256/256 - 0s - loss: 20746.1992\n",
      "256/256 - 0s - loss: 19668.7832\n",
      "256/256 - 0s - loss: 20148.5176\n",
      "256/256 - 0s - loss: 20394.0469\n",
      "256/256 - 0s - loss: 20709.4492\n",
      "Reducing exploration for all agents to 0.0043\n",
      "\n",
      "Episode 394: Starting computation.\n",
      "Episode 394: Finished running.\n",
      "Agent 0, Average Reward: -291.48\n",
      "256/256 - 0s - loss: 26702.4727\n",
      "256/256 - 0s - loss: 24628.6895\n",
      "256/256 - 0s - loss: 26422.1602\n",
      "256/256 - 0s - loss: 26865.3398\n",
      "256/256 - 0s - loss: 20119.7227\n",
      "Reducing exploration for all agents to 0.0043\n",
      "\n",
      "Episode 395: Starting computation.\n",
      "Episode 395: Finished running.\n",
      "Agent 0, Average Reward: -307.91\n",
      "256/256 - 0s - loss: 25208.7227\n",
      "256/256 - 0s - loss: 27648.6445\n",
      "256/256 - 0s - loss: 26311.4805\n",
      "256/256 - 0s - loss: 27490.1523\n",
      "256/256 - 0s - loss: 29078.8203\n",
      "Reducing exploration for all agents to 0.0042\n",
      "\n",
      "Episode 396: Starting computation.\n",
      "Episode 396: Finished running.\n",
      "Agent 0, Average Reward: -329.51\n",
      "256/256 - 0s - loss: 32121.0840\n",
      "256/256 - 0s - loss: 30240.6523\n",
      "256/256 - 0s - loss: 30240.9688\n",
      "256/256 - 0s - loss: 30433.8594\n",
      "256/256 - 0s - loss: 28755.8691\n",
      "Reducing exploration for all agents to 0.0042\n",
      "\n",
      "Episode 397: Starting computation.\n",
      "Episode 397: Finished running.\n",
      "Agent 0, Average Reward: -296.85\n",
      "256/256 - 0s - loss: 34326.3984\n",
      "256/256 - 0s - loss: 36877.4492\n",
      "256/256 - 0s - loss: 38921.8594\n",
      "256/256 - 0s - loss: 37821.8555\n",
      "256/256 - 0s - loss: 37436.0820\n",
      "Reducing exploration for all agents to 0.0041\n",
      "\n",
      "Episode 398: Starting computation.\n",
      "Episode 398: Finished running.\n",
      "Agent 0, Average Reward: -230.3\n",
      "256/256 - 0s - loss: 40489.7656\n",
      "256/256 - 0s - loss: 29860.8320\n",
      "256/256 - 0s - loss: 33367.6367\n",
      "256/256 - 0s - loss: 37269.5859\n",
      "256/256 - 0s - loss: 28577.8477\n",
      "Reducing exploration for all agents to 0.004\n",
      "\n",
      "Episode 399: Starting computation.\n",
      "Episode 399: Finished running.\n",
      "Agent 0, Average Reward: -174.64\n",
      "256/256 - 0s - loss: 34705.1875\n",
      "256/256 - 0s - loss: 21856.0586\n",
      "256/256 - 0s - loss: 31738.6484\n",
      "256/256 - 0s - loss: 27767.9434\n",
      "256/256 - 0s - loss: 26261.7227\n",
      "Reducing exploration for all agents to 0.004\n",
      "\n",
      "Episode 400: Starting computation.\n",
      "Episode 400: Finished running.\n",
      "Agent 0, Average Reward: -198.12\n",
      "256/256 - 0s - loss: 25472.3906\n",
      "256/256 - 0s - loss: 29565.5273\n",
      "256/256 - 0s - loss: 18962.4297\n",
      "256/256 - 0s - loss: 24431.3242\n",
      "256/256 - 0s - loss: 30448.9746\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0039\n",
      "\n",
      "Episode 401: Starting computation.\n",
      "Episode 401: Finished running.\n",
      "Agent 0, Average Reward: -229.56\n",
      "256/256 - 0s - loss: 23774.9551\n",
      "256/256 - 0s - loss: 26418.6230\n",
      "256/256 - 0s - loss: 28975.2773\n",
      "256/256 - 0s - loss: 22687.0957\n",
      "256/256 - 0s - loss: 27590.0938\n",
      "Reducing exploration for all agents to 0.0039\n",
      "\n",
      "Episode 402: Starting computation.\n",
      "Episode 402: Finished running.\n",
      "Agent 0, Average Reward: -224.9\n",
      "256/256 - 0s - loss: 29008.4316\n",
      "256/256 - 0s - loss: 27269.7852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 30744.2793\n",
      "256/256 - 0s - loss: 27510.2188\n",
      "256/256 - 0s - loss: 34175.9023\n",
      "Reducing exploration for all agents to 0.0038\n",
      "\n",
      "Episode 403: Starting computation.\n",
      "Episode 403: Finished running.\n",
      "Agent 0, Average Reward: -177.19\n",
      "256/256 - 0s - loss: 27637.0215\n",
      "256/256 - 0s - loss: 24875.7734\n",
      "256/256 - 0s - loss: 25942.2812\n",
      "256/256 - 0s - loss: 24379.8633\n",
      "256/256 - 0s - loss: 24962.7754\n",
      "Reducing exploration for all agents to 0.0038\n",
      "\n",
      "Episode 404: Starting computation.\n",
      "Episode 404: Finished running.\n",
      "Agent 0, Average Reward: -249.62\n",
      "256/256 - 0s - loss: 23113.3691\n",
      "256/256 - 0s - loss: 28278.5957\n",
      "256/256 - 0s - loss: 25035.1172\n",
      "256/256 - 0s - loss: 22876.6777\n",
      "256/256 - 0s - loss: 30653.9199\n",
      "Reducing exploration for all agents to 0.0037\n",
      "\n",
      "Episode 405: Starting computation.\n",
      "Episode 405: Finished running.\n",
      "Agent 0, Average Reward: -378.69\n",
      "256/256 - 0s - loss: 29398.8438\n",
      "256/256 - 0s - loss: 32692.1602\n",
      "256/256 - 0s - loss: 29283.4258\n",
      "256/256 - 0s - loss: 29022.9492\n",
      "256/256 - 0s - loss: 31901.9629\n",
      "Reducing exploration for all agents to 0.0037\n",
      "\n",
      "Episode 406: Starting computation.\n",
      "Episode 406: Finished running.\n",
      "Agent 0, Average Reward: -2024.41\n",
      "256/256 - 0s - loss: 60069.9883\n",
      "256/256 - 0s - loss: 52551.6953\n",
      "256/256 - 0s - loss: 54438.6484\n",
      "256/256 - 0s - loss: 45636.1562\n",
      "256/256 - 0s - loss: 38366.5117\n",
      "Reducing exploration for all agents to 0.0036\n",
      "\n",
      "Episode 407: Starting computation.\n",
      "Episode 407: Finished running.\n",
      "Agent 0, Average Reward: -2161.82\n",
      "256/256 - 0s - loss: 75130.1250\n",
      "256/256 - 0s - loss: 67917.0391\n",
      "256/256 - 0s - loss: 59291.4961\n",
      "256/256 - 0s - loss: 57138.2812\n",
      "256/256 - 0s - loss: 54824.6914\n",
      "Reducing exploration for all agents to 0.0036\n",
      "\n",
      "Episode 408: Starting computation.\n",
      "Episode 408: Finished running.\n",
      "Agent 0, Average Reward: -3229.89\n",
      "256/256 - 0s - loss: 81846.2734\n",
      "256/256 - 0s - loss: 73954.1172\n",
      "256/256 - 0s - loss: 45674.1602\n",
      "256/256 - 0s - loss: 48794.6992\n",
      "256/256 - 0s - loss: 46317.7305\n",
      "Reducing exploration for all agents to 0.0035\n",
      "\n",
      "Episode 409: Starting computation.\n",
      "Episode 409: Finished running.\n",
      "Agent 0, Average Reward: -104.24\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 83107.8047\n",
      "256/256 - 0s - loss: 75099.8047\n",
      "256/256 - 0s - loss: 67848.6484\n",
      "256/256 - 0s - loss: 102338.9297\n",
      "256/256 - 0s - loss: 78687.3594\n",
      "Reducing exploration for all agents to 0.0035\n",
      "\n",
      "Episode 410: Starting computation.\n",
      "Episode 410: Finished running.\n",
      "Agent 0, Average Reward: -98.83\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 70545.8203\n",
      "256/256 - 0s - loss: 46048.7227\n",
      "256/256 - 0s - loss: 30279.8594\n",
      "256/256 - 0s - loss: 37104.5859\n",
      "256/256 - 0s - loss: 45682.5508\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0034\n",
      "\n",
      "Episode 411: Starting computation.\n",
      "Episode 411: Finished running.\n",
      "Agent 0, Average Reward: -2024.78\n",
      "256/256 - 0s - loss: 122693.5781\n",
      "256/256 - 0s - loss: 123488.9297\n",
      "256/256 - 0s - loss: 115491.0234\n",
      "256/256 - 0s - loss: 105296.3750\n",
      "256/256 - 0s - loss: 85811.1250\n",
      "Reducing exploration for all agents to 0.0034\n",
      "\n",
      "Episode 412: Starting computation.\n",
      "Episode 412: Finished running.\n",
      "Agent 0, Average Reward: -2016.81\n",
      "256/256 - 0s - loss: 67441.8516\n",
      "256/256 - 0s - loss: 52811.3125\n",
      "256/256 - 0s - loss: 43601.6445\n",
      "256/256 - 0s - loss: 36229.9648\n",
      "256/256 - 0s - loss: 33603.5000\n",
      "Reducing exploration for all agents to 0.0033\n",
      "\n",
      "Episode 413: Starting computation.\n",
      "Episode 413: Finished running.\n",
      "Agent 0, Average Reward: -140.62\n",
      "256/256 - 0s - loss: 27588.8750\n",
      "256/256 - 0s - loss: 33119.2930\n",
      "256/256 - 0s - loss: 27804.2910\n",
      "256/256 - 0s - loss: 31401.3867\n",
      "256/256 - 0s - loss: 33189.9883\n",
      "Reducing exploration for all agents to 0.0033\n",
      "\n",
      "Episode 414: Starting computation.\n",
      "Episode 414: Finished running.\n",
      "Agent 0, Average Reward: -96.94\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 22993.6465\n",
      "256/256 - 0s - loss: 31718.8535\n",
      "256/256 - 0s - loss: 32406.4395\n",
      "256/256 - 0s - loss: 30304.7246\n",
      "256/256 - 0s - loss: 41216.0195\n",
      "Reducing exploration for all agents to 0.0032\n",
      "\n",
      "Episode 415: Starting computation.\n",
      "Episode 415: Finished running.\n",
      "Agent 0, Average Reward: -91.4\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "256/256 - 0s - loss: 51066.0273\n",
      "256/256 - 0s - loss: 56128.7266\n",
      "256/256 - 0s - loss: 46725.4258\n",
      "256/256 - 0s - loss: 46899.8438\n",
      "256/256 - 0s - loss: 42787.0586\n",
      "Reducing exploration for all agents to 0.0032\n",
      "\n",
      "Episode 416: Starting computation.\n",
      "Episode 416: Finished running.\n",
      "Agent 0, Average Reward: -388.32\n",
      "256/256 - 0s - loss: 57353.4219\n",
      "256/256 - 0s - loss: 42085.1719\n",
      "256/256 - 0s - loss: 59219.7266\n",
      "256/256 - 0s - loss: 51436.6289\n",
      "256/256 - 0s - loss: 39118.0742\n",
      "Reducing exploration for all agents to 0.0032\n",
      "\n",
      "Episode 417: Starting computation.\n",
      "Episode 417: Finished running.\n",
      "Agent 0, Average Reward: -1968.73\n",
      "256/256 - 0s - loss: 47389.4961\n",
      "256/256 - 0s - loss: 47687.2852\n",
      "256/256 - 0s - loss: 34428.9297\n",
      "256/256 - 0s - loss: 43689.5664\n",
      "256/256 - 0s - loss: 47383.0273\n",
      "Reducing exploration for all agents to 0.0031\n",
      "\n",
      "Episode 418: Starting computation.\n",
      "Episode 418: Finished running.\n",
      "Agent 0, Average Reward: -1989.22\n",
      "256/256 - 0s - loss: 30135.6191\n",
      "256/256 - 0s - loss: 27692.9043\n",
      "256/256 - 0s - loss: 36702.5117\n",
      "256/256 - 0s - loss: 23339.4902\n",
      "256/256 - 0s - loss: 24939.1152\n",
      "Reducing exploration for all agents to 0.0031\n",
      "\n",
      "Episode 419: Starting computation.\n",
      "Episode 419: Finished running.\n",
      "Agent 0, Average Reward: -1499.07\n",
      "256/256 - 0s - loss: 26018.2285\n",
      "256/256 - 0s - loss: 20568.2949\n",
      "256/256 - 0s - loss: 25225.2246\n",
      "256/256 - 0s - loss: 24590.4316\n",
      "256/256 - 0s - loss: 27931.9512\n",
      "Reducing exploration for all agents to 0.003\n",
      "\n",
      "Episode 420: Starting computation.\n",
      "Episode 420: Finished running.\n",
      "Agent 0, Average Reward: -290.04\n",
      "256/256 - 0s - loss: 24168.6641\n",
      "256/256 - 0s - loss: 14209.4219\n",
      "256/256 - 0s - loss: 11785.6846\n",
      "256/256 - 0s - loss: 23299.5488\n",
      "256/256 - 0s - loss: 25380.9785\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.003\n",
      "\n",
      "Episode 421: Starting computation.\n",
      "Episode 421: Finished running.\n",
      "Agent 0, Average Reward: -173.93\n",
      "256/256 - 0s - loss: 54454.0273\n",
      "256/256 - 0s - loss: 49434.2070\n",
      "256/256 - 0s - loss: 42587.5898\n",
      "256/256 - 0s - loss: 31680.2441\n",
      "256/256 - 0s - loss: 27547.9434\n",
      "Reducing exploration for all agents to 0.0029\n",
      "\n",
      "Episode 422: Starting computation.\n",
      "Episode 422: Finished running.\n",
      "Agent 0, Average Reward: -370.3\n",
      "256/256 - 0s - loss: 46963.1328\n",
      "256/256 - 0s - loss: 46926.7539\n",
      "256/256 - 0s - loss: 84307.2500\n",
      "256/256 - 0s - loss: 42128.4883\n",
      "256/256 - 0s - loss: 34771.2266\n",
      "Reducing exploration for all agents to 0.0029\n",
      "\n",
      "Episode 423: Starting computation.\n",
      "Episode 423: Finished running.\n",
      "Agent 0, Average Reward: -1644.86\n",
      "256/256 - 0s - loss: 122152.1875\n",
      "256/256 - 0s - loss: 103884.6797\n",
      "256/256 - 0s - loss: 118332.4688\n",
      "256/256 - 0s - loss: 111984.2266\n",
      "256/256 - 0s - loss: 88312.4219\n",
      "Reducing exploration for all agents to 0.0029\n",
      "\n",
      "Episode 424: Starting computation.\n",
      "Episode 424: Finished running.\n",
      "Agent 0, Average Reward: -151.4\n",
      "256/256 - 0s - loss: 110454.9062\n",
      "256/256 - 0s - loss: 101192.7266\n",
      "256/256 - 0s - loss: 110034.6406\n",
      "256/256 - 0s - loss: 87512.8516\n",
      "256/256 - 0s - loss: 64814.3086\n",
      "Reducing exploration for all agents to 0.0028\n",
      "\n",
      "Episode 425: Starting computation.\n",
      "Episode 425: Finished running.\n",
      "Agent 0, Average Reward: -1448.04\n",
      "256/256 - 0s - loss: 102205.7891\n",
      "256/256 - 0s - loss: 83656.1953\n",
      "256/256 - 0s - loss: 88393.5156\n",
      "256/256 - 0s - loss: 117360.0391\n",
      "256/256 - 0s - loss: 77874.8906\n",
      "Reducing exploration for all agents to 0.0028\n",
      "\n",
      "Episode 426: Starting computation.\n",
      "Episode 426: Finished running.\n",
      "Agent 0, Average Reward: -2040.04\n",
      "256/256 - 0s - loss: 131598.9844\n",
      "256/256 - 0s - loss: 99474.2656\n",
      "256/256 - 0s - loss: 135459.0781\n",
      "256/256 - 0s - loss: 184707.6094\n",
      "256/256 - 0s - loss: 140221.7500\n",
      "Reducing exploration for all agents to 0.0027\n",
      "\n",
      "Episode 427: Starting computation.\n",
      "Episode 427: Finished running.\n",
      "Agent 0, Average Reward: -1523.3\n",
      "256/256 - 0s - loss: 82768.2656\n",
      "256/256 - 0s - loss: 74493.1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 133771.7188\n",
      "256/256 - 0s - loss: 121582.5469\n",
      "256/256 - 0s - loss: 107124.6172\n",
      "Reducing exploration for all agents to 0.0027\n",
      "\n",
      "Episode 428: Starting computation.\n",
      "Episode 428: Finished running.\n",
      "Agent 0, Average Reward: -2012.41\n",
      "256/256 - 0s - loss: 92202.5859\n",
      "256/256 - 0s - loss: 85908.8047\n",
      "256/256 - 0s - loss: 110265.1953\n",
      "256/256 - 0s - loss: 124956.7734\n",
      "256/256 - 0s - loss: 92284.8281\n",
      "Reducing exploration for all agents to 0.0027\n",
      "\n",
      "Episode 429: Starting computation.\n",
      "Episode 429: Finished running.\n",
      "Agent 0, Average Reward: -2025.19\n",
      "256/256 - 0s - loss: 86582.6172\n",
      "256/256 - 0s - loss: 55157.8125\n",
      "256/256 - 0s - loss: 62007.4219\n",
      "256/256 - 0s - loss: 28294.1445\n",
      "256/256 - 0s - loss: 71824.1953\n",
      "Reducing exploration for all agents to 0.0026\n",
      "\n",
      "Episode 430: Starting computation.\n",
      "Episode 430: Finished running.\n",
      "Agent 0, Average Reward: -2019.3\n",
      "256/256 - 0s - loss: 20628.7109\n",
      "256/256 - 0s - loss: 16786.3770\n",
      "256/256 - 0s - loss: 34384.7617\n",
      "256/256 - 0s - loss: 38816.3906\n",
      "256/256 - 0s - loss: 20016.0645\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0026\n",
      "\n",
      "Episode 431: Starting computation.\n",
      "Episode 431: Finished running.\n",
      "Agent 0, Average Reward: -2036.45\n",
      "256/256 - 0s - loss: 115648.1016\n",
      "256/256 - 0s - loss: 93875.7969\n",
      "256/256 - 0s - loss: 63123.3008\n",
      "256/256 - 0s - loss: 32650.5215\n",
      "256/256 - 0s - loss: 12076.2100\n",
      "Reducing exploration for all agents to 0.0026\n",
      "\n",
      "Episode 432: Starting computation.\n",
      "Episode 432: Finished running.\n",
      "Agent 0, Average Reward: -2027.16\n",
      "256/256 - 0s - loss: 4055.5776\n",
      "256/256 - 0s - loss: 3815.3796\n",
      "256/256 - 0s - loss: 11943.9199\n",
      "256/256 - 0s - loss: 23539.5820\n",
      "256/256 - 0s - loss: 33262.4531\n",
      "Reducing exploration for all agents to 0.0025\n",
      "\n",
      "Episode 433: Starting computation.\n",
      "Episode 433: Finished running.\n",
      "Agent 0, Average Reward: -2000.77\n",
      "256/256 - 0s - loss: 37071.9805\n",
      "256/256 - 0s - loss: 38122.3789\n",
      "256/256 - 0s - loss: 31094.2148\n",
      "256/256 - 0s - loss: 22505.1406\n",
      "256/256 - 0s - loss: 13776.0469\n",
      "Reducing exploration for all agents to 0.0025\n",
      "\n",
      "Episode 434: Starting computation.\n",
      "Episode 434: Finished running.\n",
      "Agent 0, Average Reward: -2030.27\n",
      "256/256 - 0s - loss: 5747.1650\n",
      "256/256 - 0s - loss: 3232.2620\n",
      "256/256 - 0s - loss: 1196.4484\n",
      "256/256 - 0s - loss: 3121.7185\n",
      "256/256 - 0s - loss: 7228.3740\n",
      "Reducing exploration for all agents to 0.0025\n",
      "\n",
      "Episode 435: Starting computation.\n",
      "Episode 435: Finished running.\n",
      "Agent 0, Average Reward: -2001.23\n",
      "256/256 - 0s - loss: 13864.8408\n",
      "256/256 - 0s - loss: 13274.4551\n",
      "256/256 - 0s - loss: 13881.8408\n",
      "256/256 - 0s - loss: 12333.7959\n",
      "256/256 - 0s - loss: 7574.2061\n",
      "Reducing exploration for all agents to 0.0024\n",
      "\n",
      "Episode 436: Starting computation.\n",
      "Episode 436: Finished running.\n",
      "Agent 0, Average Reward: -2011.15\n",
      "256/256 - 0s - loss: 8011.2500\n",
      "256/256 - 0s - loss: 4257.7798\n",
      "256/256 - 0s - loss: 2225.0588\n",
      "256/256 - 0s - loss: 1982.2324\n",
      "256/256 - 0s - loss: 5955.4995\n",
      "Reducing exploration for all agents to 0.0024\n",
      "\n",
      "Episode 437: Starting computation.\n",
      "Episode 437: Finished running.\n",
      "Agent 0, Average Reward: -2027.99\n",
      "256/256 - 0s - loss: 3004.4841\n",
      "256/256 - 0s - loss: 4691.4844\n",
      "256/256 - 0s - loss: 4360.1338\n",
      "256/256 - 0s - loss: 4727.7378\n",
      "256/256 - 0s - loss: 4361.7612\n",
      "Reducing exploration for all agents to 0.0024\n",
      "\n",
      "Episode 438: Starting computation.\n",
      "Episode 438: Finished running.\n",
      "Agent 0, Average Reward: -2016.4\n",
      "256/256 - 0s - loss: 4029.6355\n",
      "256/256 - 0s - loss: 3659.5994\n",
      "256/256 - 0s - loss: 6215.3174\n",
      "256/256 - 0s - loss: 3277.9324\n",
      "256/256 - 0s - loss: 3377.1169\n",
      "Reducing exploration for all agents to 0.0023\n",
      "\n",
      "Episode 439: Starting computation.\n",
      "Episode 439: Finished running.\n",
      "Agent 0, Average Reward: -2043.17\n",
      "256/256 - 0s - loss: 5455.6533\n",
      "256/256 - 0s - loss: 2008.6558\n",
      "256/256 - 0s - loss: 2259.3521\n",
      "256/256 - 0s - loss: 2275.7175\n",
      "256/256 - 0s - loss: 3535.7000\n",
      "Reducing exploration for all agents to 0.0023\n",
      "\n",
      "Episode 440: Starting computation.\n",
      "Episode 440: Finished running.\n",
      "Agent 0, Average Reward: -2021.28\n",
      "256/256 - 0s - loss: 3278.9182\n",
      "256/256 - 0s - loss: 2490.3049\n",
      "256/256 - 0s - loss: 1867.0311\n",
      "256/256 - 0s - loss: 2260.0723\n",
      "256/256 - 0s - loss: 1224.0886\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0023\n",
      "\n",
      "Episode 441: Starting computation.\n",
      "Episode 441: Finished running.\n",
      "Agent 0, Average Reward: -2016.31\n",
      "256/256 - 0s - loss: 102570.0859\n",
      "256/256 - 0s - loss: 82687.7188\n",
      "256/256 - 0s - loss: 60346.3047\n",
      "256/256 - 0s - loss: 33996.1953\n",
      "256/256 - 0s - loss: 17067.3086\n",
      "Reducing exploration for all agents to 0.0022\n",
      "\n",
      "Episode 442: Starting computation.\n",
      "Episode 442: Finished running.\n",
      "Agent 0, Average Reward: -1996.5\n",
      "256/256 - 0s - loss: 5439.7295\n",
      "256/256 - 0s - loss: 5052.2134\n",
      "256/256 - 0s - loss: 5905.6997\n",
      "256/256 - 0s - loss: 1738.9713\n",
      "256/256 - 0s - loss: 5108.0479\n",
      "Reducing exploration for all agents to 0.0022\n",
      "\n",
      "Episode 443: Starting computation.\n",
      "Episode 443: Finished running.\n",
      "Agent 0, Average Reward: -3225.11\n",
      "256/256 - 0s - loss: 26760.9434\n",
      "256/256 - 0s - loss: 27938.0488\n",
      "256/256 - 0s - loss: 31950.4551\n",
      "256/256 - 0s - loss: 34118.8047\n",
      "256/256 - 0s - loss: 34207.2617\n",
      "Reducing exploration for all agents to 0.0022\n",
      "\n",
      "Episode 444: Starting computation.\n",
      "Episode 444: Finished running.\n",
      "Agent 0, Average Reward: -2948.99\n",
      "256/256 - 0s - loss: 113107.0859\n",
      "256/256 - 0s - loss: 71040.9297\n",
      "256/256 - 0s - loss: 35973.4609\n",
      "256/256 - 0s - loss: 36480.6953\n",
      "256/256 - 0s - loss: 51800.4766\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 445: Starting computation.\n",
      "Episode 445: Finished running.\n",
      "Agent 0, Average Reward: -1972.42\n",
      "256/256 - 0s - loss: 61929.9062\n",
      "256/256 - 0s - loss: 38721.9141\n",
      "256/256 - 0s - loss: 31996.4590\n",
      "256/256 - 0s - loss: 19802.2637\n",
      "256/256 - 0s - loss: 19697.9102\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 446: Starting computation.\n",
      "Episode 446: Finished running.\n",
      "Agent 0, Average Reward: -2032.76\n",
      "256/256 - 0s - loss: 13444.4189\n",
      "256/256 - 0s - loss: 16317.6602\n",
      "256/256 - 0s - loss: 90854.0469\n",
      "256/256 - 0s - loss: 25023.9766\n",
      "256/256 - 0s - loss: 75110.1328\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 447: Starting computation.\n",
      "Episode 447: Finished running.\n",
      "Agent 0, Average Reward: -2021.93\n",
      "256/256 - 0s - loss: 13788.3311\n",
      "256/256 - 0s - loss: 14646.2715\n",
      "256/256 - 0s - loss: 19483.1816\n",
      "256/256 - 0s - loss: 13118.7949\n",
      "256/256 - 0s - loss: 16799.5684\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 448: Starting computation.\n",
      "Episode 448: Finished running.\n",
      "Agent 0, Average Reward: -2032.45\n",
      "256/256 - 0s - loss: 8138.9868\n",
      "256/256 - 0s - loss: 6534.9448\n",
      "256/256 - 0s - loss: 6985.8496\n",
      "256/256 - 0s - loss: 3309.5312\n",
      "256/256 - 0s - loss: 4502.2573\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 449: Starting computation.\n",
      "Episode 449: Finished running.\n",
      "Agent 0, Average Reward: -2012.91\n",
      "256/256 - 0s - loss: 5705.7725\n",
      "256/256 - 0s - loss: 2189.2993\n",
      "256/256 - 0s - loss: 4765.4897\n",
      "256/256 - 0s - loss: 4155.6973\n",
      "256/256 - 0s - loss: 4496.6509\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 450: Starting computation.\n",
      "Episode 450: Finished running.\n",
      "Agent 0, Average Reward: -2008.96\n",
      "256/256 - 0s - loss: 6110.6729\n",
      "256/256 - 0s - loss: 3594.3210\n",
      "256/256 - 0s - loss: 4017.4109\n",
      "256/256 - 0s - loss: 2799.8677\n",
      "256/256 - 0s - loss: 1618.1715\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 451: Starting computation.\n",
      "Episode 451: Finished running.\n",
      "Agent 0, Average Reward: -1984.51\n",
      "256/256 - 0s - loss: 92884.0391\n",
      "256/256 - 0s - loss: 89646.1797\n",
      "256/256 - 0s - loss: 67660.5938\n",
      "256/256 - 0s - loss: 49000.2773\n",
      "256/256 - 0s - loss: 30390.2090\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 452: Starting computation.\n",
      "Episode 452: Finished running.\n",
      "Agent 0, Average Reward: -2041.81\n",
      "256/256 - 0s - loss: 16934.1992\n",
      "256/256 - 0s - loss: 18489.7656\n",
      "256/256 - 0s - loss: 10295.9062\n",
      "256/256 - 0s - loss: 3576.1765\n",
      "256/256 - 0s - loss: 7716.7441\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 453: Starting computation.\n",
      "Episode 453: Finished running.\n",
      "Agent 0, Average Reward: -3248.47\n",
      "256/256 - 0s - loss: 29956.8477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 36035.4141\n",
      "256/256 - 0s - loss: 37743.4883\n",
      "256/256 - 0s - loss: 40738.8164\n",
      "256/256 - 0s - loss: 41511.9609\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 454: Starting computation.\n",
      "Episode 454: Finished running.\n",
      "Agent 0, Average Reward: -3225.01\n",
      "256/256 - 0s - loss: 34459.5117\n",
      "256/256 - 0s - loss: 27260.6660\n",
      "256/256 - 0s - loss: 34258.5234\n",
      "256/256 - 0s - loss: 30115.8359\n",
      "256/256 - 0s - loss: 26566.1953\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 455: Starting computation.\n",
      "Episode 455: Finished running.\n",
      "Agent 0, Average Reward: -2129.13\n",
      "256/256 - 0s - loss: 45479.5273\n",
      "256/256 - 0s - loss: 49711.6328\n",
      "256/256 - 0s - loss: 39235.4922\n",
      "256/256 - 0s - loss: 38234.3672\n",
      "256/256 - 0s - loss: 25799.4824\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 456: Starting computation.\n",
      "Episode 456: Finished running.\n",
      "Agent 0, Average Reward: -1924.63\n",
      "256/256 - 0s - loss: 49860.8789\n",
      "256/256 - 0s - loss: 43197.0156\n",
      "256/256 - 0s - loss: 30711.8828\n",
      "256/256 - 0s - loss: 30693.4141\n",
      "256/256 - 0s - loss: 32202.2773\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 457: Starting computation.\n",
      "Episode 457: Finished running.\n",
      "Agent 0, Average Reward: -2028.68\n",
      "256/256 - 0s - loss: 28018.2656\n",
      "256/256 - 0s - loss: 28336.7910\n",
      "256/256 - 0s - loss: 30553.6836\n",
      "256/256 - 0s - loss: 33807.9844\n",
      "256/256 - 0s - loss: 27271.8262\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 458: Starting computation.\n",
      "Episode 458: Finished running.\n",
      "Agent 0, Average Reward: -2013.86\n",
      "256/256 - 0s - loss: 17283.9492\n",
      "256/256 - 0s - loss: 17245.5508\n",
      "256/256 - 0s - loss: 43145.8984\n",
      "256/256 - 0s - loss: 17018.1758\n",
      "256/256 - 0s - loss: 20697.0664\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 459: Starting computation.\n",
      "Episode 459: Finished running.\n",
      "Agent 0, Average Reward: -2038.55\n",
      "256/256 - 0s - loss: 2433.6050\n",
      "256/256 - 0s - loss: 1460.3834\n",
      "256/256 - 0s - loss: 1767.6488\n",
      "256/256 - 0s - loss: 4466.9810\n",
      "256/256 - 0s - loss: 2088.8525\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 460: Starting computation.\n",
      "Episode 460: Finished running.\n",
      "Agent 0, Average Reward: -2016.38\n",
      "256/256 - 0s - loss: 3186.6338\n",
      "256/256 - 0s - loss: 2811.8948\n",
      "256/256 - 0s - loss: 5845.6450\n",
      "256/256 - 0s - loss: 3939.1931\n",
      "256/256 - 0s - loss: 2282.6802\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 461: Starting computation.\n",
      "Episode 461: Finished running.\n",
      "Agent 0, Average Reward: -2011.68\n",
      "256/256 - 0s - loss: 83496.0234\n",
      "256/256 - 0s - loss: 74895.5078\n",
      "256/256 - 0s - loss: 58083.8906\n",
      "256/256 - 0s - loss: 42361.7461\n",
      "256/256 - 0s - loss: 21306.8262\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 462: Starting computation.\n",
      "Episode 462: Finished running.\n",
      "Agent 0, Average Reward: -1968.01\n",
      "256/256 - 0s - loss: 8826.7090\n",
      "256/256 - 0s - loss: 2181.7644\n",
      "256/256 - 0s - loss: 5350.7378\n",
      "256/256 - 0s - loss: 7889.3267\n",
      "256/256 - 0s - loss: 16696.0996\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 463: Starting computation.\n",
      "Episode 463: Finished running.\n",
      "Agent 0, Average Reward: -293.54\n",
      "256/256 - 0s - loss: 9611.0215\n",
      "256/256 - 0s - loss: 9371.8789\n",
      "256/256 - 0s - loss: 11745.9922\n",
      "256/256 - 0s - loss: 13586.8418\n",
      "256/256 - 0s - loss: 12811.2061\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 464: Starting computation.\n",
      "Episode 464: Finished running.\n",
      "Agent 0, Average Reward: -247.26\n",
      "256/256 - 0s - loss: 17960.3340\n",
      "256/256 - 0s - loss: 19392.3965\n",
      "256/256 - 0s - loss: 16921.2832\n",
      "256/256 - 0s - loss: 18409.8438\n",
      "256/256 - 0s - loss: 16411.7480\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 465: Starting computation.\n",
      "Episode 465: Finished running.\n",
      "Agent 0, Average Reward: -252.91\n",
      "256/256 - 0s - loss: 21360.6914\n",
      "256/256 - 0s - loss: 22895.2109\n",
      "256/256 - 0s - loss: 27456.6621\n",
      "256/256 - 0s - loss: 21326.1328\n",
      "256/256 - 0s - loss: 21832.3770\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 466: Starting computation.\n",
      "Episode 466: Finished running.\n",
      "Agent 0, Average Reward: -277.72\n",
      "256/256 - 0s - loss: 27257.8945\n",
      "256/256 - 0s - loss: 18853.1934\n",
      "256/256 - 0s - loss: 30010.7949\n",
      "256/256 - 0s - loss: 24608.2051\n",
      "256/256 - 0s - loss: 33782.5820\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 467: Starting computation.\n",
      "Episode 467: Finished running.\n",
      "Agent 0, Average Reward: -281.79\n",
      "256/256 - 0s - loss: 28798.5566\n",
      "256/256 - 0s - loss: 32797.3359\n",
      "256/256 - 0s - loss: 37726.1328\n",
      "256/256 - 0s - loss: 28848.0996\n",
      "256/256 - 0s - loss: 28328.1582\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 468: Starting computation.\n",
      "Episode 468: Finished running.\n",
      "Agent 0, Average Reward: -281.84\n",
      "256/256 - 0s - loss: 29191.2109\n",
      "256/256 - 0s - loss: 27668.2930\n",
      "256/256 - 0s - loss: 24371.2012\n",
      "256/256 - 0s - loss: 32217.9844\n",
      "256/256 - 0s - loss: 30581.2676\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 469: Starting computation.\n",
      "Episode 469: Finished running.\n",
      "Agent 0, Average Reward: -273.03\n",
      "256/256 - 0s - loss: 24323.0332\n",
      "256/256 - 0s - loss: 31386.6953\n",
      "256/256 - 0s - loss: 35497.5000\n",
      "256/256 - 0s - loss: 29120.6875\n",
      "256/256 - 0s - loss: 32540.7500\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 470: Starting computation.\n",
      "Episode 470: Finished running.\n",
      "Agent 0, Average Reward: -249.53\n",
      "256/256 - 0s - loss: 31168.5879\n",
      "256/256 - 0s - loss: 32605.9570\n",
      "256/256 - 0s - loss: 32712.4199\n",
      "256/256 - 0s - loss: 34808.7148\n",
      "256/256 - 0s - loss: 25136.3613\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 471: Starting computation.\n",
      "Episode 471: Finished running.\n",
      "Agent 0, Average Reward: -231.79\n",
      "256/256 - 0s - loss: 29205.3457\n",
      "256/256 - 0s - loss: 26459.0176\n",
      "256/256 - 0s - loss: 24977.7324\n",
      "256/256 - 0s - loss: 31678.9512\n",
      "256/256 - 0s - loss: 25281.5293\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 472: Starting computation.\n",
      "Episode 472: Finished running.\n",
      "Agent 0, Average Reward: -221.0\n",
      "256/256 - 0s - loss: 32250.6602\n",
      "256/256 - 0s - loss: 29495.5215\n",
      "256/256 - 0s - loss: 29443.0098\n",
      "256/256 - 0s - loss: 25390.7617\n",
      "256/256 - 0s - loss: 28553.9375\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 473: Starting computation.\n",
      "Episode 473: Finished running.\n",
      "Agent 0, Average Reward: -217.39\n",
      "256/256 - 0s - loss: 25023.7598\n",
      "256/256 - 0s - loss: 24397.6641\n",
      "256/256 - 0s - loss: 28252.1465\n",
      "256/256 - 0s - loss: 27807.6895\n",
      "256/256 - 0s - loss: 26946.8770\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 474: Starting computation.\n",
      "Episode 474: Finished running.\n",
      "Agent 0, Average Reward: -198.94\n",
      "256/256 - 0s - loss: 24661.6484\n",
      "256/256 - 0s - loss: 21817.8867\n",
      "256/256 - 0s - loss: 22893.5996\n",
      "256/256 - 0s - loss: 25744.5820\n",
      "256/256 - 0s - loss: 22684.1270\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 475: Starting computation.\n",
      "Episode 475: Finished running.\n",
      "Agent 0, Average Reward: -195.39\n",
      "256/256 - 0s - loss: 26845.2852\n",
      "256/256 - 0s - loss: 26137.9082\n",
      "256/256 - 0s - loss: 19916.8574\n",
      "256/256 - 0s - loss: 22986.0293\n",
      "256/256 - 0s - loss: 25419.3184\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 476: Starting computation.\n",
      "Episode 476: Finished running.\n",
      "Agent 0, Average Reward: -178.23\n",
      "256/256 - 0s - loss: 22346.1328\n",
      "256/256 - 0s - loss: 23209.6953\n",
      "256/256 - 0s - loss: 22103.3223\n",
      "256/256 - 0s - loss: 20114.1465\n",
      "256/256 - 0s - loss: 20384.5605\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 477: Starting computation.\n",
      "Episode 477: Finished running.\n",
      "Agent 0, Average Reward: -174.15\n",
      "256/256 - 0s - loss: 13776.9580\n",
      "256/256 - 0s - loss: 15557.5107\n",
      "256/256 - 0s - loss: 17841.3066\n",
      "256/256 - 0s - loss: 17554.3750\n",
      "256/256 - 0s - loss: 19646.1934\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 478: Starting computation.\n",
      "Episode 478: Finished running.\n",
      "Agent 0, Average Reward: -184.78\n",
      "256/256 - 0s - loss: 17197.4043\n",
      "256/256 - 0s - loss: 15484.1250\n",
      "256/256 - 0s - loss: 15843.6299\n",
      "256/256 - 0s - loss: 15736.4365\n",
      "256/256 - 0s - loss: 16388.1973\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 479: Starting computation.\n",
      "Episode 479: Finished running.\n",
      "Agent 0, Average Reward: -179.97\n",
      "256/256 - 0s - loss: 21931.1895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 21432.6250\n",
      "256/256 - 0s - loss: 17382.8516\n",
      "256/256 - 0s - loss: 16576.4941\n",
      "256/256 - 0s - loss: 22669.1543\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 480: Starting computation.\n",
      "Episode 480: Finished running.\n",
      "Agent 0, Average Reward: -176.76\n",
      "256/256 - 0s - loss: 20465.4922\n",
      "256/256 - 0s - loss: 22740.2363\n",
      "256/256 - 0s - loss: 20256.8906\n",
      "256/256 - 0s - loss: 18599.0156\n",
      "256/256 - 0s - loss: 18908.4180\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 481: Starting computation.\n",
      "Episode 481: Finished running.\n",
      "Agent 0, Average Reward: -180.56\n",
      "256/256 - 0s - loss: 18885.7305\n",
      "256/256 - 0s - loss: 19387.7051\n",
      "256/256 - 0s - loss: 23402.1348\n",
      "256/256 - 0s - loss: 18050.5273\n",
      "256/256 - 0s - loss: 18923.2832\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 482: Starting computation.\n",
      "Episode 482: Finished running.\n",
      "Agent 0, Average Reward: -190.33\n",
      "256/256 - 0s - loss: 22261.3262\n",
      "256/256 - 0s - loss: 22397.0332\n",
      "256/256 - 0s - loss: 21120.2441\n",
      "256/256 - 0s - loss: 20618.6191\n",
      "256/256 - 0s - loss: 20681.1504\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 483: Starting computation.\n",
      "Episode 483: Finished running.\n",
      "Agent 0, Average Reward: -233.32\n",
      "256/256 - 0s - loss: 25589.5879\n",
      "256/256 - 0s - loss: 25418.0410\n",
      "256/256 - 0s - loss: 20533.5566\n",
      "256/256 - 0s - loss: 23404.8730\n",
      "256/256 - 0s - loss: 23685.0703\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 484: Starting computation.\n",
      "Episode 484: Finished running.\n",
      "Agent 0, Average Reward: -269.3\n",
      "256/256 - 0s - loss: 24341.8223\n",
      "256/256 - 0s - loss: 22022.9531\n",
      "256/256 - 0s - loss: 18317.1777\n",
      "256/256 - 0s - loss: 27695.1465\n",
      "256/256 - 0s - loss: 30894.7754\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 485: Starting computation.\n",
      "Episode 485: Finished running.\n",
      "Agent 0, Average Reward: -295.85\n",
      "256/256 - 0s - loss: 25922.9727\n",
      "256/256 - 0s - loss: 24356.7852\n",
      "256/256 - 0s - loss: 28449.1426\n",
      "256/256 - 0s - loss: 29957.2988\n",
      "256/256 - 0s - loss: 26547.2051\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 486: Starting computation.\n",
      "Episode 486: Finished running.\n",
      "Agent 0, Average Reward: -321.5\n",
      "256/256 - 0s - loss: 40426.1406\n",
      "256/256 - 0s - loss: 35835.3906\n",
      "256/256 - 0s - loss: 26199.4316\n",
      "256/256 - 0s - loss: 32510.6738\n",
      "256/256 - 0s - loss: 36154.0508\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 487: Starting computation.\n",
      "Episode 487: Finished running.\n",
      "Agent 0, Average Reward: -304.89\n",
      "256/256 - 0s - loss: 41961.4766\n",
      "256/256 - 0s - loss: 34912.4492\n",
      "256/256 - 0s - loss: 32395.8164\n",
      "256/256 - 0s - loss: 34591.8828\n",
      "256/256 - 0s - loss: 35555.5859\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 488: Starting computation.\n",
      "Episode 488: Finished running.\n",
      "Agent 0, Average Reward: -284.85\n",
      "256/256 - 0s - loss: 40144.6445\n",
      "256/256 - 0s - loss: 32732.9199\n",
      "256/256 - 0s - loss: 43238.5820\n",
      "256/256 - 0s - loss: 36525.3047\n",
      "256/256 - 0s - loss: 33192.4688\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 489: Starting computation.\n",
      "Episode 489: Finished running.\n",
      "Agent 0, Average Reward: -248.94\n",
      "256/256 - 0s - loss: 38458.6719\n",
      "256/256 - 0s - loss: 30129.8027\n",
      "256/256 - 0s - loss: 33587.5938\n",
      "256/256 - 0s - loss: 38688.5664\n",
      "256/256 - 0s - loss: 29265.2207\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 490: Starting computation.\n",
      "Episode 490: Finished running.\n",
      "Agent 0, Average Reward: -211.12\n",
      "256/256 - 0s - loss: 24799.8633\n",
      "256/256 - 0s - loss: 32223.1211\n",
      "256/256 - 0s - loss: 30121.0801\n",
      "256/256 - 0s - loss: 36150.0625\n",
      "256/256 - 0s - loss: 34913.9766\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 491: Starting computation.\n",
      "Episode 491: Finished running.\n",
      "Agent 0, Average Reward: -185.85\n",
      "256/256 - 0s - loss: 29669.9004\n",
      "256/256 - 0s - loss: 29880.1973\n",
      "256/256 - 0s - loss: 25619.1035\n",
      "256/256 - 0s - loss: 27939.2812\n",
      "256/256 - 0s - loss: 32224.4512\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 492: Starting computation.\n",
      "Episode 492: Finished running.\n",
      "Agent 0, Average Reward: -195.54\n",
      "256/256 - 0s - loss: 30771.3691\n",
      "256/256 - 0s - loss: 25233.2441\n",
      "256/256 - 0s - loss: 24876.9395\n",
      "256/256 - 0s - loss: 28595.3027\n",
      "256/256 - 0s - loss: 23244.3164\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 493: Starting computation.\n",
      "Episode 493: Finished running.\n",
      "Agent 0, Average Reward: -217.84\n",
      "256/256 - 0s - loss: 26705.9492\n",
      "256/256 - 0s - loss: 30828.5742\n",
      "256/256 - 0s - loss: 30378.4297\n",
      "256/256 - 0s - loss: 33398.1172\n",
      "256/256 - 0s - loss: 30823.3965\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 494: Starting computation.\n",
      "Episode 494: Finished running.\n",
      "Agent 0, Average Reward: -220.86\n",
      "256/256 - 0s - loss: 32413.1465\n",
      "256/256 - 0s - loss: 30558.5957\n",
      "256/256 - 0s - loss: 40074.0938\n",
      "256/256 - 0s - loss: 35183.3516\n",
      "256/256 - 0s - loss: 33702.5547\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 495: Starting computation.\n",
      "Episode 495: Finished running.\n",
      "Agent 0, Average Reward: -189.02\n",
      "256/256 - 0s - loss: 38846.8438\n",
      "256/256 - 0s - loss: 31562.1602\n",
      "256/256 - 0s - loss: 38992.9453\n",
      "256/256 - 0s - loss: 36509.0391\n",
      "256/256 - 0s - loss: 29710.1465\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 496: Starting computation.\n",
      "Episode 496: Finished running.\n",
      "Agent 0, Average Reward: -221.32\n",
      "256/256 - 0s - loss: 34210.6523\n",
      "256/256 - 0s - loss: 35630.0000\n",
      "256/256 - 0s - loss: 41965.7539\n",
      "256/256 - 0s - loss: 34781.0938\n",
      "256/256 - 0s - loss: 34521.5703\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 497: Starting computation.\n",
      "Episode 497: Finished running.\n",
      "Agent 0, Average Reward: -300.58\n",
      "256/256 - 0s - loss: 35557.0898\n",
      "256/256 - 0s - loss: 27788.2344\n",
      "256/256 - 0s - loss: 34249.8359\n",
      "256/256 - 0s - loss: 35737.5195\n",
      "256/256 - 0s - loss: 36138.2188\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 498: Starting computation.\n",
      "Episode 498: Finished running.\n",
      "Agent 0, Average Reward: -1825.59\n",
      "256/256 - 0s - loss: 40150.1719\n",
      "256/256 - 0s - loss: 44234.2773\n",
      "256/256 - 0s - loss: 49164.3047\n",
      "256/256 - 0s - loss: 44419.9766\n",
      "256/256 - 0s - loss: 37284.0312\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 499: Starting computation.\n",
      "Episode 499: Finished running.\n",
      "Agent 0, Average Reward: -1909.31\n",
      "256/256 - 0s - loss: 47642.6523\n",
      "256/256 - 0s - loss: 41475.5742\n",
      "256/256 - 0s - loss: 30173.9941\n",
      "256/256 - 0s - loss: 31043.1777\n",
      "256/256 - 0s - loss: 20634.6309\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 500: Starting computation.\n",
      "Episode 500: Finished running.\n",
      "Agent 0, Average Reward: -318.44\n",
      "256/256 - 0s - loss: 27288.4766\n",
      "256/256 - 0s - loss: 47573.9062\n",
      "256/256 - 0s - loss: 25016.1152\n",
      "256/256 - 0s - loss: 25562.0879\n",
      "256/256 - 0s - loss: 31926.8457\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 501: Starting computation.\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is500\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.save(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DQN'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.Session_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\Balance_int3\\\\Agents_Results\\\\DQN\\\\Balance_int3_default_500_9000_DDQN\\\\Agent0_Loss.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-96aa79faa9d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", \"DQN\", Session_ID, \\\n\u001b[0;32m     19\u001b[0m                         \"Agent{}_Loss.png\".format(idx)) \n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", \"DQN\", Session_ID, \\\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vissim\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vissim\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vissim\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[0;32m   2080\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2081\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2082\u001b[1;33m                     **kwargs)\n\u001b[0m\u001b[0;32m   2083\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2084\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vissim\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m                     \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[0;32m    532\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vissim\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vissim\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[1;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m     \u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vissim\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[1;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         \u001b[0mopened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seek'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\Balance_int3\\\\Agents_Results\\\\DQN\\\\Balance_int3_default_500_9000_DDQN\\\\Agent0_Loss.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 324x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the agent training\n",
    "ploty = 1\n",
    "for idx , agent in Balance_int_MultiDQN_Agents.Agents.items():\n",
    "    print(\"Agent \"+str(idx))\n",
    "    #print(ploty)\n",
    "    #plt.subplot(14, 2, ploty)\n",
    "\n",
    "    plt.figure('6'+str(idx),figsize=(4.5, 3))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", \"DQN\", Session_ID, \\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", \"DQN\", Session_ID, \\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    ploty+=1\n",
    "    #print(ploty)\n",
    "\n",
    "    \n",
    "    #plt.subplot(14, 2, ploty)\n",
    "    plt.figure('7'+str(idx),figsize=(4.5, 3))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", \"DQN\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    print(agent.loss)\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    #Loss_rewarddf.to_csv(csv_Path,index=False)\n",
    "    ploty+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 2, Architecture, Optimizer and Memory.\n",
      "WARNING:tensorflow:From C:\\Users\\ACE\\Anaconda3\\envs\\vissim\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\ACE\\Anaconda3\\envs\\vissim\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.load(500, best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: E:\\OneDrive - University of Warwick\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int3.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 9001 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 600\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 1.18 seconds.\n",
      "\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147467259), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2a992ec70e68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBalance_int_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\OneDrive - University of Warwick\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\MasterDQN_Agent.py\u001b[0m in \u001b[0;36mdemo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                         \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\OneDrive - University of Warwick\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions, green_time)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;31m# increase the update counter by one each step (until reach simulation length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\vissim\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147467259), None)"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Balance_int_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    #Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                    \"Junction{}_Queues.png\".format(idx))               \n",
    "    #plt.savefig(Path)\n",
    "    #\n",
    "    #csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                    \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    #Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    #Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                        \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    #csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                    \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    #plt.savefig(Path)\n",
    "    #\n",
    "    #delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    #Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                        \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    #csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                    \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    #plt.savefig(Path)\n",
    "    #stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "#Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "#                        \"Total_Cumulative_Delay.png\")\n",
    "#csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "#                        \"Total_Cumulative_Delay.csv\")\n",
    "#plt.savefig(Path)\n",
    "#Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "#Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "#                        \"Total_Cumulative_stop_Delay.png\")\n",
    "#csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "#                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "#plt.savefig(Path)\n",
    "#Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "vissim_working_directory = \"E:\\\\Backup - Onedrive\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "sim_length = 3601\n",
    "agent_type = \"DDQN\"\n",
    "\n",
    "# Ojo aqui, hack\n",
    "Session_ID = \"DQN\"\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 1000\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 5000\n",
    "batch_size = 512*2\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.save(498)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for idx , agent in Balance_MultiDQN_Agents.Agents.items():\n",
    "    print(agent)\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For the agent training\n",
    "ploty = 1\n",
    "for idx , agent in Balance_MultiDQN_Agents.Agents.items():\n",
    "    print(\"Agent \"+str(idx))\n",
    "    #print(ploty)\n",
    "    #plt.subplot(14, 2, ploty)\n",
    "\n",
    "    plt.figure('6'+str(idx),figsize=(4.5, 3))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    ploty+=1\n",
    "    #print(ploty)\n",
    "\n",
    "    \n",
    "    #plt.subplot(14, 2, ploty)\n",
    "    plt.figure('7'+str(idx),figsize=(4.5, 3))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    #Loss_rewarddf.to_csv(csv_Path,index=False)\n",
    "    ploty+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Balance_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.load(498, best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight AC\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Straigth_AC\"\n",
    "\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.00001\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 0.5\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.train(200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.save(401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.load(200, best = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay = Single_Cross_Straight_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in  Single_Cross_Straight_MultiAC_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.Agents[0].Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDQN\"\n",
    "Session_ID = \"Single_Cross_Straigth_DuelingDQN20c0\"\n",
    "\n",
    "# all controller actions\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4]  }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 300\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.load(300 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Single_Cross_Straight_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in  Single_Cross_Straight_MultiDQN_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3600\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_TripleAC4test1\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "    },\n",
    "   'demand' : {\"default\" : [400,400,400,400] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 5000\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 action DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "Session_ID = \"Single_Cross_Triple4_actions\"\n",
    "#Session_ID = \"DQN\"\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{ 'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' :    {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    "         \n",
    "         'all_actions' :       {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "        },\n",
    "     'demand' : { 'default' : [400, 400, 400, 400]}\n",
    "                  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 5\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.train(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.load(best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To be arranged for multy agents\n",
    "\n",
    "queues = np.array(Episode_Queues[0])\n",
    "queues = queues.T\n",
    "\n",
    "delay = Cumulative_Episode_Delays[0]\n",
    "\n",
    "# Plot the queues\n",
    "plt.figure(1)\n",
    "for queue in queues:\n",
    "    plt.plot(queue)\n",
    "\n",
    "# plot the junctions delays\n",
    "plt.figure(2)\n",
    "plt.plot(delay)\n",
    "\n",
    "#plot the total delays \n",
    "plt.figure(3)\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "\n",
    "# Dont freak out the 2 delays are not the same because the node is not covering all the junction\n",
    "\n",
    "\"\"\"\n",
    "Because the cars never leave the nodes the delay is not computed correctly (when the agent doesn't work) \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].loss)\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "print(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_AC10\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]             \n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [300,300,300,300],\n",
    "             1 : [600,600,600,600],\n",
    "             2 : [1350,750,1350,750],\n",
    "             3 : [1500,750,1500,750],\n",
    "             4 : [1050,750,1050,750],\n",
    "             5 : [750,1050,750,1050],\n",
    "             6 : [750,1500,750,1500],\n",
    "             7 : [750,1350,750,1350],\n",
    "             8 : [600,600,600,600],\n",
    "             9 : [300,300,300,300]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.000001\n",
    "\n",
    "\n",
    "value = 5\n",
    "entropy = 500\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple8_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_DuelingDDQN20c10\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]             \n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [300,300,300,300],\n",
    "             1 : [600,600,600,600],\n",
    "             2 : [1350,750,1350,750],\n",
    "             3 : [1500,750,1500,750],\n",
    "             4 : [1050,750,1050,750],\n",
    "             5 : [750,1050,750,1050],\n",
    "             6 : [750,1500,750,1500],\n",
    "             7 : [750,1350,750,1350],\n",
    "             8 : [600,600,600,600],\n",
    "             9 : [300,300,300,300]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400 \n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.train(episodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Single_Cross_Triple8_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in Single_Cross_Triple8_MultiDQN_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Five intersection DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Five_intersection'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Five5transfert\"\n",
    "\n",
    "# all controller actions\n",
    "Five_intersection_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['11-1', '11-2', '11-3', '12-1', '12-2', '12-3', '13-1', '13-2', '13-3', '14-1', '14-2', '14-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "         },\n",
    "                  1 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['21-1', '21-2', '21-3', '22-1', '22-2', '22-3', '23-1', '23-2', '23-3', '24-1', '24-2', '24-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "        'queues_counter_ID' : [13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "         },\n",
    "                  2 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['31-1', '31-2', '31-3', '32-1', '32-2', '32-3', '33-1', '33-2', '33-3', '34-1', '34-2', '34-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [25,26,27,28,29,30,31,32,33,34,35,36]\n",
    "         },\n",
    "                  3 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['41-1', '41-2', '41-3', '42-1', '42-2', '42-3', '43-1', '43-2', '43-3', '44-1', '44-2', '44-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "          'queues_counter_ID' : [37,38,39,40,41,42,43,44,45,46,47,48]\n",
    "         },\n",
    "                  4 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['51-1', '51-2', '51-3', '52-1', '52-2', '52-3', '53-1', '53-2', '53-3', '54-1', '54-2', '54-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [49,50,51,52,53,54,55,56,57,58,59,60]\n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             \n",
    "             0 : [200,200,200,200,200,200,200,200,200,200,200,200],\n",
    "             1 : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             2 : [500,900,500,500,900,500,500,900,500,500,900,500],\n",
    "             3 : [500,1000,500,500,1000,500,500,1000,500,500,1000,500],\n",
    "             4 : [500,700,500,500,700,500,500,700,500,500,700,500],\n",
    "             5 : [500,700,500,500,700,500,500,700,500,500,700,500],\n",
    "             6 : [500,1000,500,500,1000,500,500,1000,500,500,1000,500],\n",
    "             7 : [500,900,500,500,900,500,500,900,500,500,900,500],\n",
    "             8 : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             9 : [200,200,200,200,200,200,200,200,200,200,200,200]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Five_intersection_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.save(401)\n",
    "Five_intersection_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.Agents[0].load_agent(vissim_working_directory, 'Single_Cross_Triple', 'Single_Cross_Triple8_actions',400 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay = Five_intersection_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        \n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Queue Length')\n",
    "    plt.title('Junction {} Queue length'.format(idx))\n",
    "    plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Accumulated Delay')\n",
    "    plt.title('Junction {} Delay'.format(idx))\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Accumulated Stop Delay')\n",
    "    plt.title('Junction {} Stop Delay'.format(idx))\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Global accumulated Delay')\n",
    "plt.title('Global accumulated Delay')\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Global accumulated stop Delay')\n",
    "plt.title('Global accumulated stop Delay')\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.Agents[2] = Five_intersection_MultiDQN_Agents.Agents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
