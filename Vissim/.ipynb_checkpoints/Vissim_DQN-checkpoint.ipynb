{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Model Parameters\n",
    "Random_Seed = 42\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "program = 'DQN' # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'\n",
    "state_type  = 'Queues'\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "## Load trained model?\n",
    "Demo_Mode = False\n",
    "load_trained = False\n",
    "Quickmode = True\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling flags\n",
    "# Flag for restarting the COM Server\n",
    "reset_flag = True\n",
    "#cache_flag = False\n",
    "# If a fresh start is needed, all previous results from simulations are deleted\n",
    "Start_Fresh = True\n",
    "# Debug action\n",
    "debug_action = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 100\n",
    "partial_save_at = 10\n",
    "copy_weights_frequency = 5\n",
    "reset_frequency = 10\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "simulation_length = 36000*1\n",
    "memory_population_length = simulation_length*5\n",
    "## State-Action Parameters\n",
    "state_size = 4\n",
    "action_size = 5\n",
    "# Memory Size\n",
    "memory_size = 5000\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# Learning Rate\n",
    "alpha   = 0.0001\n",
    "# Discount Factor\n",
    "gamma   = 0.9\n",
    "# Exploration Schedule\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "epsilon_decay = (epsilon_end - epsilon_start)/(episodes-1)\n",
    "#epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes+1)) # Geometric decay\n",
    "# Demand Schedule\n",
    "demands = [100, 200, 400, 600, 800, 1000]\n",
    "# Session ID\n",
    "Session_ID = 'Episodes'+str(episodes)+'_Program'+program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Loading Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 18000.0 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Deploying instance of Standard Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DQN.\n",
      "Populating memory with Random Actions....\n",
      "Episode: 1/100, Epsilon:1, Average reward: -166.19\n",
      "Prediction for [500,0,500,0] is: [[-67.811874  -23.654285    1.8364496 -17.766796   -2.0031786]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Episode: 2/100, Epsilon:0.99, Average reward: -254.65\n",
      "Prediction for [500,0,500,0] is: [[-88.04908  -29.46329   -9.863388 -23.742418 -13.165079]]\n",
      "Episode: 3/100, Epsilon:0.98, Average reward: -257.75\n",
      "Prediction for [500,0,500,0] is: [[-111.74415   -37.742523  -21.557535  -33.918312  -25.788412]]\n",
      "Episode: 4/100, Epsilon:0.97, Average reward: -289.31\n",
      "Prediction for [500,0,500,0] is: [[-141.2066    -48.88062   -33.509964  -52.123547  -36.909794]]\n",
      "Episode: 5/100, Epsilon:0.96, Average reward: -309.75\n",
      "Prediction for [500,0,500,0] is: [[-173.13829   -62.286423  -46.36932   -72.5821    -56.91672 ]]\n",
      "Episode: 6/100, Epsilon:0.95, Average reward: -190.08\n",
      "Prediction for [500,0,500,0] is: [[-214.15096  -77.97315  -66.07529  -90.65118  -75.5892 ]]\n",
      "Episode: 7/100, Epsilon:0.94, Average reward: -152.95\n",
      "Prediction for [500,0,500,0] is: [[-248.06     -93.38307  -84.95958 -114.55416  -90.24366]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Episode: 8/100, Epsilon:0.93, Average reward: -277.93\n",
      "Prediction for [500,0,500,0] is: [[-301.76163 -114.13239 -110.86244 -142.79607 -109.94791]]\n",
      "Episode: 9/100, Epsilon:0.92, Average reward: -245.89\n",
      "Prediction for [500,0,500,0] is: [[-349.79044 -140.6493  -138.98108 -171.0088  -131.21503]]\n",
      "Episode: 10/100, Epsilon:0.91, Average reward: -304.03\n",
      "Prediction for [500,0,500,0] is: [[-416.40375 -163.42714 -179.5984  -213.26799 -164.60707]]\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 10.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 11/100, Epsilon:0.9, Average reward: -193.33\n",
      "Prediction for [500,0,500,0] is: [[-492.14847 -209.85178 -217.15327 -256.1296  -211.81148]]\n",
      "Episode: 12/100, Epsilon:0.89, Average reward: -246.45\n",
      "Prediction for [500,0,500,0] is: [[-577.71545 -268.1149  -257.74875 -298.87436 -263.45978]]\n",
      "Episode: 13/100, Epsilon:0.88, Average reward: -328.44\n",
      "Prediction for [500,0,500,0] is: [[-653.6364  -327.10233 -293.48956 -344.26404 -307.76743]]\n",
      "Episode: 14/100, Epsilon:0.87, Average reward: -234.97\n",
      "Prediction for [500,0,500,0] is: [[-738.84973 -379.93643 -355.94568 -393.34647 -365.15634]]\n",
      "Episode: 15/100, Epsilon:0.86, Average reward: -175.83\n",
      "Prediction for [500,0,500,0] is: [[-809.1648  -434.46875 -411.80453 -440.58862 -412.03275]]\n",
      "Episode: 16/100, Epsilon:0.85, Average reward: -172.22\n",
      "Prediction for [500,0,500,0] is: [[-877.091   -494.62555 -476.78033 -497.68793 -455.66785]]\n",
      "Episode: 17/100, Epsilon:0.84, Average reward: -227.19\n",
      "Prediction for [500,0,500,0] is: [[-945.68274 -559.9847  -533.6471  -555.5984  -498.5918 ]]\n",
      "Episode: 18/100, Epsilon:0.83, Average reward: -347.56\n",
      "Prediction for [500,0,500,0] is: [[-988.28314 -592.94403 -580.93994 -596.28723 -541.1852 ]]\n",
      "Episode: 19/100, Epsilon:0.82, Average reward: -310.62\n",
      "Prediction for [500,0,500,0] is: [[-1018.5328   -633.5664   -615.791    -646.7064   -587.29785]]\n",
      "Episode: 20/100, Epsilon:0.81, Average reward: -328.81\n",
      "Prediction for [500,0,500,0] is: [[-1064.6613  -680.4051  -667.7016  -684.6392  -635.1197]]\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 20.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 21/100, Epsilon:0.8, Average reward: -347.15\n",
      "Prediction for [500,0,500,0] is: [[-1081.5432   -707.10345  -692.6962   -708.7455   -680.0699 ]]\n",
      "Episode: 22/100, Epsilon:0.79, Average reward: -294.95\n",
      "Prediction for [500,0,500,0] is: [[-1109.0917   -734.927    -725.14453  -753.06934  -718.98663]]\n",
      "Episode: 23/100, Epsilon:0.78, Average reward: -387.42\n",
      "Prediction for [500,0,500,0] is: [[-1126.9894   -765.48535  -761.47284  -782.57916  -758.6655 ]]\n",
      "Episode: 24/100, Epsilon:0.77, Average reward: -228.29\n",
      "Prediction for [500,0,500,0] is: [[-1125.277    -785.9393   -774.58966  -808.5589   -784.2919 ]]\n",
      "Episode: 25/100, Epsilon:0.76, Average reward: -181.58\n",
      "Prediction for [500,0,500,0] is: [[-1120.5001   -813.3703   -792.6737   -830.06946  -806.9535 ]]\n",
      "Episode: 26/100, Epsilon:0.75, Average reward: -185.72\n",
      "Prediction for [500,0,500,0] is: [[-1101.7496   -826.1479   -803.5336   -840.1325   -811.92993]]\n",
      "Episode: 27/100, Epsilon:0.74, Average reward: -346.64\n",
      "Prediction for [500,0,500,0] is: [[-1121.381    -860.0613   -837.2445   -857.85876  -846.9607 ]]\n",
      "Episode: 28/100, Epsilon:0.73, Average reward: -184.04\n",
      "Prediction for [500,0,500,0] is: [[-1119.7031   -847.963    -861.82263  -858.49054  -857.45   ]]\n",
      "Episode: 29/100, Epsilon:0.72, Average reward: -217.29\n",
      "Prediction for [500,0,500,0] is: [[-1128.0282   -863.32446  -878.1966   -879.5431   -891.8747 ]]\n",
      "Episode: 30/100, Epsilon:0.71, Average reward: -129.25\n",
      "Prediction for [500,0,500,0] is: [[-1098.8645   -845.54517  -874.0406   -872.56696  -899.5582 ]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 30.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 31/100, Epsilon:0.7, Average reward: -124.87\n",
      "Prediction for [500,0,500,0] is: [[-1084.8348   -862.3456   -885.08044  -881.82733  -910.5076 ]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Episode: 32/100, Epsilon:0.69, Average reward: -178.96\n",
      "Prediction for [500,0,500,0] is: [[-1103.3608   -890.6434   -905.42334  -894.3754   -941.89703]]\n",
      "Episode: 33/100, Epsilon:0.68, Average reward: -297.54\n",
      "Prediction for [500,0,500,0] is: [[-1085.3092   -888.48364  -904.7308   -885.9672   -934.10065]]\n",
      "Episode: 34/100, Epsilon:0.67, Average reward: -275.83\n",
      "Prediction for [500,0,500,0] is: [[-1052.4835   -879.59033  -892.37756  -870.80334  -925.29004]]\n",
      "Episode: 35/100, Epsilon:0.66, Average reward: -311.66\n",
      "Prediction for [500,0,500,0] is: [[-1051.8047   -900.36304  -895.1495   -884.84796  -930.63696]]\n",
      "Episode: 36/100, Epsilon:0.65, Average reward: -308.8\n",
      "Prediction for [500,0,500,0] is: [[-1029.7123   -896.4476   -885.44684  -874.55524  -925.8013 ]]\n",
      "Episode: 37/100, Epsilon:0.64, Average reward: -378.87\n",
      "Prediction for [500,0,500,0] is: [[-1025.1962   -897.9898   -891.07996  -862.73773  -935.95654]]\n",
      "Episode: 38/100, Epsilon:0.63, Average reward: -294.73\n",
      "Prediction for [500,0,500,0] is: [[-1013.10114  -904.0227   -877.25226  -862.9945   -929.8388 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 39/100, Epsilon:0.62, Average reward: -408.44\n",
      "Prediction for [500,0,500,0] is: [[-1027.256    -929.88086  -884.16754  -870.7904   -937.6587 ]]\n",
      "Episode: 40/100, Epsilon:0.61, Average reward: -305.58\n",
      "Prediction for [500,0,500,0] is: [[-1008.87836  -915.2108   -883.01996  -867.75653  -916.98016]]\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 40.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 41/100, Epsilon:0.6, Average reward: -237.82\n",
      "Prediction for [500,0,500,0] is: [[-1011.13885  -929.3902   -894.16766  -865.8963   -923.4145 ]]\n",
      "Episode: 42/100, Epsilon:0.59, Average reward: -275.28\n",
      "Prediction for [500,0,500,0] is: [[-1010.9687   -942.5577   -909.9915   -898.2197   -937.04114]]\n",
      "Episode: 43/100, Epsilon:0.58, Average reward: -397.83\n",
      "Prediction for [500,0,500,0] is: [[-974.94025 -915.28894 -909.4464  -897.6226  -925.5417 ]]\n",
      "Episode: 44/100, Epsilon:0.57, Average reward: -294.33\n",
      "Prediction for [500,0,500,0] is: [[-968.49304 -924.8501  -894.52026 -915.9002  -932.62836]]\n",
      "Episode: 45/100, Epsilon:0.56, Average reward: -142.68\n",
      "Prediction for [500,0,500,0] is: [[-958.04626 -941.53174 -899.56995 -945.53784 -938.41614]]\n",
      "Episode: 46/100, Epsilon:0.55, Average reward: -227.26\n",
      "Prediction for [500,0,500,0] is: [[-960.95746 -938.1169  -888.7874  -945.447   -946.3339 ]]\n",
      "Episode: 47/100, Epsilon:0.54, Average reward: -126.5\n",
      "Prediction for [500,0,500,0] is: [[-957.1628  -919.53564 -870.8452  -938.1032  -944.1905 ]]\n",
      "Episode: 48/100, Epsilon:0.53, Average reward: -187.79\n",
      "Prediction for [500,0,500,0] is: [[-960.1308 -918.2927 -877.8522 -938.7428 -950.0037]]\n",
      "Episode: 49/100, Epsilon:0.52, Average reward: -337.17\n",
      "Prediction for [500,0,500,0] is: [[-942.7564 -904.4167 -870.0095 -936.565  -919.5705]]\n",
      "Episode: 50/100, Epsilon:0.51, Average reward: -178.54\n",
      "Prediction for [500,0,500,0] is: [[-928.7767  -892.9984  -880.3902  -938.14233 -924.97534]]\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 50.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 51/100, Epsilon:0.5, Average reward: -141.88\n",
      "Prediction for [500,0,500,0] is: [[-939.92017 -904.52    -883.1418  -945.80585 -932.43353]]\n",
      "Episode: 52/100, Epsilon:0.49, Average reward: -173.87\n",
      "Prediction for [500,0,500,0] is: [[-926.3139  -883.68964 -871.9484  -941.578   -931.4978 ]]\n",
      "Episode: 53/100, Epsilon:0.48, Average reward: -105.32\n",
      "Prediction for [500,0,500,0] is: [[-912.7738  -878.4349  -863.5168  -936.7802  -922.08704]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes100_ProgramDQN_BestAgent0_Memory.p\n",
      "Episode: 54/100, Epsilon:0.47, Average reward: -224.13\n",
      "Prediction for [500,0,500,0] is: [[-910.54    -891.8391  -869.1654  -945.8189  -920.81274]]\n",
      "Episode: 55/100, Epsilon:0.46, Average reward: -273.65\n",
      "Prediction for [500,0,500,0] is: [[-908.0603  -899.8003  -881.0483  -938.2539  -929.40515]]\n",
      "Episode: 56/100, Epsilon:0.45, Average reward: -128.14\n",
      "Prediction for [500,0,500,0] is: [[-928.2529  -926.4135  -899.36914 -947.70795 -951.8237 ]]\n",
      "Episode: 57/100, Epsilon:0.44, Average reward: -226.38\n",
      "Prediction for [500,0,500,0] is: [[-939.21545 -922.4492  -909.4445  -947.9894  -956.50146]]\n",
      "Episode: 58/100, Epsilon:0.43, Average reward: -116.43\n",
      "Prediction for [500,0,500,0] is: [[-930.5678  -918.72156 -905.02765 -931.07556 -946.54346]]\n",
      "Episode: 59/100, Epsilon:0.42, Average reward: -134.58\n",
      "Prediction for [500,0,500,0] is: [[-932.3244  -922.1935  -917.61194 -936.0826  -960.35815]]\n",
      "Episode: 60/100, Epsilon:0.41, Average reward: -239.35\n",
      "Prediction for [500,0,500,0] is: [[-922.4145  -915.0051  -896.7361  -904.81396 -954.1829 ]]\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Saved Partial results at the end of episode 60.\n",
      "Sever Redispatched.\n",
      "Redispatched\n",
      "Episode: 61/100, Epsilon:0.4, Average reward: -364.31\n",
      "Prediction for [500,0,500,0] is: [[-926.4085  -909.20306 -896.13654 -915.85254 -952.6842 ]]\n",
      "Episode: 62/100, Epsilon:0.39, Average reward: -176.0\n",
      "Prediction for [500,0,500,0] is: [[-911.73444 -920.216   -886.7858  -926.2347  -951.6285 ]]\n",
      "Episode: 63/100, Epsilon:0.38, Average reward: -303.31\n",
      "Prediction for [500,0,500,0] is: [[-924.7473 -931.6802 -888.191  -937.063  -958.703 ]]\n",
      "Episode: 64/100, Epsilon:0.37, Average reward: -225.52\n",
      "Prediction for [500,0,500,0] is: [[-930.72455 -928.68207 -901.8127  -914.9722  -967.86914]]\n",
      "Episode: 65/100, Epsilon:0.36, Average reward: -253.31\n",
      "Prediction for [500,0,500,0] is: [[-944.176   -950.0788  -891.96783 -911.59686 -970.8418 ]]\n",
      "Episode: 66/100, Epsilon:0.35, Average reward: -164.21\n",
      "Prediction for [500,0,500,0] is: [[-950.9942  -949.29974 -900.63544 -913.4772  -955.755  ]]\n",
      "Episode: 67/100, Epsilon:0.34, Average reward: -309.8\n",
      "Prediction for [500,0,500,0] is: [[-957.93994 -957.67596 -910.51434 -923.535   -951.80566]]\n",
      "Episode: 68/100, Epsilon:0.33, Average reward: -309.27\n",
      "Prediction for [500,0,500,0] is: [[-954.1649 -947.6277 -905.6441 -912.8962 -955.1455]]\n",
      "Episode: 69/100, Epsilon:0.32, Average reward: -338.98\n",
      "Prediction for [500,0,500,0] is: [[-951.62067 -956.39996 -902.2274  -917.24677 -960.2364 ]]\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147467259), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4a3ea35e9a04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mSF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSet_Quickmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mSF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_simulation_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVissim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulation_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDemo_Mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# Calculate episode average reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mrun_simulation_episode\u001b[1;34m(Agents, Vissim, state_type, state_size, simulation_length, Demo_Mode)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;31m# Advance the game to the next frame based on the action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Stop the simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ACABRE~1\\AppData\\Local\\Temp\\gen_py\\3.6\\9A7C13B7-EEBA-4845-9DFE-BDF71229CE5Bx0x11x0\\ISimulation.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;34m'Runs a single simulation step.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvokeTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1610874881\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLCID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# The method SetAttValue is actually a property, but must be used as a method to correctly pass the arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147467259), None)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    \n",
    "    # If previous agents aren't to be loaded, proceed to memory population\n",
    "    if not load_trained:\n",
    "        # Initialize simulation\n",
    "        if 'Vissim' not in globals() or Vissim == None:\n",
    "            Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                        memory_population_length, Start_Fresh,\\\n",
    "                                                                        reset_flag = True, verbose = True)\n",
    "        else:\n",
    "            Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "            Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                  memory_population_length, Start_Fresh, reset_flag = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if program == \"DQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = False, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DuelingDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = False, Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = True, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DuelingDDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = True, Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    \n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), program))\n",
    "    \n",
    "    if Demo_Mode:\n",
    "        Agents = SF.load_agents(vissim_working_directory, model_name, Agents, Session_ID, best = True)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length, Demo_Mode)\n",
    "        Vissim = None\n",
    "    # Load previous trained data\n",
    "    elif load_trained:\n",
    "        Agents = SF.load_agents(vissim_working_directory, model_name, Agents, Session_ID, best = False)\n",
    "    # If previous data isn't to be loaded, have an initial longer random run to populate memory\n",
    "    else:\n",
    "        print('Populating memory with Random Actions....')\n",
    "        SF.Set_Quickmode(Vissim)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length, Demo_Mode)\n",
    "    \n",
    "    # Iterations of the simulation\n",
    "    for episode in range(episodes):\n",
    "        # Completely re-dispatch server every N iterations for performance\n",
    "        if episode % reset_frequency == 0 and episode !=0:\n",
    "            Vissim = None\n",
    "            Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                        simulation_length, Start_Fresh,\\\n",
    "                                                                        reset_flag = True, verbose = False)\n",
    "            print(\"Redispatched\")\n",
    "        else:\n",
    "            # If not the first episode, reset state at the start\n",
    "            Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                simulation_length, Start_Fresh, reset_flag = True)\n",
    "        npa = NetworkParser(Vissim) \n",
    "        for index, agent in enumerate(Agents):\n",
    "            agent.update_IDS(npa.signal_controllers_ids[index], npa)\n",
    "            agent.episode_reward = []\n",
    "        \n",
    "        # Change demand for every episode\n",
    "        if Random_Demand:\n",
    "            for vehicle_input in range(1,5):\n",
    "                Vissim.Net.VehicleInputs.ItemByKey(vehicle_input).SetAttValue('Volume(1)', demands[np.random.randint(0,len(demands)-1)])    \n",
    "        \n",
    "        # Run Episode at maximum speed\n",
    "\n",
    "        SF.Set_Quickmode(Vissim)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, Demo_Mode)\n",
    "        \n",
    "        # Calculate episode average reward\n",
    "        reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "        best_agent_weights = SF.best_agent(reward_storage, average_reward, best_agent_weights, vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "        # Train agent with experience of episode (indicated batch size)\n",
    "        for agent in Agents:\n",
    "            agent.replay(batch_size, episode)\n",
    "        # Security save for long trainings\n",
    "        if SaveResultsAgent:\n",
    "            if (episode+1)%partial_save_at == 0:\n",
    "                SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "    #Saving agents memory, weights and optimizer\n",
    "    if SaveResultsAgent:\n",
    "        SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "        print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved. Succesfully Terminated.\")\n",
    "    \n",
    "    # Plotting training progress\n",
    "    x_series = range(1,len(reward_storage)+1)\n",
    "    fit = np.polyfit(x_series,reward_storage,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average ageng reward in episode')\n",
    "    plt.title('Training evolution and trend')\n",
    "    plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Demo_Mode = True\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    \n",
    "    # If previous agents aren't to be loaded, proceed to memory population\n",
    "    if not load_trained:\n",
    "        # Initialize simulation\n",
    "        if 'Vissim' not in globals() or Vissim == None:\n",
    "            Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                        memory_population_length, Start_Fresh, reset_flag = True)\n",
    "        else:\n",
    "            Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "            Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                  memory_population_length, Start_Fresh, reset_flag = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if program == \"DQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, Vissim, DoubleDQN = False, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, Vissim, DoubleDQN = True, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif program == \"DuelingDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                           epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, DoubleDQN = True, Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "      \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    \n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), program))\n",
    "    \n",
    "    if Demo_Mode:\n",
    "        for index, agent in enumerate(Agents):\n",
    "            Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'.h5')\n",
    "            agent.model = load_model(Filename)\n",
    "            Memory_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'_Memory'+'.p')\n",
    "            agent.memory = pickle.load(open(Memory_Filename, 'rb'))\n",
    "        print('Items successfully loaded.')        \n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length, Demo_Mode = True)\n",
    "        Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " # Plotting training progress\n",
    "    x_series = range(1,len(reward_storage)+1)\n",
    "    fit = np.polyfit(x_series,reward_storage,1)\n",
    "    fit_fn = np.poly1d(fit) \n",
    "    plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average ageng reward in episode')\n",
    "    plt.title('Training evolution and trend')\n",
    "    plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-413.34192"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agents[0].model.predict(Agents[0].state)[0][Agents[0].action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'next_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fdfaf43e8323>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.9\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'next_state'"
     ]
    }
   ],
   "source": [
    "target = Agents[0].reward + 0.9 * np.max(Agents[0].target_model.predict(np.reshape(Agents[0].next_state,(1,4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'next_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-cacc9be64e06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'next_state'"
     ]
    }
   ],
   "source": [
    "Agents[0].next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
