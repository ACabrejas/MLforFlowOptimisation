{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Libraries\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Libraries\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "## Data Management Libraries\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "## Other Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Model Parameters\n",
    "Random_Seed = 42\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "program = 'DQN'\n",
    "reward_type = 'Delay'\n",
    "state_type  = 'Queues' \n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "## Load trained model?\n",
    "load_trained = False\n",
    "Quickmode = True\n",
    "# Random demand\n",
    "Random_Demand = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling flags\n",
    "# Flag for restarting the COM Server\n",
    "reset_flag = True\n",
    "# If a fresh start is needed, all previous results from simulations are deleted\n",
    "Start_Fresh = True\n",
    "# Debug action\n",
    "debug_action = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## State-Action Parameters\n",
    "state_size = 4\n",
    "action_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations\n",
    "episodes = 1000\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec)\n",
    "simulation_length = 36000*5\n",
    "# Memory Size\n",
    "memory_size = 1000\n",
    "# Learning Rate\n",
    "alpha   = 0.9\n",
    "# Discount Factor\n",
    "gamma   = 0.3\n",
    "# Exploration Schedule\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "epsilon_decay = 0.955\n",
    "# Demand Schedule\n",
    "demands = [100,200, 400, 600, 800, 1000, 1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user defined functions\n"
     ]
    }
   ],
   "source": [
    "## Basic User Defined Functions\n",
    "# Function to convert a nested tuple to a nested list\n",
    "def toList(NestedTuple):\n",
    "    return list(map(toList, NestedTuple)) if isinstance(NestedTuple, (list, tuple)) else NestedTuple\n",
    "print ('Loaded user defined functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN Agent Class\n",
    "# To access memory of agent i for data about time t:\n",
    "# s_t     = Agents[i].memory[t][0]\n",
    "# a_t     = Agents[i].memory[t][1]\n",
    "# r_t     = Agents[i].memory[t][2]\n",
    "# s_(t+1) = Agents[i].memory[t][3]\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, ID):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.gamma = gamma                    # discount rate\n",
    "        self.epsilon = epsilon_start          # starting exploration rate\n",
    "        self.epsilon_min = epsilon_end        # final exploration rate\n",
    "        self.epsilon_decay = epsilon_decay    # decay of exploration rate\n",
    "        self.learning_rate = alpha            # learning rate\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        self.state = np.reshape([0,0,0,0], [1,4])\n",
    "        self.newstate = np.reshape([0,0,0,0], [1,4])\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.episode_reward = []\n",
    "        \n",
    "    def update_IDS(self, ID):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "    \n",
    "    # DNN definition\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    # Obtain the state based on different state definitions\n",
    "    def get_state(self, state_type = state_type):\n",
    "        if state_type == 'Queues':\n",
    "            #Obtain Queue Values (average value over the last period)\n",
    "            East_Queue  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)')\n",
    "            South_Queue = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)')\n",
    "            West_Queue  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)')\n",
    "            North_Queue = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)')\n",
    "            state = [East_Queue, South_Queue, West_Queue, North_Queue]\n",
    "            state = np.reshape(state, [1,4])\n",
    "            return(state)\n",
    "        elif state_type == 'Delay':\n",
    "            pass\n",
    "        elif state_type == 'MaxFlow':\n",
    "            pass\n",
    "        elif state_type == 'FuelConsumption':\n",
    "            pass\n",
    "        elif state_type == 'NOx':\n",
    "            pass\n",
    "        elif state_type == \"COM\":\n",
    "            pass\n",
    "    \n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "        return(self.memory)\n",
    "    \n",
    "    # Choosing actions\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size) \n",
    "            self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "            #print('Chosen Random Action {}'.format(action+1))\n",
    "            return action\n",
    "        act_values = self.model.predict(state)\n",
    "        action = np.argmax(act_values[0]) \n",
    "        self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "        #print('Chosen Not-Random Action {}'.format(action+1))\n",
    "        return action  # returns action\n",
    "    \n",
    "    def get_reward(self):\n",
    "        reward = -np.absolute((self.newstate[0][0]-self.newstate[0][2])-(self.newstate[0][1]-self.newstate[0][3]))\n",
    "        self.episode_reward.append(reward)\n",
    "        return reward\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * \\\n",
    "                       np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Parser (Crawler) class definition\n",
    "class NetworkParser:\n",
    "    \n",
    "    ######################################################################################################################\n",
    "    ## Nested data structure:\n",
    "    ## \n",
    "    ## Signal Controllers = signal_controllers[signal_controller_ids]\n",
    "    ## Signal Groups      = signal_groups     [signal_controller_ids] [signal_group_id]\n",
    "    ## Signal Heads       = signal_heads      [signal_controller_ids] [signal_heads_id]\n",
    "    ## Lanes              = lanes             [signal_controller_ids] [signal_heads_id] [lane_id]\n",
    "    ##\n",
    "    ######################################################################################################################\n",
    "    ##\n",
    "    ## Accessing attributes:\n",
    "    ##\n",
    "    ## AttValue('AttName(X,Y,bla)')\n",
    "    ##\n",
    "    ## X = Simulation Number.      Values: 1,2,3.. 'Current' [single case], Avg, StdDev, Min, Max [over several sims]\n",
    "    ## Y = Time Interval Number    Values: 1,2,3, 'Current', 'Last', Avg, StdDev, Min, Max, Total\n",
    "    ## All = All vehicle classes   Values: 10, 20, All\n",
    "    ######################################################################################################################\n",
    "\n",
    "    def __init__(self):\n",
    "        ## Get all SignalControllers\n",
    "        self.signal_controllers     = toList(Vissim.Net.SignalControllers.GetAll())\n",
    "        self.signal_controllers_ids = range(len(self.signal_controllers)) #Vissim count starts at 1\n",
    "                 \n",
    "        ## Create SignalGroupContainers and unpack the SignalGroups into a list by SignalController\n",
    "        self.signal_groups = [[] for _ in self.signal_controllers_ids]\n",
    "        for SC in self.signal_controllers_ids:\n",
    "            for SG in range(1,self.signal_controllers[SC].SGs.Count+1):\n",
    "                self.signal_groups[SC].append(self.signal_controllers[SC].SGs.ItemByKey(SG))\n",
    "                \n",
    "        ## Create SignalHeadsCollection and unpack the SignalHeads into a list by SignalController\n",
    "        self.signal_heads = [[] for _ in self.signal_controllers_ids]\n",
    "        for SC in self.signal_controllers_ids:\n",
    "            for SG in range(self.signal_controllers[SC].SGs.Count):\n",
    "                self.signal_heads[SC].append(toList(self.signal_groups[SC][SG].SigHeads.GetAll())[0])\n",
    "                \n",
    "        self.lanes = [[[] for b in range(len(self.signal_heads[a])) ] for a in self.signal_controllers_ids]\n",
    "        for SC in self.signal_controllers_ids:\n",
    "            for SH in range(len(self.signal_heads[SC])):\n",
    "                self.lanes[SC][SH].append(self.signal_heads[SC][SH].Lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COMServerDispatch(reset_flag):\n",
    "    ## Connecting the COM Server => Open a new Vissim Window:\n",
    "    # Server should only be dispatched in first run. Otherwise reload model.\n",
    "    # Setting Working Directory\n",
    "    vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "    print ('Working Directory set to: ' + vissim_working_directory)\n",
    "    # Check Chache\n",
    "    try:\n",
    "        print ('Checking Presence of Pregenerated Cache.')\n",
    "        cache_flag\n",
    "    # Re-generate Cache\n",
    "    except:\n",
    "        print ('Cache NOT Present.')\n",
    "        print ('Generating Cache...')\n",
    "        Vissim = com.gencache.EnsureDispatch(\"Vissim.Vissim\") \n",
    "        print ('Cache generated.\\n')\n",
    "        cache_flag = True\n",
    "        print ('****************************')\n",
    "        print ('*   COM Server dispatched  *')\n",
    "        print ('****************************\\n')\n",
    "    # Dispatch without re-generating Cache.\n",
    "    else:\n",
    "        print ('Previous Cache Found. Dispatching...\\n')\n",
    "        Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "        print ('****************************')\n",
    "        print ('*   COM Server dispatched  *')\n",
    "        print ('****************************\\n')\n",
    "\n",
    "    ## Load the Network:\n",
    "    Filename = os.path.join(vissim_working_directory, model_name, (model_name+'.inpx'))\n",
    "    print ('Model File: ' + model_name+'.inpx')\n",
    "\n",
    "    # Additional Files\n",
    "    if flag_read_additionally == False:\n",
    "        print ('No additional files will be loaded')\n",
    "    print ('Loading...')\n",
    "    Vissim.LoadNet(Filename, flag_read_additionally)\n",
    "    print ('Load process successful')\n",
    "\n",
    "    ## Setting Simulation End\n",
    "    Vissim.Simulation.SetAttValue('SimPeriod', simulation_length)\n",
    "    print ('Simulation length set to '+str(simulation_length/10) + ' seconds.')\n",
    "    \n",
    "    ## If a fresh start is needed\n",
    "    if reset_flag == True:\n",
    "        if Start_Fresh == True:\n",
    "            # Delete all previous simulation runs first:\n",
    "            for simRun in Vissim.Net.SimulationRuns:\n",
    "                Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "            print ('Results from Previous Simulations: Deleted. Fresh Start Available.')\n",
    "\n",
    "    #Pre-fetch objects for stability\n",
    "    Simulation = Vissim.Simulation\n",
    "    print ('Fetched and containerized Simulation Object')\n",
    "    Network = Vissim.Net\n",
    "    print ('Fetched and containerized Network Object \\n')\n",
    "    print ('*******************************************************')\n",
    "    print ('*                                                     *')\n",
    "    print ('*                 SETUP COMPLETE                      *')\n",
    "    print ('*                                                     *')\n",
    "    print ('*******************************************************\\n')\n",
    "    return(Vissim,Simulation,Network, cache_flag)\n",
    "\n",
    "def COMServerReload(Vissim, reset_flag):\n",
    "    ## Connecting the COM Server => Open a new Vissim Window:\n",
    "    # Server should only be dispatched in first run. Otherwise reload model.\n",
    "    # Setting Working Directory\n",
    "    vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "    ## Load the Network:\n",
    "    Filename = os.path.join(vissim_working_directory, model_name, (model_name+'.inpx'))\n",
    "\n",
    "    print('Reoading...')\n",
    "    Vissim.LoadNet(Filename, flag_read_additionally)\n",
    "\n",
    "    ## Setting Simulation End\n",
    "    Vissim.Simulation.SetAttValue('SimPeriod', simulation_length)\n",
    "\n",
    "    ## If a fresh start is needed\n",
    "    if reset_flag == True:\n",
    "        if Start_Fresh == True:\n",
    "            # Delete all previous simulation runs first:\n",
    "            for simRun in Vissim.Net.SimulationRuns:\n",
    "                Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "    \n",
    "    #Pre-fetch objects for stability\n",
    "    Simulation = Vissim.Simulation\n",
    "    Network = Vissim.Net\n",
    "    return(Simulation,Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Checking Presence of Pregenerated Cache.\n",
      "Cache NOT Present.\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Model File: Single_Cross_Straight.inpx\n",
      "No additional files will be loaded\n",
      "Loading...\n",
      "Load process successful\n",
      "Simulation length set to 18000.0 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Episode: 1/1000, Average reward: -73.73352826135007\n",
      "Reoading...\n",
      "Episode: 2/1000, Average reward: -184.08888301880137\n",
      "Reoading...\n",
      "Episode: 3/1000, Average reward: -4.997747186161857\n",
      "Reoading...\n",
      "Episode: 4/1000, Average reward: -117.83987883726068\n",
      "Reoading...\n",
      "Episode: 5/1000, Average reward: -84.36136589594162\n",
      "Reoading...\n",
      "Episode: 6/1000, Average reward: -88.91039138219003\n",
      "Reoading...\n",
      "Episode: 7/1000, Average reward: -265.4542051272661\n",
      "Reoading...\n",
      "Episode: 8/1000, Average reward: -188.08847428218127\n",
      "Reoading...\n",
      "Episode: 9/1000, Average reward: -10.875490114831578\n",
      "Reoading...\n",
      "Episode: 10/1000, Average reward: -26.493066872312735\n",
      "Reoading...\n",
      "Episode: 11/1000, Average reward: -32.13660271707433\n",
      "Reoading...\n",
      "Episode: 12/1000, Average reward: -101.64437934140565\n",
      "Reoading...\n",
      "Episode: 13/1000, Average reward: -260.9156239507959\n",
      "Reoading...\n",
      "Episode: 14/1000, Average reward: -166.03112363747732\n",
      "Reoading...\n",
      "Episode: 15/1000, Average reward: -44.460541920179985\n",
      "Reoading...\n",
      "Episode: 16/1000, Average reward: -13.315121971743919\n",
      "Reoading...\n",
      "Episode: 17/1000, Average reward: -108.9585984785574\n",
      "Reoading...\n",
      "Episode: 18/1000, Average reward: -53.01456493093084\n",
      "Reoading...\n",
      "Episode: 19/1000, Average reward: -135.66293166428892\n",
      "Reoading...\n",
      "Episode: 20/1000, Average reward: -136.50078445524633\n",
      "Reoading...\n",
      "Episode: 21/1000, Average reward: -222.00339490897025\n",
      "Reoading...\n",
      "Episode: 22/1000, Average reward: -186.4798298382055\n",
      "Reoading...\n",
      "Episode: 23/1000, Average reward: -37.51488344025792\n",
      "Reoading...\n",
      "Episode: 24/1000, Average reward: -44.95218745579681\n",
      "Reoading...\n",
      "Episode: 25/1000, Average reward: -18.634460113048522\n",
      "Reoading...\n",
      "Episode: 26/1000, Average reward: -151.74151925887622\n",
      "Reoading...\n",
      "Episode: 27/1000, Average reward: -164.6358384556295\n",
      "Reoading...\n",
      "Episode: 28/1000, Average reward: -175.00592679400066\n",
      "Reoading...\n",
      "Episode: 29/1000, Average reward: -54.00079073481245\n",
      "Reoading...\n",
      "Episode: 30/1000, Average reward: -38.16435753918126\n",
      "Reoading...\n",
      "Episode: 31/1000, Average reward: -52.45075320215379\n",
      "Reoading...\n",
      "Episode: 32/1000, Average reward: -138.65960957497705\n",
      "Reoading...\n",
      "Episode: 33/1000, Average reward: -197.863996365059\n",
      "Reoading...\n",
      "Episode: 34/1000, Average reward: -228.22469804983388\n",
      "Reoading...\n",
      "Episode: 35/1000, Average reward: -10.75491825521094\n",
      "Reoading...\n",
      "Episode: 36/1000, Average reward: -235.30673158652482\n",
      "Reoading...\n",
      "Episode: 37/1000, Average reward: -182.5727480040357\n",
      "Reoading...\n",
      "Episode: 38/1000, Average reward: -184.30903247151187\n",
      "Reoading...\n",
      "Episode: 39/1000, Average reward: -221.423602061582\n",
      "Reoading...\n",
      "Episode: 40/1000, Average reward: -31.703079260469714\n",
      "Reoading...\n",
      "Episode: 41/1000, Average reward: -45.20577856381702\n",
      "Reoading...\n",
      "Episode: 42/1000, Average reward: -5.170143856642401\n",
      "Reoading...\n",
      "Episode: 43/1000, Average reward: -22.382823540214\n",
      "Reoading...\n",
      "Episode: 44/1000, Average reward: -23.467378414258768\n",
      "Reoading...\n",
      "Episode: 45/1000, Average reward: -228.43908696762006\n",
      "Reoading...\n",
      "Episode: 46/1000, Average reward: -11.691948707544858\n",
      "Reoading...\n",
      "Episode: 47/1000, Average reward: -15.748142977522757\n",
      "Reoading...\n",
      "Episode: 48/1000, Average reward: -230.41933798632869\n",
      "Reoading...\n",
      "Episode: 49/1000, Average reward: -232.02154184505181\n",
      "Reoading...\n",
      "Episode: 50/1000, Average reward: -198.89835336107492\n",
      "Reoading...\n",
      "Episode: 51/1000, Average reward: -36.269142389404685\n",
      "Reoading...\n",
      "Episode: 52/1000, Average reward: -221.33339259782556\n",
      "Reoading...\n",
      "Episode: 53/1000, Average reward: -12.152493446958447\n",
      "Reoading...\n",
      "Episode: 54/1000, Average reward: -12.749373922034298\n",
      "Reoading...\n",
      "Episode: 55/1000, Average reward: -7.1844184227815155\n",
      "Reoading...\n",
      "Episode: 56/1000, Average reward: -217.71434497358402\n",
      "Reoading...\n",
      "Episode: 57/1000, Average reward: -181.2489019306126\n",
      "Reoading...\n",
      "Episode: 58/1000, Average reward: -5.084945609272007\n",
      "Reoading...\n",
      "Episode: 59/1000, Average reward: -8.84471360642169\n",
      "Reoading...\n",
      "Episode: 60/1000, Average reward: -19.02017649505581\n",
      "Reoading...\n",
      "Episode: 61/1000, Average reward: -5.6008660120644915\n",
      "Reoading...\n",
      "Episode: 62/1000, Average reward: -6.202518691006362\n",
      "Reoading...\n",
      "Episode: 63/1000, Average reward: -229.16080507832345\n",
      "Reoading...\n",
      "Episode: 64/1000, Average reward: -214.66697317894025\n",
      "Reoading...\n",
      "Episode: 65/1000, Average reward: -11.177257654522316\n",
      "Reoading...\n",
      "Episode: 66/1000, Average reward: -209.01086373126986\n",
      "Reoading...\n",
      "Episode: 67/1000, Average reward: -229.56202999373895\n",
      "Reoading...\n",
      "Episode: 68/1000, Average reward: -223.20413717694896\n",
      "Reoading...\n",
      "Episode: 69/1000, Average reward: -227.92378137366615\n",
      "Reoading...\n",
      "Episode: 70/1000, Average reward: -8.823205038999333\n",
      "Reoading...\n",
      "Episode: 71/1000, Average reward: -209.10331927948027\n",
      "Reoading...\n",
      "Episode: 72/1000, Average reward: -16.092724668126202\n",
      "Reoading...\n",
      "Episode: 73/1000, Average reward: -229.95761976093632\n",
      "Reoading...\n",
      "Episode: 74/1000, Average reward: -205.70219460896968\n",
      "Reoading...\n",
      "Episode: 75/1000, Average reward: -40.04191345812848\n",
      "Reoading...\n",
      "Episode: 76/1000, Average reward: -31.86816218579677\n",
      "Reoading...\n",
      "Episode: 77/1000, Average reward: -34.28699115947021\n",
      "Reoading...\n",
      "Episode: 78/1000, Average reward: -27.14292232051911\n",
      "Reoading...\n",
      "Episode: 79/1000, Average reward: -221.7432476481549\n",
      "Reoading...\n",
      "Episode: 80/1000, Average reward: -132.50578806705798\n",
      "Reoading...\n",
      "Episode: 81/1000, Average reward: -12.9974899432246\n",
      "Reoading...\n",
      "Episode: 82/1000, Average reward: -139.30634206940277\n",
      "Reoading...\n",
      "Episode: 83/1000, Average reward: -11.882477081424241\n",
      "Reoading...\n",
      "Episode: 84/1000, Average reward: -160.95548172183146\n",
      "Reoading...\n",
      "Episode: 85/1000, Average reward: -15.479568704103528\n",
      "Reoading...\n",
      "Episode: 86/1000, Average reward: -225.04190312279007\n",
      "Reoading...\n",
      "Episode: 87/1000, Average reward: -46.23814216820067\n",
      "Reoading...\n",
      "Episode: 88/1000, Average reward: -14.374516426038642\n",
      "Reoading...\n",
      "Episode: 89/1000, Average reward: -15.215913850962538\n",
      "Reoading...\n",
      "Episode: 90/1000, Average reward: -152.42193920474833\n",
      "Reoading...\n",
      "Episode: 91/1000, Average reward: -156.43921552132298\n",
      "Reoading...\n",
      "Episode: 92/1000, Average reward: -29.204028149415077\n",
      "Reoading...\n",
      "Episode: 93/1000, Average reward: -35.761604749405514\n",
      "Reoading...\n",
      "Episode: 94/1000, Average reward: -20.26219159841279\n",
      "Reoading...\n",
      "Episode: 95/1000, Average reward: -217.11912812088966\n",
      "Reoading...\n",
      "Episode: 96/1000, Average reward: -182.0083810250951\n",
      "Reoading...\n",
      "Episode: 97/1000, Average reward: -170.51623918186402\n",
      "Reoading...\n",
      "Episode: 98/1000, Average reward: -186.2888663116109\n",
      "Reoading...\n",
      "Episode: 99/1000, Average reward: -34.75277866149698\n",
      "Reoading...\n",
      "Episode: 100/1000, Average reward: -15.785015353774272\n",
      "Reoading...\n",
      "Episode: 101/1000, Average reward: -6.6272267551060455\n",
      "Reoading...\n",
      "Episode: 102/1000, Average reward: -232.22979118335152\n",
      "Reoading...\n",
      "Episode: 103/1000, Average reward: -15.367581699379883\n",
      "Reoading...\n",
      "Episode: 104/1000, Average reward: -224.7933691235981\n",
      "Reoading...\n",
      "Episode: 105/1000, Average reward: -221.26221287438926\n",
      "Reoading...\n",
      "Episode: 106/1000, Average reward: -7.788855356392018\n",
      "Reoading...\n",
      "Episode: 107/1000, Average reward: -30.416740146146463\n",
      "Reoading...\n",
      "Episode: 108/1000, Average reward: -14.868993096750941\n",
      "Reoading...\n",
      "Episode: 109/1000, Average reward: -186.041100717007\n",
      "Reoading...\n",
      "Episode: 110/1000, Average reward: -192.86935899689934\n",
      "Reoading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 111/1000, Average reward: -150.05794849196616\n",
      "Reoading...\n",
      "Episode: 112/1000, Average reward: -8.434443929392893\n",
      "Reoading...\n",
      "Episode: 113/1000, Average reward: -27.37796189730452\n",
      "Reoading...\n",
      "Episode: 114/1000, Average reward: -47.10864213193855\n",
      "Reoading...\n",
      "Episode: 115/1000, Average reward: -13.52950105127025\n",
      "Reoading...\n",
      "Episode: 116/1000, Average reward: -13.818504997491576\n",
      "Reoading...\n",
      "Episode: 117/1000, Average reward: -187.22944030041526\n",
      "Reoading...\n",
      "Episode: 118/1000, Average reward: -148.93763363342\n",
      "Reoading...\n",
      "Episode: 119/1000, Average reward: -61.570963318634696\n",
      "Reoading...\n",
      "Episode: 120/1000, Average reward: -34.86729558822377\n",
      "Reoading...\n",
      "Episode: 121/1000, Average reward: -225.04916765301837\n",
      "Reoading...\n",
      "Episode: 122/1000, Average reward: -32.05216432288999\n",
      "Reoading...\n",
      "Episode: 123/1000, Average reward: -26.904188443073828\n",
      "Reoading...\n",
      "Episode: 124/1000, Average reward: -211.37027488173104\n",
      "Reoading...\n",
      "Episode: 125/1000, Average reward: -43.057800847139795\n",
      "Reoading...\n",
      "Episode: 126/1000, Average reward: -169.5460218113149\n",
      "Reoading...\n",
      "Episode: 127/1000, Average reward: -191.62214632744437\n",
      "Reoading...\n",
      "Episode: 128/1000, Average reward: -26.65364310357223\n",
      "Reoading...\n",
      "Episode: 129/1000, Average reward: -232.25830707767287\n",
      "Reoading...\n",
      "Episode: 130/1000, Average reward: -14.102701200460054\n",
      "Reoading...\n",
      "Episode: 131/1000, Average reward: -28.738931514371068\n",
      "Reoading...\n",
      "Episode: 132/1000, Average reward: -30.908129569022076\n",
      "Reoading...\n",
      "Episode: 133/1000, Average reward: -355.44391605002505\n",
      "Reoading...\n",
      "Episode: 134/1000, Average reward: -12.549826361946973\n",
      "Reoading...\n",
      "Episode: 135/1000, Average reward: -52.73398412520636\n",
      "Reoading...\n",
      "Episode: 136/1000, Average reward: -152.9784216307128\n",
      "Reoading...\n",
      "Episode: 137/1000, Average reward: -191.18570575907043\n",
      "Reoading...\n",
      "Episode: 138/1000, Average reward: -170.32336784160964\n",
      "Reoading...\n",
      "Episode: 139/1000, Average reward: -188.12036155270522\n",
      "Reoading...\n",
      "Episode: 140/1000, Average reward: -164.01095554276586\n",
      "Reoading...\n",
      "Episode: 141/1000, Average reward: -219.96043663222568\n",
      "Reoading...\n",
      "Episode: 142/1000, Average reward: -11.357823035087083\n",
      "Reoading...\n",
      "Episode: 143/1000, Average reward: -51.41550312131693\n",
      "Reoading...\n",
      "Episode: 144/1000, Average reward: -200.28472480451327\n",
      "Reoading...\n",
      "Episode: 145/1000, Average reward: -42.10346852850196\n",
      "Reoading...\n",
      "Episode: 146/1000, Average reward: -33.89804169500698\n",
      "Reoading...\n",
      "Episode: 147/1000, Average reward: -186.40089471515873\n",
      "Reoading...\n",
      "Episode: 148/1000, Average reward: -40.49201683808757\n",
      "Reoading...\n",
      "Episode: 149/1000, Average reward: -146.19208923070371\n",
      "Reoading...\n",
      "Episode: 150/1000, Average reward: -39.443770722832454\n",
      "Reoading...\n",
      "Episode: 151/1000, Average reward: -375.0629811464732\n",
      "Reoading...\n",
      "Episode: 152/1000, Average reward: -24.13629614977215\n",
      "Reoading...\n",
      "Episode: 153/1000, Average reward: -4.555149126660865\n",
      "Reoading...\n",
      "Episode: 154/1000, Average reward: -119.32067027851396\n",
      "Reoading...\n",
      "Episode: 155/1000, Average reward: -20.391615600503137\n",
      "Reoading...\n",
      "Episode: 156/1000, Average reward: -24.206477579360207\n",
      "Reoading...\n",
      "Episode: 157/1000, Average reward: -25.971591246587447\n",
      "Reoading...\n",
      "Episode: 158/1000, Average reward: -376.83025895888005\n",
      "Reoading...\n",
      "Episode: 159/1000, Average reward: -178.06320400090044\n",
      "Reoading...\n",
      "Episode: 160/1000, Average reward: -37.19642278490024\n",
      "Reoading...\n",
      "Episode: 161/1000, Average reward: -189.05188734252073\n",
      "Reoading...\n",
      "Episode: 162/1000, Average reward: -22.110368786278425\n",
      "Reoading...\n",
      "Episode: 163/1000, Average reward: -167.17134907040648\n",
      "Reoading...\n",
      "Episode: 164/1000, Average reward: -183.3329598226997\n",
      "Reoading...\n",
      "Episode: 165/1000, Average reward: -12.970409248889025\n",
      "Reoading...\n",
      "Episode: 166/1000, Average reward: -107.39328847841644\n",
      "Reoading...\n",
      "Episode: 167/1000, Average reward: -177.47365244310103\n",
      "Reoading...\n",
      "Episode: 168/1000, Average reward: -28.320580341061696\n",
      "Reoading...\n",
      "Episode: 169/1000, Average reward: -63.91543095144991\n",
      "Reoading...\n",
      "Episode: 170/1000, Average reward: -165.32485563623666\n",
      "Reoading...\n",
      "Episode: 171/1000, Average reward: -195.19297002391443\n",
      "Reoading...\n",
      "Episode: 172/1000, Average reward: -47.920899718936866\n",
      "Reoading...\n",
      "Episode: 173/1000, Average reward: -12.640910977731682\n",
      "Reoading...\n",
      "Episode: 174/1000, Average reward: -62.07052094094448\n",
      "Reoading...\n",
      "Episode: 175/1000, Average reward: -145.80143333940393\n",
      "Reoading...\n",
      "Episode: 176/1000, Average reward: -26.48482478391257\n",
      "Reoading...\n",
      "Episode: 177/1000, Average reward: -163.57920686518017\n",
      "Reoading...\n",
      "Episode: 178/1000, Average reward: -40.05978483487407\n",
      "Reoading...\n",
      "Episode: 179/1000, Average reward: -147.81309339458787\n",
      "Reoading...\n",
      "Episode: 180/1000, Average reward: -200.33019573877377\n",
      "Reoading...\n",
      "Episode: 181/1000, Average reward: -54.87601568529592\n",
      "Reoading...\n",
      "Episode: 182/1000, Average reward: -27.234026627424218\n",
      "Reoading...\n",
      "Episode: 183/1000, Average reward: -147.12277491584433\n",
      "Reoading...\n",
      "Episode: 184/1000, Average reward: -16.850882829616737\n",
      "Reoading...\n",
      "Episode: 185/1000, Average reward: -190.9341976991282\n",
      "Reoading...\n",
      "Episode: 186/1000, Average reward: -7.541225953930492\n",
      "Reoading...\n",
      "Episode: 187/1000, Average reward: -180.63383491683504\n",
      "Reoading...\n",
      "Episode: 188/1000, Average reward: -123.34173441246817\n",
      "Reoading...\n",
      "Episode: 189/1000, Average reward: -26.028660485456964\n",
      "Reoading...\n",
      "Episode: 190/1000, Average reward: -12.658121311756094\n",
      "Reoading...\n",
      "Episode: 191/1000, Average reward: -36.58308883504839\n",
      "Reoading...\n",
      "Episode: 192/1000, Average reward: -27.475598957491187\n",
      "Reoading...\n",
      "Episode: 193/1000, Average reward: -141.88105762964355\n",
      "Reoading...\n",
      "Episode: 194/1000, Average reward: -149.27908319002964\n",
      "Reoading...\n",
      "Episode: 195/1000, Average reward: -170.58228693464926\n",
      "Reoading...\n",
      "Episode: 196/1000, Average reward: -19.06592409505597\n",
      "Reoading...\n",
      "Episode: 197/1000, Average reward: -199.5437484190023\n",
      "Reoading...\n",
      "Episode: 198/1000, Average reward: -169.13184089520232\n",
      "Reoading...\n",
      "Episode: 199/1000, Average reward: -227.76600507387496\n",
      "Reoading...\n",
      "Episode: 200/1000, Average reward: -165.5778515525865\n",
      "Reoading...\n",
      "Episode: 201/1000, Average reward: -16.369095567637657\n",
      "Reoading...\n",
      "Episode: 202/1000, Average reward: -26.443561928899555\n",
      "Reoading...\n",
      "Episode: 203/1000, Average reward: -19.87142972819317\n",
      "Reoading...\n",
      "Episode: 204/1000, Average reward: -4.7559789035311155\n",
      "Reoading...\n",
      "Episode: 205/1000, Average reward: -246.28937459983203\n",
      "Reoading...\n",
      "Episode: 206/1000, Average reward: -21.899545925803924\n",
      "Reoading...\n",
      "Episode: 207/1000, Average reward: -212.23550212212172\n",
      "Reoading...\n",
      "Episode: 208/1000, Average reward: -26.42717039288514\n",
      "Reoading...\n",
      "Episode: 209/1000, Average reward: -7.669686370901094\n",
      "Reoading...\n",
      "Episode: 210/1000, Average reward: -15.553095813668016\n",
      "Reoading...\n",
      "Episode: 211/1000, Average reward: -11.320121982858208\n",
      "Reoading...\n",
      "Episode: 212/1000, Average reward: -232.7470808022421\n",
      "Reoading...\n",
      "Episode: 213/1000, Average reward: -201.22206715782042\n",
      "Reoading...\n",
      "Episode: 214/1000, Average reward: -22.403784786460275\n",
      "Reoading...\n",
      "Episode: 215/1000, Average reward: -221.7019381999402\n",
      "Reoading...\n",
      "Episode: 216/1000, Average reward: -7.548020892282567\n",
      "Reoading...\n",
      "Episode: 217/1000, Average reward: -209.5758276750047\n",
      "Reoading...\n",
      "Episode: 218/1000, Average reward: -16.32801360565543\n",
      "Reoading...\n",
      "Episode: 219/1000, Average reward: -5.109039661245234\n",
      "Reoading...\n",
      "Episode: 220/1000, Average reward: -8.17272394108753\n",
      "Reoading...\n",
      "Episode: 221/1000, Average reward: -22.521326994397622\n",
      "Reoading...\n",
      "Episode: 222/1000, Average reward: -10.900172009510868\n",
      "Reoading...\n",
      "Episode: 223/1000, Average reward: -231.4081341542873\n",
      "Reoading...\n",
      "Episode: 224/1000, Average reward: -197.671244395631\n",
      "Reoading...\n",
      "Episode: 225/1000, Average reward: -4.804093365082042\n",
      "Reoading...\n",
      "Episode: 226/1000, Average reward: -204.81026041439569\n",
      "Reoading...\n",
      "Episode: 227/1000, Average reward: -18.133283149217455\n",
      "Reoading...\n",
      "Episode: 228/1000, Average reward: -203.9452064879691\n",
      "Reoading...\n",
      "Episode: 229/1000, Average reward: -14.011860620063606\n",
      "Reoading...\n",
      "Episode: 230/1000, Average reward: -16.088775164458173\n",
      "Reoading...\n",
      "Episode: 231/1000, Average reward: -199.11073043816026\n",
      "Reoading...\n",
      "Episode: 232/1000, Average reward: -212.2252572436108\n",
      "Reoading...\n",
      "Episode: 233/1000, Average reward: -13.847652814762032\n",
      "Reoading...\n",
      "Episode: 234/1000, Average reward: -6.935616463838686\n",
      "Reoading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 235/1000, Average reward: -238.51325373659768\n",
      "Reoading...\n",
      "Episode: 236/1000, Average reward: -200.4949435091604\n",
      "Reoading...\n",
      "Episode: 237/1000, Average reward: -222.8104955747995\n",
      "Reoading...\n",
      "Episode: 238/1000, Average reward: -21.746278806028933\n",
      "Reoading...\n",
      "Episode: 239/1000, Average reward: -215.5653209594323\n",
      "Reoading...\n",
      "Episode: 240/1000, Average reward: -17.396279672435927\n",
      "Reoading...\n",
      "Episode: 241/1000, Average reward: -12.749217969028372\n",
      "Reoading...\n",
      "Episode: 242/1000, Average reward: -18.246622934376035\n",
      "Reoading...\n",
      "Episode: 243/1000, Average reward: -12.356212319631867\n",
      "Reoading...\n",
      "Episode: 244/1000, Average reward: -31.971252833360598\n",
      "Reoading...\n",
      "Episode: 245/1000, Average reward: -27.184743575382274\n",
      "Reoading...\n",
      "Episode: 246/1000, Average reward: -10.060917231234736\n",
      "Reoading...\n",
      "Episode: 247/1000, Average reward: -9.049799964034085\n",
      "Reoading...\n",
      "Episode: 248/1000, Average reward: -18.77806497344904\n",
      "Reoading...\n",
      "Episode: 249/1000, Average reward: -10.793560654700986\n",
      "Reoading...\n",
      "Episode: 250/1000, Average reward: -5.202777107508897\n",
      "Reoading...\n",
      "Episode: 251/1000, Average reward: -191.91670467730586\n",
      "Reoading...\n",
      "Episode: 252/1000, Average reward: -206.35860682104249\n",
      "Reoading...\n",
      "Episode: 253/1000, Average reward: -207.5342886519132\n",
      "Reoading...\n",
      "Episode: 254/1000, Average reward: -222.95720925136538\n",
      "Reoading...\n",
      "Episode: 255/1000, Average reward: -15.103561585200382\n",
      "Reoading...\n",
      "Episode: 256/1000, Average reward: -222.72232565790185\n",
      "Reoading...\n",
      "Episode: 257/1000, Average reward: -201.1576761464609\n",
      "Reoading...\n",
      "Episode: 258/1000, Average reward: -27.772028546861932\n",
      "Reoading...\n",
      "Episode: 259/1000, Average reward: -8.290536428144664\n",
      "Reoading...\n",
      "Episode: 260/1000, Average reward: -12.020110804506027\n",
      "Reoading...\n",
      "Episode: 261/1000, Average reward: -215.02993601751865\n",
      "Reoading...\n",
      "Episode: 262/1000, Average reward: -18.46066056765747\n",
      "Reoading...\n",
      "Episode: 263/1000, Average reward: -13.285609451399466\n",
      "Reoading...\n",
      "Episode: 264/1000, Average reward: -20.52768908852926\n",
      "Reoading...\n",
      "Episode: 265/1000, Average reward: -205.4345299389429\n",
      "Reoading...\n",
      "Episode: 266/1000, Average reward: -15.344935505700091\n",
      "Reoading...\n",
      "Episode: 267/1000, Average reward: -6.991298213990209\n",
      "Reoading...\n",
      "Episode: 268/1000, Average reward: -212.69495093799094\n",
      "Reoading...\n",
      "Episode: 269/1000, Average reward: -6.073031526262744\n",
      "Reoading...\n",
      "Episode: 270/1000, Average reward: -189.82592380651758\n",
      "Reoading...\n",
      "Episode: 271/1000, Average reward: -18.242621049187967\n",
      "Reoading...\n",
      "Episode: 272/1000, Average reward: -225.90747549751768\n",
      "Reoading...\n",
      "Episode: 273/1000, Average reward: -217.1994162404221\n",
      "Reoading...\n",
      "Episode: 274/1000, Average reward: -183.87872424063912\n",
      "Reoading...\n",
      "Episode: 275/1000, Average reward: -7.683408136820431\n",
      "Reoading...\n",
      "Episode: 276/1000, Average reward: -7.852066023692938\n",
      "Reoading...\n",
      "Episode: 277/1000, Average reward: -3.0437081305611025\n",
      "Reoading...\n",
      "Episode: 278/1000, Average reward: -4.682823850888868\n",
      "Reoading...\n",
      "Episode: 279/1000, Average reward: -4.595977613717182\n",
      "Reoading...\n",
      "Episode: 280/1000, Average reward: -220.96128862651145\n",
      "Reoading...\n",
      "Episode: 281/1000, Average reward: -4.334515868078761\n",
      "Reoading...\n",
      "Episode: 282/1000, Average reward: -4.505184266286984\n",
      "Reoading...\n",
      "Episode: 283/1000, Average reward: -218.55990970794235\n",
      "Reoading...\n",
      "Episode: 284/1000, Average reward: -214.13567801701163\n",
      "Reoading...\n",
      "Episode: 285/1000, Average reward: -11.800511500673474\n",
      "Reoading...\n",
      "Episode: 286/1000, Average reward: -220.2436103978221\n",
      "Reoading...\n",
      "Episode: 287/1000, Average reward: -4.231649632981034\n",
      "Reoading...\n",
      "Episode: 288/1000, Average reward: -5.8928403952778865\n",
      "Reoading...\n",
      "Episode: 289/1000, Average reward: -7.203534956461725\n",
      "Reoading...\n",
      "Episode: 290/1000, Average reward: -242.34496253928032\n",
      "Reoading...\n",
      "Episode: 291/1000, Average reward: -238.21548931853226\n",
      "Reoading...\n",
      "Episode: 292/1000, Average reward: -18.48504991392441\n",
      "Reoading...\n",
      "Episode: 293/1000, Average reward: -238.2785204717025\n",
      "Reoading...\n",
      "Episode: 294/1000, Average reward: -233.37810700553445\n",
      "Reoading...\n",
      "Episode: 295/1000, Average reward: -236.8193907227706\n",
      "Reoading...\n",
      "Episode: 296/1000, Average reward: -6.3809319621357625\n",
      "Reoading...\n",
      "Episode: 297/1000, Average reward: -222.3660251397199\n",
      "Reoading...\n",
      "Episode: 298/1000, Average reward: -191.25794762255947\n",
      "Reoading...\n",
      "Episode: 299/1000, Average reward: -6.4613088735731194\n",
      "Reoading...\n",
      "Episode: 300/1000, Average reward: -18.667767653975243\n",
      "Reoading...\n",
      "Episode: 301/1000, Average reward: -214.07980235391335\n",
      "Reoading...\n",
      "Episode: 302/1000, Average reward: -210.94412112047482\n",
      "Reoading...\n",
      "Episode: 303/1000, Average reward: -211.8302056599082\n",
      "Reoading...\n",
      "Episode: 304/1000, Average reward: -217.6569366207216\n",
      "Reoading...\n",
      "Episode: 305/1000, Average reward: -13.117751142020678\n",
      "Reoading...\n",
      "Episode: 306/1000, Average reward: -242.88812046411633\n",
      "Reoading...\n",
      "Episode: 307/1000, Average reward: -18.95181762994698\n",
      "Reoading...\n",
      "Episode: 308/1000, Average reward: -236.066520977202\n",
      "Reoading...\n",
      "Episode: 309/1000, Average reward: -7.8823438983721275\n",
      "Reoading...\n",
      "Episode: 310/1000, Average reward: -208.9394147555213\n",
      "Reoading...\n",
      "Episode: 311/1000, Average reward: -244.44854982534642\n",
      "Reoading...\n",
      "Episode: 312/1000, Average reward: -5.6363846921280665\n",
      "Reoading...\n",
      "Episode: 313/1000, Average reward: -219.31377183353638\n",
      "Reoading...\n",
      "Episode: 314/1000, Average reward: -5.14536853563046\n",
      "Reoading...\n",
      "Episode: 315/1000, Average reward: -3.712002426883606\n",
      "Reoading...\n",
      "Episode: 316/1000, Average reward: -235.22828282845737\n",
      "Reoading...\n",
      "Episode: 317/1000, Average reward: -7.995678105064327\n",
      "Reoading...\n",
      "Episode: 318/1000, Average reward: -5.972060180413336\n",
      "Reoading...\n",
      "Episode: 319/1000, Average reward: -20.90950950048679\n",
      "Reoading...\n",
      "Episode: 320/1000, Average reward: -3.88670341866605\n",
      "Reoading...\n",
      "Episode: 321/1000, Average reward: -9.613908725845155\n",
      "Reoading...\n",
      "Episode: 322/1000, Average reward: -7.1906519724414615\n",
      "Reoading...\n",
      "Episode: 323/1000, Average reward: -8.062703686992604\n",
      "Reoading...\n",
      "Episode: 324/1000, Average reward: -5.488080722415225\n",
      "Reoading...\n",
      "Episode: 325/1000, Average reward: -223.28404972144833\n",
      "Reoading...\n",
      "Episode: 326/1000, Average reward: -239.34245245607863\n",
      "Reoading...\n",
      "Episode: 327/1000, Average reward: -3.9925100466958123\n",
      "Reoading...\n",
      "Episode: 328/1000, Average reward: -235.17968076901508\n",
      "Reoading...\n",
      "Episode: 329/1000, Average reward: -16.012451938938217\n",
      "Reoading...\n",
      "Episode: 330/1000, Average reward: -18.122491628021848\n",
      "Reoading...\n",
      "Episode: 331/1000, Average reward: -6.074197248079718\n",
      "Reoading...\n",
      "Episode: 332/1000, Average reward: -4.141678953281202\n",
      "Reoading...\n",
      "Episode: 333/1000, Average reward: -10.02933390288479\n",
      "Reoading...\n",
      "Episode: 334/1000, Average reward: -19.390425186385773\n",
      "Reoading...\n",
      "Episode: 335/1000, Average reward: -238.4668547983202\n",
      "Reoading...\n",
      "Episode: 336/1000, Average reward: -214.02673086286433\n",
      "Reoading...\n",
      "Episode: 337/1000, Average reward: -241.7396760865992\n",
      "Reoading...\n",
      "Episode: 338/1000, Average reward: -4.573630293840171\n",
      "Reoading...\n",
      "Episode: 339/1000, Average reward: -244.12681851122122\n",
      "Reoading...\n",
      "Episode: 340/1000, Average reward: -8.061435831817025\n",
      "Reoading...\n",
      "Episode: 341/1000, Average reward: -241.86746696590092\n",
      "Reoading...\n",
      "Episode: 342/1000, Average reward: -4.73583146134923\n",
      "Reoading...\n",
      "Episode: 343/1000, Average reward: -232.21343544056907\n",
      "Reoading...\n",
      "Episode: 344/1000, Average reward: -240.51970034910224\n",
      "Reoading...\n",
      "Episode: 345/1000, Average reward: -241.63636998741086\n",
      "Reoading...\n",
      "Episode: 346/1000, Average reward: -221.92722542551363\n",
      "Reoading...\n",
      "Episode: 347/1000, Average reward: -221.8421649919728\n",
      "Reoading...\n",
      "Episode: 348/1000, Average reward: -5.597599161418589\n",
      "Reoading...\n",
      "Episode: 349/1000, Average reward: -228.67569795482078\n",
      "Reoading...\n",
      "Episode: 350/1000, Average reward: -11.551172391967262\n",
      "Reoading...\n",
      "Episode: 351/1000, Average reward: -204.94879580950587\n",
      "Reoading...\n",
      "Episode: 352/1000, Average reward: -223.53764393616822\n",
      "Reoading...\n",
      "Episode: 353/1000, Average reward: -217.51479571308337\n",
      "Reoading...\n",
      "Episode: 354/1000, Average reward: -6.377076245253309\n",
      "Reoading...\n",
      "Episode: 355/1000, Average reward: -233.81395813613833\n",
      "Reoading...\n",
      "Episode: 356/1000, Average reward: -234.79431947807092\n",
      "Reoading...\n",
      "Episode: 357/1000, Average reward: -19.58288949816165\n",
      "Reoading...\n",
      "Episode: 358/1000, Average reward: -12.357875516870083\n",
      "Reoading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 359/1000, Average reward: -14.874485794090768\n",
      "Reoading...\n",
      "Episode: 360/1000, Average reward: -5.766534066348461\n",
      "Reoading...\n",
      "Episode: 361/1000, Average reward: -6.461649292876534\n",
      "Reoading...\n",
      "Episode: 362/1000, Average reward: -229.39450553532805\n",
      "Reoading...\n",
      "Episode: 363/1000, Average reward: -5.837369518709666\n",
      "Reoading...\n",
      "Episode: 364/1000, Average reward: -7.286323614103021\n",
      "Reoading...\n",
      "Episode: 365/1000, Average reward: -6.072270582789088\n",
      "Reoading...\n",
      "Episode: 366/1000, Average reward: -6.873588553536047\n",
      "Reoading...\n",
      "Episode: 367/1000, Average reward: -11.95544119414678\n",
      "Reoading...\n",
      "Episode: 368/1000, Average reward: -235.93853126842802\n",
      "Reoading...\n",
      "Episode: 369/1000, Average reward: -238.57979977849675\n",
      "Reoading...\n",
      "Episode: 370/1000, Average reward: -221.28888782898946\n",
      "Reoading...\n",
      "Episode: 371/1000, Average reward: -221.39677700090886\n",
      "Reoading...\n",
      "Episode: 372/1000, Average reward: -7.254906434603169\n",
      "Reoading...\n",
      "Episode: 373/1000, Average reward: -238.73216494788656\n",
      "Reoading...\n",
      "Episode: 374/1000, Average reward: -207.05178762733706\n",
      "Reoading...\n",
      "Episode: 375/1000, Average reward: -229.60019684934863\n",
      "Reoading...\n",
      "Episode: 376/1000, Average reward: -17.920048600522193\n",
      "Reoading...\n",
      "Episode: 377/1000, Average reward: -214.85142215367202\n",
      "Reoading...\n",
      "Episode: 378/1000, Average reward: -6.7611940578814105\n",
      "Reoading...\n",
      "Episode: 379/1000, Average reward: -209.00511917554218\n",
      "Reoading...\n",
      "Episode: 380/1000, Average reward: -222.9420001316796\n",
      "Reoading...\n",
      "Episode: 381/1000, Average reward: -186.96561290663408\n",
      "Reoading...\n",
      "Episode: 382/1000, Average reward: -225.9658070729538\n",
      "Reoading...\n",
      "Episode: 383/1000, Average reward: -226.22689884600217\n",
      "Reoading...\n",
      "Episode: 384/1000, Average reward: -12.469248099074552\n",
      "Reoading...\n",
      "Episode: 385/1000, Average reward: -170.8578465748462\n",
      "Reoading...\n",
      "Episode: 386/1000, Average reward: -44.663953761106214\n",
      "Reoading...\n",
      "Episode: 387/1000, Average reward: -28.272488384971915\n",
      "Reoading...\n",
      "Episode: 388/1000, Average reward: -62.79469639954113\n",
      "Reoading...\n",
      "Episode: 389/1000, Average reward: -36.08648240252412\n",
      "Reoading...\n",
      "Episode: 390/1000, Average reward: -44.59196858159842\n",
      "Reoading...\n",
      "Episode: 391/1000, Average reward: -20.259162384138815\n",
      "Reoading...\n",
      "Episode: 392/1000, Average reward: -358.24023053840745\n",
      "Reoading...\n",
      "Episode: 393/1000, Average reward: -41.799229358300394\n",
      "Reoading...\n",
      "Episode: 394/1000, Average reward: -174.27012988364774\n",
      "Reoading...\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147023170, 'The remote procedure call failed.', None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7b38c913cf07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;31m# Advance the game to the next frame based on the action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# Stop the simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ACABRE~1\\AppData\\Local\\Temp\\gen_py\\3.6\\9A7C13B7-EEBA-4845-9DFE-BDF71229CE5Bx0x11x0\\ISimulation.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;34m'Runs a single simulation step.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvokeTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1610874881\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLCID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# The method SetAttValue is actually a property, but must be used as a method to correctly pass the arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147023170, 'The remote procedure call failed.', None, None)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize simulation\n",
    "    if 'Vissim' not in globals() or Vissim == None:\n",
    "        Vissim, Simulation, Network, cache_flag = COMServerDispatch(reset_flag = True)\n",
    "    else:\n",
    "        Simulation, Network = COMServerReload(Vissim, reset_flag = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser()\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    Agents = [DQNAgent(state_size, action_size, ID) for ID in npa.signal_controllers_ids] \n",
    "    \n",
    "    # Load previous trained data\n",
    "    if load_trained:\n",
    "        print('Loading Pre-Trained Data')\n",
    "        for index, agent in enumerate(Agents):\n",
    "            Filename = os.path.join(vissim_working_directory, model_name, 'Agent'+str(index)+'_'+model_name+'.h5')\n",
    "            agent.model = load_model(Filename)\n",
    "    \n",
    "    # Iterations of the simulation\n",
    "    for e in range(episodes):\n",
    "        done = False\n",
    "        # If not the first episode, reset state at the start\n",
    "        if e != 0:\n",
    "            Simulation, Network = COMServerReload(Vissim, reset_flag = False)\n",
    "            npa = NetworkParser() \n",
    "            for index, agent in enumerate(Agents):\n",
    "                agent.update_IDS(npa.signal_controllers_ids[index])\n",
    "                agent.episode_reward = []\n",
    "        \n",
    "        # Change demand for every episode\n",
    "        if Random_Demand:\n",
    "            for vehicle_input in range(1,5):\n",
    "                Vissim.Net.VehicleInputs.ItemByKey(vehicle_input).SetAttValue('Volume(1)', demands[np.random.randint(0,6)])\n",
    "        \n",
    "        # Use max speed for Simulator\n",
    "        if Quickmode:\n",
    "            # Set speed parameters in Vissim\n",
    "            Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)\n",
    "            Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",1)\n",
    "            Vissim.SuspendUpdateGUI()\n",
    "\n",
    "        # Set cycle time to start of cycle\n",
    "        cycle_t = 0\n",
    "\n",
    "        # time_t represents each timestep of the simulation\n",
    "        for time_t in range(simulation_length+1):\n",
    "            # If the cycle for the current program is over\n",
    "            if cycle_t == 900:\n",
    "                for agent in Agents:\n",
    "                    agent.newstate = agent.get_state()\n",
    "                    agent.action   = agent.act(agent.newstate)\n",
    "                    agent.reward   = agent.get_reward()\n",
    "                    agent.memory   = agent.remember(agent.state, agent.action, agent.reward, agent.newstate)\n",
    "                    agent.state    = agent.newstate\n",
    "                cycle_t = 0\n",
    "            else:\n",
    "                cycle_t += 1\n",
    "\n",
    "            # Advance the game to the next frame based on the action.\n",
    "            Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "        # Stop the simulation    \n",
    "        Vissim.Simulation.Stop()\n",
    "\n",
    "        # Calculate episode average reward\n",
    "        average_reward = []\n",
    "        for agent in Agents:\n",
    "            average_agent_reward = np.average(agent.episode_reward)\n",
    "            average_reward.append(average_agent_reward)\n",
    "        average_reward = np.average(average_reward)\n",
    "\n",
    "        if len(Agents)>1:\n",
    "            # Print the score and break out of the loop\n",
    "            print(\"Episode: {}/{}, Average reward: {}\".format(e+1, episodes, average_reward))\n",
    "            for agent in enumerate(Agents):\n",
    "                print(\"Agent {}, Average agent reward: {}\".format(agent, average_agent_reward[agent]))\n",
    "        else:\n",
    "            print(\"Episode: {}/{}, Average reward: {}\".format(e+1, episodes, average_reward))\n",
    "        \n",
    "        done = True\n",
    "        # Train agent with experience of episode (indicated batch size)\n",
    "        agent.replay(32)\n",
    "        \n",
    "        if e%200 == 0:\n",
    "            for index, agent in enumerate(Agents):\n",
    "                Filename = os.path.join(vissim_working_directory, model_name, 'PartialSave_'+str(e)+'_Agent'+str(index)+'_'+model_name+'.h5')\n",
    "                agent.model.save(Filename)\n",
    "\n",
    "\n",
    "    #Saving agents memory, weights and optimizer\n",
    "    for index,agent in enumerate(Agents):    \n",
    "        Filename = os.path.join(vissim_working_directory, model_name, 'Agent'+str(index)+'_'+model_name+'.h5')\n",
    "        print('Saving architecture, weights and optimizer state for agent{}'.format(index))\n",
    "        agent.model.save(Filename)\n",
    "    print('Model Trained and Saved. Succesfully Terminated.')\n",
    "    \n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.reshape([0,0,0,0], [1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-186.65543, -200.45123, -114.64673, -199.88553, -198.01128]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =[100,0,100,0]\n",
    "b = np.reshape(a,[1,4])\n",
    "a1=Agents[0].model.predict(b)\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1=Agents[0].model.predict(np.reshape([0,0,0,0], [1,4]))\n",
    "a2=Agents[0].model.predict(np.reshape([0,30,0,30], [1,4]))\n",
    "a1-a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([0,20,0,20])\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = random.sample(Agents[0].memory, 3)\n",
    "for state, action, reward, next_state in minibatch:\n",
    "    st = state\n",
    "    st = np.reshape(st, [1,4])\n",
    "\n",
    "print(type(st))\n",
    "st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving agents memory\n",
    "    for index,agent in enumerate(Agents):    \n",
    "        Filename = os.path.join(vissim_working_directory, model_name, 'Agent'+str(index)+'_'+model_name+'.h5')\n",
    "        print('Saving architecture, weights and optimizer state for agent{}'.format(index))\n",
    "        agent.model.save(Filename)\n",
    "    print('Model Trained and Saved. Succesfully Terminated.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
