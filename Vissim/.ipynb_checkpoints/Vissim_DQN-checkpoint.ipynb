{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import Simulator_Functions as SF\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 400\n",
    "partial_save_at = 100\n",
    "copy_weights_frequency = 5\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 7\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = (simulation_length-1)*5+1\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "\n",
    "# Hyperparameters\n",
    "PER_activated = True\n",
    "batch_size = 256\n",
    "memory_size = 1000\n",
    "alpha   = 0.0001\n",
    "gamma   = 0.95\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [1 * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "\n",
    "# Demand Schedule\n",
    "high_demand = 600\n",
    "low_demand = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"populate\" = population of memory, generation of initial memory file\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "## Network Model Parameters\n",
    "\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "agent_type = 'DQN' # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'\n",
    "state_type  = 'Queues'\n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "SaveResultsAgent = False\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Session ID\n",
    "Session_ID = 'Episodes_'+str(episodes)+'_Agent_'+agent_type+\"_Actions_\"+action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting exploration schedule\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = np.array(range(1,episodes+1))\n",
    "y_series = epsilon_sequence[0:episodes]\n",
    "plt.plot(x_series, y_series, '-b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Ratio of random exploration')\n",
    "plt.title('Exploration schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Loading Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 18001 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Deploying instance of Standard Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DQN.\n",
      "Previous Experience Found: Loading into agent\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "PER memory pre-populated. Starting Training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a498b186cc4672b3a16f31604996db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=400)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update counter is: 1\n",
      "Episode: 1/400, Epsilon:1.0, Average reward: -64.14\n",
      "Prediction for [500,0,500,0] is: [[-0.00014277 -0.00027938]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes_400_Agent_DQN_Actions_phases_BestAgent0_Memory.p\n",
      "Update counter is: 1\n",
      "Episode: 2/400, Epsilon:0.98, Average reward: -92.5\n",
      "Prediction for [500,0,500,0] is: [[-0.00106238 -0.00257535]]\n",
      "Update counter is: 1\n",
      "Episode: 3/400, Epsilon:0.97, Average reward: -57.38\n",
      "Prediction for [500,0,500,0] is: [[-0.00204068 -0.0049377 ]]\n",
      "New best agent found. Saved in C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Single_Cross_Straight_Episodes_400_Agent_DQN_Actions_phases_BestAgent0_Memory.p\n",
      "Update counter is: 1\n",
      "Episode: 4/400, Epsilon:0.95, Average reward: -78.51\n",
      "Prediction for [500,0,500,0] is: [[-0.00297807 -0.00737356]]\n",
      "Update counter is: 1\n",
      "Episode: 5/400, Epsilon:0.93, Average reward: -231.5\n",
      "Prediction for [500,0,500,0] is: [[-0.00391206 -0.00958232]]\n",
      "Weights succesfully copied to Target model.\n",
      "Update counter is: 1\n",
      "Episode: 6/400, Epsilon:0.92, Average reward: -114.38\n",
      "Prediction for [500,0,500,0] is: [[-0.00520023 -0.01182117]]\n",
      "Update counter is: 1\n",
      "Episode: 7/400, Epsilon:0.9, Average reward: -99.13\n",
      "Prediction for [500,0,500,0] is: [[-0.00667659 -0.01458439]]\n",
      "Update counter is: 1\n",
      "Episode: 8/400, Epsilon:0.89, Average reward: -112.55\n",
      "Prediction for [500,0,500,0] is: [[-0.00825994 -0.01728226]]\n",
      "Update counter is: 1\n",
      "Episode: 9/400, Epsilon:0.87, Average reward: -254.49\n",
      "Prediction for [500,0,500,0] is: [[-0.00969927 -0.01986278]]\n",
      "Update counter is: 1\n",
      "Episode: 10/400, Epsilon:0.86, Average reward: -150.8\n",
      "Prediction for [500,0,500,0] is: [[-0.01117567 -0.02247146]]\n",
      "Weights succesfully copied to Target model.\n",
      "Update counter is: 1\n",
      "Episode: 11/400, Epsilon:0.84, Average reward: -95.63\n",
      "Prediction for [500,0,500,0] is: [[-0.01267894 -0.02525802]]\n",
      "Update counter is: 1\n",
      "Episode: 12/400, Epsilon:0.83, Average reward: -267.97\n",
      "Prediction for [500,0,500,0] is: [[-0.01419675 -0.02791437]]\n",
      "Update counter is: 1\n",
      "Episode: 13/400, Epsilon:0.81, Average reward: -153.68\n",
      "Prediction for [500,0,500,0] is: [[-0.01562377 -0.03070736]]\n",
      "Update counter is: 1\n",
      "Episode: 14/400, Epsilon:0.8, Average reward: -149.92\n",
      "Prediction for [500,0,500,0] is: [[-0.01732509 -0.0331797 ]]\n",
      "Update counter is: 1\n",
      "Episode: 15/400, Epsilon:0.78, Average reward: -168.08\n",
      "Prediction for [500,0,500,0] is: [[-0.01899503 -0.03567563]]\n",
      "Weights succesfully copied to Target model.\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147467259), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bdee868511e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mSF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSet_Quickmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             SF.run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, timesteps_per_second,\\\n\u001b[1;32m--> 120\u001b[1;33m                                       seconds_per_green, seconds_per_yellow, mode, PER_activated)\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;31m# Calculate episode average reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mrun_simulation_episode\u001b[1;34m(Agents, Vissim, state_type, state_size, simulation_length, timesteps_per_second, seconds_per_green, seconds_per_yellow, mode, PER_activated)\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;31m# Advance the game to the next second (proportionally to the simulator resolution).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                         \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147467259), None)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                memory_population_length, timesteps_per_second,\\\n",
    "                                                                delete_results = True, verbose = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type == \"DQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = False, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif agent_type == \"DuelingDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = False, Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif agent_type == \"DDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True, Dueling = False) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    elif agent_type == \"DuelingDDQN\":\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True, Dueling = True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "    if mode == \"demo\":\n",
    "        timesteps_per_second = 10\n",
    "        Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "        Agents = SF.load_agents(vissim_working_directory, model_name, Agents, Session_ID, best = True)\n",
    "        for agent in Agents:\n",
    "            agent.epsilon = 0\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length,\\\n",
    "                                  timesteps_per_second, seconds_per_green, seconds_per_yellow, mode, PER_activated)\n",
    "        Vissim = None\n",
    "    \n",
    "    ## EXECUTION IN DEBUGGING MODE (slow, extra messages)\n",
    "    elif mode == \"debug\":\n",
    "        timesteps_per_second = 10\n",
    "        Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length,\\\n",
    "                                  timesteps_per_second, seconds_per_green, seconds_per_yellow, mode, PER_activated)\n",
    "        Vissim = None\n",
    "        \n",
    "    ## EXECUTION OF MEMORY POPULATION and creation of memory files\n",
    "    elif mode == \"populate\":\n",
    "        SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "        if PER_activated:\n",
    "            memory, Agents, runflag = SF.PER_prepopulate_memory(Agents, Vissim, state_type, state_size, memory_size,\\\n",
    "                                               vissim_working_directory, model_name, Session_ID, seconds_per_green,\\\n",
    "                                                                seconds_per_green, timesteps_per_second)\n",
    "            print(\"PER memory prepopulated with {} entries\".format(memory_size))\n",
    "        Vissim = None\n",
    "        \n",
    "    ## EXECUTION OF TEST MODE (fast, best agents, more metrics out)    \n",
    "    elif mode == \"test\":\n",
    "        pass\n",
    "    \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\":\n",
    "        # Load previous memory if available, else create it\n",
    "        if PER_activated:\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            memory, Agents, runflag = SF.PER_prepopulate_memory(Agents, Vissim, state_type, state_size, memory_size,\\\n",
    "                                               vissim_working_directory, model_name, Session_ID, seconds_per_green,\\\n",
    "                                                                seconds_per_green, timesteps_per_second)\n",
    "            print('PER memory pre-populated. Starting Training.\\n')\n",
    "        else:\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, state_size, memory_population_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow, mode, PER_activated)\n",
    "            print('Normal memory pre-populated. Starting Training.\\n')\n",
    "            runflag = True\n",
    "            \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                # Run Network Parser and ensure agents are linked to their intersections\n",
    "                #npa.update(Vissim) \n",
    "                npa = NetworkParser(Vissim)\n",
    "                for index, agent in enumerate(Agents):\n",
    "                    #agent.update_IDS(npa.signal_controllers_ids[index], npa)\n",
    "                    agent.update_IDS(agent.signal_id, npa)\n",
    "                    agent.episode_reward = []\n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "        \n",
    "            # Run Episode at maximum speed\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, mode, PER_activated)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights = SF.best_agent(reward_storage, average_reward, best_agent_weights,\\\n",
    "                                               vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "            # Train agent with experience of episode and copy weights when necessary\n",
    "            # Update exploration rate\n",
    "            for agent in Agents:\n",
    "                agent.replay_batch(batch_size, episode)\n",
    "                agent.epsilon = epsilon_sequence[episode+1]\n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
