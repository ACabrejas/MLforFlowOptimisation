{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import math\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from Simulator_Functions import Select_Vissim_Mode\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PER\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 25\n",
    "partial_save_at =  5 # 100 \n",
    "copy_weights_frequency = 2 #10\n",
    "\n",
    "# Hyperparameters\n",
    "Surtrac = False\n",
    "PER_activated = True\n",
    "batch_size = 64\n",
    "memory_size = 1024\n",
    "alpha   = 0.000065\n",
    "gamma   = 0.95\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = batch_size*seconds_per_green*2 +1\n",
    "if PER_activated:\n",
    "    memory_population_length = int(memory_size*seconds_per_green*1.6) +1\n",
    "\n",
    "# Vissim autosave the result of the simulation    \n",
    "delete_results = True\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = 4\n",
    "action_size = 8\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP)\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":600, 'm':300, 'l':150}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep_25_A_DuelingDDQN_State_Queues_Act_phases_Rew_Queues\n"
     ]
    }
   ],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"populate\" = population of memory, generation of initial memory file\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "if mode == 'demo' :\n",
    "    simulation_length = 3600\n",
    "    demand_list = [[demand['l'], demand['l']]]\n",
    "    demand_change_timesteps = simulation_length\n",
    "    \n",
    "    \n",
    "if mode == 'test' : \n",
    "    simulation_length = 3600\n",
    "    demand_change_timesteps = 450\n",
    "    demand = {\"h\":800, 'm':400, 'l':200}\n",
    "    demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "                  [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "                  [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "                  [demand['l'], demand['h']], [demand['l'], demand['m']]]\n",
    "    delete_results = False\n",
    "\n",
    "model_name  = 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "# vissim_working_directory = 'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "agent_type = 'DuelingDDQN'        # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'   \n",
    "# 'Queues'          Sum of the queues for all lanes in intersection\n",
    "# 'QueuesDiff'      Difference in queue lengths in last timestep\n",
    "# 'QueuesDiffSC'    10000* QueuesDiff - Queues^2\n",
    "# 'TotalDelayDiff'\n",
    "\n",
    "state_type  = 'Queues'    # 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig'\n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Session ID\n",
    "#Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "\n",
    "# Adding the state type to the Session_ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_State_\"+state_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "print(Session_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting exploration schedule\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = np.array(range(1,episodes+1))\n",
    "y_series = epsilon_sequence[0:episodes]\n",
    "plt.plot(x_series, y_series, '-b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Ratio of random exploration')\n",
    "plt.title('Exploration schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 9831 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DuelingDDQN.\n",
      "After 0 timesteps, memory is 0.0 percent full\n",
      "After 1000 timesteps, memory is 8.59 percent full\n",
      "After 2000 timesteps, memory is 17.38 percent full\n",
      "After 3000 timesteps, memory is 26.07 percent full\n",
      "After 4000 timesteps, memory is 34.96 percent full\n",
      "After 5000 timesteps, memory is 43.65 percent full\n",
      "After 6000 timesteps, memory is 52.44 percent full\n",
      "After 7000 timesteps, memory is 60.94 percent full\n",
      "After 8000 timesteps, memory is 69.34 percent full\n",
      "After 9000 timesteps, memory is 77.93 percent full\n",
      "After 10000 timesteps, memory is 86.43 percent full\n",
      "After 11000 timesteps, memory is 95.21 percent full\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(32, 4), b.shape=(4, 64), m=32, n=64, k=4\n\t [[{{node dense/MatMul}}]]\n\t [[{{node policy/add}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f4b7f339b877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m                                                         \u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSession_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                                                         \u001b[0mseconds_per_green\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_green\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                                                         demand_list, demand_change_timesteps, PER_activated)\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Memory pre-populated. Starting Training.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mprepopulate_memory\u001b[1;34m(Agents, Vissim, state_type, reward_type, state_size, memory_size, vissim_working_directory, model_name, Session_ID, seconds_per_green, seconds_per_yellow, timesteps_per_second, demand_list, demand_change_timesteps, PER_activated)\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;31m# Fit once to calculate importance sampling weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                         \u001b[0mupdate_priority_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# Stop the simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                 \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mupdate_priority_weights\u001b[1;34m(agent, memory_size)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoubleDQN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m                 \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnext_action\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       return training_arrays.predict_loop(\n\u001b[1;32m-> 1113\u001b[1;33m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(32, 4), b.shape=(4, 64), m=32, n=64, k=4\n\t [[{{node dense/MatMul}}]]\n\t [[{{node policy/add}}]]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                memory_population_length if mode == 'training' else simulation_length , timesteps_per_second,\\\n",
    "                                                                delete_results = delete_results, verbose = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or agent_type ==\"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or agent_type == \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION, POPULATION, DEBUG OR TEST ITERATION\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents, reward_storage = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = True)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0 #1\n",
    "        \n",
    "        # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"populate\":\n",
    "            if PER_activated:\n",
    "                memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                                vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                                seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                                demand_list, demand_change_timesteps, PER_activated)\n",
    "                print(\"PER memory prepopulated with {} entries\".format(memory_size))\n",
    "        \n",
    "        Vissim = None\n",
    "     \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\":\n",
    "        # Load previous memory if available, else create it\n",
    "        SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "        memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "        print('Memory pre-populated. Starting Training.\\n')\n",
    "        \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                # Run Network Parser and ensure agents are linked to their intersections\n",
    "                npa = NetworkParser(Vissim)\n",
    "                for index, agent in enumerate(Agents):\n",
    "                    agent.update_IDS(agent.signal_id, npa)\n",
    "                    agent.episode_reward = []\n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "                    \n",
    "            # Run Episode at maximum speed\n",
    "            SF.Select_Vissim_Mode(Vissim, mode)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "           \n",
    "             # Train agent with experience of episode and copy weights when necessary\n",
    "            # Update exploration rate\n",
    "            for agent in Agents: \n",
    "                for _ in range(3):\n",
    "                    agent.learn_batch(batch_size, episode)\n",
    "            # Copy weights \n",
    "                if (episode+1) % agent.copy_weights_frequency == 0 and episode != 0:\n",
    "                    agent.copy_weights()\n",
    "                agent.epsilon = epsilon_sequence[episode+1]\n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHORT PRETRAINED FROM MEMORY DEMO\n",
    "# Initialize storage\n",
    "reward_storage = []\n",
    "best_agent_weights = []\n",
    "best_agent_memory = []\n",
    "reward_plot = np.zeros([episodes,])\n",
    "loss_plot = np.zeros([episodes,])\n",
    "\n",
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n",
    "\n",
    "# Setting Random Seed\n",
    "Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "# Deploy Network Parser (crawl network)\n",
    "npa = NetworkParser(Vissim)\n",
    "print('NetworkParser has succesfully crawled the model network.')\n",
    "\n",
    "# Initialize agents\n",
    "if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "    Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                       gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                       DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                       Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "    agents_deployed = True\n",
    "else:\n",
    "    print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "    quit()\n",
    "if agents_deployed:\n",
    "    print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "\n",
    "#    memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "#                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "#                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "#                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "#print('Memory pre-populated. Starting Training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_heads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups[i].SetAttValue(\"SigState\", \"RED\")\n",
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SignalHeadsCollection and unpack the SignalHeads into a list by SignalController\n",
    "signal_heads = [[] for _ in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    print(SC)\n",
    "    for SG in range(signal_controllers[SC].SGs.Count):\n",
    "        print(SG)\n",
    "        signal_heads[SC].append(toList(signal_groups[SC][SG].SigHeads.GetAll())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanes = [[[] for b in range(len(signal_heads[a])) ] for a in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    for SH in range(len(signal_heads[SC])):\n",
    "        lanes[SC][SH].append(signal_heads[SC][SH].Lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser2(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa.signal_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(6000):\n",
    "    if i % 5 == 0:\n",
    "        Agents[0].copy_weights()\n",
    "    Agents[0].learn_batch(64, 0)\n",
    "    print(\"Epoch {}:\".format(i))\n",
    "    print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape([50,0,50,0], [1,4])))\\\n",
    "          + (\"OK\" if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1]  else \"NO\"))\n",
    "    true1 = True if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1] else False\n",
    "    print(\"Prediction for [0,50,0,50] is: {}\".format(Agents[0].model.predict(np.reshape([0,50,0,50], [1,4])))\\\n",
    "         + (\"OK\" if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1]  else \"NO\"))\n",
    "    true2 = True if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1] else False\n",
    "    if true1 and true2 == True and i>100:\n",
    "        print(\"FOUND CANDIDATE AT EPOCH {}. TERMINATING\".format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A TEST RUN\n",
    "SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "#timesteps_per_second = 10\n",
    "#Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)'))\n",
    "print(Vissim.Net.QueueCounters.ItemByKey(5).AttValue('QStops(Current,Last)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(Filename+\"bla\"):\n",
    "    Vissim.LoadNet(Filename+\"bla\")\n",
    "else:\n",
    "    raise Exception(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.SetAttValue('SimPeriod', sim_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
