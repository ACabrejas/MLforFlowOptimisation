{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import Simulator_Functions as SF\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 13\n",
    "partial_save_at = 100\n",
    "copy_weights_frequency = 3\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = 4\n",
    "action_size = 8\n",
    "\n",
    "# Hyperparameters\n",
    "PER_activated = True\n",
    "batch_size = 256\n",
    "memory_size = 256*4\n",
    "alpha   = 0.001\n",
    "gamma   = 0.95\n",
    "memory_population_length = 1024*10\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP)\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":800, 'm':400, 'l':200}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"populate\" = population of memory, generation of initial memory file\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "## Network Model Parameters\n",
    "\n",
    "model_name  = 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "agent_type = 'DuelingDDQN'         # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'   \n",
    "# 'Queues'          Sum of the queues for all lanes in intersection\n",
    "# 'QueuesDiff'      Difference in queue lengths in last timestep\n",
    "# 'QueuesDiffSC'    10000* QueuesDiff - Queues^2\n",
    "# 'TotalDelayDiff'\n",
    "state_type  = 'Queues'    \n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Session ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting exploration schedule\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = np.array(range(1,episodes+1))\n",
    "y_series = epsilon_sequence[0:episodes]\n",
    "plt.plot(x_series, y_series, '-b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Ratio of random exploration')\n",
    "plt.title('Exploration schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                memory_population_length, timesteps_per_second,\\\n",
    "                                                                delete_results = True, verbose = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION, POPULATION, DEBUG OR TEST ITERATION\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = True)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0\n",
    "        \n",
    "        # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"populate\":\n",
    "            if PER_activated:\n",
    "                memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                                vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                                seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                                demand_list, demand_change_timesteps, PER_activated)\n",
    "                print(\"PER memory prepopulated with {} entries\".format(memory_size))\n",
    "        \n",
    "        Vissim = None\n",
    "     \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\":\n",
    "        # Load previous memory if available, else create it\n",
    "        SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "        memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "        print('Memory pre-populated. Starting Training.\\n')\n",
    "        \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                # Run Network Parser and ensure agents are linked to their intersections\n",
    "                npa = NetworkParser(Vissim)\n",
    "                for index, agent in enumerate(Agents):\n",
    "                    agent.update_IDS(agent.signal_id, npa)\n",
    "                    agent.episode_reward = []\n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "                    \n",
    "            # Run Episode at maximum speed\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "            # Train agent with experience of episode and copy weights when necessary\n",
    "            # Update exploration rate\n",
    "            for agent in Agents:\n",
    "                agent.learn_batch(batch_size, episode)\n",
    "                agent.epsilon = epsilon_sequence[episode+1]\n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHORT PRETRAINED FROM MEMORY DEMO\n",
    "# Initialize storage\n",
    "reward_storage = []\n",
    "best_agent_weights = []\n",
    "best_agent_memory = []\n",
    "reward_plot = np.zeros([episodes,])\n",
    "loss_plot = np.zeros([episodes,])\n",
    "\n",
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n",
    "\n",
    "# Setting Random Seed\n",
    "Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "# Deploy Network Parser (crawl network)\n",
    "npa = NetworkParser(Vissim)\n",
    "print('NetworkParser has succesfully crawled the model network.')\n",
    "\n",
    "# Initialize agents\n",
    "if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "    Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                       gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                       DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                       Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "    agents_deployed = True\n",
    "else:\n",
    "    print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "    quit()\n",
    "if agents_deployed:\n",
    "    print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "\n",
    "#    memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "#                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "#                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "#                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "#print('Memory pre-populated. Starting Training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = NetworkParser(Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents[0].signal_groups[i].SetAttValue(\"SigState\", \"AMBER\")\n",
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanes = [[[] for b in range(len(signal_heads[a])) ] for a in signal_controllers_ids]\n",
    "for SC in signal_controllers_ids:\n",
    "    for SH in range(len(signal_heads[SC])):\n",
    "        lanes[SC][SH].append(signal_heads[SC][SH].Lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(6000):\n",
    "    if i % 5 == 0:\n",
    "        Agents[0].copy_weights()\n",
    "    Agents[0].learn_batch(64, 0)\n",
    "    print(\"Epoch {}:\".format(i))\n",
    "    print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape([50,0,50,0], [1,4])))\\\n",
    "          + (\"OK\" if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1]  else \"NO\"))\n",
    "    true1 = True if Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][0] < Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))[0][1] else False\n",
    "    print(\"Prediction for [0,50,0,50] is: {}\".format(Agents[0].model.predict(np.reshape([0,50,0,50], [1,4])))\\\n",
    "         + (\"OK\" if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1]  else \"NO\"))\n",
    "    true2 = True if Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][0] > Agents[0].model.predict(np.reshape([0,50,0,50], [1,4]))[0][1] else False\n",
    "    if true1 and true2 == True and i>100:\n",
    "        print(\"FOUND CANDIDATE AT EPOCH {}. TERMINATING\".format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A TEST RUN\n",
    "SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "#timesteps_per_second = 10\n",
    "#Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()\n",
    "time_t+=1\n",
    "Agents = SF.Agents_update(Agents, Vissim, state_type, reward_type, state_size, seconds_per_green, seconds_per_yellow, mode, time_t)\n",
    "previous_action = Agents[0].compatible_actions[Agents[0].action]\n",
    "print(\"Previous:\"+str(previous_action))\n",
    "current_action = Agents[0].compatible_actions[Agents[0].newaction]\n",
    "print(\"Current:\"+str(current_action))\n",
    "Agents[0].transition_vector = np.subtract(previous_action, current_action)\n",
    "Agents[0].transition_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_t = 0\n",
    "# Cycle through all agents and update them\n",
    "Agents = SF.Agents_update(Agents, Vissim, state_type, reward_type, state_size, seconds_per_green, seconds_per_yellow, mode, \n",
    "time_t)\n",
    "\n",
    "# Advance the game to the next second (proportionally to the simulator resolution).\n",
    "for _ in range(0, timesteps_per_second):\n",
    "    Vissim.Simulation.RunSingleStep()\n",
    "    time_t += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.SetAttValue('SimPeriod', sim_length*timesteps_per_second)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
