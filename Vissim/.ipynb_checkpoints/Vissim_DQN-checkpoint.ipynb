{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import Simulator_Functions as SF\n",
    "from RLAgents import DQNAgent\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 1000\n",
    "partial_save_at = 100\n",
    "copy_weights_frequency = 5\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = (simulation_length-1)*200+1\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "\n",
    "# Hyperparameters\n",
    "PER_activated = True\n",
    "batch_size = 256\n",
    "memory_size = 10000\n",
    "alpha   = 0.01\n",
    "gamma   = 0.95\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [1 * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP)\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":800, 'm':400, 'l':200}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"demo\"\n",
    "# \"populate\" = population of memory, generation of initial memory file\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "## Network Model Parameters\n",
    "\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "agent_type = 'DuelingDDQN'         # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'   \n",
    "# 'Queues'          Sum of the queues for all lanes in intersection\n",
    "# 'QueuesDiff'      Difference in queue lengths in last timestep\n",
    "# 'QueuesDiffSC'    10000* QueuesDiff - Queues^2\n",
    "state_type  = 'Queues'    \n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Session ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting exploration schedule\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = np.array(range(1,episodes+1))\n",
    "y_series = epsilon_sequence[0:episodes]\n",
    "plt.plot(x_series, y_series, '-b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Ratio of random exploration')\n",
    "plt.title('Exploration schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Loading Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 720001 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Deploying instance of Double Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DuelingDDQN.\n",
      "Loading Pre-Trained Agent, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Agent Reward in this cycle is : 0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -3.84\n",
      "Agent Reward in this cycle is : -17.46\n",
      "Agent Reward in this cycle is : -17.06\n",
      "Agent Reward in this cycle is : -23.97\n",
      "Agent Reward in this cycle is : -30.46\n",
      "Agent Reward in this cycle is : -32.39\n",
      "Agent Reward in this cycle is : -35.98\n",
      "Agent Reward in this cycle is : -35.98\n",
      "Agent Reward in this cycle is : -35.98\n",
      "Agent Reward in this cycle is : -35.98\n",
      "Agent Reward in this cycle is : -35.98\n",
      "Agent Reward in this cycle is : -49.47\n",
      "Agent Reward in this cycle is : -47.96\n",
      "Agent Reward in this cycle is : -47.96\n",
      "Agent Reward in this cycle is : -47.96\n",
      "Agent Reward in this cycle is : -54.33\n",
      "Agent Reward in this cycle is : -61.1\n",
      "Agent Reward in this cycle is : -60.73\n",
      "Agent Reward in this cycle is : -60.73\n",
      "Agent Reward in this cycle is : -60.73\n",
      "Agent Reward in this cycle is : -68.84\n",
      "Agent Reward in this cycle is : -73.4\n",
      "Agent Reward in this cycle is : -79.62\n",
      "Agent Reward in this cycle is : -87.3\n",
      "Agent Reward in this cycle is : -87.0\n",
      "Agent Reward in this cycle is : -96.04\n",
      "Agent Reward in this cycle is : -102.72\n",
      "Agent Reward in this cycle is : -105.96\n",
      "Agent Reward in this cycle is : -105.96\n",
      "Agent Reward in this cycle is : -105.96\n",
      "Agent Reward in this cycle is : -113.56\n",
      "Agent Reward in this cycle is : -112.14\n",
      "Agent Reward in this cycle is : -118.36\n",
      "Agent Reward in this cycle is : -135.51\n",
      "Agent Reward in this cycle is : -151.03\n",
      "Agent Reward in this cycle is : -155.12\n",
      "Agent Reward in this cycle is : -163.97\n",
      "Agent Reward in this cycle is : -178.55\n",
      "Agent Reward in this cycle is : -189.25\n",
      "Agent Reward in this cycle is : -201.29\n",
      "Agent Reward in this cycle is : -228.54\n",
      "Agent Reward in this cycle is : -246.28\n",
      "Agent Reward in this cycle is : -267.91\n",
      "Agent Reward in this cycle is : -272.39\n",
      "Agent Reward in this cycle is : -285.04\n",
      "Agent Reward in this cycle is : -306.67\n",
      "Agent Reward in this cycle is : -349.19\n",
      "Agent Reward in this cycle is : -352.53\n",
      "Agent Reward in this cycle is : -363.65\n",
      "Agent Reward in this cycle is : -386.96\n",
      "Agent Reward in this cycle is : -393.58\n",
      "Agent Reward in this cycle is : -395.78\n",
      "Agent Reward in this cycle is : -428.33\n",
      "Agent Reward in this cycle is : -458.55\n",
      "Agent Reward in this cycle is : -465.97\n",
      "Agent Reward in this cycle is : -472.73\n",
      "Agent Reward in this cycle is : -477.36\n",
      "Agent Reward in this cycle is : -492.9\n",
      "Agent Reward in this cycle is : -509.52\n",
      "Agent Reward in this cycle is : -515.27\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n",
      "Agent Reward in this cycle is : -513.15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b265bf07ba52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n\u001b[0;32m     43\u001b[0m                                   \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_green\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_yellow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                                   demand_list, demand_change_timesteps, mode, PER_activated)\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mVissim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mrun_simulation_episode\u001b[1;34m(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second, seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode, PER_activated)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[1;31m# Advance the game to the next second (proportionally to the simulator resolution).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                         \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                memory_population_length, timesteps_per_second,\\\n",
    "                                                                delete_results = True, verbose = True)\n",
    "        \n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "        Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                           gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                           DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                           Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "    if mode == \"demo\":\n",
    "        timesteps_per_second = 10\n",
    "        Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "        Agents = SF.load_agents(vissim_working_directory, model_name, Agents, Session_ID, best = True)\n",
    "        for agent in Agents:\n",
    "            agent.epsilon = 0\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                                  timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                  demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        Vissim = None\n",
    "    \n",
    "    ## EXECUTION IN DEBUGGING MODE (slow, extra messages)\n",
    "    elif mode == \"debug\":\n",
    "        timesteps_per_second = 10\n",
    "        Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "        SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                                  timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                  demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        Vissim = None\n",
    "        \n",
    "    ## EXECUTION OF MEMORY POPULATION and creation of memory files\n",
    "    elif mode == \"populate\":\n",
    "        SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "        if PER_activated:\n",
    "            memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                                vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                                seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                                demand_list, demand_change_timesteps, PER_activated)\n",
    "            print(\"PER memory prepopulated with {} entries\".format(memory_size))\n",
    "        Vissim = None\n",
    "        \n",
    "    ## EXECUTION OF TEST MODE (fast, best agents, more metrics out)    \n",
    "    elif mode == \"test\":\n",
    "        pass\n",
    "    \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\":\n",
    "        # Load previous memory if available, else create it\n",
    "        SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "        memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "        print('Memory pre-populated. Starting Training.\\n')\n",
    "        \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                # Run Network Parser and ensure agents are linked to their intersections\n",
    "                npa = NetworkParser(Vissim)\n",
    "                for index, agent in enumerate(Agents):\n",
    "                    agent.update_IDS(agent.signal_id, npa)\n",
    "                    agent.episode_reward = []\n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "                    \n",
    "            # Run Episode at maximum speed\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "        \n",
    "            # Train agent with experience of episode and copy weights when necessary\n",
    "            # Update exploration rate\n",
    "            for agent in Agents:\n",
    "                agent.learn_batch(batch_size, episode)\n",
    "                agent.epsilon = epsilon_sequence[episode+1]\n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected non-empty vector for x",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-cb16162b5f61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_series\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_storage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mfit_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoly1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_series\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_series\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_series\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'--r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\numpy\\lib\\polynomial.py\u001b[0m in \u001b[0;36mpolyfit\u001b[1;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected 1D vector for x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected non-empty vector for x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected 1D or 2D array for y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected non-empty vector for x"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x324 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Loading Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 720001 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Deploying instance of Double Deep Q Learning Agent(s)\n",
      "Deployed 1 agent(s) of the Class DuelingDDQN.\n",
      "Previous Experience Found: Loading into agent\n",
      "Memory pre-populated. Starting Training.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SHORT PRETRAINED FROM MEMORY DEMO\n",
    "# Initialize storage\n",
    "reward_storage = []\n",
    "best_agent_weights = []\n",
    "best_agent_memory = []\n",
    "reward_plot = np.zeros([episodes,])\n",
    "loss_plot = np.zeros([episodes,])\n",
    "\n",
    "# Initialize simulation\n",
    "Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                            memory_population_length, timesteps_per_second,\\\n",
    "                                                            delete_results = True, verbose = True)\n",
    "\n",
    "# Setting Random Seed\n",
    "Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "# Deploy Network Parser (crawl network)\n",
    "npa = NetworkParser(Vissim)\n",
    "print('NetworkParser has succesfully crawled the model network.')\n",
    "\n",
    "# Initialize agents\n",
    "if agent_type in ['DQN', 'DuelingDQN', 'DDQN', 'DuelingDDQN'] :\n",
    "    Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                       gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                       DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                       Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] \n",
    "    agents_deployed = True\n",
    "else:\n",
    "    print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "    quit()\n",
    "if agents_deployed:\n",
    "    print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "\n",
    "    memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                        demand_list, demand_change_timesteps, PER_activated)\n",
    "print('Memory pre-populated. Starting Training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for [50,0,50,0] is: [[-96.21001 -93.7373 ]]\n",
      "Prediction for [50,0,50,0] is: [[-88.19781 -91.48635]]\n",
      "Prediction for [50,0,50,0] is: [[-95.69573 -98.98147]]\n",
      "Prediction for [50,0,50,0] is: [[ -95.6585   -100.413124]]\n",
      "Prediction for [50,0,50,0] is: [[-92.56124 -98.35698]]\n",
      "Prediction for [50,0,50,0] is: [[-93.65682 -99.50793]]\n",
      "Prediction for [50,0,50,0] is: [[ -98.91864 -103.29671]]\n",
      "Prediction for [50,0,50,0] is: [[ -99.93705 -107.03251]]\n",
      "Prediction for [50,0,50,0] is: [[-1.4952635 -1.5935124]]\n",
      "Prediction for [50,0,50,0] is: [[-2.1193347 -2.200062 ]]\n",
      "Prediction for [50,0,50,0] is: [[-2.6158724 -2.636978 ]]\n",
      "Prediction for [50,0,50,0] is: [[-3.0373406 -3.0526803]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7d17dcbc4c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Prediction for [50,0,50,0] is: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\RLAgents.py\u001b[0m in \u001b[0;36mlearn_batch\u001b[1;34m(self, batch_size, episode)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mtarget_f_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_f_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_f_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1434\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m         \u001b[1;31m# TODO(mrry): Switch to raising an exception from the SWIG wrapper.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScopedTFStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(600):\n",
    "    Agents[0].learn_batch(1024, 0)\n",
    "    print(\"Prediction for [50,0,50,0] is: {}\".format(Agents[0].model.predict(np.reshape([50,0,50,0], [1,4]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Reward in this cycle is : 0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -3.84\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -5.26\n",
      "Agent Reward in this cycle is : -6.95\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -9.06\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -6.15\n",
      "Agent Reward in this cycle is : -4.34\n",
      "Agent Reward in this cycle is : -17.11\n",
      "Agent Reward in this cycle is : -24.0\n",
      "Agent Reward in this cycle is : -23.9\n",
      "Agent Reward in this cycle is : -23.9\n",
      "Agent Reward in this cycle is : -37.41\n",
      "Agent Reward in this cycle is : -16.82\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -12.04\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -1.57\n",
      "Agent Reward in this cycle is : -4.65\n",
      "Agent Reward in this cycle is : -1.92\n",
      "Agent Reward in this cycle is : -18.03\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -6.17\n",
      "Agent Reward in this cycle is : -22.99\n",
      "Agent Reward in this cycle is : -53.21\n",
      "Agent Reward in this cycle is : -5.1\n",
      "Agent Reward in this cycle is : -8.29\n",
      "Agent Reward in this cycle is : -36.97\n",
      "Agent Reward in this cycle is : -20.06\n",
      "Agent Reward in this cycle is : -48.0\n",
      "Agent Reward in this cycle is : -101.72\n",
      "Agent Reward in this cycle is : -108.19\n",
      "Agent Reward in this cycle is : -96.57\n",
      "Agent Reward in this cycle is : -12.13\n",
      "Agent Reward in this cycle is : -36.01\n",
      "Agent Reward in this cycle is : -56.0\n",
      "Agent Reward in this cycle is : -92.57\n",
      "Agent Reward in this cycle is : -35.03\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -14.4\n",
      "Agent Reward in this cycle is : -7.44\n",
      "Agent Reward in this cycle is : -20.44\n",
      "Agent Reward in this cycle is : -6.92\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -6.57\n",
      "Agent Reward in this cycle is : -11.91\n",
      "Agent Reward in this cycle is : -15.65\n",
      "Agent Reward in this cycle is : -4.07\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -4.36\n",
      "Agent Reward in this cycle is : -8.13\n",
      "Agent Reward in this cycle is : -17.9\n",
      "Agent Reward in this cycle is : -23.47\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -6.79\n",
      "Agent Reward in this cycle is : -11.57\n",
      "Agent Reward in this cycle is : -25.47\n",
      "Agent Reward in this cycle is : -14.14\n",
      "Agent Reward in this cycle is : -18.72\n",
      "Agent Reward in this cycle is : -13.58\n",
      "Agent Reward in this cycle is : -6.08\n",
      "Agent Reward in this cycle is : -7.27\n",
      "Agent Reward in this cycle is : -12.11\n",
      "Agent Reward in this cycle is : -18.7\n",
      "Agent Reward in this cycle is : -35.06\n",
      "Agent Reward in this cycle is : -18.83\n",
      "Agent Reward in this cycle is : -14.51\n",
      "Agent Reward in this cycle is : -12.09\n",
      "Agent Reward in this cycle is : -7.48\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -10.4\n",
      "Agent Reward in this cycle is : -18.84\n",
      "Agent Reward in this cycle is : -20.68\n",
      "Agent Reward in this cycle is : -20.57\n",
      "Agent Reward in this cycle is : -2.48\n",
      "Agent Reward in this cycle is : -12.14\n",
      "Agent Reward in this cycle is : -12.09\n",
      "Agent Reward in this cycle is : -27.96\n",
      "Agent Reward in this cycle is : -37.37\n",
      "Agent Reward in this cycle is : -42.5\n",
      "Agent Reward in this cycle is : -50.57\n",
      "Agent Reward in this cycle is : -82.18\n",
      "Agent Reward in this cycle is : -45.99\n",
      "Agent Reward in this cycle is : -62.82\n",
      "Agent Reward in this cycle is : -64.45\n",
      "Agent Reward in this cycle is : -138.52\n",
      "Agent Reward in this cycle is : -198.55\n",
      "Agent Reward in this cycle is : -225.02\n",
      "Agent Reward in this cycle is : -102.73\n",
      "Agent Reward in this cycle is : -120.78\n",
      "Agent Reward in this cycle is : -192.68\n",
      "Agent Reward in this cycle is : -180.02\n",
      "Agent Reward in this cycle is : -76.01\n",
      "Agent Reward in this cycle is : -109.58\n",
      "Agent Reward in this cycle is : -103.5\n",
      "Agent Reward in this cycle is : -42.66\n",
      "Agent Reward in this cycle is : -34.61\n",
      "Agent Reward in this cycle is : -55.87\n",
      "Agent Reward in this cycle is : -95.42\n",
      "Agent Reward in this cycle is : -116.52\n",
      "Agent Reward in this cycle is : -139.21\n",
      "Agent Reward in this cycle is : -137.28\n",
      "Agent Reward in this cycle is : -135.78\n",
      "Agent Reward in this cycle is : -167.09\n",
      "Agent Reward in this cycle is : -181.63\n",
      "Agent Reward in this cycle is : -144.28\n",
      "Agent Reward in this cycle is : -6.97\n",
      "Agent Reward in this cycle is : -27.91\n",
      "Agent Reward in this cycle is : -33.86\n",
      "Agent Reward in this cycle is : -18.22\n",
      "Agent Reward in this cycle is : -5.33\n",
      "Agent Reward in this cycle is : -19.2\n",
      "Agent Reward in this cycle is : -29.27\n",
      "Agent Reward in this cycle is : -21.22\n",
      "Agent Reward in this cycle is : -21.56\n",
      "Agent Reward in this cycle is : -25.99\n",
      "Agent Reward in this cycle is : -32.77\n",
      "Agent Reward in this cycle is : -44.17\n",
      "Agent Reward in this cycle is : -51.27\n",
      "Agent Reward in this cycle is : -56.05\n",
      "Agent Reward in this cycle is : -60.35\n",
      "Agent Reward in this cycle is : -6.98\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -42.37\n",
      "Agent Reward in this cycle is : -56.35\n",
      "Agent Reward in this cycle is : -78.32\n",
      "Agent Reward in this cycle is : -100.36\n",
      "Agent Reward in this cycle is : -87.22\n",
      "Agent Reward in this cycle is : -23.25\n",
      "Agent Reward in this cycle is : -10.34\n",
      "Agent Reward in this cycle is : -28.53\n",
      "Agent Reward in this cycle is : -1.16\n",
      "Agent Reward in this cycle is : -16.17\n",
      "Agent Reward in this cycle is : -25.18\n",
      "Agent Reward in this cycle is : -5.53\n",
      "Agent Reward in this cycle is : -26.23\n",
      "Agent Reward in this cycle is : -24.02\n",
      "Agent Reward in this cycle is : -27.13\n",
      "Agent Reward in this cycle is : -30.4\n",
      "Agent Reward in this cycle is : -53.12\n",
      "Agent Reward in this cycle is : -17.1\n",
      "Agent Reward in this cycle is : -28.7\n",
      "Agent Reward in this cycle is : -47.67\n",
      "Agent Reward in this cycle is : -3.74\n",
      "Agent Reward in this cycle is : -14.04\n",
      "Agent Reward in this cycle is : -35.14\n",
      "Agent Reward in this cycle is : -65.22\n",
      "Agent Reward in this cycle is : -20.0\n",
      "Agent Reward in this cycle is : -13.83\n",
      "Agent Reward in this cycle is : -6.81\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -7.33\n",
      "Agent Reward in this cycle is : -10.53\n",
      "Agent Reward in this cycle is : -0.16\n",
      "Agent Reward in this cycle is : -8.14\n",
      "Agent Reward in this cycle is : -7.76\n",
      "Agent Reward in this cycle is : -31.5\n",
      "Agent Reward in this cycle is : -26.85\n",
      "Agent Reward in this cycle is : -1.17\n",
      "Agent Reward in this cycle is : -7.61\n",
      "Agent Reward in this cycle is : -1.24\n",
      "Agent Reward in this cycle is : -25.39\n",
      "Agent Reward in this cycle is : -84.62\n",
      "Agent Reward in this cycle is : -133.82\n",
      "Agent Reward in this cycle is : -145.62\n",
      "Agent Reward in this cycle is : -123.78\n",
      "Agent Reward in this cycle is : -100.14\n",
      "Agent Reward in this cycle is : -120.63\n",
      "Agent Reward in this cycle is : -133.06\n",
      "Agent Reward in this cycle is : -195.41\n",
      "Agent Reward in this cycle is : -254.88\n",
      "Agent Reward in this cycle is : -197.61\n",
      "Agent Reward in this cycle is : -255.09\n",
      "Agent Reward in this cycle is : -268.19\n",
      "Agent Reward in this cycle is : -200.78\n",
      "Agent Reward in this cycle is : -216.42\n",
      "Agent Reward in this cycle is : -227.35\n",
      "Agent Reward in this cycle is : -200.86\n",
      "Agent Reward in this cycle is : -170.26\n",
      "Agent Reward in this cycle is : -199.24\n",
      "Agent Reward in this cycle is : -210.51\n",
      "Agent Reward in this cycle is : -256.23\n",
      "Agent Reward in this cycle is : -281.91\n",
      "Agent Reward in this cycle is : -314.45\n",
      "Agent Reward in this cycle is : -395.84\n",
      "Agent Reward in this cycle is : -466.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Reward in this cycle is : -484.6\n",
      "Agent Reward in this cycle is : -546.33\n",
      "Agent Reward in this cycle is : -298.75\n",
      "Agent Reward in this cycle is : -195.6\n",
      "Agent Reward in this cycle is : -280.56\n",
      "Agent Reward in this cycle is : -343.2\n",
      "Agent Reward in this cycle is : -342.79\n",
      "Agent Reward in this cycle is : -348.99\n",
      "Agent Reward in this cycle is : -380.49\n",
      "Agent Reward in this cycle is : -254.58\n",
      "Agent Reward in this cycle is : -226.37\n",
      "Agent Reward in this cycle is : -255.26\n",
      "Agent Reward in this cycle is : -278.39\n",
      "Agent Reward in this cycle is : -226.85\n",
      "Agent Reward in this cycle is : -203.95\n",
      "Agent Reward in this cycle is : -58.05\n",
      "Agent Reward in this cycle is : -61.74\n",
      "Agent Reward in this cycle is : -51.76\n",
      "Agent Reward in this cycle is : -67.37\n",
      "Agent Reward in this cycle is : -88.1\n",
      "Agent Reward in this cycle is : -37.76\n",
      "Agent Reward in this cycle is : -5.02\n",
      "Agent Reward in this cycle is : -19.6\n",
      "Agent Reward in this cycle is : -24.94\n",
      "Agent Reward in this cycle is : -27.35\n",
      "Agent Reward in this cycle is : -18.39\n",
      "Agent Reward in this cycle is : -6.6\n",
      "Agent Reward in this cycle is : -7.75\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -10.63\n",
      "Agent Reward in this cycle is : -5.14\n",
      "Agent Reward in this cycle is : -27.94\n",
      "Agent Reward in this cycle is : -37.78\n",
      "Agent Reward in this cycle is : -20.9\n",
      "Agent Reward in this cycle is : -53.55\n",
      "Agent Reward in this cycle is : -102.81\n",
      "Agent Reward in this cycle is : -123.95\n",
      "Agent Reward in this cycle is : -163.11\n",
      "Agent Reward in this cycle is : -215.28\n",
      "Agent Reward in this cycle is : -191.05\n",
      "Agent Reward in this cycle is : -135.74\n",
      "Agent Reward in this cycle is : -138.18\n",
      "Agent Reward in this cycle is : -192.59\n",
      "Agent Reward in this cycle is : -229.35\n",
      "Agent Reward in this cycle is : -237.68\n",
      "Agent Reward in this cycle is : -117.5\n",
      "Agent Reward in this cycle is : -162.69\n",
      "Agent Reward in this cycle is : -207.55\n",
      "Agent Reward in this cycle is : -133.56\n",
      "Agent Reward in this cycle is : -129.51\n",
      "Agent Reward in this cycle is : -112.78\n",
      "Agent Reward in this cycle is : -113.93\n",
      "Agent Reward in this cycle is : -62.66\n",
      "Agent Reward in this cycle is : -88.08\n",
      "Agent Reward in this cycle is : -78.54\n",
      "Agent Reward in this cycle is : -39.86\n",
      "Agent Reward in this cycle is : -59.48\n",
      "Agent Reward in this cycle is : -67.06\n",
      "Agent Reward in this cycle is : -81.44\n",
      "Agent Reward in this cycle is : -101.73\n",
      "Agent Reward in this cycle is : -116.03\n",
      "Agent Reward in this cycle is : -134.88\n",
      "Agent Reward in this cycle is : -120.17\n",
      "Agent Reward in this cycle is : -149.97\n",
      "Agent Reward in this cycle is : -26.16\n",
      "Agent Reward in this cycle is : -35.07\n",
      "Agent Reward in this cycle is : -58.34\n",
      "Agent Reward in this cycle is : -76.6\n",
      "Agent Reward in this cycle is : -58.43\n",
      "Agent Reward in this cycle is : -18.7\n",
      "Agent Reward in this cycle is : -36.85\n",
      "Agent Reward in this cycle is : -102.02\n",
      "Agent Reward in this cycle is : -87.76\n",
      "Agent Reward in this cycle is : -56.34\n",
      "Agent Reward in this cycle is : -19.47\n",
      "Agent Reward in this cycle is : -25.08\n",
      "Agent Reward in this cycle is : -39.69\n",
      "Agent Reward in this cycle is : -36.2\n",
      "Agent Reward in this cycle is : -59.99\n",
      "Agent Reward in this cycle is : -0.0\n",
      "Agent Reward in this cycle is : -14.45\n",
      "Agent Reward in this cycle is : -10.58\n",
      "Agent Reward in this cycle is : -26.01\n",
      "Agent Reward in this cycle is : -7.82\n",
      "Agent Reward in this cycle is : -16.15\n",
      "Agent Reward in this cycle is : -48.95\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147467259), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-16c5de5459b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n\u001b[0;32m      7\u001b[0m                           \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_green\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_yellow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                           demand_list, demand_change_timesteps, mode, PER_activated)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mVissim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mrun_simulation_episode\u001b[1;34m(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second, seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode, PER_activated)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[1;31m# Advance the game to the next second (proportionally to the simulator resolution).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                         \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissimgpu\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147467259), None)"
     ]
    }
   ],
   "source": [
    "## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "timesteps_per_second = 10\n",
    "Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "for agent in Agents:\n",
    "    agent.epsilon = 0\n",
    "SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, memory_population_length,\\\n",
    "                          timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                          demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Agents[0].state)\n",
    "print(Agents[0].action)\n",
    "print(Agents[0].reward)\n",
    "print(Agents[0].newstate)\n",
    "print(Agents[0].newaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Double Deep Q Learning Agent(s)\n"
     ]
    }
   ],
   "source": [
    " Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size,\\\n",
    "                       gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "                       DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "                       Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for ID in npa.signal_controllers_ids] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Memory_Filename = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID, model_name+ '_Agent'+str(index)+'_Memory'+'.p')\n",
    "a = pickle.load(open(Memory_Filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_queue_sum = np.sum([0 if state is None else state for state in Agents[0].newstate[0]])\n",
    "current_queue_sum**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array([0 if state is None else state for state in Agents[0].newstate[0]])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in Agents:\n",
    "    memory = pickle.load(open(prepopulation_filename, 'rb'))\n",
    "    for s,a,r,s_ in memory:\n",
    "        agent.remember(s,a,r,s_)\n",
    "        # FCalculate importance sampling weights\n",
    "    update_priority_weights(agent, memory_size)\n",
    "    # No simulation ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-12.045227, -11.181825]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agents[0].model.predict(np.reshape([3,0,3,0], [1,4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "vissimgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
