{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: GPU DEVICE NOT FOUND.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "## FA Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "# Import Keras is TF 1.    \n",
    "#from keras.models import load_model\n",
    "\n",
    "## Data Management Modules\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "## Math Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Our Modules\n",
    "from COMServer import COMServerDispatch\n",
    "import Agents \n",
    "import Simulation_Environments as SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Actor_Critic_Agents as AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Actor_Critic_Agents' from 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\NSW\\\\Actor_Critic_Agents.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload modules for debugging\n",
    "from imp import reload\n",
    "reload(Agents)\n",
    "reload(SE)\n",
    "reload(AC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():     \n",
    "    # Initialize agent with dimension of state and action space\n",
    "    def __init__(self,state_size, action_size,actions):\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.actions = actions\n",
    "\n",
    "    # Choose and action\n",
    "    def Action(self, state, actions=None):\n",
    "        if actions is None:\n",
    "            actions = self.actions\n",
    "        pass\n",
    "    \n",
    "    # Learning routine\n",
    "    def Learn(self,sarsa):\n",
    "        pass\n",
    "    \n",
    "    # learn from a batch\n",
    "    def Learn_Batch(self,Sarsa, batch_size=32):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "'''\n",
    "Easy Q_learner Q_Function\n",
    "'''\n",
    "class Q_function(Agent):\n",
    "    def __init__(self, actions = None):\n",
    "        # Q function\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        # number of visits\n",
    "        self.N = defaultdict(lambda: defaultdict(float))\n",
    "        self.actions = actions\n",
    "\n",
    "    def Check(self,state,actions=None):\n",
    "        if actions is None :\n",
    "            actions = self.actions\n",
    "        \n",
    "        if state not in self.Q.keys():\n",
    "            for action in actions:\n",
    "                self.Q[state][action] = 0\n",
    "\n",
    "    def Max(self,state):\n",
    "        Q_maximum = np.max(list(self.Q[state].values()))\n",
    "        return Q_maximum\n",
    "\n",
    "    def Action(self,state,epsilon=0):\n",
    "        if np.random.rand() < epsilon :\n",
    "            idx = np.random.randint(len(actions))\n",
    "            action = actions[idx]\n",
    "        else :\n",
    "            self.Check(state,actions)\n",
    "            action = max(self.Q[state], key=self.Q[state].get)\n",
    "        return action\n",
    "\n",
    "    def Learn(self,sars,learning_rate=0.1,discount_factor=0.5):\n",
    "        state, action, reward, next_state = sars\n",
    "        # Check if state,action and next_state are in Q\n",
    "        self.Check(state)\n",
    "        self.Check(next_state)\n",
    "        self.N_update(state,action)\n",
    "\n",
    "        dQ = reward \\\n",
    "            + discount_factor * self.Max(next_state) \\\n",
    "            - self.Q[state][action]\n",
    "        self.Q[state][action] = self.Q[state][action] + learning_rate * dQ \n",
    "        \n",
    "        return self.Q\n",
    "\n",
    "    def N_update(self,state,action,actions=None):\n",
    "        if actions is None :\n",
    "            actions = self.actions\n",
    "        \n",
    "        if state not in self.N.keys():\n",
    "            for action in actions:\n",
    "                self.N[state][action] = 0 \n",
    "        self.N[state][action] = self.N[state][action] + 1\n",
    "        return self.N[state][action]\n",
    "\n",
    "    def Print(self):\n",
    "        for state in Q_fn.Q.keys():\n",
    "            for action in Q_fn.Q[state].keys():\n",
    "                print(state,action,Q_fn.N[state][action],Q_fn.Q[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim'\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "actions = [(1,0,1,0),\\\n",
    "            (0,1,0,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed load attempt 1/5. Re-attempting.\n",
      "Server Dispatched.\n"
     ]
    }
   ],
   "source": [
    "Env= SE.Q_Sim_Env(None,model_name,vissim_working_directory)\n",
    "#---\n",
    "# # Some code to reload Simulator instance (without rebooting Vissim)\n",
    "# Vissim = Env.Vissim\n",
    "# Env = SE.Q_Sim_Env(Vissim,model_name,vissim_working_directory)\n",
    "#---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "MW = Agents.MaxWeight(4,2,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actions[0]\n",
    "for _ in range(25):\n",
    "    state, reward, done = Env.Step(action)\n",
    "    action = MW.Action(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actions[0]\n",
    "state, reward, done = Env.Step(action)\n",
    "\n",
    "for _ in range(10):\n",
    "    next_state, reward, done = Env.Step(action) \n",
    "    Q_fn.Learn([state,action,reward, next_state])\n",
    "    state = next_state\n",
    "    action = Q_fn.Action(state,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raymond Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Raymond Agent\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "#Code adapted from http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "    def call(self, logits):\n",
    "        # sample a random categorical action from given logits\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlp_policy')\n",
    "        # no tf.get_variable(), just simple Keras API\n",
    "        self.hidden1 = kl.Dense(128, activation='relu')\n",
    "        self.hidden2 = kl.Dense(128, activation='relu')\n",
    "        self.value = kl.Dense(1, name='value')\n",
    "        # logits are unnormalized log probabilities\n",
    "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "        # separate hidden layers from the same input tensor\n",
    "        hidden_logs = self.hidden1(x)\n",
    "        hidden_vals = self.hidden2(x)\n",
    "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # executes call() under the hood\n",
    "        logits, value = self.predict(obs)\n",
    "        action = self.dist.predict(logits)\n",
    "        # a simpler option, will become clear later why we don't use it\n",
    "        # action = tf.random.categorical(logits, 1)\n",
    "        return action , value\n",
    "  \n",
    "    \n",
    "class ACAgent(Agent):\n",
    "\n",
    "    def __init__(self,\\\n",
    "                 state_size,\\\n",
    "                 action_size,\\\n",
    "                 # ID,\\\n",
    "                 # state_type,\\\n",
    "                 # npa,\\\n",
    "                 n_step_size=32,\\\n",
    "                 gamma=0.99,\\\n",
    "                 alpha=0.000065): #,\n",
    "                 #Vissim):\n",
    "                \n",
    "\n",
    "\n",
    "        print(\"Deploying instance of Actor_Critic Agent(s)\")\n",
    "\n",
    "        #just temporary\n",
    "        self.epsilon=0\n",
    "\n",
    "        # Model\n",
    "        # hyperparameters for loss terms and Agent\n",
    "        self.params = {'value': 0.5, 'entropy': 0.0001, 'gamma': gamma}\n",
    "        self.model = Model(action_size)\n",
    "        self.model.compile(\n",
    "            optimizer=ko.RMSprop(lr=alpha),\n",
    "            # define separate losses for policy logits and value estimate\n",
    "            loss=[self._logits_loss, self._value_loss]\n",
    "        )\n",
    "\n",
    "#         # Agent Junction ID and Controller ID\n",
    "#         self.signal_id = ID\n",
    "#         self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "#         self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "\n",
    "        # Potential actions (compatible phases) and transitions\n",
    "        self.update_counter = 1                                 # Timesteps until next update\n",
    "        self.compatible_actions = [[0,1,0,1],[1,0,1,0]]         # Potential actions (compatible phases), 1 means green\n",
    "\n",
    "        # Internal State Traffic Control Variables\n",
    "        self.intermediate_phase = False                         # Boolean indicating an ongoing green-red or red-green transition\n",
    "        self.transition_vector = []                             # Vector that will store the transitions between updates\n",
    "\n",
    "\n",
    "        # Initial Setup of S, A, R, S_\n",
    "        self.state = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.newstate = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.action = 0\n",
    "        self.newaction = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        # Metrics Storage Initialization\n",
    "        self.episode_reward = []\n",
    "        self.loss = []\n",
    "\n",
    "        # The memory will store (state , action , reward, next_state) in a batch\n",
    "        self.memory = deque(maxlen=n_step_size)\n",
    "        self.n_step_size = n_step_size\n",
    "\n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append([state, action, reward, next_state])\n",
    "\n",
    "#     # Update the Junction IDs for the agent\n",
    "#     def update_IDS(self, ID, npa):\n",
    "#         self.signal_id = ID\n",
    "#         self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "#         self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "    # Need to test before loading to build the graph (surely an other way to do it ...)\n",
    "    def test(self):\n",
    "        _,_ = self.model.action_value(np.empty((1,self.state_size)))\n",
    "\n",
    "\n",
    "    def _value_loss(self, returns, value):\n",
    "        # value loss is typically MSE between value estimates and returns\n",
    "        return self.params['value']*kls.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, acts_and_advs, logits):\n",
    "        # a trick to input actions and advantages through same API\n",
    "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
    "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
    "        # from_logits argument ensures transformation into normalized probabilities\n",
    "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        # policy loss is defined by policy gradients, weighted by advantages\n",
    "        # note: we only calculate the loss on the actions we've actually taken\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "        # entropy loss can be calculated via CE over itself\n",
    "        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n",
    "        # here signs are flipped because optimizer minimizes\n",
    "        return policy_loss - self.params['entropy']*entropy_loss\n",
    "\n",
    "    def _returns_advantages(self, rewards, values, next_value):\n",
    "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # returns are calculated as discounted sum of future rewards\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1]\n",
    "        returns = returns[:-1]\n",
    "        # advantages are returns - baseline, value estimates in our case\n",
    "        advantages = returns - values\n",
    "        return returns, advantages\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        action, _ = self.model.action_value(state)\n",
    "        return np.squeeze(action, axis=-1)\n",
    "\n",
    "    #Performing step of gradient descent on the agent memory\n",
    "    def learn(self):\n",
    "\n",
    "\n",
    "        Sample = np.array(self.memory)\n",
    "\n",
    "        states, actions, rewards, next_state  = np.concatenate(Sample[:,0], axis=0), Sample[:,1].astype('int32'), Sample[:,2], np.concatenate(Sample[:,3], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        _, values = self.model.action_value(states)\n",
    "        values = values.squeeze()\n",
    "\n",
    "        _, next_value  = self.model.action_value(next_state)\n",
    "\n",
    "        next_value = next_value[-1]\n",
    "\n",
    "\n",
    "\n",
    "        returns, advs = self._returns_advantages(rewards, values, next_value)\n",
    "\n",
    "        # a trick to input actions and advantages through same API\n",
    "\n",
    "        acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "\n",
    "        # performs a full training step on the collected batch\n",
    "        # note: no need to mess around with gradients, Keras API handles it\n",
    "        losses = self.model.train_on_batch(states, [acts_and_advs, returns])\n",
    "\n",
    "        #print(losses)\n",
    "\n",
    "\n",
    "\n",
    "    # def train(self, env, batch_sz=32, updates=1000):\n",
    "    #     # storage helpers for a single batch of data\n",
    "    #     actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "    #     rewards, dones, values = np.empty((3, batch_sz))\n",
    "    #     observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "    #     # training loop: collect samples, send to optimizer, repeat updates times\n",
    "    #     ep_rews = [0.0]\n",
    "    #     next_obs = env.reset()\n",
    "    #     for update in range(updates):\n",
    "    #         for step in range(batch_sz):\n",
    "    #             observations[step] = next_obs.copy()\n",
    "    #             actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "    #             next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "    #               ep_rews[-1] += rewards[step]\n",
    "    #             if dones[step]:\n",
    "    #                 ep_rews.append(0.0)\n",
    "    #                 next_obs = env.reset()\n",
    "\n",
    "    #           _, next_value = self.model.action_value(next_obs[None, :])\n",
    "    #         returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "    #         # a trick to input actions and advantages through same API\n",
    "    #         acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "    #         # performs a full training step on the collected batch\n",
    "    #         # note: no need to mess around with gradients, Keras API handles it\n",
    "    #         losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "    #     return ep_rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actions[0]\n",
    "state, reward, done = Env.Step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=np.reshape(state, [1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s)\n"
     ]
    }
   ],
   "source": [
    "AC = ACAgent(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-2d2963e9ea86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-c5be36768f7c>\u001b[0m in \u001b[0;36maction_value\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# executes call() under the hood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# a simpler option, will become clear later why we don't use it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[1;31m# function we recompile the metrics based on the updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m   \u001b[1;31m# sample_weight_mode value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m   \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m   \u001b[1;31m# Prepare validation data. Hold references to the iterator and the input list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[1;34m(model, mode)\u001b[0m\n\u001b[0;32m    553\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m   2048\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2050\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2051\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2038\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'predict_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2040\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   2041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[0;32m   3538\u001b[0m       raise ValueError('Session keyword arguments are not support during '\n\u001b[0;32m   3539\u001b[0m                        'eager execution. You passed: %s' % (kwargs,))\n\u001b[1;32m-> 3540\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mEagerExecutionFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3542\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, name)\u001b[0m\n\u001b[0;32m   3462\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_references\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_freezable_vars_to_feed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdates_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3464\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3466\u001b[0m       exec_graph.inputs = self._input_references + list(\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.action_value(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-586bae2d61a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-c5be36768f7c>\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-c5be36768f7c>\u001b[0m in \u001b[0;36maction_value\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# executes call() under the hood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# a simpler option, will become clear later why we don't use it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[1;31m# function we recompile the metrics based on the updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m   \u001b[1;31m# sample_weight_mode value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m   \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m   \u001b[1;31m# Prepare validation data. Hold references to the iterator and the input list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[1;34m(model, mode)\u001b[0m\n\u001b[0;32m    553\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m   2048\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2050\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2051\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2038\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'predict_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2040\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   2041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[0;32m   3538\u001b[0m       raise ValueError('Session keyword arguments are not support during '\n\u001b[0;32m   3539\u001b[0m                        'eager execution. You passed: %s' % (kwargs,))\n\u001b[1;32m-> 3540\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mEagerExecutionFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3542\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, name)\u001b[0m\n\u001b[0;32m   3462\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_references\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_freezable_vars_to_feed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdates_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3464\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3466\u001b[0m       exec_graph.inputs = self._input_references + list(\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "AC.choose_action(np.array([state]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alvaro DQN\n",
    "\n",
    "Main changes:\n",
    "\n",
    "    -- changed Action function name\n",
    "        \n",
    "    -- get state and get reward have been passed to the simulator\n",
    "    \n",
    "    \n",
    "    -- various default parameters are set \n",
    "    \n",
    "  \n",
    "    -- removed state, npa, type as that is a simulator property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALvaro DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self,\\\n",
    "                 state_size=4,\\\n",
    "                 action_size=2,\\\n",
    "                 ID=1,\\\n",
    "                 state_type,\\\n",
    "                 npa,\\\n",
    "                 memory_size,\\\n",
    "                 gamma,\\\n",
    "                 epsilon_start,\\\n",
    "                 epsilon_end,\\\n",
    "                 epsilon_decay,\\\n",
    "                 alpha,\\\n",
    "                 copy_weights_frequency,\\\n",
    "                 Vissim,\\\n",
    "                 PER_activated,\\\n",
    "                 DoubleDQN,\\\n",
    "                 Dueling):\n",
    "        \n",
    "        # Agent Junction ID and Controller ID\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        \n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Agent Hyperparameters\n",
    "        self.gamma = gamma                    # discount rate\n",
    "        self.epsilon = epsilon_start          # starting exploration rate\n",
    "        self.epsilon_min = epsilon_end        # final exploration rate\n",
    "        self.epsilon_decay = epsilon_decay    # decay of exploration rate\n",
    "        self.learning_rate = alpha            # learning rate\n",
    "\n",
    "        # Agent Architecture\n",
    "        self.DoubleDQN = DoubleDQN            # Double Deep Q Network Flag\n",
    "        self.Dueling = Dueling                # Dueling Q Networks Flag\n",
    "        self.PER_activated = PER_activated    # Prioritized Experience Replay Flag\n",
    "\n",
    "        # Model and target networks\n",
    "        self.copy_weights_frequency = copy_weights_frequency    # Frequency to copy weights to target network\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # Architecture Debug Messages\n",
    "        if self.DoubleDQN:\n",
    "            if self.Dueling:\n",
    "                print(\"Deploying instance of Dueling Double Deep Q Learning Agent(s)\")\n",
    "            else:\n",
    "                print(\"Deploying instance of Double Deep Q Learning Agent(s)\")\n",
    "        else:\n",
    "            if self.Dueling:\n",
    "                print(\"Deploying instance of Dueling Deep Q Learning Agent(s)\")\n",
    "            else:\n",
    "                print(\"Deploying instance of Standard Deep Q Learning Agent(s)\")\n",
    "\n",
    "        # Initial Setup of S, A, R, S_\n",
    "        self.state = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.newstate = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        # Metrics Storage Initialization\n",
    "        self.episode_reward = []\n",
    "        self.loss = []\n",
    "\n",
    "        if self.PER_activated:\n",
    "            # If PER_activated spawn BinaryTree and Memory object to store priorities and experiences\n",
    "            self.memory = PER.Memory(memory_size)\n",
    "        else:\n",
    "            # Else use the deque structure to only store experiences which will be sampled uniformly\n",
    "            self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "    # Update the Junction IDs for the agent\n",
    "    def update_IDS(self, ID, npa):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "    \n",
    "    # Agent Neural Network definition\n",
    "    def _build_model(self):\n",
    "        if self.Dueling:\n",
    "            # Architecture for the Neural Net in the Dueling Deep Q-Learning Model\n",
    "            #model = Sequential()\n",
    "            input_layer = Input(shape = (self.state_size,))\n",
    "            dense1 = Dense(24, input_dim=self.state_size, activation='relu')(input_layer)\n",
    "            #dense2 = Dense(48, activation='relu')(dense1)\n",
    "            #flatten = Flatten()(dense2)\n",
    "            fc1 = Dense(48)(dense1)\n",
    "            dueling_actions = Dense(self.action_size)(fc1)\n",
    "            fc2 = Dense(48)(dense1)\n",
    "            dueling_values = Dense(1)(fc2)\n",
    "\n",
    "            def dueling_operator(duel_input):\n",
    "                duel_v = duel_input[0]\n",
    "                duel_a = duel_input[1]\n",
    "                return (duel_v + (duel_a - K.mean(duel_a, axis = 1, keepdims = True)))\n",
    "\n",
    "            policy = Lambda(dueling_operator, name = 'policy')([dueling_values, dueling_actions])\n",
    "            model = Model(inputs=[input_layer], outputs=[policy])\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            return(model)\n",
    "        else:\n",
    "            # Architecture for the Neural Net in Deep-Q learning Model (also Double version)\n",
    "            model = Sequential()\n",
    "            model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "            model.add(Dense(48, activation='relu'))\n",
    "            model.add(Dense(self.action_size, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            return model\n",
    "    \n",
    "    # Obtain the state based on different state definitions\n",
    "    def get_state(self, state_type, state_size, Vissim):\n",
    "        if state_type == 'Queues':\n",
    "            #Obtain Queue Values (average value over the last period)\n",
    "            West_Queue  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)')\n",
    "            South_Queue = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)')\n",
    "            East_Queue  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)')\n",
    "            North_Queue = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)')\n",
    "            state = [West_Queue, South_Queue, East_Queue, North_Queue]\n",
    "            state = np.reshape(state, [1,state_size])\n",
    "            return(state)\n",
    "        elif state_type == 'Delay':\n",
    "            # Obtain Delay Values (average delay in lane * nr cars in queue)\n",
    "            West_Delay    = Vissim.Net.DelayMeasurements.ItemByKey(1).AttValue('VehDelay(Current,Last,All)') \n",
    "            West_Stopped  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QStops(Current,Last)')\n",
    "            South_Delay   = Vissim.Net.DelayMeasurements.ItemByKey(2).AttValue('VehDelay(Current,Last,All)') \n",
    "            South_Stopped = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QStops(Current,Last)')\n",
    "            East_Delay    = Vissim.Net.DelayMeasurements.ItemByKey(3).AttValue('VehDelay(Current,Last,All)') \n",
    "            East_Stopped  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QStops(Current,Last)')\n",
    "            North_Delay   = Vissim.Net.DelayMeasurements.ItemByKey(4).AttValue('VehDelay(Current,Last,All)') \n",
    "            North_Stopped = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QStops(Current,Last)')\n",
    "            \n",
    "            pre_state = [West_Delay, South_Delay, East_Delay, North_Delay, West_Stopped, South_Stopped, East_Stopped, North_Stopped]\n",
    "            pre_state = [0 if state is None else state for state in pre_state]\n",
    "            \n",
    "            state = [pre_state[0]*pre_state[4], pre_state[1]*pre_state[5], pre_state[2]*pre_state[6], pre_state[3]*pre_state[7]]\n",
    "            state = np.reshape(state, [1,state_size])\n",
    "            return(state)\n",
    "        elif state_type == 'MaxFlow':\n",
    "            pass\n",
    "        elif state_type == 'FuelConsumption':\n",
    "            pass\n",
    "        elif state_type == 'NOx':\n",
    "            pass\n",
    "        elif state_type == \"COM\":\n",
    "            pass\n",
    "    \n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        if self.PER_activated:\n",
    "            experience = (state, action, reward, next_state)\n",
    "            self.memory.store(experience)\n",
    "        else:\n",
    "            self.memory.append((state, action, reward, next_state))\n",
    "    \n",
    "    # Choosing actions\n",
    "    def Action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size) \n",
    "            self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "            #print('Chosen Random Action {}'.format(action+1))\n",
    "            return action\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            action = np.argmax(act_values[0]) \n",
    "            self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "            #print('Chosen Not-Random Action {}'.format(action+1))\n",
    "            return action  # returns action\n",
    "    \n",
    "    def get_reward(self):\n",
    "        #reward = -np.absolute((self.newstate[0][0]-self.newstate[0][2])-(self.newstate[0][1]-self.newstate[0][3])) - \n",
    "        #reward = -np.sum(Agents[0].newstate[0])\n",
    "        reward = -np.sum([0 if state is None else state for state in self.newstate[0]])\n",
    "        #print(reward)\n",
    "\n",
    "        self.episode_reward.append(reward)\n",
    "        return reward\n",
    "    \n",
    "    def replay_single(self, batch_size, episode, loss):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "\n",
    "            if self.DoubleDQN:\n",
    "                next_action = np.argmax(self.target_model.predict(np.reshape(next_state,(1,self.state_size))), axis=1)\n",
    "                target = reward + self.gamma * self.target_model.predict(np.reshape(next_state,(1,self.state_size)))[0][next_action]\n",
    "            else:\n",
    "                target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "                # No fixed targets version\n",
    "                #target = reward + self.gamma * np.max(self.model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            self.loss.append(self.model.history.history['loss'][0])\n",
    "\n",
    "        # Exploration rate decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon += self.epsilon_decay\n",
    "        # Copy weights every 5 episodes\n",
    "        if (episode+1) % self.copy_weights_frequency == 0 and episode != 0:\n",
    "            self.copy_weights()   \n",
    "   \n",
    "    def replay_batch(self, batch_size, episode, loss):\n",
    "        state_vector = []\n",
    "        target_f_vector = []\n",
    "        absolute_errors = [] \n",
    "\n",
    "        if self.PER_activated:\n",
    "            tree_idx, minibatch, ISWeights_mb = self.memory.sample(batch_size)\n",
    "            minibatch = [item[0] for item in minibatch]\n",
    "            #return(minibatch)\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "            if self.DoubleDQN:\n",
    "                next_action = np.argmax(self.target_model.predict(np.reshape(next_state,(1,self.state_size))), axis=1)\n",
    "                target = reward + self.gamma * self.target_model.predict(np.reshape(next_state,(1,self.state_size)))[0][next_action]\n",
    "            else:\n",
    "                # Fixed Q-Target\n",
    "                target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "                # No fixed targets version\n",
    "                #target = reward + self.gamma * np.max(self.model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "\n",
    "            # This section incorporates the reward into the prediction and calculates the absolute error between old and new\n",
    "            target_f = self.model.predict(state)\n",
    "            absolute_errors.append(abs(target_f[0][action] - target)[0])\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            state_vector.append(state[0])\n",
    "            target_f_vector.append(target_f[0])\n",
    "\n",
    "        state_matrix = np.asarray(state_vector)\n",
    "        target_f_matrix = np.asarray(target_f_vector)\n",
    "\n",
    "        self.model.fit(state_matrix, target_f_matrix, epochs=1, verbose=0)\n",
    "        self.loss.append(self.model.history.history['loss'])\n",
    "\n",
    "        if self.PER_activated:\n",
    "            #Update priority\n",
    "            self.memory.batch_update(tree_idx, absolute_errors)\n",
    "\n",
    "        # Exploration rate decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon += self.epsilon_decay\n",
    "        # Copy weights every 5 episodes\n",
    "        if (episode+1) % self.copy_weights_frequency == 0 and episode != 0:\n",
    "            self.copy_weights()   \n",
    "\n",
    "    # Copy weights function\n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(\"Weights succesfully copied to Target model.\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
