{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_critic_class import ACAgent\n",
    "from MasterAC_Agent import MasterAC_Agent\n",
    "from MasterDQN_Agent import MasterDQN_Agent\n",
    "\n",
    "# Network Specific Libraries\n",
    "from Balance_Functions import balance_dictionary\n",
    "\n",
    "# General Libraries\n",
    "import numpy as np \n",
    "import pylab as plt\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = \"E:\\Backup - Onedrive\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\"\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "agent_type = 'AC'\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Agent hyperparameters\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "timesteps_per_second = 1\n",
    "\n",
    "\n",
    "# for the monitoring only for AC\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiAc_Agents.train(1000)\n",
    "\n",
    "Balance_MultiAc_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n",
    "\n",
    "Balance_MultiAc_Agents.load(best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Agents = []\n",
    "for idx, info in Balance_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        \n",
    "        \n",
    "        # Only for AC\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance RL DQN Partial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = 1\n",
    "map_name  = 'Balance_int'+str(intersection)\n",
    "model_name = map_name\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "vissim_working_directory = \"E:\\\\Backup - Onedrive\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "\n",
    "sim_length = 3601\n",
    "agent_type = \"DDQN\"\n",
    "actions = 'default'     # 'default' or 'all'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 500\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 5000\n",
    "batch_size = 512\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "# Ojo aqui, hack\n",
    "Session_ID = map_name + \"_\" + actions + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type\n",
    "\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "partial_dictionary = {\"junctions\": {(intersection-1): Balance_dictionary[\"junctions\"][intersection-1]},\\\n",
    "                      \"demand\": Balance_dictionary[\"demand\"]}\n",
    "\n",
    "alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, partial_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the agent training\n",
    "ploty = 1\n",
    "for idx , agent in Balance_int_MultiDQN_Agents.Agents.items():\n",
    "    print(\"Agent \"+str(idx))\n",
    "    #print(ploty)\n",
    "    #plt.subplot(14, 2, ploty)\n",
    "\n",
    "    plt.figure('6'+str(idx),figsize=(4.5, 3))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    ploty+=1\n",
    "    #print(ploty)\n",
    "\n",
    "    \n",
    "    #plt.subplot(14, 2, ploty)\n",
    "    plt.figure('7'+str(idx),figsize=(4.5, 3))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    #Loss_rewarddf.to_csv(csv_Path,index=False)\n",
    "    ploty+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.load(400, best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Balance_int_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    #Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                    \"Junction{}_Queues.png\".format(idx))               \n",
    "    #plt.savefig(Path)\n",
    "    #\n",
    "    #csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                    \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    #Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    #Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                        \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    #csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                    \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    #plt.savefig(Path)\n",
    "    #\n",
    "    #delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    #Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                        \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    #csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "    #                    \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    #plt.savefig(Path)\n",
    "    #stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "#Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "#                        \"Total_Cumulative_Delay.png\")\n",
    "#csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "#                        \"Total_Cumulative_Delay.csv\")\n",
    "#plt.savefig(Path)\n",
    "#Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "#Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "#                        \"Total_Cumulative_stop_Delay.png\")\n",
    "#csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "#                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "#plt.savefig(Path)\n",
    "#Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance RL DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "vissim_working_directory = \"E:\\\\Backup - Onedrive\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "sim_length = 3601\n",
    "agent_type = \"DDQN\"\n",
    "\n",
    "# Ojo aqui, hack\n",
    "Session_ID = \"DQN\"\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 1000\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 5000\n",
    "batch_size = 512*2\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAEyCAYAAADqTulnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3debyVVb3H8c+Xw6AyqYADIuBsZDhwJCwrckAwBUtSUcrpal4jzWt1tTS16+uW3Uwty9ScTc0xcUQjh8wcQHEAUklRUVRQFJwBf/eP9RzZHM6wD+fs8+x99vf9eq3X3s+4f/t5bfidtZ71rKWIwMzMzCpPp7wDMDMzs9XjJG5mZlahnMTNzMwqlJO4mZlZhXISNzMzq1BO4mZmZhXKSdysTEi6VNLp7fh5d0g6uL0+rymS7pX0H210rlMlXdnW+5qVIydxsxaSNFfSB5LeLSjn5h1XUxpKVhExJiIuyysmM2u9znkHYFah9o6Iv+YdBICkzhGxLO84zKz9uSZu1oYknSfp+oLlMyRNVTJS0jxJP5a0MKvRH9TEuY6QNEfSW5ImS+pfsC0kfVfSc8Bz2bpzJL0sabGk6ZK+lK0fDfwY2D9rNXgiW/9pE7akTpJOkvSipDckXS6pd7ZtcPZ5B0t6KYv9J03EvaekWZKWSHpF0g8Kto2TNCOL8d9ZbHUGSfpHdtxdkvoWHDdC0oOS3pb0hKSRBds2kXRfdtzdQOFxIyXNqxffXEm7NRJ7o59jVo6cxM3a1vHAUEmHZEn0cODgWDG+8QakJLMRcDBwgaSt6p9E0i7Az4H9gA2BF4Fr6u22D/B5YEi2/CiwHbAucBVwnaQ1IuJO4H+BP0dEj4jYtoG4D8nKV4FNgR5A/VsEOwNbAbsCP5X0mUauwUXAdyKiJ7AN8LfsOw0HLgd+CKwNfBmYW3DcgcChwHpAV+AH2XEbAbcBp2ff7QfADZL6ZcddBUwnXdf/IV3XFivic8zKjpO42er5S1ZbqytHAETE+8BE4NfAlcD3ImJevWNPjoiPIuI+UtLYr4HzHwRcHBGPRcRHwInATpIGF+zz84h4KyI+yD77yoh4MyKWRcSZQDdS0i3GQcCvI+L5iHg3+7wDJBXecjstIj6IiCeAJ4CG/hgAWAoMkdQrIhZFxGPZ+sOz73R3RHwSEa9ExL8KjrskIp7Nvs+1pD9IIF3P2yPi9uy4u4FpwJ6SBgI7suKa3g/cUuR3rq/Rz1nN85mVnJO42erZJyLWLigX1m2IiEeA5wGRklGhRRHxXsHyi0B/VtU/21Z3zneBN0k1+DovFx4g6XhJsyW9I+ltoDcFTcvNWOnzsvedgfUL1r1W8P59Um29IfuSEt+LWTP3Ttn6jYF/NxFDY+cfBHyz8I8mUqvAhlncDV3T1dHU55iVJSdxszYm6bukWvCrwI/qbV5HUveC5YHZfvW9SkoqdefsDvQBXinYJwq2fwn4b1Ktfp2IWBt4h/SHxEr7NmKlz8viWga83sxxq4iIRyNiHKlZ/C+s+EPmZWCzlp4vO+6Ken80dY+IXwDzafia1nkPWKtuQVIN0FjzeFOfY1aWnMTN2pCkLUn3VCcC3wJ+JGm7erudJqlrlnj3Aq5r4FRXAYdK2k5SN9I97YcjYm4jH92TlHQXAJ0l/RToVbD9dWCwpMb+zV8NHJd1EuvBinvoLer1nn2vgyT1joilwGJgebb5ouw77Zp1pNtI0tZFnPZKYG9Je0iqkbRG1mFtQES8SGryrrumOwN7Fxz7LLCGpK9J6gKcRPoDq0Wf05JrYNaenMTNVs8tWvk58Zuy+8dXAmdExBMR8RypV/gVWSKG1GS8iFTz/RNwVL37wgBExFTgZOAGUm1zM+CAJuKZAtxBSlovAh+ycnN73R8Kb0p6jFVdDFwB3A+8kB3/veYuQiO+BcyVtBg4ivQHTd1thkOBs0itBPexcu2/QRHxMjCOdC0XkL7XD1nx/9eBpA5+bwGnkDrP1R37DnA08EdSK8Z7QP0+CsV+jlnZ0YpOs2ZWStnjSldGhGt2ZtYm/BemmZlZhXISNzMzq1BuTjczM6tQrombmZlVKCdxMzOzClVxs5j17ds3Bg8enHcYZmZm7WL69OkLI6LBQYoqLokPHjyYadOm5R2GmZlZu5DU6FDCbk43MzOrUE7iZmZmFcpJ3MzMrEI5iZuZmVUoJ3EzM7MKVbIkLuliSW9IerqR7ZL0G0lzJD0paYdSxWJmZtYRlbImfikwuontY4AtsnIkcF4JYzEzM+twSpbEI+J+0vy+jRkHXB7JQ8DakjYsVTxmZmYdTZ73xDcCXi5YnpetazezZsFpp8Enn7Tnp5qZmbWNPJO4GljX4JRqko6UNE3StAULFrRZAI89BqeeCo8/3manNDMzazd5JvF5wMYFywOAVxvaMSIuiIjaiKjt16/B4WNXy267pde7726zU5qZmbWbPJP4ZODbWS/1EcA7ETG/PQPYYAMYOhTuuqs9P9XMzKxtlGwCFElXAyOBvpLmAacAXQAi4g/A7cCewBzgfeDQUsXSlFGj4Jxz4L33oHv3PCIwMzNbPSVL4hExoZntAXy3VJ9frFGj4Fe/gvvvhzFj8o7GzMyseFU/YtvOO8Maa7hJ3czMKk/VJ/E114Qvfcmd28zMrPJUfRKH1KQ+cya88krekZiZmRXPSZyUxMG1cTMzqyxO4sDnPgfrr+8kbmZmlcVJHJBg991TEvcQrGZmVimcxDOjRsGCBfDEE3lHYmZmVhwn8YyHYDUzs0rjJJ7ZcMN0b3zKlLwjMTMzK46TeIHRo+Hvf4clS/KOxMzMrHlO4gW+9jVYuhSmTs07EjMzs+Y5iRf4whegVy+4/fa8IzEzM2uek3iBLl1SL/Xbb4eIvKMxMzNrmpN4PXvumYZfffLJvCMxMzNrmpN4PaNHp1c3qZuZWblzEq9nww1hhx2cxM3MrPw5iTfga1+DBx+ERYvyjsTMzKxxTuIN2HPPNIa6B34xM7Ny5iTegB13hD593KRuZmblzUm8ATU1qYPbHXd4VjMzMytfTuKN2HNPWLgQpk3LOxIzM7OGOYk3Yo89oFMnuOWWvCMxMzNrmJN4I/r0gZ13hsmT847EzMysYU7iTRg3Lo3c9sILeUdiZma2KifxJowbl15dGzczs3LkJN6EzTaDz34Wbr4570jMzMxW5STejLFj4f774a238o7EzMxsZU7izRg3DpYv98AvZmZWfpzEm7HjjmlSFDepm5lZuXESb0anTrD33nDnnfDRR3lHY2ZmtoKTeBHGjYN334V77sk7EjMzsxWcxIuwyy7Qvbub1M3MrLw4iRdhjTXShCiTJ3tCFDMzKx9O4kXaZx949VV49NG8IzEzM0ucxIu0997QpQtcf33ekZiZmSVO4kXq3RtGjUpJPCLvaMzMzEqcxCWNlvSMpDmSTmhg+0BJ90h6XNKTkvYsZTytNX48zJ0Ljz2WdyRmZmYlTOKSaoDfAWOAIcAESUPq7XYScG1EbA8cAPy+VPG0hbFjoXNnN6mbmVl5aDaJS/qGpOckvSNpsaQlkhYXce7hwJyIeD4iPgauAcbV2yeAXtn73sCrLQm+va27Luy6q5vUzcysPBRTE/8lMDYiekdEr4joGRG9mj0KNgJeLliel60rdCowUdI84Hbge0WcN1fjx8OcOWmecTMzszwVk8Rfj4jZq3FuNbCufv11AnBpRAwA9gSukLRKTJKOlDRN0rQFCxasRihtZ599oKbGTepmZpa/YpL4NEl/ljQha1r/hqRvFHHcPGDjguUBrNpcfjhwLUBE/BNYA+hb/0QRcUFE1EZEbb9+/Yr46NLp2xdGjoTrrnOTupmZ5auYJN4LeB8YBeydlb2KOO5RYAtJm0jqSuq4NrnePi8BuwJI+gwpiedb1S7C+PHwzDMwa1bekZiZWTXr3NwOEXHo6pw4IpZJmgRMAWqAiyNipqSfAdMiYjJwPHChpONITe2HRJR//fbrX4ejj05N6p/9bN7RmJlZtVJzOVPSAOC3wBdJifYB4NiImFf68FZVW1sb06ZNy+OjVzJyJLzxBsycCWro7r+ZmVkbkDQ9Imob2lZMc/olpGbw/qTe5bdk66raAQfA7NnupW5mZvkpJon3i4hLImJZVi4F8u1dVgbGj08Dv1x9dd6RmJlZtSomiS+UNFFSTVYmAm+WOrBy17dvGkv96qs9PamZmeWjmCR+GLAf8BowHxifrat6EybASy/BP/+ZdyRmZlaNiumd/hIwth1iqTjjxsGaa6ba+Be/mHc0ZmZWbRpN4pJ+FBG/lPRbVh1pjYg4pqSRVYCePdM849deC2efne6Rm5mZtZemmtPrhlqdBkxvoBipSX3BApg6Ne9IzMys2jRad4yIW7K370fEdYXbJH2zpFFVkDFjoHdvuOoq2GOPvKMxM7NqUkzHthOLXFeVunWDffeFm26CDz7IOxozM6smTd0TH0OaWWwjSb8p2NQLWFbqwCrJgQfCxRfDrbfCN91GYWZm7aSpmvirpPvhH7LyvfDJgBuOC4wcCf37wxVX5B2JmZlVk6buiT8BPCHpqohY2o4xVZyaGpg4Ec48E15/HdZfP++IzMysGhRzT3ywpOslzZL0fF0peWQV5uCDYfny1MHNzMysPRQ7Acp5pPvgXwUuB9xwXM+QIVBbC5ddlnckZmZWLYpJ4mtGxFTStKUvRsSpwC6lDasyHXwwPPFEKmZmZqVWTBL/UFIn4DlJkyR9HVivxHFVpAkToEsX18bNzKx9FJPEvw+sBRwDDAMmAgeXMqhK1adPGob1T3+Cpe4KaGZmJdZkEpdUA+wXEe9GxLyIODQi9o2Ih9opvopz8MHwxhtw5515R2JmZh1dk0k8IpYDwySpneKpeGPGQL9+blI3M7PSK2berceBmyVdB7xXtzIibixZVBWsS5c0gtt558HChdC3b94RmZlZR1XMPfF1gTdJPdL3zspepQyq0h12GHz8sUdwMzOz0lLEKlOFl7Xa2tqYNm1a3mE0a8QIWLwYZs4E34wwM7PVJWl6RNQ2tK3ZmrikAZJukvSGpNcl3SBpQNuH2bEccQTMng0PPph3JGZm1lEVO2LbZKA/sBFwS7bOmrD//tCjB1x4Yd6RmJlZR1VMEu8XEZdExLKsXAr0K3FcFa9Hj9TB7dpr4e23847GzMw6omKS+EJJEyXVZGUiqaObNeOII+CDD9LgL2ZmZm2tmCR+GLAf8BowHxifrbNmDBsG22+fmtQrrP+gmZlVgGaTeES8FBFjI6JfRKwXEftExIvtEVylk1Jt/IknoAI61JuZWYVpdLAXSb8FGq0/RsQxJYmogznwQPjBD+CCC2DHHfOOxszMOpKmRmxz3bEN9O6dZje76ir45S9hnXXyjsjMzDqKRpN4RKw0+rekXml1LCl5VB3Md78LF10El14Kxx2XdzRmZtZRFDPYS62kp4AngaclPSFpWOlD6zi23x6+8AX4/e/hk0/yjsbMzDqKYnqnXwwcHRGDI2IQ8F082EuLTZoEc+bAXXflHYmZmXUUxSTxJRHx97qFiHgAcJN6C+27L6y/Ppx7bt6RmJlZR1FMEn9E0vmSRkr6iqTfA/dK2kHSDqUOsKPo2hW+8x24/XZ4/vm8ozEzs46gmCS+HbAlcApwKvAZ4AvAmcCvmjpQ0mhJz0iaI+mERvbZT9IsSTMlXdWi6CvMkUdCp05prnEzM7PWKtlUpJJqgGeB3YF5wKPAhIiYVbDPFsC1wC4RsUjSehHxRlPnrZSpSBuz337w17/CvHmw1lp5R2NmZuWutVORXiGpd8HyIElTi/jc4cCciHg+Ij4GrgHG1dvnCOB3EbEIoLkE3hFMmgSLFnk8dTMza71imtMfAB6WtKekI4C7gbOLOG4j4OWC5XnZukJbAltK+oekhySNLiboSvalL6VHzs46y4+bmZlZ6zQ1YhsAEXG+pJnAPcBCYPuIeK2Ic6uh0zXw+VsAI4EBwN8lbRMRK03eKelI4EiAgQMHFvHR5UuC//ov+Na3YMoUGDMm74jMzKxSFdOc/i3Ss+LfBi4Fbpe0bRHnngdsXLA8AHi1gX1ujoilEfEC8Awpqa8kIi6IiNqIqO3Xr/KnMt9/f9hoIzjzzLwjMTOzSlZMc/q+wM4RcXVEnAgcBVzWzDGQOrJtIWkTSV2BA4DJ9fb5C/BVAEl9Sc3rHf4BrC5d4JhjYOpUmDEj72jMzKxSFTMV6T6FHc4i4hFSp7XmjlsGTAKmALOBayNipqSfSRqb7TYFeFPSLFJz/Q8j4s3V+B4V54gjoHv3dG/czMxsdTT7iJmkLYHzgPUjYhtJQ4GxEXF6ewRYX6U/Ylbo2GPTM+Nz50L//nlHY2Zm5ahVj5gBFwInAksBIuJJUtO4tdKxx8Ly5R6K1czMVk8xSXytrAm90LJSBFNtNt0Uvv51+MMfYIlHozczsxYqJokvlLQZ2eNhksYD80saVRX50Y/S4C8XXJB3JGZmVmmKSeLfBc4Htpb0CvB9Ug91awPDh8Ouu6bHzT78MO9ozMyskhTTO/35iNgN6AdsHRE7R8SLpQ+tevz4xzB/PlxWzIN7ZmZmmWJq4gBExHsR4Tu3JfDVr8LnPw9nnAHL3NvAzMyKVHQSt9KRUm38hRfgz3/OOxozM6sUTuJlYq+9YJtt4Oc/98QoZmZWnGYnQMnmBf8aMLhw/4j4denCqj6dOsGJJ8JBB8Ett8C4+pO2mpmZ1VNMTfwW4BCgD9CzoFgb228/2Gwz+NnPoJmB9MzMzJqviQMDImJoySMxOneGk06CQw+FyZNdGzczs6YVUxO/Q9KokkdiAEycCFtsAaec4nvjZmbWtGKS+EPATZI+kLRY0hJJi0sdWLXq3Dkl8CeegBtvzDsaMzMrZ8Uk8TOBnUhjqPeKiJ4R0avEcVW1Aw6ArbdOyXz58ryjMTOzclVMEn8OeDqam7PU2kxNDZx6KsyaBddem3c0ZmZWroqZT/xSYFPgDuCjuvV5PWLWkeYTb8onn8C228LHH8PMmamZ3czMqk9r5xN/AZgKdMWPmLWbTp3gtNPg2Wfh6qvzjsbMzMpRszXxT3eUegIREe+WNqSmVUtNHFJtfNgweOcdmD0bunXLOyIzM2tvraqJS9pG0uPA08BMSdMlfbatg7RVdeoEv/hFGlP9D3/IOxozMys3xTSnXwD8V0QMiohBwPHAhaUNy+qMGgW77Qb/8z/w9tt5R2NmZuWkmCTePSLuqVuIiHuB7iWLyFYipSlK33wzvZqZmdUpJok/L+lkSYOzchKps5u1kx12SBOjnH02zJuXdzRmZlYuiknihwH9gBuBm7L3h5YyKFvV6aenjm4//WnekZiZWbloNolHxKKIOCYidoiI7SPi2IhY1B7B2QqDB8P3vgeXXQZPPZV3NGZmVg4afcRM0i1Ao8+fRcTYUgXVlGp6xKy+t95KU5WOGAF33JF3NGZm1h5W9xGzX5HGTX8B+IDUI/1C4F3S42bWztZdF04+Ge68E267Le9ozMwsb8UMu3p/RHy5uXXtpZpr4pCGYR06NN0ff+opDwBjZtbRtXbY1X6SNi042Sakzm2Wg65dUy/1556Dc87JOxozM8tTMUn8OOBeSfdKuhe4B/h+SaOyJo0eDXvvnQaAmT8/72jMzCwvxfROvxPYAjg2K1tFxJRSB2ZN+/WvU9P6CSfkHYmZmeWlmJo4wDDgs8C2wP6Svl26kKwYm28Oxx8Pl18ODz2UdzRmZpaHYiZAuYLUU31nYMesNHiD3drXj38M/fvDpEmwfHne0ZiZWXvrXMQ+tcCQKHbOUms3PXrAmWfChAnw+9+nwWDMzKx6FNOc/jSwQakDsdWz//6wxx7wk594XHUzs2pTTBLvC8ySNEXS5LpS6sCsOBKcdx4sWwbHHJN3NGZm1p6KaU4/tdRBWOtssgmcckrqqX7zzTBuXN4RmZlZe2h2xLZWnVwaDZwD1AB/jIhfNLLfeOA6YMeIaHI4tmofsa0xS5fCsGGwaBHMmgU9e+YdkZmZtYVWjdgmaYSkRyW9K+ljScslLS7iuBrgd8AYYAgwQdKQBvbrCRwDPNzcOa1xXbrA+efDK6+k8dXNzKzjK+ae+LnABOA5YE3gP7J1zRkOzImI5yPiY+AaoKGG3v8Bfgl8WFTE1qiddoL//E/4zW/gwQfzjsbMzEqtqMFeImIOUBMRyyPiEmBkEYdtBLxcsDwvW/cpSdsDG0fErcWFa835xS9g4EA45BB4//28ozEzs1IqJom/L6krMEPSLyUdB3Qv4jg1sO7TG/CSOgFnAcc3eyLpSEnTJE1bsGBBER9dvXr2hIsvThOknHRS3tGYmVkpFZPEv5XtNwl4D9gY2LeI4+Zl+9YZALxasNwT2IY0ucpcYAQwWdIqN+8j4oKIqI2I2n79PIFac3bZBY4+Os129sADeUdjZmal0mTv9Kxz2mURMbHFJ5Y6A88CuwKvAI8CB0bEzEb2vxf4gXunt413303zjtfUwIwZ0L2YthMzMys7q907PSKWk+YT79rSD42IZaTa+xRgNnBtRMyU9DNJY1t6PmuZHj1Ss/qcOWmMdTMz63iafU5c0vnADsBkUnM6ABHx69KG1jDXxFvmmGPgt7+Fu+6C3XfPOxozM2upVj0nTrqPfWu2b8+CYhXgjDNgyBD49rfBfQLNzDqWZoddjYjT2iMQK40114Srr4bhw+HQQ+GWW9J462ZmVvmKek7cKtvQofB//we33Qa/+13e0ZiZWVtxEq8SkybB174GP/gBPPVU3tGYmVlbaDSJSzoje/1m+4VjpSLBJZfAOuvAAQd4NDczs46gqZr4npK6ACe2VzBWWv36weWXw+zZcNRRUMIJ7MzMrB00lcTvBBYCQyUtlrSk8LWd4rM2tvvuae7xK66ACy7IOxozM2uNRpN4RPwwInoDt0VEr4joWfjajjFaGzv5ZBg9Oj1D7kfuzcwqV7Md2yJinKT1Je2VFQ9eXuE6dYIrr4QNNoDx4+Gtt/KOyMzMVkezSTzr2PYI8E1gP+ARSeNLHZiVVp8+cN118OqrMHEifPJJ3hGZmVlLFfOI2UnAjhFxcER8GxgOnFzasKw9DB8O55wDd9zhaUvNzCpRsyO2AZ0i4o2C5Tfx8+UdxlFHweOPw89/DttsAwcemHdEZmZWrGKS+J2SpgBXZ8v7A7eXLiRrTxKcey488wwcdhhsvnmqoZuZWfkrpmPbD4HzgaHAtsAFEfHfpQ7M2k/XrnDDDbDhhrDPPvDKK3lHZGZmxSimJk5E3AjcWOJYLEd9+8LkyfCFL6REft99sNZaeUdlZmZN8b1t+9TnPgd/+hNMn57ujS9fnndEZmbWFCdxW8nYsfCb38DNN6fBYDw0q5lZ+SqqOV1SV2DLbPGZiFhaupAsb5Mmwcsvwy9/CQMHwn+7B4SZWVlqNolLGglcBswFBGws6eCIuL+0oVmefv7zlMhPOAEGDICDDso7IjMzq6+YmviZwKiIeAZA0pakx82GlTIwy1enTmnq0tdeg0MPTSO8jR6dd1RmZlaomHviXeoSOEBEPAt0KV1IVi66dYMbb0yDwHz963C/217MzMpKMUl8mqSLJI3MyoXA9FIHZuVh7bVhyhTYZBPYay945JG8IzIzszrFJPH/BGYCxwDHArOAo0oZlJWXfv3g7rvT6+jR8OSTeUdkZmYAigp7hqi2tjameRLsXMydCzvvDEuXpqb1rbbKOyIzs45P0vSIqG1oW6M1cUnXZq9PSXqyfilVsFa+Bg+GqVPT+69+Ff71r1zDMTOrek31Tj82e92rPQKxyrDVVvC3v8Guu8JXvpKS+jbb5B2VmVl1arQmHhHzs7dHR8SLhQU4un3Cs3L02c/CvfdCTQ2MHAkzZuQdkZlZdSqmY9vuDawb09aBWGXZeut0X3yttWCXXcDdFMzM2l9T98T/U9JTwFb17oe/APieuLH55imR9+6dmtf9HLmZWftqqiZ+FbA3MDl7rSvDImJiO8RmFWDw4JS8+/eHUaPgppvyjsjMrHo0dU/8nYiYGxETsvvgHwAB9JA0sN0itLK38cbwwAOw/fYwfjycf37eEZmZVYdm74lL2lvSc8ALwH2kiVDuKHFcVmH69IG//hXGjIGjjoLTTvM0pmZmpVZMx7bTgRHAsxGxCbAr8I+SRmUVqXv31Jx+yCFw6qlw5JHw8cd5R2Vm1nEVk8SXRsSbQCdJnSLiHmC7EsdlFapLF7j4YvjJT+CPf4Q99oA338w7KjOzjqmYJP62pB7A/cCfJJ0DLCttWFbJJDj9dLjiCnjwQfj85z26m5lZKRSTxMcB7wPHAXcC/yb1Um+WpNGSnpE0R9IJDWz/L0mzskfXpkoa1JLgrbxNnJgGhVmyBEaMgLvuyjsiM7OOpdkkHhHvRcQnEbEsIi4DfgeMbu44STXZvmOAIcAESUPq7fY4UBsRQ4HrgV+29AtYedtppzR96aBBqdPbGWe4w5uZWVtparCXXpJOlHSupFFKJgHPA/sVce7hwJyIeD4iPgauIdXqPxUR90TE+9niQ8CA1fsaVs4GDYJ//CM9fnbCCfCNb8A77+QdlZlZ5WuqJn4FsBXwFPAfwF3AN4FxETGuiePqbAS8XLA8L1vXmMPxo2sdVo8ecM01cNZZcOutUFvrecnNzFqrqSS+aUQcEhHnAxOAWmCviCh2ugs1sK7BhlRJE7Pz/18j24+UNE3StAULFhT58VZuJPj+9+Gee+C999J98ksucfO6mdnqaiqJL617ExHLgRciYkkLzj0P2LhgeQDwav2dJO0G/AQYGxEfNXSiiLggImojorZfv34tCMHK0c47w2OPpSR+2GFwwAGwaFHeUZmZVZ6mkvi2khZnZQkwtO69pMVFnPtRYAtJm0jqChxAGof9U5K2B84nJfA3VvdLWOXZYAO4+2743/+FG2+EbbeFv/8976jMzCpLU2On10REr6z0jIjOBe97NXfiiFgGTAKmALOBayNipqSfSRqb7fZ/QA/gOkkzJE1u5HTWAdXUwIknpmfJu3VLc5OffDIsXdrsoWZmBigq7IZkbW1tTPPk1R3OkiVwzDFw6aWwww7pXvnQoXlHZWaWP0nTI6K2oW3FDNtFFnIAAA58SURBVPZiVnI9e6bEff31MG8eDBuWxl/32OtmZo1zEreysu++MHMm7L9/mgltxx1h+vS8ozIzK09O4lZ2+vaFK6+Em2+GBQvS2OvHH5+a3M3MbAUncStbY8emWvmhh8Kvfw1bbw3XXuvnys3M6jiJW1lbZx248EL45z9h/fVTM/uoUfDMM3lHZmaWPydxqwgjRsCjj8Jvf5teP/e51MTuQWLMrJo5iVvFqKmBSZNSLfxb30rjsG+2GZx9tnuxm1l1chK3irP++nDRRTBjRppI5bjjYMgQuOEG3y83s+riJG4Va+hQmDIF7rgD1lgjTXU6YkRa52RuZtXASdwqmgSjR6da+R//CK+/npZ33hmmTnUyN7OOzUncOoTOneHww+HZZ+EPf4CXXoLddkvjsf/tb07mZtYxOYlbh9K1K3znO/Dcc6kn+3PPwa67pgFjbrgBli/PO0Izs7bjJG4d0hprpJ7s//53qpm/9Va6Z/6Zz8AFF8CHH+YdoZlZ6zmJW4e25pqpZv7MM3DdddC7d1oePBh+9jOYPz/vCM3MVp+TuFWFmppUE3/kkXSPfIcd4JRTYOBAmDABHnjA983NrPI4iVtVkeCrX4Xbb0+d4L73vfSI2pe+BNtvn4Z49UQrZlYpnMStam2xRZpY5ZVX4Pzz4ZNP4MgjYYMN4JBD4L77XDs3s/LmJG5Vr3v3lLyfeAIefBAOOghuvDE9nrb55nD66fDyy3lHaWa2Kidxs4wEO+2Ueq/Pnw+XX57umZ98MgwaBF/5Cpx3HrzxRt6RmpklTuJmDejePU2ycs896TG1U05Jyfvoo6F//zQd6kUXeRY1M8uXosJu+tXW1sa0adPyDsOqUAQ89RT8+c+p/Pvf0KVLGhlu7FjYe2/YaKO8ozSzjkbS9IiobXCbk7hZy0XA9Okpmd90U0rokGZVGzs2laFDUxO9mVlrOImblVAEzJ4Nkyen8tBDad3AgWkyllGjYJddYJ118o7UzCqRk7hZO3rtNbjtNrjlljSwzJIl0KkTDB+eEvruu6ex3Lt0yTtSM6sETuJmOVm6NI0Sd9ddqTzySHoevWdP+OIX4ctfTqW2Frp1yztaMytHTuJmZWLRolQ7/+tf4e9/h5kz0/o11oARI1Yk9eHDU6I3M3MSNytTCxemcdvvvz+Vxx9PNXUJhgxJze7Dh6fXbbZJ86abWXVxEjerEIsXp1HjHn44Nb0//DC8+WbatuaaMGwY7LhjGud9223T1Kq+t27WsTmJm1WoCHjhhZWT+uOPr5gPvWvXVGPfbruU1Ote3RPerONwEjfrQJYtSzOwzZiRxnufMSOVwuFg+/dPtfT6Zf31/ey6WaVxEjerAq+9tiKpz5yZnl3/17/g3XdX7LP22isS+pZbwmabrSi9euUXu5k1rqkk7m4yZh3EBhukssceK9ZFpKlWZ89eudx666oTufTps3JSryuDBqWavTvVmZUf/7M068AkGDAgld13X3nb4sXw/PNpyNjC8s9/puFkP/lkxb6dOqVEvvHGaSS6jTde+f3AgdC3r5vqzdqbk7hZlerVK3WE2267Vbd9/DG8+GJK8i+9lOZTr3udPh3+8hf46KOVj+nadUVrwIYbrnhff93666fn4s2s9ZzEzWwVXbvCFluk0pAIWLAgJfW6BD9vHrz+ero3//zz6VG5BQsaPn7ttWG99VLtva706dPw+7590/41NaX7vmaVyknczFpMSkl4vfXSs+uNWbo0JfL581Nyryvz56f1CxfC3Lmpdr9w4aq1+8LPW3fdVHr3Tkl97bVXvG9oXeH7nj3TLQGzjqakSVzSaOAcoAb4Y0T8ot72bsDlwDDgTWD/iJhbypjMrP106ZLupffv3/y+EfDee2lwm4ULUyl8v3BhGrb2nXfg7bfh1VfT69tvw/vvN3/+7t2hR4/iSs+eKy93754G22morLGG/0Cw/JQsiUuqAX4H7A7MAx6VNDkiZhXsdjiwKCI2l3QAcAawf6liMrPyJa1ImoMGtezYpUtXJPf6r3Xv33svPW5XWN5+O90GKFzXWGtAU7p1azzBN7SuW7d0y6Jr19a9r7/cpUt6iqBTJ3cyrBalrIkPB+ZExPMAkq4BxgGFSXwccGr2/nrgXEmKSnt43cxy1aXLivvnrbV06coJf8mS9PrBB6tXFi9OfQXqlj/8MH3GRx+lUqr/7Tp3TqUusReWhta1dN+amlQ6dVr1taF1zb229hgplXJ4X1PTfqMmljKJbwS8XLA8D/h8Y/tExDJJ7wB9gIUljMvMrFFduqy4p94eli9Pyfzjj1Np7H1T2+reL1u2oixduvJyU+vrr6v7Q6Op45cvT48hNvZa974arb12uvXTHkqZxBtqzKn/N2cx+yDpSOBIgIEDB7Y+MjOzMlFTA2utlUpHFNF8wq+f+FvyWvc+YkX55JN833ft2n7Xt5RJfB6wccHyAODVRvaZJ6kz0Bt4q/6JIuIC4AJIw66WJFozM2tzkkf7K6VS9ql8FNhC0iaSugIHAJPr7TMZODh7Px74m++Hm5mZFadkfx9l97gnAVNIj5hdHBEzJf0MmBYRk4GLgCskzSHVwA8oVTxmZmYdTUkbOSLiduD2eut+WvD+Q+CbpYzBzMyso/IQBWZmZhXKSdzMzKxCOYmbmZlVKCdxMzOzCuUkbmZmVqGcxM3MzCqUKm1sFUkLgBfb8JR98VjtreVr2Hq+hm3D17H1fA1br62v4aCI6NfQhopL4m1N0rSIqM07jkrma9h6voZtw9ex9XwNW689r6Gb083MzCqUk7iZmVmFchLPZkezVvE1bD1fw7bh69h6voat127XsOrviZuZmVUq18TNzMwqVNUmcUmjJT0jaY6kE/KOp5xJ2ljSPZJmS5op6dhs/bqS7pb0XPa6TrZekn6TXdsnJe2Q7zcoH5JqJD0u6dZseRNJD2fX8M+Sumbru2XLc7Ltg/OMu1xIWlvS9ZL+lf0ed/LvsGUkHZf9O35a0tWS1vDvsHmSLpb0hqSnC9a1+Lcn6eBs/+ckHdzauKoyiUuqAX4HjAGGABMkDck3qrK2DDg+Ij4DjAC+m12vE4CpEbEFMDVbhnRdt8jKkcB57R9y2ToWmF2wfAZwVnYNFwGHZ+sPBxZFxObAWdl+BucAd0bE1sC2pGvp32GRJG0EHAPURsQ2QA1wAP4dFuNSYHS9dS367UlaFzgF+DwwHDilLvGvtoiougLsBEwpWD4RODHvuCqlADcDuwPPABtm6zYEnsnenw9MKNj/0/2quQADsn/ouwC3AiINCNE52/7p7xKYAuyUve+c7ae8v0PO168X8EL96+DfYYuu4UbAy8C62e/qVmAP/w6Lvn6DgacLllv02wMmAOcXrF9pv9UpVVkTZ8UPuc68bJ01I2tO2x54GFg/IuYDZK/rZbv5+jbsbOBHwCfZch/g7YhYli0XXqdPr2G2/Z1s/2q2KbAAuCS7JfFHSd3x77BoEfEK8CvgJWA+6Xc1Hf8OV1dLf3tt/pus1iSuBta5m34zJPUAbgC+HxGLm9q1gXVVfX0l7QW8ERHTC1c3sGsUsa1adQZ2AM6LiO2B91jRfNkQX8N6sqbbccAmQH+gO6nptz7/DlunsevW5tezWpP4PGDjguUBwKs5xVIRJHUhJfA/RcSN2erXJW2Ybd8QeCNb7+u7qi8CYyXNBa4hNamfDawtqXO2T+F1+vQaZtt7A2+1Z8BlaB4wLyIezpavJyV1/w6LtxvwQkQsiIilwI3AF/DvcHW19LfX5r/Jak3ijwJbZD0yu5I6dkzOOaayJUnARcDsiPh1wabJQF3vyoNJ98rr1n8766E5AninrsmpWkXEiRExICIGk35vf4uIg4B7gPHZbvWvYd21HZ/tX9U1oIh4DXhZ0lbZql2BWfh32BIvASMkrZX9u667hv4drp6W/vamAKMkrZO1iozK1q2+vDsK5NhBYU/gWeDfwE/yjqecC7AzqcnnSWBGVvYk3RubCjyXva6b7S9S7/9/A0+ResLm/j3KpQAjgVuz95sCjwBzgOuAbtn6NbLlOdn2TfOOuxwKsB0wLfst/gVYx7/DFl/D04B/AU8DVwDd/Dss6rpdTepHsJRUoz58dX57wGHZ9ZwDHNrauDxim5mZWYWq1uZ0MzOziuckbmZmVqGcxM3MzCqUk7iZmVmFchI3MzOrUE7iZh2QpOWSZhSUJmfqk3SUpG+3wefOldS3tecxs+L4ETOzDkjSuxHRI4fPnUt6JnZhe3+2WTVyTdysimQ15TMkPZKVzbP1p0r6Qfb+GEmzsnmQr8nWrSvpL9m6hyQNzdb3kXRXNiHJ+RSMDS1pYvYZMySdrzSXeo2kS7O5rJ+SdFwOl8Gsw3ASN+uY1qzXnL5/wbbFETEcOJc0fnt9JwDbR8RQ4Khs3WnA49m6HwOXZ+tPAR6INCHJZGAggKTPAPsDX4yI7YDlwEGkEdc2iohtIuJzwCVt+J3Nqk7n5ncxswr0QZY8G3J1wetZDWx/EviTpL+QhjaFNPTuvgAR8besBt4b+DLwjWz9bZIWZfvvCgwDHk1DdLMmaXKIW4BNJf0WuA24a/W/opm5Jm5WfaKR93W+Rhr3eRgwPZu9qqkpFBs6h4DLImK7rGwVEadGxCJgW+Be4LvAH1fzO5gZTuJm1Wj/gtd/Fm6Q1AnYOCLuAX4ErA30AO4nNYcjaSSwMNKc8oXrx5AmJIE0GcR4Setl29aVNCjrud4pIm4ATiZNJWpmq8nN6WYd05qSZhQs3xkRdY+ZdZP0MOmP+An1jqsBrsyaygWcFRFvSzoVuETSk8D7rJh+8TTgakmPAfeRprokImZJOgm4K/vDYCmp5v1Bdp66CsSJbfeVzaqPHzEzqyJ+BMysY3FzupmZWYVyTdzMzKxCuSZuZmZWoZzEzczMKpSTuJmZWYVyEjczM6tQTuJmZmYVyknczMysQv0/ye4suYZuXhsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 0: SETTING UP AGENT\n",
      "WARNING:tensorflow:From C:\\Users\\ACE\\Anaconda3\\envs\\vissim\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 48)                432       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,283\n",
      "Trainable params: 5,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 0\n",
      "\n",
      "INTERSECTION 1: SETTING UP AGENT\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 48)                432       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,283\n",
      "Trainable params: 5,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 1\n",
      "\n",
      "INTERSECTION 2: SETTING UP AGENT\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 14)]              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 48)                720       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4)                 196       \n",
      "=================================================================\n",
      "Total params: 5,620\n",
      "Trainable params: 5,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 2\n",
      "\n",
      "INTERSECTION 3: SETTING UP AGENT\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 48)                432       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,283\n",
      "Trainable params: 5,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 3\n",
      "\n",
      "INTERSECTION 4: SETTING UP AGENT\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 48)                336       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,187\n",
      "Trainable params: 5,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 4\n",
      "\n",
      "INTERSECTION 5: SETTING UP AGENT\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 4,994\n",
      "Trainable params: 4,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 5\n",
      "\n",
      "INTERSECTION 6: SETTING UP AGENT\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 48)                336       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 5,138\n",
      "Trainable params: 5,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 6\n",
      "\n",
      "INTERSECTION 7: SETTING UP AGENT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 48)                384       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,235\n",
      "Trainable params: 5,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 7\n",
      "\n",
      "INTERSECTION 8: SETTING UP AGENT\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 4,994\n",
      "Trainable params: 4,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 8\n",
      "\n",
      "INTERSECTION 9: SETTING UP AGENT\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 48)                240       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 9\n",
      "\n",
      "INTERSECTION 10: SETTING UP AGENT\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 48)                240       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 10\n",
      "\n",
      "INTERSECTION 11: SETTING UP AGENT\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 48)                240       \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 11\n",
      "\n",
      "INTERSECTION 12: SETTING UP AGENT\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 48)                384       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 4)                 196       \n",
      "=================================================================\n",
      "Total params: 5,284\n",
      "Trainable params: 5,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 12\n",
      "\n",
      "INTERSECTION 13: SETTING UP AGENT\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 48)                336       \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,187\n",
      "Trainable params: 5,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Double Deep Q Learning Agent(s) at Intersection 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Experience Found: Loading into agents\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.save(498)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx , agent in Balance_MultiDQN_Agents.Agents.items():\n",
    "    print(agent)\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For the agent training\n",
    "ploty = 1\n",
    "for idx , agent in Balance_MultiDQN_Agents.Agents.items():\n",
    "    print(\"Agent \"+str(idx))\n",
    "    #print(ploty)\n",
    "    #plt.subplot(14, 2, ploty)\n",
    "\n",
    "    plt.figure('6'+str(idx),figsize=(4.5, 3))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    ploty+=1\n",
    "    #print(ploty)\n",
    "\n",
    "    \n",
    "    #plt.subplot(14, 2, ploty)\n",
    "    plt.figure('7'+str(idx),figsize=(4.5, 3))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    #Loss_rewarddf.to_csv(csv_Path,index=False)\n",
    "    ploty+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Balance_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.load(498, best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight AC\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Straigth_AC\"\n",
    "\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.00001\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 0.5\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.train(200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.save(401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.load(200, best = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay = Single_Cross_Straight_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in  Single_Cross_Straight_MultiAC_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.Agents[0].Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDQN\"\n",
    "Session_ID = \"Single_Cross_Straigth_DuelingDQN20c0\"\n",
    "\n",
    "# all controller actions\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4]  }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 300\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.load(300 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Single_Cross_Straight_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in  Single_Cross_Straight_MultiDQN_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3600\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_TripleAC4test1\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "    },\n",
    "   'demand' : {\"default\" : [400,400,400,400] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 5000\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 action DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "Session_ID = \"Single_Cross_Triple4_actions\"\n",
    "#Session_ID = \"DQN\"\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{ 'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' :    {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    "         \n",
    "         'all_actions' :       {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "        },\n",
    "     'demand' : { 'default' : [400, 400, 400, 400]}\n",
    "                  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 5\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.train(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.load(best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To be arranged for multy agents\n",
    "\n",
    "queues = np.array(Episode_Queues[0])\n",
    "queues = queues.T\n",
    "\n",
    "delay = Cumulative_Episode_Delays[0]\n",
    "\n",
    "# Plot the queues\n",
    "plt.figure(1)\n",
    "for queue in queues:\n",
    "    plt.plot(queue)\n",
    "\n",
    "# plot the junctions delays\n",
    "plt.figure(2)\n",
    "plt.plot(delay)\n",
    "\n",
    "#plot the total delays \n",
    "plt.figure(3)\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "\n",
    "# Dont freak out the 2 delays are not the same because the node is not covering all the junction\n",
    "\n",
    "\"\"\"\n",
    "Because the cars never leave the nodes the delay is not computed correctly (when the agent doesn't work) \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].loss)\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "print(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_AC10\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]             \n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [300,300,300,300],\n",
    "             1 : [600,600,600,600],\n",
    "             2 : [1350,750,1350,750],\n",
    "             3 : [1500,750,1500,750],\n",
    "             4 : [1050,750,1050,750],\n",
    "             5 : [750,1050,750,1050],\n",
    "             6 : [750,1500,750,1500],\n",
    "             7 : [750,1350,750,1350],\n",
    "             8 : [600,600,600,600],\n",
    "             9 : [300,300,300,300]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.000001\n",
    "\n",
    "\n",
    "value = 5\n",
    "entropy = 500\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple8_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_DuelingDDQN20c10\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]             \n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [300,300,300,300],\n",
    "             1 : [600,600,600,600],\n",
    "             2 : [1350,750,1350,750],\n",
    "             3 : [1500,750,1500,750],\n",
    "             4 : [1050,750,1050,750],\n",
    "             5 : [750,1050,750,1050],\n",
    "             6 : [750,1500,750,1500],\n",
    "             7 : [750,1350,750,1350],\n",
    "             8 : [600,600,600,600],\n",
    "             9 : [300,300,300,300]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400 \n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.train(episodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Single_Cross_Triple8_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in Single_Cross_Triple8_MultiDQN_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Five intersection DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Five_intersection'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Five5transfert\"\n",
    "\n",
    "# all controller actions\n",
    "Five_intersection_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['11-1', '11-2', '11-3', '12-1', '12-2', '12-3', '13-1', '13-2', '13-3', '14-1', '14-2', '14-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "         },\n",
    "                  1 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['21-1', '21-2', '21-3', '22-1', '22-2', '22-3', '23-1', '23-2', '23-3', '24-1', '24-2', '24-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "        'queues_counter_ID' : [13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "         },\n",
    "                  2 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['31-1', '31-2', '31-3', '32-1', '32-2', '32-3', '33-1', '33-2', '33-3', '34-1', '34-2', '34-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [25,26,27,28,29,30,31,32,33,34,35,36]\n",
    "         },\n",
    "                  3 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['41-1', '41-2', '41-3', '42-1', '42-2', '42-3', '43-1', '43-2', '43-3', '44-1', '44-2', '44-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "          'queues_counter_ID' : [37,38,39,40,41,42,43,44,45,46,47,48]\n",
    "         },\n",
    "                  4 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['51-1', '51-2', '51-3', '52-1', '52-2', '52-3', '53-1', '53-2', '53-3', '54-1', '54-2', '54-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [49,50,51,52,53,54,55,56,57,58,59,60]\n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             \n",
    "             0 : [200,200,200,200,200,200,200,200,200,200,200,200],\n",
    "             1 : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             2 : [500,900,500,500,900,500,500,900,500,500,900,500],\n",
    "             3 : [500,1000,500,500,1000,500,500,1000,500,500,1000,500],\n",
    "             4 : [500,700,500,500,700,500,500,700,500,500,700,500],\n",
    "             5 : [500,700,500,500,700,500,500,700,500,500,700,500],\n",
    "             6 : [500,1000,500,500,1000,500,500,1000,500,500,1000,500],\n",
    "             7 : [500,900,500,500,900,500,500,900,500,500,900,500],\n",
    "             8 : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             9 : [200,200,200,200,200,200,200,200,200,200,200,200]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Five_intersection_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.save(401)\n",
    "Five_intersection_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.Agents[0].load_agent(vissim_working_directory, 'Single_Cross_Triple', 'Single_Cross_Triple8_actions',400 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay = Five_intersection_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        \n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Queue Length')\n",
    "    plt.title('Junction {} Queue length'.format(idx))\n",
    "    plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Accumulated Delay')\n",
    "    plt.title('Junction {} Delay'.format(idx))\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Accumulated Stop Delay')\n",
    "    plt.title('Junction {} Stop Delay'.format(idx))\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Global accumulated Delay')\n",
    "plt.title('Global accumulated Delay')\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Global accumulated stop Delay')\n",
    "plt.title('Global accumulated stop Delay')\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.Agents[2] = Five_intersection_MultiDQN_Agents.Agents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
