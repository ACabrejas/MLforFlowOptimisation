{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_Critic_Class import ACAgent\n",
    "from MasterAC_Agent import MasterAC_Agent\n",
    "from MasterDQN_Agent import MasterDQN_Agent\n",
    "# Network Specific Libraries\n",
    "from Balance_Functions import balance_dictionary\n",
    "\n",
    "import numpy as np \n",
    "import pylab as plt\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%load_ext tensorboard\n",
    "\n",
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "agent_type = 'AC'\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n"
     ]
    }
   ],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  960       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  630       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_2 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,307\n",
      "Trainable params: 12,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_3 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_4 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_5 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_6 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,373\n",
      "Trainable params: 11,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_7 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,522\n",
      "Trainable params: 11,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_9 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_10  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_11  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_12  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_13  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agent hyperparameters\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "timesteps_per_second = 1\n",
    "\n",
    "\n",
    "# for the monitoring only for AC\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 1800 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 1.06 seconds.\n",
      "\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4674e+03 - output_1_loss: -5.4941e+03 - output_2_loss: 26.7329\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4573e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 8.7875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4571e+03 - output_1_loss: -3.4649e+03 - output_2_loss: 7.8728\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4351e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 30.4049\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4579e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 7.8736\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4345e+03 - output_1_loss: -5.4942e+03 - output_2_loss: 59.6747\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4028e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 63.3638\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.1407e+03 - output_1_loss: -5.4775e+03 - output_2_loss: 1336.8276\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4578e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 8.3335\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4838e+03 - output_1_loss: -5.4917e+03 - output_2_loss: 7.8809\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4579e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 7.6377\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3989e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 67.8662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3942e+03 - output_1_loss: -5.4950e+03 - output_2_loss: 100.7995\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4669e+03 - output_1_loss: -5.4914e+03 - output_2_loss: 24.5284\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4366e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 29.7350\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4647e+03 - output_1_loss: -5.4932e+03 - output_2_loss: 28.5185\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4127e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 53.6793\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7591e+03 - output_1_loss: -5.4779e+03 - output_2_loss: 1718.8265\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4034e+03 - output_1_loss: -3.4631e+03 - output_2_loss: 59.6264\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4085e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 57.6951\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3064e+03 - output_1_loss: -5.4780e+03 - output_2_loss: 3171.5784\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4052e+03 - output_1_loss: -3.4649e+03 - output_2_loss: 59.6575\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4042e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 62.2442\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4304e+03 - output_1_loss: -5.4893e+03 - output_2_loss: 58.8286\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3441e+03 - output_1_loss: -6.9336e+03 - output_2_loss: 3589.4758\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2592e+03 - output_1_loss: -5.4939e+03 - output_2_loss: 234.6320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3040e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 191.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4152e+03 - output_1_loss: -5.4903e+03 - output_2_loss: 3075.0439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4706e+03 - output_1_loss: -5.4926e+03 - output_2_loss: 21.9963\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 521.7256 - output_1_loss: -5.4806e+03 - output_2_loss: 6002.3423\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4103e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 55.3710\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4084e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 57.6409\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3051e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 162.1778\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3335e+03 - output_1_loss: -5.4892e+03 - output_2_loss: 155.6323\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1819.0571 - output_1_loss: -6.9427e+03 - output_2_loss: 8761.7373\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3562e+03 - output_1_loss: -3.4647e+03 - output_2_loss: 108.4424\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3712e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 95.7029\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4742e+03 - output_1_loss: -5.4930e+03 - output_2_loss: 18.7905\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2795e+03 - output_1_loss: -5.4951e+03 - output_2_loss: 215.6326\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0717e+03 - output_1_loss: -5.4923e+03 - output_2_loss: 420.5308\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4132e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 53.0968\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1654e+03 - output_1_loss: -3.4638e+03 - output_2_loss: 298.4696\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.5359e+01 - output_1_loss: -5.5001e+03 - output_2_loss: 5444.6997\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2299e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 235.7737\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3948.8652 - output_1_loss: -5.4912e+03 - output_2_loss: 9440.1025\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4111e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 55.2020\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2091e+03 - output_1_loss: -5.4920e+03 - output_2_loss: 282.8970\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3405.9683 - output_1_loss: -6.9428e+03 - output_2_loss: 10348.7227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.4390e+03 - output_1_loss: -5.4884e+03 - output_2_loss: 1049.4071\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3813e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 85.5684\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9990e+03 - output_1_loss: -3.4639e+03 - output_2_loss: 464.8617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3037e+03 - output_1_loss: -5.4956e+03 - output_2_loss: 191.9274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7757.2300 - output_1_loss: -5.5053e+03 - output_2_loss: 13262.5225\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1719e+03 - output_1_loss: -5.4964e+03 - output_2_loss: 324.4840\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4160e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 50.3849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3507.9150 - output_1_loss: -6.9495e+03 - output_2_loss: 10457.4414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.7211e+02 - output_1_loss: -5.5035e+03 - output_2_loss: 4931.3711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2630e+03 - output_1_loss: -3.4642e+03 - output_2_loss: 201.1702\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3864e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 80.3817\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4142e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 52.1730\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1501e+03 - output_1_loss: -5.4938e+03 - output_2_loss: 343.6420\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9373e+03 - output_1_loss: -5.4978e+03 - output_2_loss: 560.5225\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0904e+03 - output_1_loss: -5.4883e+03 - output_2_loss: 1397.9475\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0328e+03 - output_1_loss: -3.4643e+03 - output_2_loss: 431.5408\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9223e+03 - output_1_loss: -6.9323e+03 - output_2_loss: 9.9646\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.2387e+02 - output_1_loss: -5.5054e+03 - output_2_loss: 4681.5757\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3465e+03 - output_1_loss: -5.4909e+03 - output_2_loss: 144.3887\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2227e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 244.1309\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6782.2891 - output_1_loss: -5.5110e+03 - output_2_loss: 12293.3057\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3402e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 126.9989\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4181e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 48.2067\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9984e+03 - output_1_loss: -5.4957e+03 - output_2_loss: 497.2899\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4179.5610 - output_1_loss: -6.9522e+03 - output_2_loss: 11131.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3930e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 73.6994\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0577e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 408.1644\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1802e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 287.3578\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.3095e+03 - output_1_loss: -5.5003e+03 - output_2_loss: 1190.7996\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4972e+03 - output_1_loss: -5.4961e+03 - output_2_loss: 1998.8519\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4234e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 42.9220\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.8560e+03 - output_1_loss: -6.9327e+03 - output_2_loss: 76.6538\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1507e+03 - output_1_loss: -5.5045e+03 - output_2_loss: 4353.7397\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2472e+03 - output_1_loss: -5.4946e+03 - output_2_loss: 247.3658\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4015e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 64.9801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9720.1504 - output_1_loss: -5.5175e+03 - output_2_loss: 15237.6367\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7870e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 678.6085\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6945e+03 - output_1_loss: -5.4977e+03 - output_2_loss: 803.1250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5271.9473 - output_1_loss: -6.9546e+03 - output_2_loss: 12226.5596\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2584e+03 - output_1_loss: -5.5029e+03 - output_2_loss: 2244.5134\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5662e+03 - output_1_loss: -5.4992e+03 - output_2_loss: 2932.9849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4086e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 58.0641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0889e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 377.4721\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.6563e+03 - output_1_loss: -6.9345e+03 - output_2_loss: 278.2399\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -7.2234e+02 - output_1_loss: -5.5057e+03 - output_2_loss: 4783.3950\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2028e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 264.7776\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4130e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 53.4109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12117.8867 - output_1_loss: -5.5200e+03 - output_2_loss: 17637.9121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.9639e+03 - output_1_loss: -5.4999e+03 - output_2_loss: 1535.9913\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1961e+03 - output_1_loss: -5.4945e+03 - output_2_loss: 298.4611\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.6000e+03 - output_1_loss: -5.5059e+03 - output_2_loss: 3905.8779\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5572e+03 - output_1_loss: -5.5037e+03 - output_2_loss: 3946.5056\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2334e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 233.9507\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14568.5195 - output_1_loss: -5.5222e+03 - output_2_loss: 20090.6855\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4596e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 6.4242\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4283e+03 - output_1_loss: -3.4609e+03 - output_2_loss: 1032.5601\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8042.8574 - output_1_loss: -6.9573e+03 - output_2_loss: 15000.1260\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4259e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 40.4475\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1303e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 336.3617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.5402e+03 - output_1_loss: -6.9403e+03 - output_2_loss: 1400.1085\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6012e+02 - output_1_loss: -5.5067e+03 - output_2_loss: 5246.5918\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4284e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 37.8029\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17665.4180 - output_1_loss: -5.5230e+03 - output_2_loss: 23188.4121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3310e+03 - output_1_loss: -5.5000e+03 - output_2_loss: 2168.9807\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11599.0605 - output_1_loss: -6.9601e+03 - output_2_loss: 18559.1270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2065e+03 - output_1_loss: -5.4952e+03 - output_2_loss: 288.6704\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1155.4141 - output_1_loss: -5.5096e+03 - output_2_loss: 6664.9717\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 235.8999 - output_1_loss: -5.5076e+03 - output_2_loss: 5743.4604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4419e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 24.3456\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4336e+03 - output_1_loss: -6.9224e+03 - output_2_loss: 3488.8076\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 545.9902 - output_1_loss: -5.5080e+03 - output_2_loss: 6053.9902\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2278e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 239.7099\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4495e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 16.5247\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4568e+03 - output_1_loss: -3.4647e+03 - output_2_loss: 7.8845\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9537e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 1513.4128\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1857e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 280.9407\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15107.4707 - output_1_loss: -6.9628e+03 - output_2_loss: 22070.2207\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2201e+03 - output_1_loss: -5.4955e+03 - output_2_loss: 275.3521\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2595e+02 - output_1_loss: -6.9263e+03 - output_2_loss: 6400.3667\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4592e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 6.6802\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20184.8223 - output_1_loss: -5.5160e+03 - output_2_loss: 25700.7793\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4581e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 7.6130\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.9415e+02 - output_1_loss: -5.5064e+03 - output_2_loss: 4912.1997\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5808.1738 - output_1_loss: -5.5132e+03 - output_2_loss: 11321.4131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3849.2295 - output_1_loss: -5.5106e+03 - output_2_loss: 9359.8057\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4479e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 18.2404\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1827.2114 - output_1_loss: -5.5098e+03 - output_2_loss: 7336.9922\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2376e+03 - output_1_loss: -5.4955e+03 - output_2_loss: 257.8956\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2959e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 171.2751\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4011.9595 - output_1_loss: -6.9412e+03 - output_2_loss: 10953.1553\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9811e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 1486.6107\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4630e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 2.8185\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2609e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 205.6325\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15022.5488 - output_1_loss: -6.9647e+03 - output_2_loss: 21987.2324\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22859.0449 - output_1_loss: -5.5160e+03 - output_2_loss: 28375.0801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4586e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 7.2715\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4490.4844 - output_1_loss: -5.5138e+03 - output_2_loss: 10004.2637\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4329.7402 - output_1_loss: -5.4986e+03 - output_2_loss: 9828.3447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7778.8208 - output_1_loss: -5.5150e+03 - output_2_loss: 13293.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4435e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 22.8014\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3339.2749 - output_1_loss: -5.5114e+03 - output_2_loss: 8850.6904\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0619e+03 - output_1_loss: -5.4971e+03 - output_2_loss: 435.2445\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3493e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 117.5455\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9096.7402 - output_1_loss: -6.9583e+03 - output_2_loss: 16055.0254\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7763e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 1693.5874\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4554e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 10.7114\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4591e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 6.9053\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3254e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 140.8640\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15328.8320 - output_1_loss: -6.9639e+03 - output_2_loss: 22292.7324\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3540.0879 - output_1_loss: -5.5065e+03 - output_2_loss: 9046.5928\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11950.5391 - output_1_loss: -5.5152e+03 - output_2_loss: 17465.7461\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5117.5112 - output_1_loss: -5.5131e+03 - output_2_loss: 10630.6455\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3591e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 107.8703\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15471.8730 - output_1_loss: -6.9667e+03 - output_2_loss: 22438.5430\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26216.3535 - output_1_loss: -5.5203e+03 - output_2_loss: 31736.6426\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8296e+03 - output_1_loss: -3.4701e+03 - output_2_loss: 1640.5071\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5741.6045 - output_1_loss: -5.5115e+03 - output_2_loss: 11253.0781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4372e+03 - output_1_loss: -3.4651e+03 - output_2_loss: 27.9491\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4595e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 6.4956\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3730e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 94.0414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16116.8906 - output_1_loss: -6.9646e+03 - output_2_loss: 23081.4609\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8135e+03 - output_1_loss: -5.4975e+03 - output_2_loss: 684.0123\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7106.7915 - output_1_loss: -5.5143e+03 - output_2_loss: 12621.0586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3636e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 103.4216\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21989.9473 - output_1_loss: -6.9726e+03 - output_2_loss: 28962.5859\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29771.7266 - output_1_loss: -5.5260e+03 - output_2_loss: 35297.7031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3841e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 81.9498\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3958.5059 - output_1_loss: -5.5094e+03 - output_2_loss: 9467.9180\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18269.8789 - output_1_loss: -5.5191e+03 - output_2_loss: 23788.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4481e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 18.2154\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2610e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 206.6664\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0159e+03 - output_1_loss: -3.4697e+03 - output_2_loss: 1453.7469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8538.7070 - output_1_loss: -5.5142e+03 - output_2_loss: 14052.8789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4398e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 25.8525\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16696.2266 - output_1_loss: -6.9642e+03 - output_2_loss: 23660.4688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9608e+03 - output_1_loss: -5.4956e+03 - output_2_loss: 534.7779\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29263.8242 - output_1_loss: -5.5145e+03 - output_2_loss: 34778.3633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4240e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 42.5766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9447.0566 - output_1_loss: -5.5169e+03 - output_2_loss: 14963.9541\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3957e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 71.0450\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29387.8086 - output_1_loss: -6.9768e+03 - output_2_loss: 36364.6484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33750.1133 - output_1_loss: -5.5304e+03 - output_2_loss: 39280.5469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1519e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 1317.4025\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4220e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 44.2810\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2751.2183 - output_1_loss: -5.5079e+03 - output_2_loss: 8259.1162\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4058e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 60.8601\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2843e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 183.0662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17044.8477 - output_1_loss: -6.9662e+03 - output_2_loss: 24011.0059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3915e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 75.3990\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12355.9648 - output_1_loss: -5.5191e+03 - output_2_loss: 17875.1074\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3478e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 119.4200\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10588.2852 - output_1_loss: -5.5140e+03 - output_2_loss: 16102.2754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2742e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 1194.8967\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0644e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 431.5412\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27823.9844 - output_1_loss: -5.5049e+03 - output_2_loss: 33328.9141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35011.0469 - output_1_loss: -6.9602e+03 - output_2_loss: 41971.2227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38029.5508 - output_1_loss: -5.5339e+03 - output_2_loss: 43563.4219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14494.6113 - output_1_loss: -5.5210e+03 - output_2_loss: 20015.6328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3870e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 79.7407\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3298e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 137.5389\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2013.5874 - output_1_loss: -5.5078e+03 - output_2_loss: 7521.3457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4175e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 48.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4955e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 973.2300\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2949e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 172.2894\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19686.0508 - output_1_loss: -6.9698e+03 - output_2_loss: 26655.8125\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1246e+03 - output_1_loss: -5.4962e+03 - output_2_loss: 371.6681\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4020e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 64.7790\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17010.1445 - output_1_loss: -5.5214e+03 - output_2_loss: 22531.5098\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15850.9883 - output_1_loss: -5.5219e+03 - output_2_loss: 21372.8984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3012e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 166.2437\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27939.9941 - output_1_loss: -5.5143e+03 - output_2_loss: 33454.3164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8418e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 626.5009\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0941e+03 - output_1_loss: -5.4965e+03 - output_2_loss: 402.4124\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4091e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 57.5341\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 42970.1523 - output_1_loss: -6.9711e+03 - output_2_loss: 49941.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32089.7168 - output_1_loss: -5.5283e+03 - output_2_loss: 37618.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4071e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 58.1100\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2317.5225 - output_1_loss: -5.5094e+03 - output_2_loss: 7826.9214\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4202e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 46.3073\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3082e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 158.8514\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17852.8027 - output_1_loss: -6.9670e+03 - output_2_loss: 24819.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25842.7109 - output_1_loss: -5.5273e+03 - output_2_loss: 31370.0234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 51686.5078 - output_1_loss: -6.9723e+03 - output_2_loss: 58658.8516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10738.3750 - output_1_loss: -5.5156e+03 - output_2_loss: 16253.9590\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3617e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 105.0666\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3364e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 130.7592\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28921.1562 - output_1_loss: -5.5187e+03 - output_2_loss: 34439.8086\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2047e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 262.6805\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3255e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 141.4536\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1950e+03 - output_1_loss: -5.4958e+03 - output_2_loss: 300.7761\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4181e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 48.3435\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34605.7344 - output_1_loss: -5.5293e+03 - output_2_loss: 40135.0781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35629.4062 - output_1_loss: -5.5340e+03 - output_2_loss: 41163.3594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2844.7095 - output_1_loss: -5.5096e+03 - output_2_loss: 8354.3457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3231e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 144.2394\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3205e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 146.6833\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3521e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 114.8333\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19350.8789 - output_1_loss: -6.9691e+03 - output_2_loss: 26320.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30237.9375 - output_1_loss: -5.5222e+03 - output_2_loss: 35760.0977\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3483e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 118.4501\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 55169.8359 - output_1_loss: -6.9702e+03 - output_2_loss: 62140.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13659.1406 - output_1_loss: -5.5197e+03 - output_2_loss: 19178.8262\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 47919.2891 - output_1_loss: -5.5399e+03 - output_2_loss: 53459.1719\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3050e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 162.0065\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23119.4688 - output_1_loss: -6.9665e+03 - output_2_loss: 30085.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2286e+03 - output_1_loss: -5.4955e+03 - output_2_loss: 266.9031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3663e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 100.7327\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3579e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 108.6809\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39974.1523 - output_1_loss: -5.5287e+03 - output_2_loss: 45502.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 430.6719 - output_1_loss: -5.5059e+03 - output_2_loss: 5936.6191\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3649e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 101.9838\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2881e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 179.3224\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3228e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 144.4907\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 59154.0664 - output_1_loss: -5.5392e+03 - output_2_loss: 64693.2539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29144.7891 - output_1_loss: -5.5220e+03 - output_2_loss: 34666.7461\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3831e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 83.5090\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28644.4844 - output_1_loss: -6.9646e+03 - output_2_loss: 35609.0586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 61088.7734 - output_1_loss: -6.9737e+03 - output_2_loss: 68062.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 46301.4375 - output_1_loss: -5.5267e+03 - output_2_loss: 51828.1836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14075.9053 - output_1_loss: -5.5207e+03 - output_2_loss: 19596.6289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3745e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 92.2968\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2968e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 170.6520\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 69872.5703 - output_1_loss: -5.5415e+03 - output_2_loss: 75414.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2033e+03 - output_1_loss: -5.4960e+03 - output_2_loss: 292.7291\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28506.4238 - output_1_loss: -5.5225e+03 - output_2_loss: 34028.9609\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3652e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 100.5744\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3667e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 100.1493\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3456e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 121.3539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.9620e+02 - output_1_loss: -5.5055e+03 - output_2_loss: 5109.3198\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3017e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 165.4913\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3869e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 79.8171\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0311e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 437.5071\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36356.6133 - output_1_loss: -6.9713e+03 - output_2_loss: 43327.9414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3766e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 90.0356\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3657e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 101.2304\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 85120.9609 - output_1_loss: -6.9845e+03 - output_2_loss: 92105.4688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35337.8242 - output_1_loss: -5.5201e+03 - output_2_loss: 40857.8789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7989.0498 - output_1_loss: -5.5128e+03 - output_2_loss: 13501.8037\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3141e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 153.0346\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 79901.4141 - output_1_loss: -5.5467e+03 - output_2_loss: 85448.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1612e+03 - output_1_loss: -5.4961e+03 - output_2_loss: 334.8285\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21330.0039 - output_1_loss: -5.5181e+03 - output_2_loss: 26848.0625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3603e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 106.5720\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 48672.8906 - output_1_loss: -6.9729e+03 - output_2_loss: 55645.8281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.4480e+03 - output_1_loss: -5.5014e+03 - output_2_loss: 4053.3811\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28793.8320 - output_1_loss: -5.5170e+03 - output_2_loss: 34310.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4032e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 63.2690\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8734e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 595.6968\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2232e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 244.1068\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3902e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 76.4269\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2811e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 186.4331\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3746e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 92.2849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 105691.7656 - output_1_loss: -6.9892e+03 - output_2_loss: 112680.9766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8463.6084 - output_1_loss: -5.5145e+03 - output_2_loss: 13978.1396\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22624.3320 - output_1_loss: -5.5141e+03 - output_2_loss: 28138.4375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 81563.7891 - output_1_loss: -5.5488e+03 - output_2_loss: 87112.5547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1079e+03 - output_1_loss: -5.4960e+03 - output_2_loss: 388.1377\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18364.7715 - output_1_loss: -5.5154e+03 - output_2_loss: 23880.1230\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 43838.5664 - output_1_loss: -6.9639e+03 - output_2_loss: 50802.4766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5258e+03 - output_1_loss: -5.5005e+03 - output_2_loss: 3974.6853\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2935e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 173.9414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4253e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 40.9420\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0319e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 436.3106\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2585e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 208.3877\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3699e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 97.0760\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3129e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 153.1692\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 131335.0781 - output_1_loss: -7.0004e+03 - output_2_loss: 138335.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9715.9355 - output_1_loss: -5.5172e+03 - output_2_loss: 15233.1523\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5658e+02 - output_1_loss: -5.5041e+03 - output_2_loss: 5147.5645\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0260e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 442.4652\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21867.6016 - output_1_loss: -5.5179e+03 - output_2_loss: 27385.5312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 93102.3906 - output_1_loss: -5.5541e+03 - output_2_loss: 98656.4453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9934e+03 - output_1_loss: -5.4969e+03 - output_2_loss: 503.4916\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12239.7871 - output_1_loss: -5.5094e+03 - output_2_loss: 17749.1465\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8992e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 569.5577\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 47097.4297 - output_1_loss: -6.9753e+03 - output_2_loss: 54072.6836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3373e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 129.3754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11488.8887 - output_1_loss: -5.5184e+03 - output_2_loss: 17007.2500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2997e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 167.7603\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7070e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 762.1323\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20751.6680 - output_1_loss: -5.5103e+03 - output_2_loss: 26262.0137\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2856e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 181.1121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4105e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 56.0832\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7969e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 672.2050\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 143171.7656 - output_1_loss: -6.9928e+03 - output_2_loss: 150164.5938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 59204.8750 - output_1_loss: -6.9882e+03 - output_2_loss: 66193.0547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 728.3999 - output_1_loss: -5.5067e+03 - output_2_loss: 6235.0864\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13170.9414 - output_1_loss: -5.5192e+03 - output_2_loss: 18690.1641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 105718.5156 - output_1_loss: -5.5595e+03 - output_2_loss: 111278.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6968e+03 - output_1_loss: -5.4971e+03 - output_2_loss: 800.2239\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11039.9727 - output_1_loss: -5.5134e+03 - output_2_loss: 16553.3887\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3264e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 140.1456\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4242e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 42.1963\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3105e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 156.7217\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1706e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 297.0749\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8893e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 579.2546\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14888.3633 - output_1_loss: -5.5041e+03 - output_2_loss: 20392.5020\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9168e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 551.7407\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3188e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 148.0457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 174391.7812 - output_1_loss: -7.0104e+03 - output_2_loss: 181402.1875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 78234.1641 - output_1_loss: -6.9989e+03 - output_2_loss: 85233.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2161e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 251.6958\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3776.0420 - output_1_loss: -5.5115e+03 - output_2_loss: 9287.5361\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7168.7129 - output_1_loss: -5.5112e+03 - output_2_loss: 12679.9189\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 124882.4453 - output_1_loss: -5.5668e+03 - output_2_loss: 130449.2422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8117e+03 - output_1_loss: -5.4965e+03 - output_2_loss: 684.7556\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0405e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 427.7640\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9236e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 544.7510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12411.1543 - output_1_loss: -5.5078e+03 - output_2_loss: 17918.9941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10661.2598 - output_1_loss: -5.5149e+03 - output_2_loss: 16176.1152\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4312e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 35.1165\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9614e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 506.5060\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 100401.9922 - output_1_loss: -7.0089e+03 - output_2_loss: 107410.9062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9736e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 495.1401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 140791.1875 - output_1_loss: -5.5709e+03 - output_2_loss: 146362.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8737e+03 - output_1_loss: -5.4968e+03 - output_2_loss: 623.0263\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: -3.3422e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 124.4141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 201370.7656 - output_1_loss: -7.0153e+03 - output_2_loss: 208386.0938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4334e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 32.8347\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10585.0645 - output_1_loss: -5.5182e+03 - output_2_loss: 16103.2559\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0424e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 425.3544\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8453.9199 - output_1_loss: -5.5148e+03 - output_2_loss: 13968.6846\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6796e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 789.9556\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9202e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 548.4773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9663e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 501.7124\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17312.8359 - output_1_loss: -5.5153e+03 - output_2_loss: 22828.1289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8242.7656 - output_1_loss: -5.5111e+03 - output_2_loss: 13753.8662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 155710.4688 - output_1_loss: -5.5699e+03 - output_2_loss: 161280.3438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 124489.5312 - output_1_loss: -7.0232e+03 - output_2_loss: 131512.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18383.1074 - output_1_loss: -5.5249e+03 - output_2_loss: 23908.0293\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0373e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 430.6789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.7468e+03 - output_1_loss: -5.4979e+03 - output_2_loss: 751.1051\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1356e+03 - output_1_loss: -3.4707e+03 - output_2_loss: 1335.0604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5648e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 904.6701\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9435e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 524.7654\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3533e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 113.2127\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 210899.4688 - output_1_loss: -7.0074e+03 - output_2_loss: 217906.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 170343.5938 - output_1_loss: -5.5674e+03 - output_2_loss: 175911.0156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4239e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 42.6134\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 157628.9375 - output_1_loss: -7.0381e+03 - output_2_loss: 164666.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5257.7046 - output_1_loss: -5.5075e+03 - output_2_loss: 10765.2217\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26637.5977 - output_1_loss: -5.5302e+03 - output_2_loss: 32167.8262\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18896.8047 - output_1_loss: -5.5194e+03 - output_2_loss: 24416.2441\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9575.7891 - output_1_loss: -5.5146e+03 - output_2_loss: 15090.3945\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5662e+03 - output_1_loss: -3.4715e+03 - output_2_loss: 1905.3274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9741e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 494.1108\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4101e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 56.5437\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0544e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 413.5660\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3727.8560 - output_1_loss: -5.5064e+03 - output_2_loss: 9234.2930\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.3296e+03 - output_1_loss: -5.4993e+03 - output_2_loss: 1169.7339\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36253.5898 - output_1_loss: -5.5356e+03 - output_2_loss: 41789.1602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8181e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 650.3220\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3408e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 126.1153\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 232141.3438 - output_1_loss: -7.0226e+03 - output_2_loss: 239163.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 160014.5312 - output_1_loss: -5.5595e+03 - output_2_loss: 165574.0469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 183786.5938 - output_1_loss: -7.0207e+03 - output_2_loss: 190807.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3957e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 71.1189\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0787e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 389.2149\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23825.4668 - output_1_loss: -5.5223e+03 - output_2_loss: 29347.7734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7786.8428 - output_1_loss: -5.5137e+03 - output_2_loss: 13300.5527\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 516.4307 - output_1_loss: -5.5032e+03 - output_2_loss: 6019.6201\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.8365e+03 - output_1_loss: -5.5009e+03 - output_2_loss: 1664.4032\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0146e+03 - output_1_loss: -3.4704e+03 - output_2_loss: 1455.7814\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 46840.6094 - output_1_loss: -5.5407e+03 - output_2_loss: 52381.3398\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1204e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 346.9777\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 128449.6484 - output_1_loss: -5.5536e+03 - output_2_loss: 134003.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3574e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 109.6909\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7203e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 748.5811\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1463e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 321.4413\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33363.8242 - output_1_loss: -5.5031e+03 - output_2_loss: 38866.9102\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3709e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 95.8516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 242249.5781 - output_1_loss: -7.0123e+03 - output_2_loss: 249261.9062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1601e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 1309.7153\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2078e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 259.1107\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 98128.0781 - output_1_loss: -5.5424e+03 - output_2_loss: 103670.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 160295.5312 - output_1_loss: -7.0185e+03 - output_2_loss: 167313.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11821.9805 - output_1_loss: -5.5177e+03 - output_2_loss: 17339.6328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -7.7833e+02 - output_1_loss: -5.5010e+03 - output_2_loss: 4722.6880\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0103e+02 - output_1_loss: -5.5083e+03 - output_2_loss: 5107.2319\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2353e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 232.4903\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34192.2656 - output_1_loss: -5.5267e+03 - output_2_loss: 39718.9336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7079e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 761.0250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45239.8633 - output_1_loss: -5.5130e+03 - output_2_loss: 50752.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3655e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 101.3041\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 249383.6562 - output_1_loss: -7.0344e+03 - output_2_loss: 256418.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3092e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 1160.1167\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1354e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 332.7510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5055.5601 - output_1_loss: -5.4965e+03 - output_2_loss: 10552.0430\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7256e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 743.2185\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3012e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 165.3689\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 92054.5469 - output_1_loss: -5.5468e+03 - output_2_loss: 97601.3750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 192407.5781 - output_1_loss: -7.0383e+03 - output_2_loss: 199445.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4128e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 1056.0355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8871.5215 - output_1_loss: -5.5149e+03 - output_2_loss: 14386.4287\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.8946e+02 - output_1_loss: -5.5026e+03 - output_2_loss: 4613.1099\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2896e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 177.7200\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 41174.6133 - output_1_loss: -5.5326e+03 - output_2_loss: 46707.2539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 57159.5156 - output_1_loss: -5.5263e+03 - output_2_loss: 62685.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2150e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 252.6819\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 204830.3438 - output_1_loss: -7.0157e+03 - output_2_loss: 211846.0938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 239110.0625 - output_1_loss: -7.0553e+03 - output_2_loss: 246165.3438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4072e+03 - output_1_loss: -3.4692e+03 - output_2_loss: 1061.9771\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2416e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 225.8398\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7194.4131 - output_1_loss: -5.5121e+03 - output_2_loss: 12706.5225\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6845.0596 - output_1_loss: -5.5030e+03 - output_2_loss: 12348.0908\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2314e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 236.3214\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45329.6680 - output_1_loss: -5.5376e+03 - output_2_loss: 50867.2227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8882e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 579.8702\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3940e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 72.1598\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 67915.8125 - output_1_loss: -5.5349e+03 - output_2_loss: 73450.7422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 67073.5078 - output_1_loss: -5.5335e+03 - output_2_loss: 72607.0234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8132e+03 - output_1_loss: -5.5010e+03 - output_2_loss: 2687.8342\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3665e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 1102.9630\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3087e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 158.3804\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6373.0659 - output_1_loss: -5.5107e+03 - output_2_loss: 11883.7412\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0036e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 464.9655\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 165415.2500 - output_1_loss: -6.9999e+03 - output_2_loss: 172415.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 76643.3203 - output_1_loss: -5.5406e+03 - output_2_loss: 82183.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45205.1602 - output_1_loss: -5.5233e+03 - output_2_loss: 50728.4648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 223572.7344 - output_1_loss: -7.0460e+03 - output_2_loss: 230618.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3640e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 1105.8070\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4485.4004 - output_1_loss: -5.5074e+03 - output_2_loss: 9992.8408\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2724e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 194.9577\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32337.5449 - output_1_loss: -5.5239e+03 - output_2_loss: 37861.4180\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0472e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 420.3544\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4422e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 23.6714\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 87884.5547 - output_1_loss: -5.5464e+03 - output_2_loss: 93430.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8414e+03 - output_1_loss: -5.5029e+03 - output_2_loss: 2661.5173\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2757e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 191.6700\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11055.9248 - output_1_loss: -5.5181e+03 - output_2_loss: 16573.9941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3256e+03 - output_1_loss: -3.4700e+03 - output_2_loss: 1144.3535\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1382e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 329.5432\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 152548.3594 - output_1_loss: -7.0039e+03 - output_2_loss: 159552.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2916e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 175.5539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2629e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 204.0017\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4192e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 47.2503\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28483.9648 - output_1_loss: -5.5168e+03 - output_2_loss: 34000.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 226909.9688 - output_1_loss: -7.0496e+03 - output_2_loss: 233959.5625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3687.3579 - output_1_loss: -5.5084e+03 - output_2_loss: 9195.7256\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30405.6738 - output_1_loss: -5.5238e+03 - output_2_loss: 35929.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20278.9766 - output_1_loss: -5.5199e+03 - output_2_loss: 25798.8359\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1803e+03 - output_1_loss: -3.4703e+03 - output_2_loss: 1290.0110\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2337e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 233.8009\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3987e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 68.0707\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 70795.2891 - output_1_loss: -5.5366e+03 - output_2_loss: 76331.9297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4091.7598 - output_1_loss: -5.5102e+03 - output_2_loss: 9601.9814\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2533e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 214.2243\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0762e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 391.8085\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 134409.5781 - output_1_loss: -7.0013e+03 - output_2_loss: 141410.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3770.1313 - output_1_loss: -5.5088e+03 - output_2_loss: 9278.9521\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3161e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 150.6965\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26780.8730 - output_1_loss: -5.5263e+03 - output_2_loss: 32307.1309\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12398.4922 - output_1_loss: -5.4853e+03 - output_2_loss: 17883.8301\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 194173.5625 - output_1_loss: -7.0381e+03 - output_2_loss: 201211.6875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0524e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 415.8604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2370e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 230.6427\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27187.0586 - output_1_loss: -5.5211e+03 - output_2_loss: 32708.1895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1537e+03 - output_1_loss: -3.4718e+03 - output_2_loss: 2318.0371\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3902.9072 - output_1_loss: -5.5099e+03 - output_2_loss: 9412.7773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3486e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 117.7235\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4290e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 37.3001\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 80365.6484 - output_1_loss: -5.5416e+03 - output_2_loss: 85907.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3124.3359 - output_1_loss: -5.4863e+03 - output_2_loss: 8610.5889\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1492e+02 - output_1_loss: -5.5032e+03 - output_2_loss: 5388.3130\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1389e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 328.7017\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 120720.9922 - output_1_loss: -7.0028e+03 - output_2_loss: 127723.7734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25085.3066 - output_1_loss: -5.5280e+03 - output_2_loss: 30613.2695\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2323e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 235.3318\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 204124.3594 - output_1_loss: -7.0469e+03 - output_2_loss: 211171.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2469e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 220.3928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40398.6758 - output_1_loss: -5.5225e+03 - output_2_loss: 45921.1914\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.8402e+02 - output_1_loss: -3.4717e+03 - output_2_loss: 2487.6599\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1849e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 282.4481\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2435.9775 - output_1_loss: -5.5063e+03 - output_2_loss: 7942.2583\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3932e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 72.9088\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26923.0098 - output_1_loss: -5.5301e+03 - output_2_loss: 32453.1191\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3917e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 75.1446\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 55897.5586 - output_1_loss: -5.5219e+03 - output_2_loss: 61419.4883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1366.0996 - output_1_loss: -5.4916e+03 - output_2_loss: 6857.6826\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1265.0854 - output_1_loss: -5.5072e+03 - output_2_loss: 6772.2598\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 78954.3203 - output_1_loss: -6.9833e+03 - output_2_loss: 85937.5859\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1113e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 356.9402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0037e+02 - output_1_loss: -3.4732e+03 - output_2_loss: 3372.8518\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2163e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 250.8251\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 218681.2969 - output_1_loss: -7.0530e+03 - output_2_loss: 225734.2656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 808.1602 - output_1_loss: -5.5042e+03 - output_2_loss: 6312.3247\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4206e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 45.3920\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 31306.4238 - output_1_loss: -5.5329e+03 - output_2_loss: 36839.3281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2919e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 175.1160\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3765e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 90.4629\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 43962.0391 - output_1_loss: -5.5153e+03 - output_2_loss: 49477.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35810.7734 - output_1_loss: -5.5193e+03 - output_2_loss: 41330.0586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 49294.0742 - output_1_loss: -6.9669e+03 - output_2_loss: 56260.9492\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7867e+02 - output_1_loss: -3.4727e+03 - output_2_loss: 3194.0178\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2013e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 266.2255\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4691.4370 - output_1_loss: -5.5002e+03 - output_2_loss: 10191.6367\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 235340.8594 - output_1_loss: -7.0570e+03 - output_2_loss: 242397.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2142.4155 - output_1_loss: -5.5081e+03 - output_2_loss: 7650.5542\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.9529e+02 - output_1_loss: -5.5018e+03 - output_2_loss: 4506.4707\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3473e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 119.8414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1607e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 307.2690\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4405e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 25.3690\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 41652.3828 - output_1_loss: -5.5387e+03 - output_2_loss: 47191.0352\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2954e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 171.4953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40190.5625 - output_1_loss: -5.5148e+03 - output_2_loss: 45705.3828\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.8585e+02 - output_1_loss: -3.4712e+03 - output_2_loss: 2585.3284\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0946e+03 - output_1_loss: -5.5022e+03 - output_2_loss: 3407.5510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 41815.0859 - output_1_loss: -5.5240e+03 - output_2_loss: 47339.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38644.8398 - output_1_loss: -6.9702e+03 - output_2_loss: 45615.0273\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2677e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 199.5054\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8373.8555 - output_1_loss: -5.5061e+03 - output_2_loss: 13879.9941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 235892.1094 - output_1_loss: -7.0566e+03 - output_2_loss: 242948.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1299.7451 - output_1_loss: -5.5061e+03 - output_2_loss: 6805.8403\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4536e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 12.1462\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3854e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 81.2455\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3049e+03 - output_1_loss: -3.4703e+03 - output_2_loss: 2165.3792\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6553e+03 - output_1_loss: -5.5010e+03 - output_2_loss: 1845.6960\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8901e+03 - output_1_loss: -3.4690e+03 - output_2_loss: 578.8692\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 49295.2969 - output_1_loss: -5.5323e+03 - output_2_loss: 54827.5586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27589.6074 - output_1_loss: -6.9679e+03 - output_2_loss: 34557.5391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23888.4160 - output_1_loss: -5.5216e+03 - output_2_loss: 29409.9980\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1593e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 308.6329\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24979.0000 - output_1_loss: -5.5099e+03 - output_2_loss: 30488.9375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 772.6631 - output_1_loss: -5.5050e+03 - output_2_loss: 6277.6841\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8661e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 1603.1876\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3621e+03 - output_1_loss: -5.5024e+03 - output_2_loss: 2140.3359\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3341e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 132.5961\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12614.7168 - output_1_loss: -5.5119e+03 - output_2_loss: 18126.6016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 186070.0469 - output_1_loss: -7.0354e+03 - output_2_loss: 193105.4531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4608e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 4.8200\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3302e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 137.0514\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3373e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 1131.1997\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0973e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 370.3867\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38415.7891 - output_1_loss: -5.5257e+03 - output_2_loss: 43941.4648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19009.5234 - output_1_loss: -6.9590e+03 - output_2_loss: 25968.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39802.9961 - output_1_loss: -5.5360e+03 - output_2_loss: 45338.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2756e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 191.4589\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25352.5156 - output_1_loss: -5.5169e+03 - output_2_loss: 30869.4492\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3896e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 76.7263\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 54.0889 - output_1_loss: -5.5034e+03 - output_2_loss: 5557.5332\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2118.6079 - output_1_loss: -5.5115e+03 - output_2_loss: 7630.1001\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6898e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 777.8401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1499e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 317.6819\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34734.3984 - output_1_loss: -5.5241e+03 - output_2_loss: 40258.4727\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19285.6504 - output_1_loss: -5.5186e+03 - output_2_loss: 24804.2188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 192646.2656 - output_1_loss: -7.0406e+03 - output_2_loss: 199686.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4618e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 3.8204\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: -3.2920e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 174.8938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3764e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 90.2222\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28199.4609 - output_1_loss: -5.5248e+03 - output_2_loss: 33724.2930\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16778.6309 - output_1_loss: -5.5046e+03 - output_2_loss: 22283.2520\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9639.9043 - output_1_loss: -6.9469e+03 - output_2_loss: 16586.8145\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9860e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 480.8551\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39635.4844 - output_1_loss: -5.5357e+03 - output_2_loss: 45171.2227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28834.1914 - output_1_loss: -5.5199e+03 - output_2_loss: 34354.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4182e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 48.2366\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 188759.2812 - output_1_loss: -7.0436e+03 - output_2_loss: 195802.9062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.3294e+02 - output_1_loss: -5.5014e+03 - output_2_loss: 4568.5044\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9789e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 489.5372\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13013.9668 - output_1_loss: -5.5126e+03 - output_2_loss: 18526.5215\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4627e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 2.8427\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2272e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 238.9906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3027e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 164.0031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3899e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 76.5857\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15065.9268 - output_1_loss: -5.5142e+03 - output_2_loss: 20580.1191\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22116.9082 - output_1_loss: -5.5166e+03 - output_2_loss: 27633.4707\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4010e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 65.6581\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2951e+03 - output_1_loss: -5.4990e+03 - output_2_loss: 3203.9099\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21147.9922 - output_1_loss: -5.4956e+03 - output_2_loss: 26643.6055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10709.2988 - output_1_loss: -6.9526e+03 - output_2_loss: 17661.9238\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45798.8438 - output_1_loss: -5.5392e+03 - output_2_loss: 51337.9961\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8995e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 569.0480\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 185055.8906 - output_1_loss: -7.0421e+03 - output_2_loss: 192097.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4645e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 1.1343\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4147e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 50.9123\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3621e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 104.8778\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3707e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 96.3207\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28278.3789 - output_1_loss: -5.5225e+03 - output_2_loss: 33800.8789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17499.5176 - output_1_loss: -5.5180e+03 - output_2_loss: 23017.4688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3313e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 135.1614\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6455e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 823.9233\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18238.9180 - output_1_loss: -5.5197e+03 - output_2_loss: 23758.6270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23555.3281 - output_1_loss: -5.5231e+03 - output_2_loss: 29078.4766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 193898.7812 - output_1_loss: -7.0437e+03 - output_2_loss: 200942.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3334e+03 - output_1_loss: -5.4976e+03 - output_2_loss: 2164.1572\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4254e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 40.0192\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7723e+03 - output_1_loss: -3.4692e+03 - output_2_loss: 696.9510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3971.6743 - output_1_loss: -6.9431e+03 - output_2_loss: 10914.7617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29496.4141 - output_1_loss: -5.5218e+03 - output_2_loss: 35018.2148\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4645e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 1.1197\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4003e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 66.3315\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4375e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 27.7195\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1286e+03 - output_1_loss: -3.4705e+03 - output_2_loss: 1341.8768\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19457.0918 - output_1_loss: -5.5179e+03 - output_2_loss: 24974.9629\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14409.7949 - output_1_loss: -5.5148e+03 - output_2_loss: 19924.6113\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3394e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 127.4012\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9393e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 528.9004\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11205.4121 - output_1_loss: -5.5104e+03 - output_2_loss: 16715.8223\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24102.0020 - output_1_loss: -5.5230e+03 - output_2_loss: 29625.0312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 137467.7656 - output_1_loss: -7.0167e+03 - output_2_loss: 144484.4844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.5683e+03 - output_1_loss: -5.4949e+03 - output_2_loss: 926.6272\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4044e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 62.1289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4458e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 19.4747\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3209e+03 - output_1_loss: -3.4710e+03 - output_2_loss: 2150.1680\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.8420e+02 - output_1_loss: -6.9396e+03 - output_2_loss: 6055.4268\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34823.6758 - output_1_loss: -5.5283e+03 - output_2_loss: 40352.0234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4645e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 1.1047\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3808e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 85.9206\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0358e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 431.9761\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2470e+03 - output_1_loss: -5.4933e+03 - output_2_loss: 246.2569\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3493e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 117.8721\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21106.2441 - output_1_loss: -5.5209e+03 - output_2_loss: 26627.1895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14218.9746 - output_1_loss: -5.5150e+03 - output_2_loss: 19733.9824\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13338.3857 - output_1_loss: -5.5141e+03 - output_2_loss: 18852.4746\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33213.1992 - output_1_loss: -5.5313e+03 - output_2_loss: 38744.4648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 143151.0938 - output_1_loss: -7.0256e+03 - output_2_loss: 150176.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2961e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 171.2289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2778e+03 - output_1_loss: -5.4947e+03 - output_2_loss: 216.8816\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9031e+03 - output_1_loss: -3.4705e+03 - output_2_loss: 1567.4812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1853e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 1283.3273\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0677e+03 - output_1_loss: -6.9432e+03 - output_2_loss: 5875.5854\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32614.9062 - output_1_loss: -5.5253e+03 - output_2_loss: 38140.1836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1863e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 281.6616\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22210.1875 - output_1_loss: -5.5273e+03 - output_2_loss: 27737.4883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4645e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 1.0892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1305e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 336.7306\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17505.1445 - output_1_loss: -5.5166e+03 - output_2_loss: 23021.7031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1222.2288 - output_1_loss: -3.4709e+03 - output_2_loss: 4693.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 31013.9609 - output_1_loss: -5.5266e+03 - output_2_loss: 36540.5391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7370.0562 - output_1_loss: -5.5073e+03 - output_2_loss: 12877.3779\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22837.9531 - output_1_loss: -5.5235e+03 - output_2_loss: 28361.4941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25332.3008 - output_1_loss: -5.5296e+03 - output_2_loss: 30861.9141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 93422.3594 - output_1_loss: -7.0007e+03 - output_2_loss: 100423.0703\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3607e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 106.0519\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3122e+03 - output_1_loss: -5.4955e+03 - output_2_loss: 183.2320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2569e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 1211.8923\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 931.0405 - output_1_loss: -6.9440e+03 - output_2_loss: 7875.0151\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3026e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 164.5829\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14440.2871 - output_1_loss: -5.5126e+03 - output_2_loss: 19952.8516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4406e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 25.7200\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0293e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 438.8504\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2349e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 260.9630\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.5454e+02 - output_1_loss: -3.4705e+03 - output_2_loss: 2615.9636\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3234e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 1145.2919\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33912.7969 - output_1_loss: -5.5299e+03 - output_2_loss: 39442.6602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7748.1089 - output_1_loss: -5.5094e+03 - output_2_loss: 13257.5312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30541.6504 - output_1_loss: -5.5301e+03 - output_2_loss: 36071.7773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26675.8047 - output_1_loss: -5.5307e+03 - output_2_loss: 32206.4766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95024.5312 - output_1_loss: -7.0079e+03 - output_2_loss: 102032.4062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3642e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 101.9224\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3723e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 94.3438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9535e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 515.0063\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4649.7549 - output_1_loss: -6.9521e+03 - output_2_loss: 11601.8662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3330e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 133.8410\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9970.9795 - output_1_loss: -5.5072e+03 - output_2_loss: 15478.1416\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37794.7734 - output_1_loss: -5.5321e+03 - output_2_loss: 43326.9023\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9106e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 558.0749\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0515e+03 - output_1_loss: -5.5008e+03 - output_2_loss: 1449.3636\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.1968e+02 - output_1_loss: -3.4712e+03 - output_2_loss: 2851.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3862e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 1082.1860\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3426e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 124.0568\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4108.0083 - output_1_loss: -5.5046e+03 - output_2_loss: 9612.6182\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25065.5469 - output_1_loss: -5.5233e+03 - output_2_loss: 30588.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7146.4805 - output_1_loss: -5.5072e+03 - output_2_loss: 12653.6309\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 44130.2539 - output_1_loss: -5.5409e+03 - output_2_loss: 49671.1484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 78342.3672 - output_1_loss: -6.9943e+03 - output_2_loss: 85336.6328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3711e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 95.4915\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 43977.0078 - output_1_loss: -5.5353e+03 - output_2_loss: 49512.2734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3874e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 79.0322\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5384e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 931.1870\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8692.8145 - output_1_loss: -6.9592e+03 - output_2_loss: 15651.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6694e+02 - output_1_loss: -3.4717e+03 - output_2_loss: 3104.7397\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5285e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 939.3553\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 693.7866 - output_1_loss: -5.5016e+03 - output_2_loss: 6195.3877\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.2930e+03 - output_1_loss: -5.4995e+03 - output_2_loss: 1206.5299\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9996e+03 - output_1_loss: -3.4703e+03 - output_2_loss: 1470.7037\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3245e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 142.4514\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33029.5820 - output_1_loss: -5.5284e+03 - output_2_loss: 38557.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2126.6108 - output_1_loss: -5.5016e+03 - output_2_loss: 7628.1714\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 78539.2891 - output_1_loss: -5.5552e+03 - output_2_loss: 84094.4688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 98875.7031 - output_1_loss: -7.0053e+03 - output_2_loss: 105880.9766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3758e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 90.9827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26201.2402 - output_1_loss: -5.5154e+03 - output_2_loss: 31716.6230\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4080e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 58.2040\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13110.7617 - output_1_loss: -6.9648e+03 - output_2_loss: 20075.5215\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0880e+03 - output_1_loss: -5.5003e+03 - output_2_loss: 1412.2344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -7.0889e+02 - output_1_loss: -3.4706e+03 - output_2_loss: 2761.7148\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5220e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 946.2744\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2578e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 209.7304\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 739.9854 - output_1_loss: -5.5049e+03 - output_2_loss: 6244.8701\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7017e+03 - output_1_loss: -5.4974e+03 - output_2_loss: 2795.7271\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3202e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 147.0934\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4102e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 56.0081\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5991e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 869.1403\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34385.5352 - output_1_loss: -5.5301e+03 - output_2_loss: 39915.6406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 103548.2812 - output_1_loss: -5.5621e+03 - output_2_loss: 109110.4297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95282.9609 - output_1_loss: -6.9993e+03 - output_2_loss: 102282.2422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27483.7129 - output_1_loss: -5.5260e+03 - output_2_loss: 33009.7227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16096.7695 - output_1_loss: -6.9667e+03 - output_2_loss: 23063.4648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5677e+03 - output_1_loss: -5.5019e+03 - output_2_loss: 1934.1591\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.7689e+02 - output_1_loss: -3.4701e+03 - output_2_loss: 2593.2256\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8017e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 665.2045\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: -3.2892e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 178.0237\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3924.9492 - output_1_loss: -5.5124e+03 - output_2_loss: 9437.3682\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0739e+03 - output_1_loss: -5.4992e+03 - output_2_loss: 1425.3304\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 89458.5156 - output_1_loss: -7.0027e+03 - output_2_loss: 96461.1953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3518e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 115.0622\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28860.3945 - output_1_loss: -5.5275e+03 - output_2_loss: 34387.9180\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4157e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 50.3661\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6552e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 812.7574\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22219.7852 - output_1_loss: -6.9736e+03 - output_2_loss: 29193.3613\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24959.3477 - output_1_loss: -5.5173e+03 - output_2_loss: 30476.6758\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1688.4355 - output_1_loss: -5.5103e+03 - output_2_loss: 7198.7593\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 103216.0859 - output_1_loss: -5.5588e+03 - output_2_loss: 108774.8672\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2706e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 196.6885\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3600e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 106.7368\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33526.4023 - output_1_loss: -5.5308e+03 - output_2_loss: 39057.2188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4235e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 42.4767\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1033e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 2366.1565\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0506e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 416.5147\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6880e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 780.3602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0453e+03 - output_1_loss: -5.5029e+03 - output_2_loss: 4457.6411\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19086.9375 - output_1_loss: -5.5138e+03 - output_2_loss: 24600.7422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9446.9863 - output_1_loss: -5.5186e+03 - output_2_loss: 14965.5938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6483e+03 - output_1_loss: -5.4944e+03 - output_2_loss: 846.0557\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 89705.6641 - output_1_loss: -5.5470e+03 - output_2_loss: 95252.6406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 93929.7812 - output_1_loss: -7.0074e+03 - output_2_loss: 100937.1484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2457e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 221.7487\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3700e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 96.6187\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4347e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 31.1001\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15345.0195 - output_1_loss: -6.9629e+03 - output_2_loss: 22307.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1616e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 305.8840\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11806.9473 - output_1_loss: -5.5098e+03 - output_2_loss: 17316.7051\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40510.5352 - output_1_loss: -5.5347e+03 - output_2_loss: 46045.2227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2450e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 222.4544\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1723e+03 - output_1_loss: -3.4699e+03 - output_2_loss: 2297.5320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6577e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 810.9034\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2051.2642 - output_1_loss: -5.5093e+03 - output_2_loss: 7560.5859\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17919.7070 - output_1_loss: -5.5239e+03 - output_2_loss: 23443.6426\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6746e+03 - output_1_loss: -5.4951e+03 - output_2_loss: 820.4864\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 86317.5469 - output_1_loss: -5.5391e+03 - output_2_loss: 91856.6641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 102767.2969 - output_1_loss: -7.0058e+03 - output_2_loss: 109773.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3155.3906 - output_1_loss: -5.5051e+03 - output_2_loss: 8660.4990\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3828e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 83.6993\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4383e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 27.8565\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1244e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 343.6405\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15923.5781 - output_1_loss: -6.9641e+03 - output_2_loss: 22887.6836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5004e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 1968.8292\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3291e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 136.6942\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27244.3457 - output_1_loss: -5.5273e+03 - output_2_loss: 32771.6523\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.7799e+03 - output_1_loss: -5.4965e+03 - output_2_loss: 716.6423\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 121831.3594 - output_1_loss: -5.5561e+03 - output_2_loss: 127387.4219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 133087.2031 - output_1_loss: -7.0173e+03 - output_2_loss: 140104.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14280.1494 - output_1_loss: -5.5079e+03 - output_2_loss: 19788.0117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0238e+02 - output_1_loss: -5.5022e+03 - output_2_loss: 5299.7856\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3998e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 66.4696\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3442e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 122.7950\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8448e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 622.8399\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0254e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 442.7180\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4667.3423 - output_1_loss: -5.5129e+03 - output_2_loss: 10180.2695\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 163977.2812 - output_1_loss: -7.0224e+03 - output_2_loss: 170999.7031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14274.9707 - output_1_loss: -6.9591e+03 - output_2_loss: 21234.0254\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1423e+03 - output_1_loss: -5.5002e+03 - output_2_loss: 3357.8943\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4021e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 64.4759\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0560e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 1412.7814\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2489e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 218.6108\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3358e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 130.0202\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9471e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 520.6562\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8866e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 582.3044\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7726.5659 - output_1_loss: -5.5152e+03 - output_2_loss: 13241.7910\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36639.1758 - output_1_loss: -5.5286e+03 - output_2_loss: 42167.7891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9542e+03 - output_1_loss: -5.4961e+03 - output_2_loss: 541.8439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 87872.3828 - output_1_loss: -5.5417e+03 - output_2_loss: 93414.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16577.4570 - output_1_loss: -5.5166e+03 - output_2_loss: 22094.0625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1268e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 341.2457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1276e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 340.4587\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7702e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 698.8941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11751.0459 - output_1_loss: -5.5191e+03 - output_2_loss: 17270.1035\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 64313.0859 - output_1_loss: -5.5312e+03 - output_2_loss: 69844.2422\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 178362.3750 - output_1_loss: -7.0365e+03 - output_2_loss: 185398.9062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13033.4629 - output_1_loss: -6.9588e+03 - output_2_loss: 19992.2480\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 189.3975 - output_1_loss: -5.5066e+03 - output_2_loss: 5696.0059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4502e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 15.8634\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1799e+03 - output_1_loss: -3.4701e+03 - output_2_loss: 1290.1766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1997e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 266.9795\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24661.4355 - output_1_loss: -5.5181e+03 - output_2_loss: 30179.4863\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0299e+03 - output_1_loss: -5.4947e+03 - output_2_loss: 464.8393\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3155e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 1153.8901\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8228.7051 - output_1_loss: -5.5012e+03 - output_2_loss: 13729.9502\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20880.4785 - output_1_loss: -5.5263e+03 - output_2_loss: 26406.7676\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 44041.3359 - output_1_loss: -5.5172e+03 - output_2_loss: 49558.4883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14625.6670 - output_1_loss: -6.9638e+03 - output_2_loss: 21589.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7657e+03 - output_1_loss: -3.4692e+03 - output_2_loss: 703.4939\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3016e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 165.0847\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2914e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 174.8303\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21690.4180 - output_1_loss: -5.5131e+03 - output_2_loss: 27203.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0961e+03 - output_1_loss: -5.4942e+03 - output_2_loss: 398.0994\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 201996.3906 - output_1_loss: -7.0459e+03 - output_2_loss: 209042.3125\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5868e+03 - output_1_loss: -5.5001e+03 - output_2_loss: 2913.2632\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28946.8496 - output_1_loss: -5.5113e+03 - output_2_loss: 34458.1055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4331e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 33.3479\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2155e+03 - output_1_loss: -3.4696e+03 - output_2_loss: 1254.1141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9391e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 529.0442\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7579e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 710.2274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6978.4458 - output_1_loss: -5.5068e+03 - output_2_loss: 12485.2812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32083.4570 - output_1_loss: -5.5336e+03 - output_2_loss: 37617.0156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1649e+03 - output_1_loss: -5.4936e+03 - output_2_loss: 328.7041\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 211090.0781 - output_1_loss: -7.0486e+03 - output_2_loss: 218138.6406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5249.8447 - output_1_loss: -6.9494e+03 - output_2_loss: 12199.2402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3124e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 154.1926\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3377e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 128.8117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20317.0762 - output_1_loss: -5.5155e+03 - output_2_loss: 25832.5684\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7069e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 761.8670\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1780.9805 - output_1_loss: -5.5089e+03 - output_2_loss: 7289.9214\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17661.5469 - output_1_loss: -5.5078e+03 - output_2_loss: 23169.3145\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4461e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 20.0159\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2264e+03 - output_1_loss: -3.4699e+03 - output_2_loss: 1243.4988\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3262e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 140.2458\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3878e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 78.6613\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9423e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 525.1787\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3963.1060 - output_1_loss: -5.4969e+03 - output_2_loss: 9460.0244\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18256.8203 - output_1_loss: -5.5191e+03 - output_2_loss: 23775.9570\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1227.8784 - output_1_loss: -5.5091e+03 - output_2_loss: 6737.0283\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 219994.2188 - output_1_loss: -7.0502e+03 - output_2_loss: 227044.4531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1330.8667 - output_1_loss: -6.9448e+03 - output_2_loss: 8275.6494\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12329.6172 - output_1_loss: -5.5097e+03 - output_2_loss: 17839.2910\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4491e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 16.9943\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1240e+03 - output_1_loss: -3.4721e+03 - output_2_loss: 2348.0967\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16262.4902 - output_1_loss: -5.5123e+03 - output_2_loss: 21774.8203\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9466e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 521.2778\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3148e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 152.0302\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3282.6895 - output_1_loss: -5.4954e+03 - output_2_loss: 8778.0908\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1607.8379 - output_1_loss: -5.5085e+03 - output_2_loss: 7116.2998\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17461.9336 - output_1_loss: -5.5171e+03 - output_2_loss: 22979.0586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4528e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 13.1219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8776e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 591.0576\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9994e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 467.9889\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24066.4102 - output_1_loss: -5.5263e+03 - output_2_loss: 29592.6875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3047e+02 - output_1_loss: -5.5007e+03 - output_2_loss: 5270.2773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4556e+03 - output_1_loss: -3.4699e+03 - output_2_loss: 1014.2401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 164603.2656 - output_1_loss: -7.0242e+03 - output_2_loss: 171627.4375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3213.5059 - output_1_loss: -5.4950e+03 - output_2_loss: 8708.4902\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8375e+03 - output_1_loss: -6.9404e+03 - output_2_loss: 5102.9097\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7460e+03 - output_1_loss: -3.4706e+03 - output_2_loss: 1724.6000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11836.7129 - output_1_loss: -5.5084e+03 - output_2_loss: 17345.1152\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3612e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 105.0412\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1707e+03 - output_1_loss: -3.4701e+03 - output_2_loss: 1299.4117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0843e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 382.6995\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1361.9688 - output_1_loss: -5.5033e+03 - output_2_loss: 6865.2261\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20311.0352 - output_1_loss: -5.5161e+03 - output_2_loss: 25827.1484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4309e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 35.5544\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32702.2930 - output_1_loss: -5.5316e+03 - output_2_loss: 38233.9023\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6547.4722 - output_1_loss: -5.5048e+03 - output_2_loss: 12052.2852\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3206.2886 - output_1_loss: -5.5108e+03 - output_2_loss: 8717.0977\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8852e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 582.6217\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3668e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 99.2489\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1249e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 342.4652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 197742.2031 - output_1_loss: -7.0406e+03 - output_2_loss: 204782.7656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1446e+03 - output_1_loss: -5.4921e+03 - output_2_loss: 3347.4573\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7749e+03 - output_1_loss: -6.9381e+03 - output_2_loss: 3163.2034\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.2152e+03 - output_1_loss: -3.4717e+03 - output_2_loss: 2256.5212\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28206.6562 - output_1_loss: -5.5238e+03 - output_2_loss: 33730.4062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4087e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 58.0146\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8734e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 593.6072\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6046.1191 - output_1_loss: -5.5127e+03 - output_2_loss: 11558.8662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 130.5112 - output_1_loss: -5.4996e+03 - output_2_loss: 5630.1099\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9984e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 468.9416\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2045e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 262.5990\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0783e+03 - output_1_loss: -3.4716e+03 - output_2_loss: 2393.3342\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35880.5469 - output_1_loss: -5.5243e+03 - output_2_loss: 41404.8164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37570.0078 - output_1_loss: -5.5342e+03 - output_2_loss: 43104.2344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7360.3750 - output_1_loss: -5.5147e+03 - output_2_loss: 12875.1191\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3828e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 84.1239\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3998e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 66.0041\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9076e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 559.8669\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 243931.8281 - output_1_loss: -7.0533e+03 - output_2_loss: 250985.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7592e+03 - output_1_loss: -5.4924e+03 - output_2_loss: 2733.2666\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1417e+03 - output_1_loss: -6.9476e+03 - output_2_loss: 5805.8984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14320.2988 - output_1_loss: -5.5201e+03 - output_2_loss: 19840.4434\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1532e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 313.5916\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45831.9844 - output_1_loss: -5.5383e+03 - output_2_loss: 51370.2461\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0326e+03 - output_1_loss: -5.5009e+03 - output_2_loss: 3468.3274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3562e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 110.9028\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3695e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 97.0050\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3349e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 131.2643\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3423e+03 - output_1_loss: -5.4990e+03 - output_2_loss: 4156.7231\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0967e+03 - output_1_loss: -3.4715e+03 - output_2_loss: 2374.8301\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40819.2227 - output_1_loss: -5.5270e+03 - output_2_loss: 46346.1914\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4856.8589 - output_1_loss: -5.4980e+03 - output_2_loss: 10354.8916\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9351e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 532.2305\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 228598.0469 - output_1_loss: -7.0422e+03 - output_2_loss: 235640.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1278.0645 - output_1_loss: -6.9498e+03 - output_2_loss: 8227.8711\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17067.4023 - output_1_loss: -5.5229e+03 - output_2_loss: 22590.2715\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.0950e+02 - output_1_loss: -5.5048e+03 - output_2_loss: 4595.2466\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1549e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 312.6181\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2961e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 171.1959\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3741e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 91.6572\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24149.4883 - output_1_loss: -5.5168e+03 - output_2_loss: 29666.3047\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3945e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 72.1236\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9572e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 509.9759\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5930e+03 - output_1_loss: -5.4926e+03 - output_2_loss: 2899.6506\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1373e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 330.3732\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2262e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 241.4270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3966e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 68.9227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.4570e+03 - output_1_loss: -3.4709e+03 - output_2_loss: 2013.9222\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45181.8516 - output_1_loss: -5.5291e+03 - output_2_loss: 50710.9336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4968.7271 - output_1_loss: -5.5057e+03 - output_2_loss: 10474.4463\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 224489.4531 - output_1_loss: -7.0465e+03 - output_2_loss: 231535.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5379.3452 - output_1_loss: -6.9560e+03 - output_2_loss: 12335.3525\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10551.2646 - output_1_loss: -5.5121e+03 - output_2_loss: 16063.3721\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7347.7549 - output_1_loss: -5.5137e+03 - output_2_loss: 12861.4463\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4101e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 56.3694\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0155e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 451.3214\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0945e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 373.3459\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1562e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 311.6355\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0261e+03 - output_1_loss: -3.4696e+03 - output_2_loss: 1443.5077\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36860.1250 - output_1_loss: -5.5278e+03 - output_2_loss: 42387.9023\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 221899.9688 - output_1_loss: -7.0496e+03 - output_2_loss: 228949.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2578e+03 - output_1_loss: -5.4983e+03 - output_2_loss: 2240.4343\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7829.7651 - output_1_loss: -5.5088e+03 - output_2_loss: 13338.5537\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9163e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 551.5801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3995e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 66.6059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36567.5781 - output_1_loss: -5.5222e+03 - output_2_loss: 42089.7656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4869.8550 - output_1_loss: -5.5077e+03 - output_2_loss: 10377.5605\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9908.2148 - output_1_loss: -6.9614e+03 - output_2_loss: 16869.6582\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1037.8481 - output_1_loss: -5.5020e+03 - output_2_loss: 6539.8267\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3872e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 79.3587\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0651e+03 - output_1_loss: -5.5025e+03 - output_2_loss: 3437.3281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1472e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 320.2820\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1606e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 307.3440\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3129e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 154.0151\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2147e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 1254.8375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20558.6289 - output_1_loss: -5.5124e+03 - output_2_loss: 26071.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4510.0410 - output_1_loss: -5.5075e+03 - output_2_loss: 10017.5068\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 156910.3906 - output_1_loss: -7.0210e+03 - output_2_loss: 163931.3594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5895.2666 - output_1_loss: -5.5071e+03 - output_2_loss: 11402.3496\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2998e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 167.2954\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0082e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 459.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1173e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 350.2975\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39462.6953 - output_1_loss: -5.5269e+03 - output_2_loss: 44989.5508\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7689e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 699.0632\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23479.1914 - output_1_loss: -6.9748e+03 - output_2_loss: 30453.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3658.4497 - output_1_loss: -5.5096e+03 - output_2_loss: 9168.0908\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4446.4746 - output_1_loss: -5.5082e+03 - output_2_loss: 9954.6338\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1275.8076 - output_1_loss: -5.5096e+03 - output_2_loss: 6785.4438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1913e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 276.1456\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2485e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 218.6367\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4114e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 54.7603\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24157.1621 - output_1_loss: -5.5202e+03 - output_2_loss: 29677.4082\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8448e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 623.1357\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 178313.8281 - output_1_loss: -7.0339e+03 - output_2_loss: 185347.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35651.9531 - output_1_loss: -6.9835e+03 - output_2_loss: 42635.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14779.5781 - output_1_loss: -5.5207e+03 - output_2_loss: 20300.2852\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1847e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 281.7239\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3716.8291 - output_1_loss: -5.5123e+03 - output_2_loss: 9229.1416\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0796e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 388.6721\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2216e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 245.1319\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2695e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 197.3815\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34609.0352 - output_1_loss: -5.5233e+03 - output_2_loss: 40132.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6280.9727 - output_1_loss: -5.5133e+03 - output_2_loss: 11794.2822\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6557.1348 - output_1_loss: -5.5123e+03 - output_2_loss: 12069.4336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9765e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 491.1729\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23580.2461 - output_1_loss: -5.5247e+03 - output_2_loss: 29104.9824\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2703e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 195.6827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3684e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 98.4293\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5587e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 911.0821\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1568e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 310.6494\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2523e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 214.9909\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12859.9004 - output_1_loss: -5.5081e+03 - output_2_loss: 18368.0215\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 147620.8750 - output_1_loss: -7.0199e+03 - output_2_loss: 154640.7656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 47571.2773 - output_1_loss: -6.9905e+03 - output_2_loss: 54561.7852\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1979e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 269.3795\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.2886e+02 - output_1_loss: -5.5026e+03 - output_2_loss: 5073.7290\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3649e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 100.6545\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32977.1680 - output_1_loss: -5.5261e+03 - output_2_loss: 38503.2500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1048e+03 - output_1_loss: -3.4706e+03 - output_2_loss: 1365.8285\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0466e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 421.5171\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2399e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 227.3903\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2422.5449 - output_1_loss: -5.5050e+03 - output_2_loss: 7927.5112\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7628.8262 - output_1_loss: -5.5115e+03 - output_2_loss: 13140.3340\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11074.6934 - output_1_loss: -5.5137e+03 - output_2_loss: 16588.3613\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1924e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 274.9510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4032e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 63.0095\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3675e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 97.9524\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11574.7852 - output_1_loss: -5.5099e+03 - output_2_loss: 17084.6641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 189727.5625 - output_1_loss: -7.0335e+03 - output_2_loss: 196761.0938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5745e+03 - output_1_loss: -3.4715e+03 - output_2_loss: 1896.9965\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 68033.3828 - output_1_loss: -7.0034e+03 - output_2_loss: 75036.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 57.7988 - output_1_loss: -5.5050e+03 - output_2_loss: 5562.7617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11821.4434 - output_1_loss: -5.5153e+03 - output_2_loss: 17336.7168\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3375e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 129.6331\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29377.4668 - output_1_loss: -5.5257e+03 - output_2_loss: 34903.1641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2105e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 256.4930\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3021e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 164.6447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4058e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 60.6667\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10845.2773 - output_1_loss: -5.5091e+03 - output_2_loss: 16354.4033\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2430.4648 - output_1_loss: -5.5054e+03 - output_2_loss: 7935.8560\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5713.2378 - output_1_loss: -5.5117e+03 - output_2_loss: 11224.9346\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3392e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 127.8454\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 368.6025 - output_1_loss: -5.5061e+03 - output_2_loss: 5874.7305\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3366e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 130.4280\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 148829.1875 - output_1_loss: -7.0096e+03 - output_2_loss: 155838.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1982e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 1271.2812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3508e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 115.5332\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 65381.4688 - output_1_loss: -6.9992e+03 - output_2_loss: 72380.6484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14057.6904 - output_1_loss: -5.5163e+03 - output_2_loss: 19574.0391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1934e+03 - output_1_loss: -3.4704e+03 - output_2_loss: 1277.0366\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22700.3477 - output_1_loss: -5.5215e+03 - output_2_loss: 28221.8867\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2957.6206 - output_1_loss: -5.5112e+03 - output_2_loss: 8468.8320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1930e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 274.4849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3858e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 80.8492\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1004e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 367.7844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2176.0435 - output_1_loss: -5.4999e+03 - output_2_loss: 7675.8940\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5278.6226 - output_1_loss: -5.5121e+03 - output_2_loss: 10790.7119\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9789.6406 - output_1_loss: -5.5158e+03 - output_2_loss: 15305.4805\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4187e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 47.2399\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3461e+03 - output_1_loss: -3.4717e+03 - output_2_loss: 2125.5264\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7409.6411 - output_1_loss: -5.5165e+03 - output_2_loss: 12926.1689\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 170618.6250 - output_1_loss: -7.0259e+03 - output_2_loss: 177644.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1351e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 1331.9708\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0094e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 459.0982\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 80419.9375 - output_1_loss: -7.0050e+03 - output_2_loss: 87424.9688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7026.4028 - output_1_loss: -5.5141e+03 - output_2_loss: 12540.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4417e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 23.9183\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10500.7666 - output_1_loss: -5.5141e+03 - output_2_loss: 16014.8467\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21768.6660 - output_1_loss: -5.5216e+03 - output_2_loss: 27290.2734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5243e+02 - output_1_loss: -3.4729e+03 - output_2_loss: 3120.5195\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3005e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 165.8972\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4153e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 50.6897\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.7947e+02 - output_1_loss: -5.5014e+03 - output_2_loss: 4821.8867\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1828.7007 - output_1_loss: -5.4999e+03 - output_2_loss: 7328.6177\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9498e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 1519.9872\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9488e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 519.8010\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11164.9541 - output_1_loss: -5.5187e+03 - output_2_loss: 16683.6250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4469e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 18.5513\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9926.8438 - output_1_loss: -5.5131e+03 - output_2_loss: 15439.9570\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26546.8047 - output_1_loss: -5.5269e+03 - output_2_loss: 32073.6992\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9306.8926 - output_1_loss: -5.5177e+03 - output_2_loss: 14824.5742\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 145488.6250 - output_1_loss: -7.0260e+03 - output_2_loss: 152514.6094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 91758.1562 - output_1_loss: -7.0109e+03 - output_2_loss: 98769.0703\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9171e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 551.3289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11183.4062 - output_1_loss: -5.5136e+03 - output_2_loss: 16697.0371\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7845e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 1684.9269\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9709e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 497.6948\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4006e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 65.9094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5605.5063 - output_1_loss: -5.5089e+03 - output_2_loss: 11114.3701\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2009.0562 - output_1_loss: -5.5019e+03 - output_2_loss: 7510.9268\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3467e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 1122.1163\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19766.2754 - output_1_loss: -5.5244e+03 - output_2_loss: 25290.6895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4458e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 19.5543\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27903.2930 - output_1_loss: -5.5223e+03 - output_2_loss: 33425.6289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2639.9585 - output_1_loss: -5.5017e+03 - output_2_loss: 8141.6987\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 129445.8359 - output_1_loss: -7.0233e+03 - output_2_loss: 136469.1250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5535e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 915.9596\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1666e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 301.3155\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95033.7812 - output_1_loss: -7.0114e+03 - output_2_loss: 102045.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1420e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 323.5871\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4485e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 16.9100\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27067.7812 - output_1_loss: -5.5270e+03 - output_2_loss: 32594.8262\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 619.3167 - output_1_loss: -3.4739e+03 - output_2_loss: 4093.2485\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 913.0068 - output_1_loss: -5.5003e+03 - output_2_loss: 6413.3418\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1900.4766 - output_1_loss: -5.5008e+03 - output_2_loss: 7401.3262\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 129620.6953 - output_1_loss: -7.0237e+03 - output_2_loss: 136644.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6215e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 846.6082\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9829e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 485.5144\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28420.2148 - output_1_loss: -5.5278e+03 - output_2_loss: 33947.9727\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1658e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 299.7052\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21035.7051 - output_1_loss: -5.5186e+03 - output_2_loss: 26554.3359\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1829.3022 - output_1_loss: -5.5049e+03 - output_2_loss: 7334.2271\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0794e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 386.8055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95309.0547 - output_1_loss: -7.0030e+03 - output_2_loss: 102312.0547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37731.9102 - output_1_loss: -5.5310e+03 - output_2_loss: 43262.8906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4496e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 15.8548\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23811.1602 - output_1_loss: -5.5221e+03 - output_2_loss: 29333.2422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.6187e+02 - output_1_loss: -3.4703e+03 - output_2_loss: 2508.4346\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8534.2559 - output_1_loss: -5.5137e+03 - output_2_loss: 14047.9473\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1017.0103 - output_1_loss: -5.5007e+03 - output_2_loss: 6517.6646\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 124210.9297 - output_1_loss: -7.0208e+03 - output_2_loss: 131231.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8814e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 585.9069\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9808e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 487.3682\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1627e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 304.0851\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27657.7051 - output_1_loss: -5.5254e+03 - output_2_loss: 33183.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7405.6216 - output_1_loss: -5.5149e+03 - output_2_loss: 12920.4805\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.4227e+02 - output_1_loss: -3.4709e+03 - output_2_loss: 2828.6748\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0936e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 371.8213\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 127521.6641 - output_1_loss: -7.0098e+03 - output_2_loss: 134531.4375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17672.8047 - output_1_loss: -5.5214e+03 - output_2_loss: 23194.2324\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 121673.2422 - output_1_loss: -7.0190e+03 - output_2_loss: 128692.2188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0647e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 402.0173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21647.6699 - output_1_loss: -5.5113e+03 - output_2_loss: 27158.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4246e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 41.6621\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38278.8438 - output_1_loss: -5.5345e+03 - output_2_loss: 43813.3242\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36171.6367 - output_1_loss: -5.5318e+03 - output_2_loss: 41703.4414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 971.3364 - output_1_loss: -5.5032e+03 - output_2_loss: 6474.5786\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3544e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 2113.9402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1135e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 352.6763\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0058e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 462.0865\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28687.8887 - output_1_loss: -5.5296e+03 - output_2_loss: 34217.4414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 122727.0469 - output_1_loss: -7.0171e+03 - output_2_loss: 129744.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2091e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 256.5009\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16352.9941 - output_1_loss: -5.5227e+03 - output_2_loss: 21875.6855\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4146e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 51.8222\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 71800.2812 - output_1_loss: -6.9773e+03 - output_2_loss: 78777.5391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1906e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 276.3750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.6451e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 1822.7173\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1292e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 337.0947\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9940e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 474.0572\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39358.2109 - output_1_loss: -5.5349e+03 - output_2_loss: 44893.1523\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29195.9062 - output_1_loss: -5.5232e+03 - output_2_loss: 34719.1406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2276e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 237.8250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25121.4688 - output_1_loss: -5.5234e+03 - output_2_loss: 30644.9043\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28399.1328 - output_1_loss: -5.5262e+03 - output_2_loss: 33925.3164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.4200e+02 - output_1_loss: -5.4994e+03 - output_2_loss: 4657.3965\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0552e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 413.0083\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45359.9141 - output_1_loss: -6.9501e+03 - output_2_loss: 52310.0156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 68198.6719 - output_1_loss: -6.9823e+03 - output_2_loss: 75180.9297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1889e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 277.0254\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25877.0156 - output_1_loss: -5.5288e+03 - output_2_loss: 31405.8379\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3785e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 87.1613\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9994e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 1467.6641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9805e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 487.6534\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 44586.4648 - output_1_loss: -5.5373e+03 - output_2_loss: 50123.8086\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39828.8008 - output_1_loss: -5.5319e+03 - output_2_loss: 45360.7109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3061e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 160.0892\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36844.7539 - output_1_loss: -5.5261e+03 - output_2_loss: 42370.8164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22495.3984 - output_1_loss: -5.5214e+03 - output_2_loss: 28016.7832\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6366e+03 - output_1_loss: -5.4932e+03 - output_2_loss: 2856.6658\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3129e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 153.6561\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 42846.7461 - output_1_loss: -6.9648e+03 - output_2_loss: 49811.5508\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 79006.8438 - output_1_loss: -6.9986e+03 - output_2_loss: 86005.4062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4445e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 1021.5818\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2156e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 250.0949\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0146e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 453.4760\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33326.6797 - output_1_loss: -5.5326e+03 - output_2_loss: 38859.3086\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18844.2402 - output_1_loss: -5.5121e+03 - output_2_loss: 24356.3008\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3216e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 144.8631\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3797e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 86.6046\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25747.4883 - output_1_loss: -5.5164e+03 - output_2_loss: 31263.9082\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45250.8984 - output_1_loss: -5.5345e+03 - output_2_loss: 50785.4023\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9977e+03 - output_1_loss: -3.4651e+03 - output_2_loss: 467.3880\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2194e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 246.2830\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3402e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 125.6286\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26584.6348 - output_1_loss: -5.5202e+03 - output_2_loss: 32104.8105\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6148e+03 - output_1_loss: -5.4967e+03 - output_2_loss: 1881.9777\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36587.6641 - output_1_loss: -5.5324e+03 - output_2_loss: 42120.0430\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32209.3672 - output_1_loss: -6.9626e+03 - output_2_loss: 39172.0117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40644.5469 - output_1_loss: -6.9683e+03 - output_2_loss: 47612.8320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3662e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 100.2197\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1641e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 302.8722\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2659e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 199.4447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3692e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 96.6209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16281.3613 - output_1_loss: -5.5100e+03 - output_2_loss: 21791.3555\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0626e+03 - output_1_loss: -5.5015e+03 - output_2_loss: 3438.9475\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3293e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 137.1030\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23970.4023 - output_1_loss: -6.9588e+03 - output_2_loss: 30929.1992\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34254.5469 - output_1_loss: -5.5282e+03 - output_2_loss: 39782.7773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 46809.2656 - output_1_loss: -5.5336e+03 - output_2_loss: 52342.8164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0560e+03 - output_1_loss: -3.4647e+03 - output_2_loss: 408.6846\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2052e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 261.3447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23156.3223 - output_1_loss: -5.5201e+03 - output_2_loss: 28676.3828\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38923.4531 - output_1_loss: -5.5310e+03 - output_2_loss: 44454.4531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 98012.5391 - output_1_loss: -7.0123e+03 - output_2_loss: 105024.8203\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4104e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 55.5555\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 43654.3203 - output_1_loss: -5.5293e+03 - output_2_loss: 49183.6641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1889e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 277.9821\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3692e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 96.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23072.5703 - output_1_loss: -5.5221e+03 - output_2_loss: 28594.6270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19583.4980 - output_1_loss: -5.5124e+03 - output_2_loss: 25095.8945\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5408e+03 - output_1_loss: -5.4962e+03 - output_2_loss: 1955.3827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3366e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 129.6785\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20107.0312 - output_1_loss: -6.9581e+03 - output_2_loss: 27065.0957\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23542.2207 - output_1_loss: -5.5130e+03 - output_2_loss: 29055.1875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4242e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 41.4671\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9111e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 555.9776\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2101e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 256.9419\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20499.0977 - output_1_loss: -5.4967e+03 - output_2_loss: 25995.7734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15153.2402 - output_1_loss: -5.5131e+03 - output_2_loss: 20666.3379\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4718e+03 - output_1_loss: -5.4971e+03 - output_2_loss: 2025.3632\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3445e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 121.6775\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 42237.4922 - output_1_loss: -6.9742e+03 - output_2_loss: 49211.7266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14249.3125 - output_1_loss: -6.9581e+03 - output_2_loss: 21207.4062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16424.2344 - output_1_loss: -5.5073e+03 - output_2_loss: 21931.4922\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20600.9199 - output_1_loss: -5.5058e+03 - output_2_loss: 26106.7363\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4255e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 40.1278\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7703e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 697.5087\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3379e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 127.3756\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3706e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 95.3956\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25738.5215 - output_1_loss: -5.5251e+03 - output_2_loss: 31263.5918\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6890e+03 - output_1_loss: -5.4990e+03 - output_2_loss: 1809.9249\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37319.8281 - output_1_loss: -6.9719e+03 - output_2_loss: 44291.7539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3534e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 112.3754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13729.0166 - output_1_loss: -5.5014e+03 - output_2_loss: 19230.4121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4346e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 31.0913\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29612.0742 - output_1_loss: -5.5225e+03 - output_2_loss: 35134.6133\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3472e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 118.0966\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10317.0195 - output_1_loss: -5.5118e+03 - output_2_loss: 15828.8604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6553e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 814.0457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9410.8965 - output_1_loss: -6.9545e+03 - output_2_loss: 16365.3525\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16100.0312 - output_1_loss: -5.5146e+03 - output_2_loss: 21614.6387\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37609.4609 - output_1_loss: -6.9726e+03 - output_2_loss: 44582.0156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1344e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 331.1570\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3913e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 74.2588\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23801.2520 - output_1_loss: -5.5246e+03 - output_2_loss: 29325.8945\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35501.2734 - output_1_loss: -5.5286e+03 - output_2_loss: 41029.8789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4079e+03 - output_1_loss: -5.5010e+03 - output_2_loss: 2093.1299\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4356e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 29.7446\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9497.9023 - output_1_loss: -6.9559e+03 - output_2_loss: 16453.8418\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19014.1738 - output_1_loss: -5.5184e+03 - output_2_loss: 24532.5566\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4125e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 53.9808\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4254e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 40.1486\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4269e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 38.8630\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22154.1953 - output_1_loss: -5.5240e+03 - output_2_loss: 27678.2383\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39153.6055 - output_1_loss: -5.5303e+03 - output_2_loss: 44683.8867\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0939e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 373.4188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10841.6689 - output_1_loss: -5.5006e+03 - output_2_loss: 16342.2598\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 63713.0938 - output_1_loss: -6.9944e+03 - output_2_loss: 70707.4844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3586e+03 - output_1_loss: -3.4713e+03 - output_2_loss: 2112.7393\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4351e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 30.1272\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29007.4805 - output_1_loss: -5.5305e+03 - output_2_loss: 34538.0117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24431.1348 - output_1_loss: -5.5239e+03 - output_2_loss: 29954.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7891.4541 - output_1_loss: -5.5172e+03 - output_2_loss: 13408.6494\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1161e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 351.1761\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3369.3335 - output_1_loss: -6.9343e+03 - output_2_loss: 10303.6064\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4303e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 35.3897\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4251e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 40.4846\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4128e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 53.6159\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32480.7344 - output_1_loss: -5.5284e+03 - output_2_loss: 38009.0859\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18212.7891 - output_1_loss: -5.5001e+03 - output_2_loss: 23712.8887\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6719.5747 - output_1_loss: -5.5054e+03 - output_2_loss: 12224.9688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1596e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 307.5918\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 88989.0000 - output_1_loss: -7.0071e+03 - output_2_loss: 95996.0547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6888e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 778.2141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4308e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 35.2856\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7605.9312 - output_1_loss: -5.5075e+03 - output_2_loss: 13113.4316\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12665.7637 - output_1_loss: -5.5046e+03 - output_2_loss: 18170.4121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4242e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 41.2953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5715.4028 - output_1_loss: -5.5125e+03 - output_2_loss: 11227.8652\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3602.6094 - output_1_loss: -6.9415e+03 - output_2_loss: 10544.0918\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1620e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 305.1803\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4322e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 33.3413\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2742e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 193.0640\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4314e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 34.3088\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8397.2773 - output_1_loss: -5.5111e+03 - output_2_loss: 13908.3789\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 30795.0859 - output_1_loss: -5.5258e+03 - output_2_loss: 36320.8945\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9039.5723 - output_1_loss: -5.4976e+03 - output_2_loss: 14537.2178\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3075.4624 - output_1_loss: -5.4982e+03 - output_2_loss: 8573.6924\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 123600.7344 - output_1_loss: -7.0204e+03 - output_2_loss: 130621.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4417e+03 - output_1_loss: -3.4690e+03 - output_2_loss: 1027.2437\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4026.7534 - output_1_loss: -6.9522e+03 - output_2_loss: 10978.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16766.5566 - output_1_loss: -5.5147e+03 - output_2_loss: 22281.2617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1675e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 299.5209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4362e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 29.2156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8321e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 636.6849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2017e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 265.9252\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6477.8120 - output_1_loss: -5.5131e+03 - output_2_loss: 11990.8750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5252.3535 - output_1_loss: -5.4937e+03 - output_2_loss: 10746.0439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3002.3730 - output_1_loss: -5.5015e+03 - output_2_loss: 8503.8281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5656.1436 - output_1_loss: -3.4783e+03 - output_2_loss: 9134.4248\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4326e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 33.0943\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13934.4121 - output_1_loss: -5.5190e+03 - output_2_loss: 19453.4551\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20469.4551 - output_1_loss: -5.5081e+03 - output_2_loss: 25977.5059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9688e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 499.6701\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4992.7974 - output_1_loss: -5.4987e+03 - output_2_loss: 10491.4658\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 150437.4531 - output_1_loss: -7.0269e+03 - output_2_loss: 157464.3125\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8862.4482 - output_1_loss: -6.9605e+03 - output_2_loss: 15822.9385\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5091.0562 - output_1_loss: -5.4985e+03 - output_2_loss: 10589.5312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2160e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 250.6002\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4255e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 40.6932\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1953e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 271.7359\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28992.9102 - output_1_loss: -5.5216e+03 - output_2_loss: 34514.4844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11263.3672 - output_1_loss: -5.5153e+03 - output_2_loss: 16778.6934\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 209.2539 - output_1_loss: -5.4925e+03 - output_2_loss: 5701.7988\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2467.1350 - output_1_loss: -3.4746e+03 - output_2_loss: 5941.7739\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4356e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 29.9049\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2179e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 248.9673\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26944.7812 - output_1_loss: -5.5153e+03 - output_2_loss: 32460.0801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4181e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 48.1904\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2444e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 221.9999\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1828e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 284.4078\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4404.1304 - output_1_loss: -5.4960e+03 - output_2_loss: 9900.1201\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 142375.4688 - output_1_loss: -7.0203e+03 - output_2_loss: 149395.8125\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10399.2383 - output_1_loss: -6.9615e+03 - output_2_loss: 17360.6992\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 130.8242 - output_1_loss: -5.4948e+03 - output_2_loss: 5625.6479\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2758.5312 - output_1_loss: -5.5019e+03 - output_2_loss: 8260.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 31603.1562 - output_1_loss: -5.5266e+03 - output_2_loss: 37129.7734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8932e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 575.4412\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6053.9478 - output_1_loss: -5.4903e+03 - output_2_loss: 11544.2344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4014e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 65.0217\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1695e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 297.6947\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 129500.0156 - output_1_loss: -7.0165e+03 - output_2_loss: 136516.5469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8129.8877 - output_1_loss: -3.4788e+03 - output_2_loss: 11608.6748\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4422e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 23.1044\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2095e+01 - output_1_loss: -5.4976e+03 - output_2_loss: 5465.4844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3067.6230 - output_1_loss: -5.5037e+03 - output_2_loss: 8571.3281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20332.7969 - output_1_loss: -5.5107e+03 - output_2_loss: 25843.4668\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2122e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 254.7350\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35061.8828 - output_1_loss: -5.5302e+03 - output_2_loss: 40592.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3199e+03 - output_1_loss: -3.4700e+03 - output_2_loss: 1150.1135\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6255.3179 - output_1_loss: -5.5033e+03 - output_2_loss: 11758.6562\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4680.9727 - output_1_loss: -5.4858e+03 - output_2_loss: 10166.7783\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6586e+03 - output_1_loss: -3.4690e+03 - output_2_loss: 810.3340\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 837.5723 - output_1_loss: -6.9363e+03 - output_2_loss: 7773.8833\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10969.8379 - output_1_loss: -3.4801e+03 - output_2_loss: 14449.9629\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8738e+03 - output_1_loss: -5.4958e+03 - output_2_loss: 3621.9929\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4309e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 35.0178\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38451.7656 - output_1_loss: -5.5332e+03 - output_2_loss: 43984.9336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9917e+03 - output_1_loss: -3.4705e+03 - output_2_loss: 1478.8467\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.0472e+02 - output_1_loss: -3.4721e+03 - output_2_loss: 2567.3936\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 136503.4688 - output_1_loss: -7.0206e+03 - output_2_loss: 143524.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4405e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 24.7943\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1565.8379 - output_1_loss: -5.4981e+03 - output_2_loss: 7063.9673\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20949.6641 - output_1_loss: -5.5150e+03 - output_2_loss: 26464.6895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2581e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 208.0989\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8456e+03 - output_1_loss: -5.4972e+03 - output_2_loss: 3651.6677\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4149.8066 - output_1_loss: -5.5002e+03 - output_2_loss: 9649.9727\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4881.6748 - output_1_loss: -5.4992e+03 - output_2_loss: 10380.8623\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38993.0000 - output_1_loss: -5.5310e+03 - output_2_loss: 44524.0469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0235e+02 - output_1_loss: -6.9380e+03 - output_2_loss: 6635.6763\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 417.5334 - output_1_loss: -3.4736e+03 - output_2_loss: 3891.1399\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8848.5732 - output_1_loss: -3.4785e+03 - output_2_loss: 12327.0820\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4381e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 27.0665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4318e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 34.0075\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9574e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 2538.4680\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8320e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 635.1604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40078.5820 - output_1_loss: -5.5308e+03 - output_2_loss: 45609.3516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 112477.9922 - output_1_loss: -7.0106e+03 - output_2_loss: 119488.5547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1845e+03 - output_1_loss: -6.9387e+03 - output_2_loss: 5754.2578\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1675.5537 - output_1_loss: -5.4992e+03 - output_2_loss: 7174.7627\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13354.8223 - output_1_loss: -5.5083e+03 - output_2_loss: 18863.0781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2448e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 221.8829\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3762.0405 - output_1_loss: -5.5000e+03 - output_2_loss: 9262.0059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6309.6426 - output_1_loss: -5.5046e+03 - output_2_loss: 11814.2070\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.0987e+02 - output_1_loss: -3.4703e+03 - output_2_loss: 2660.4312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 117057.8672 - output_1_loss: -7.0124e+03 - output_2_loss: 124070.2500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9922.8125 - output_1_loss: -3.4786e+03 - output_2_loss: 13401.3877\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4386e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 26.6330\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 368.6724 - output_1_loss: -6.9455e+03 - output_2_loss: 7314.1973\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1542.3652 - output_1_loss: -5.5008e+03 - output_2_loss: 7043.1826\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3989e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 67.6695\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0949e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 372.9554\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.8345e+03 - output_1_loss: -5.4981e+03 - output_2_loss: 1663.5649\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7333e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 734.8328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5721.4346 - output_1_loss: -5.5069e+03 - output_2_loss: 11228.2920\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 362.7041 - output_1_loss: -5.5012e+03 - output_2_loss: 5863.8911\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 42870.1094 - output_1_loss: -5.5321e+03 - output_2_loss: 48402.2266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11009.2520 - output_1_loss: -5.5101e+03 - output_2_loss: 16519.3379\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2497e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 217.6040\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5941.6602 - output_1_loss: -5.5116e+03 - output_2_loss: 11453.2656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3865e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 80.2690\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8811e+03 - output_1_loss: -5.5030e+03 - output_2_loss: 3621.9041\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5550.3623 - output_1_loss: -5.5080e+03 - output_2_loss: 11058.3760\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.4388e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 2030.0977\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 112878.2578 - output_1_loss: -7.0140e+03 - output_2_loss: 119892.2734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5819.0137 - output_1_loss: -3.4691e+03 - output_2_loss: 9288.1621\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7266e+03 - output_1_loss: -6.9341e+03 - output_2_loss: 5207.5737\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2301e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 236.5362\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0518e+03 - output_1_loss: -3.4704e+03 - output_2_loss: 1418.6056\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6609.1299 - output_1_loss: -5.5124e+03 - output_2_loss: 12121.5566\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8666e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 600.3607\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3777e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 89.0724\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0991e+03 - output_1_loss: -5.4944e+03 - output_2_loss: 4395.3237\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26294.1016 - output_1_loss: -5.5130e+03 - output_2_loss: 31807.1328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7458e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 1722.9231\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7581.2637 - output_1_loss: -5.5070e+03 - output_2_loss: 13088.2754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 99633.3359 - output_1_loss: -7.0096e+03 - output_2_loss: 106642.9766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5012.9902 - output_1_loss: -3.4699e+03 - output_2_loss: 8482.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2526e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 213.8373\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1439e+03 - output_1_loss: -5.5044e+03 - output_2_loss: 4360.4692\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9098e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 556.9252\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1278e+02 - output_1_loss: -5.4986e+03 - output_2_loss: 5285.8403\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.2561e+03 - output_1_loss: -5.4921e+03 - output_2_loss: 4236.0205\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8398e+03 - output_1_loss: -6.9336e+03 - output_2_loss: 5093.8018\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3073.0874 - output_1_loss: -3.4687e+03 - output_2_loss: 6541.7505\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9416e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 525.5270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8655.8389 - output_1_loss: -5.5140e+03 - output_2_loss: 14169.8096\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4098e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 56.3645\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2837e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 182.4114\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 52323.5664 - output_1_loss: -5.5380e+03 - output_2_loss: 57861.5352\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2368e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 2259.0786\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0567e+03 - output_1_loss: -3.4712e+03 - output_2_loss: 2414.5134\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4180.3037 - output_1_loss: -5.5064e+03 - output_2_loss: 9686.7178\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37707.3086 - output_1_loss: -6.9491e+03 - output_2_loss: 44656.4102\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2672.0769 - output_1_loss: -3.4680e+03 - output_2_loss: 6140.0308\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9719e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 494.5956\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6813.0366 - output_1_loss: -5.5148e+03 - output_2_loss: 12327.8213\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9511e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 515.4209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.7546e+02 - output_1_loss: -5.5041e+03 - output_2_loss: 4828.6362\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3741e+03 - output_1_loss: -6.9394e+03 - output_2_loss: 5565.3027\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11263.3945 - output_1_loss: -5.5185e+03 - output_2_loss: 16781.9355\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3769e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 89.9279\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3118e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 154.6741\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8657e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 602.1826\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32716.4062 - output_1_loss: -5.5273e+03 - output_2_loss: 38243.6836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4176e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 1051.7843\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0537e+03 - output_1_loss: -5.4888e+03 - output_2_loss: 2435.1064\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8826e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 1584.5024\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2888.8062 - output_1_loss: -5.5098e+03 - output_2_loss: 8398.6045\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39898.8750 - output_1_loss: -6.9754e+03 - output_2_loss: 46874.3164\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 1531.2378 - output_1_loss: -3.4713e+03 - output_2_loss: 5002.5239\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3601.3081 - output_1_loss: -5.5097e+03 - output_2_loss: 9110.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3747e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 92.1087\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7615e+03 - output_1_loss: -5.4966e+03 - output_2_loss: 2735.0969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2824e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 184.6835\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1640e+03 - output_1_loss: -3.4702e+03 - output_2_loss: 1306.1908\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1514e+03 - output_1_loss: -6.9381e+03 - output_2_loss: 3786.7048\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1622e+03 - output_1_loss: -5.4888e+03 - output_2_loss: 2326.5959\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9963e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 1470.2336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5460.2153 - output_1_loss: -5.5130e+03 - output_2_loss: 10973.1758\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11922.9336 - output_1_loss: -5.5188e+03 - output_2_loss: 17441.7422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.5992e+01 - output_1_loss: -3.4710e+03 - output_2_loss: 3384.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14896.5117 - output_1_loss: -5.5229e+03 - output_2_loss: 20419.4434\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35801.3164 - output_1_loss: -5.5333e+03 - output_2_loss: 41334.6602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8528e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 614.2698\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2357e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 231.9507\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2092e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 258.5176\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15087.2910 - output_1_loss: -6.9328e+03 - output_2_loss: 22020.0801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2325e+03 - output_1_loss: -5.4894e+03 - output_2_loss: 2256.9294\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 649.7180 - output_1_loss: -3.4720e+03 - output_2_loss: 4121.7017\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1013e+03 - output_1_loss: -5.4968e+03 - output_2_loss: 2395.4990\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6900e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 777.9205\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22317.5645 - output_1_loss: -5.5274e+03 - output_2_loss: 27844.9980\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.2632e+03 - output_1_loss: -6.9354e+03 - output_2_loss: 2672.2361\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9765e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 492.0970\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0615e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 1404.5604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8225.9277 - output_1_loss: -5.5157e+03 - output_2_loss: 13741.5820\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5932.7280 - output_1_loss: -5.5018e+03 - output_2_loss: 11434.5605\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5778e+03 - output_1_loss: -5.4927e+03 - output_2_loss: 1914.8755\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26548.0898 - output_1_loss: -5.5261e+03 - output_2_loss: 32074.1934\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0384e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 428.0693\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3726e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 93.4813\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4753.9492 - output_1_loss: -3.4762e+03 - output_2_loss: 8230.1123\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.6853e+03 - output_1_loss: -3.4710e+03 - output_2_loss: 1785.7531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28468.0352 - output_1_loss: -5.5312e+03 - output_2_loss: 33999.2812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28885.1367 - output_1_loss: -6.9704e+03 - output_2_loss: 35855.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1220e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 1346.4790\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21255.5000 - output_1_loss: -5.5255e+03 - output_2_loss: 26780.9551\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.8959e+03 - output_1_loss: -5.4918e+03 - output_2_loss: 1595.9624\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5624e+03 - output_1_loss: -5.4941e+03 - output_2_loss: 1931.6854\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27018.6426 - output_1_loss: -5.5251e+03 - output_2_loss: 32543.7207\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2230e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 242.8170\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8152e+03 - output_1_loss: -6.9291e+03 - output_2_loss: 2113.9287\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3741e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 91.7723\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11907.1748 - output_1_loss: -3.4807e+03 - output_2_loss: 15387.9053\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2245e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 242.5403\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3825e+02 - output_1_loss: -3.4727e+03 - output_2_loss: 2934.4351\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9482.7168 - output_1_loss: -5.5152e+03 - output_2_loss: 14997.9434\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12973.1035 - output_1_loss: -5.5071e+03 - output_2_loss: 18480.1543\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 66710.4688 - output_1_loss: -6.9946e+03 - output_2_loss: 73705.0938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3744e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 91.5227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21344.6289 - output_1_loss: -3.4852e+03 - output_2_loss: 24829.8457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2401e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 226.7663\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4907e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 977.4266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 313.2300 - output_1_loss: -3.4731e+03 - output_2_loss: 3786.3018\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24241.1875 - output_1_loss: -5.5245e+03 - output_2_loss: 29765.6855\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15676.1484 - output_1_loss: -5.5212e+03 - output_2_loss: 21197.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.9785e+03 - output_1_loss: -5.4929e+03 - output_2_loss: 1514.4086\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.1705e+02 - output_1_loss: -5.5047e+03 - output_2_loss: 4887.6919\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26824.3359 - output_1_loss: -5.5248e+03 - output_2_loss: 32349.1582\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2674e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 198.1201\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.3692e+03 - output_1_loss: -6.9371e+03 - output_2_loss: 2567.8953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30957.8242 - output_1_loss: -3.4897e+03 - output_2_loss: 34447.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1032.2368 - output_1_loss: -3.4728e+03 - output_2_loss: 4505.0010\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19539.3379 - output_1_loss: -5.5238e+03 - output_2_loss: 25063.1055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3277.0435 - output_1_loss: -5.5099e+03 - output_2_loss: 8786.9365\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20585.2734 - output_1_loss: -5.5210e+03 - output_2_loss: 26106.2441\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23387.6406 - output_1_loss: -6.9401e+03 - output_2_loss: 30327.7383\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2645e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 201.7402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3866e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 79.1377\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2229e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 244.3292\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9476e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 518.0439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17647.1387 - output_1_loss: -5.5138e+03 - output_2_loss: 23160.9648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0246e+03 - output_1_loss: -5.4894e+03 - output_2_loss: 1464.7906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40235.3203 - output_1_loss: -3.4931e+03 - output_2_loss: 43728.4219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25431.2168 - output_1_loss: -5.5246e+03 - output_2_loss: 30955.8242\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.5368e+03 - output_1_loss: -6.9422e+03 - output_2_loss: 2405.4204\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 21244.9375 - output_1_loss: -6.9256e+03 - output_2_loss: 28170.5391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2929e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 173.7850\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0568e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 411.3524\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1307e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 337.0772\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9713e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 494.6091\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1546e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 2313.2185\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8212.8320 - output_1_loss: -5.5000e+03 - output_2_loss: 13712.7930\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7942e+03 - output_1_loss: -5.4962e+03 - output_2_loss: 1701.9781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3495e+03 - output_1_loss: -5.4923e+03 - output_2_loss: 3142.7305\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11271.1641 - output_1_loss: -5.5049e+03 - output_2_loss: 16776.0801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24773.7891 - output_1_loss: -5.5249e+03 - output_2_loss: 30298.7266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37512.3750 - output_1_loss: -5.5269e+03 - output_2_loss: 43039.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8542e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 614.4839\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22490.5801 - output_1_loss: -3.4757e+03 - output_2_loss: 25966.2754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8099e+03 - output_1_loss: -3.4690e+03 - output_2_loss: 659.0971\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3515e+03 - output_1_loss: -6.9389e+03 - output_2_loss: 1587.4618\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 68768.9062 - output_1_loss: -6.9875e+03 - output_2_loss: 75756.4219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3304e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 135.6409\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9509e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 516.5064\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3649e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 2103.0237\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5050.5703 - output_1_loss: -5.4995e+03 - output_2_loss: 10550.0732\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.3356e+03 - output_1_loss: -5.4928e+03 - output_2_loss: 1157.2128\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3575e+03 - output_1_loss: -3.4701e+03 - output_2_loss: 1112.5250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4543e+03 - output_1_loss: -5.4917e+03 - output_2_loss: 3037.3845\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11783.9648 - output_1_loss: -5.5122e+03 - output_2_loss: 17296.1758\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24268.5684 - output_1_loss: -5.5244e+03 - output_2_loss: 29793.0059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6008e+03 - output_1_loss: -6.9425e+03 - output_2_loss: 2341.6924\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20890.4316 - output_1_loss: -5.5109e+03 - output_2_loss: 26401.2988\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3336e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 132.3534\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1746e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 291.6332\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19773.6211 - output_1_loss: -3.4735e+03 - output_2_loss: 23247.1113\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2779.6592 - output_1_loss: -5.5032e+03 - output_2_loss: 8282.8506\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.2811e+03 - output_1_loss: -5.5045e+03 - output_2_loss: 4223.3862\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0094e+03 - output_1_loss: -3.4705e+03 - output_2_loss: 1461.0852\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6697e+03 - output_1_loss: -5.4944e+03 - output_2_loss: 2824.6975\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23909.4805 - output_1_loss: -5.5243e+03 - output_2_loss: 29433.7578\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 79473.3516 - output_1_loss: -6.9925e+03 - output_2_loss: 86465.8281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0334e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 433.3156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0773e+02 - output_1_loss: -3.4721e+03 - output_2_loss: 3264.3916\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1894e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 276.3533\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25695.6172 - output_1_loss: -3.4844e+03 - output_2_loss: 29180.0371\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4941.1606 - output_1_loss: -5.4987e+03 - output_2_loss: 10439.8213\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6482.2710 - output_1_loss: -5.5147e+03 - output_2_loss: 11996.9229\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8181e+03 - output_1_loss: -6.9375e+03 - output_2_loss: 2119.4287\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20711.7051 - output_1_loss: -5.5180e+03 - output_2_loss: 26229.7266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3490e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 116.7516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0565e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 410.0297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.1912e+02 - output_1_loss: -3.4718e+03 - output_2_loss: 3052.6343\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 542.5483 - output_1_loss: -5.4935e+03 - output_2_loss: 6036.0405\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2302e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 235.4670\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23674.0098 - output_1_loss: -3.4849e+03 - output_2_loss: 27158.9551\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7529e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 714.5781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3522.6504 - output_1_loss: -5.5100e+03 - output_2_loss: 9032.6816\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2508.0063 - output_1_loss: -5.4927e+03 - output_2_loss: 8000.6904\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14857.5273 - output_1_loss: -5.5015e+03 - output_2_loss: 20358.9922\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95244.9922 - output_1_loss: -6.9996e+03 - output_2_loss: 102244.6250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29085.7188 - output_1_loss: -5.5261e+03 - output_2_loss: 34611.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3531e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 113.3344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0004e+02 - output_1_loss: -5.5001e+03 - output_2_loss: 5000.0269\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26582.2617 - output_1_loss: -3.4866e+03 - output_2_loss: 30068.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8732e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 593.7410\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0047e+03 - output_1_loss: -6.9422e+03 - output_2_loss: 2937.5139\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 112.5879 - output_1_loss: -5.4943e+03 - output_2_loss: 5606.9282\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3844.7837 - output_1_loss: -5.4907e+03 - output_2_loss: 9335.4785\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0531e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 412.2707\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5777e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 1889.3402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 511.5591 - output_1_loss: -5.4972e+03 - output_2_loss: 6008.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2195e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 247.0602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5261.2422 - output_1_loss: -5.5115e+03 - output_2_loss: 10772.7373\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 94130.6172 - output_1_loss: -6.9971e+03 - output_2_loss: 101127.7031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 200.3086 - output_1_loss: -5.5039e+03 - output_2_loss: 5704.2534\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27176.0293 - output_1_loss: -5.5257e+03 - output_2_loss: 32701.7559\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2288.6440 - output_1_loss: -5.4924e+03 - output_2_loss: 7781.0625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4087e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 56.5910\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9784e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 1487.5868\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1186.2217 - output_1_loss: -5.5006e+03 - output_2_loss: 6686.7861\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2196e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 247.1799\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12079.5625 - output_1_loss: -3.4708e+03 - output_2_loss: 15550.3555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9218e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 545.1068\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7703e+03 - output_1_loss: -6.9434e+03 - output_2_loss: 3173.1677\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4105.0635 - output_1_loss: -5.5067e+03 - output_2_loss: 9611.7939\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1290e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 336.6344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4278e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 38.5353\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1254e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 1340.7809\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 395.7124 - output_1_loss: -5.4924e+03 - output_2_loss: 5888.1455\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 80690.6641 - output_1_loss: -6.9270e+03 - output_2_loss: 87617.6406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3416.3418 - output_1_loss: -5.5105e+03 - output_2_loss: 8926.8750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8400e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 628.7681\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7445.6943 - output_1_loss: -3.4678e+03 - output_2_loss: 10913.4561\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9527e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 514.0069\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1831e+03 - output_1_loss: -6.9480e+03 - output_2_loss: 4764.9819\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30397.4883 - output_1_loss: -5.5288e+03 - output_2_loss: 35926.2695\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -7.4508e+02 - output_1_loss: -5.4941e+03 - output_2_loss: 4748.9849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6585e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 810.4109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1596e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 308.3274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 316.3423 - output_1_loss: -5.4937e+03 - output_2_loss: 5810.0278\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7952e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 670.3038\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 274.6094 - output_1_loss: -5.4925e+03 - output_2_loss: 5767.1406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5989.9473 - output_1_loss: -5.5097e+03 - output_2_loss: 11499.6211\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 76260.2578 - output_1_loss: -6.9121e+03 - output_2_loss: 83172.3828\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3259.2197 - output_1_loss: -6.9568e+03 - output_2_loss: 10216.0576\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40276.3828 - output_1_loss: -5.5352e+03 - output_2_loss: 45811.6094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1490.1514 - output_1_loss: -5.5025e+03 - output_2_loss: 6992.6479\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1428.7388 - output_1_loss: -5.5052e+03 - output_2_loss: 6933.9502\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1473e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 319.3578\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4952.4321 - output_1_loss: -3.4670e+03 - output_2_loss: 8419.4414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8853e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 582.3242\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9472e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 517.9919\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.4727e+02 - output_1_loss: -5.4914e+03 - output_2_loss: 5344.1260\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16028.9590 - output_1_loss: -5.5180e+03 - output_2_loss: 21546.9395\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0679e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 397.3020\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2230.6582 - output_1_loss: -5.5028e+03 - output_2_loss: 7733.5073\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3489e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 117.6461\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 223.9541 - output_1_loss: -5.4942e+03 - output_2_loss: 5718.1104\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 84188.5391 - output_1_loss: -6.9788e+03 - output_2_loss: 91167.3750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1907e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 275.6846\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4591.5259 - output_1_loss: -3.4701e+03 - output_2_loss: 8061.6401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9801e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 487.2158\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9806e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 484.7388\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9085.8662 - output_1_loss: -6.9636e+03 - output_2_loss: 16049.4316\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 86552.3125 - output_1_loss: -5.5557e+03 - output_2_loss: 92108.0234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23585.3887 - output_1_loss: -5.5222e+03 - output_2_loss: 29107.5762\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3419.0435 - output_1_loss: -5.5045e+03 - output_2_loss: 8923.5029\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8966e+03 - output_1_loss: -5.4910e+03 - output_2_loss: 2594.3921\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2383.4297 - output_1_loss: -5.5077e+03 - output_2_loss: 7891.1245\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 77.4873 - output_1_loss: -5.4996e+03 - output_2_loss: 5577.1089\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 89595.5547 - output_1_loss: -6.9863e+03 - output_2_loss: 96581.8984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1321e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 334.8402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19884.8867 - output_1_loss: -6.9746e+03 - output_2_loss: 26859.5293\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2433e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 222.4016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2775e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 189.7401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2499e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 216.0766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3705.6897 - output_1_loss: -3.4656e+03 - output_2_loss: 7171.2739\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9915e+03 - output_1_loss: -3.4650e+03 - output_2_loss: 473.4732\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 103.5239 - output_1_loss: -5.5006e+03 - output_2_loss: 5604.1533\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 41531.4219 - output_1_loss: -5.5277e+03 - output_2_loss: 47059.1680\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10611.9785 - output_1_loss: -5.5039e+03 - output_2_loss: 16115.8467\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26203.6465 - output_1_loss: -6.9787e+03 - output_2_loss: 33182.3594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3375.6001 - output_1_loss: -5.5046e+03 - output_2_loss: 8880.1689\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7066e+03 - output_1_loss: -5.4960e+03 - output_2_loss: 2789.3638\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 969.0322 - output_1_loss: -5.5034e+03 - output_2_loss: 6472.4292\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 61817.3984 - output_1_loss: -6.9309e+03 - output_2_loss: 68748.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4159.3428 - output_1_loss: -3.4633e+03 - output_2_loss: 7622.6895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2618e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 204.5352\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1027e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 363.7409\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1875.0488 - output_1_loss: -5.5047e+03 - output_2_loss: 7379.7017\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1027e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 364.4272\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3020e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 164.9974\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35920.1797 - output_1_loss: -6.9851e+03 - output_2_loss: 42905.2383\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.4774e+02 - output_1_loss: -5.5031e+03 - output_2_loss: 5055.4058\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3096.6646 - output_1_loss: -5.5090e+03 - output_2_loss: 8605.7109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3338e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 131.8130\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45599.8008 - output_1_loss: -5.5220e+03 - output_2_loss: 51121.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15334.5488 - output_1_loss: -5.5156e+03 - output_2_loss: 20850.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1468e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 320.6439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1547e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 311.5442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12943.0391 - output_1_loss: -5.5157e+03 - output_2_loss: 18458.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 75870.5391 - output_1_loss: -6.9737e+03 - output_2_loss: 82844.2109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3978.7710 - output_1_loss: -3.4704e+03 - output_2_loss: 7449.1641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6281.0308 - output_1_loss: -5.5126e+03 - output_2_loss: 11793.6084\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2130e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 252.2473\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 44471.1484 - output_1_loss: -5.5191e+03 - output_2_loss: 49990.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7087e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 760.2060\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3353e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 131.1073\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2394e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 226.5566\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 57167.2734 - output_1_loss: -6.9960e+03 - output_2_loss: 64163.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2528e+03 - output_1_loss: -5.4972e+03 - output_2_loss: 2244.3984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5029e+02 - output_1_loss: -5.5028e+03 - output_2_loss: 5152.5454\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3389e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 126.6111\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28940.3457 - output_1_loss: -5.5285e+03 - output_2_loss: 34468.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16270.4512 - output_1_loss: -5.5217e+03 - output_2_loss: 21792.1641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18385.0508 - output_1_loss: -5.5202e+03 - output_2_loss: 23905.2168\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0028e+03 - output_1_loss: -3.4704e+03 - output_2_loss: 1467.6719\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2806e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 185.7271\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 53415.5078 - output_1_loss: -6.9403e+03 - output_2_loss: 60355.8164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2127.7817 - output_1_loss: -3.4634e+03 - output_2_loss: 5591.2139\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2155e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 249.7158\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33695.2734 - output_1_loss: -5.5171e+03 - output_2_loss: 39212.3633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3273e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 139.3907\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 79635.3125 - output_1_loss: -7.0002e+03 - output_2_loss: 86635.5625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 976.2480 - output_1_loss: -3.4746e+03 - output_2_loss: 4450.8979\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2977e+03 - output_1_loss: -5.5025e+03 - output_2_loss: 3204.8127\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.8959e+02 - output_1_loss: -5.5007e+03 - output_2_loss: 4511.0957\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3638e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 101.8783\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13330.5430 - output_1_loss: -5.5021e+03 - output_2_loss: 18832.5957\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6676e+02 - output_1_loss: -3.4629e+03 - output_2_loss: 3096.1292\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4518.2554 - output_1_loss: -5.4944e+03 - output_2_loss: 10012.6240\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2181e+03 - output_1_loss: -3.4651e+03 - output_2_loss: 247.0447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27425.2305 - output_1_loss: -5.5061e+03 - output_2_loss: 32931.3398\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13365.2012 - output_1_loss: -5.4854e+03 - output_2_loss: 18850.5605\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3427e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 123.6069\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2903e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 177.2046\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2779.6555 - output_1_loss: -3.4761e+03 - output_2_loss: 6255.7642\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 76608.9297 - output_1_loss: -6.9824e+03 - output_2_loss: 83591.3047\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6015e+03 - output_1_loss: -5.4998e+03 - output_2_loss: 2898.3289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13319.8008 - output_1_loss: -5.4974e+03 - output_2_loss: 18817.2207\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 56487.6758 - output_1_loss: -6.9695e+03 - output_2_loss: 63457.1914\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27415.9609 - output_1_loss: -5.4793e+03 - output_2_loss: 32895.2305\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3595e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 106.5754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7931e+03 - output_1_loss: -5.4956e+03 - output_2_loss: 1702.5653\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3676e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 97.9331\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5778.6465 - output_1_loss: -3.4778e+03 - output_2_loss: 9256.4541\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 100945.6016 - output_1_loss: -6.9956e+03 - output_2_loss: 107941.1953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7412e+03 - output_1_loss: -5.4998e+03 - output_2_loss: 2758.5750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.9387e+02 - output_1_loss: -3.4645e+03 - output_2_loss: 2870.6775\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5575.4131 - output_1_loss: -5.5035e+03 - output_2_loss: 11078.8682\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1588e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 308.1761\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17433.6289 - output_1_loss: -5.5177e+03 - output_2_loss: 22951.3516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3807e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 85.2984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37733.6016 - output_1_loss: -6.9461e+03 - output_2_loss: 44679.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14534.2402 - output_1_loss: -5.5112e+03 - output_2_loss: 20045.4238\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2860e+03 - output_1_loss: -5.5028e+03 - output_2_loss: 3216.7546\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39241.6602 - output_1_loss: -5.4593e+03 - output_2_loss: 44700.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22711.7695 - output_1_loss: -5.5221e+03 - output_2_loss: 28233.8750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3839e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 81.8383\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1481e+03 - output_1_loss: -5.5054e+03 - output_2_loss: 4357.2988\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3750e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 90.3744\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1790.2239 - output_1_loss: -3.4700e+03 - output_2_loss: 5260.2354\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 62070.6953 - output_1_loss: -6.9466e+03 - output_2_loss: 69017.3203\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.3781e+02 - output_1_loss: -3.4637e+03 - output_2_loss: 2625.8901\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3653.6284 - output_1_loss: -5.4963e+03 - output_2_loss: 9149.9219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0462e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 421.4565\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10953.3184 - output_1_loss: -5.5093e+03 - output_2_loss: 16462.6660\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2353e+03 - output_1_loss: -5.5007e+03 - output_2_loss: 2265.3257\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3965e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 69.1827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34168.7734 - output_1_loss: -6.9598e+03 - output_2_loss: 41128.5586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3884e+03 - output_1_loss: -3.4651e+03 - output_2_loss: 76.6757\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17043.1621 - output_1_loss: -5.4794e+03 - output_2_loss: 22522.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9822.0977 - output_1_loss: -5.5115e+03 - output_2_loss: 15333.5654\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18607.0664 - output_1_loss: -5.4811e+03 - output_2_loss: 24088.1816\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.8177e+03 - output_1_loss: -5.4990e+03 - output_2_loss: 1681.3160\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3832e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 82.3554\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27915.2344 - output_1_loss: -6.9544e+03 - output_2_loss: 34869.6719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1475e+03 - output_1_loss: -5.4986e+03 - output_2_loss: 2351.0320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3548.5908 - output_1_loss: -3.4744e+03 - output_2_loss: 7022.9629\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 87304.1094 - output_1_loss: -6.9877e+03 - output_2_loss: 94291.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6308e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 834.7415\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4143.6470 - output_1_loss: -5.5050e+03 - output_2_loss: 9648.6230\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1682e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 297.5978\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3945e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 71.0019\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20986.6211 - output_1_loss: -5.4741e+03 - output_2_loss: 26460.7168\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4153e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 49.8929\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3890e+03 - output_1_loss: -3.4650e+03 - output_2_loss: 75.9342\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4700.6919 - output_1_loss: -3.4766e+03 - output_2_loss: 8177.2427\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 164423.0781 - output_1_loss: -7.0303e+03 - output_2_loss: 171453.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6892e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 776.0209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6980.3433 - output_1_loss: -5.5118e+03 - output_2_loss: 12492.1338\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32979.3320 - output_1_loss: -5.5103e+03 - output_2_loss: 38489.6055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19451.5508 - output_1_loss: -5.5200e+03 - output_2_loss: 24971.5176\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.1732e+03 - output_1_loss: -5.4899e+03 - output_2_loss: 1316.7209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22276.6973 - output_1_loss: -6.9460e+03 - output_2_loss: 29222.7246\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0162e+03 - output_1_loss: -5.4994e+03 - output_2_loss: 2483.2827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4209e+03 - output_1_loss: -3.4650e+03 - output_2_loss: 44.1320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6231.4980 - output_1_loss: -3.4780e+03 - output_2_loss: 9709.4961\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1938e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 271.6483\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3898e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 75.5188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6852.6099 - output_1_loss: -5.5118e+03 - output_2_loss: 12364.4541\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28135.0273 - output_1_loss: -5.5049e+03 - output_2_loss: 33639.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20963.7734 - output_1_loss: -5.5104e+03 - output_2_loss: 26474.1543\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3883e+03 - output_1_loss: -3.4649e+03 - output_2_loss: 76.5770\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 107229.3203 - output_1_loss: -6.9983e+03 - output_2_loss: 114227.6484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7685e+03 - output_1_loss: -3.4649e+03 - output_2_loss: 696.4391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23619.4336 - output_1_loss: -5.5210e+03 - output_2_loss: 29140.4297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.4721e+03 - output_1_loss: -5.4945e+03 - output_2_loss: 1022.3660\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4048e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 61.0270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16190.7324 - output_1_loss: -6.9483e+03 - output_2_loss: 23139.0293\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2800e+03 - output_1_loss: -5.4967e+03 - output_2_loss: 2216.6848\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-1\n",
      "Dumping agent-1 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-3\n",
      "Dumping agent-3 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-4\n",
      "Dumping agent-4 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-5\n",
      "Dumping agent-5 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-6\n",
      "Dumping agent-6 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-7\n",
      "Dumping agent-7 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-8\n",
      "Dumping agent-8 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-9\n",
      "Dumping agent-9 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-10\n",
      "Dumping agent-10 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-11\n",
      "Dumping agent-11 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-12\n",
      "Dumping agent-12 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiAc_Agents.train(1000)\n",
    "\n",
    "Balance_MultiAc_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_28  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_29  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  960       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  630       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_30  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,307\n",
      "Trainable params: 12,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_31  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_32  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_33  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_34  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,373\n",
      "Trainable params: 11,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_35  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,522\n",
      "Trainable params: 11,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_36  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_37  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_38  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_39  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_40  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_41  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_28  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_29  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  960       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  630       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_30  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,307\n",
      "Trainable params: 12,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_31  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_32  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_33  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_34  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,373\n",
      "Trainable params: 11,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_35  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,522\n",
      "Trainable params: 11,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_36  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_37  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_38  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_39  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_40  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_41  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n",
    "\n",
    "Balance_MultiAc_Agents.load(best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Agents = []\n",
    "for idx, info in Balance_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patate\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -72.0, -64.0, -6.0, -71.0, -6.0, -6.0, -63.0, -19.0, -72.0] \n",
      " [-56.0, -72.0, -90.0, -15.0, -69.0, -25.0, -49.0, -78.0, -24.0, -72.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.33, 0.35, 0.32], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34], [0.34, 0.34, 0.33], [0.32, 0.35, 0.32], [0.34, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.34 0.33]\n",
      "Agent 1 : Predicted Values and True Return : \n",
      " [-32.0, -71.0, -44.0, -28.0, -54.0, -60.0, -5.0, -75.0, -93.0, -5.0] \n",
      " [-55.0, -76.0, -118.0, -105.0, -85.0, -82.0, -64.0, -52.0, -87.0, -37.0]\n",
      "Agent 1 : Proba distribution on those states : \n",
      " [[0.33, 0.34, 0.33], [0.32, 0.34, 0.34], [0.3, 0.36, 0.34], [0.33, 0.33, 0.33], [0.32, 0.35, 0.33], [0.33, 0.33, 0.34], [0.33, 0.34, 0.34], [0.32, 0.35, 0.33], [0.33, 0.34, 0.33], [0.33, 0.34, 0.34]]\n",
      "Agent 1 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.34]\n",
      "Agent 2 : Predicted Values and True Return : \n",
      " [-7.0, -108.0, -72.0, -127.0, -14.0, -146.0, -38.0, -193.0, -161.0, -161.0] \n",
      " [-31.0, -178.0, -160.0, -175.0, -42.0, -171.0, -58.0, -159.0, -153.0, -153.0]\n",
      "Agent 2 : Proba distribution on those states : \n",
      " [[0.25, 0.26, 0.24, 0.25], [0.24, 0.23, 0.26, 0.26], [0.26, 0.24, 0.25, 0.25], [0.24, 0.24, 0.22, 0.31], [0.24, 0.26, 0.25, 0.25], [0.23, 0.23, 0.26, 0.28], [0.24, 0.25, 0.24, 0.27], [0.26, 0.22, 0.27, 0.25], [0.26, 0.24, 0.22, 0.28], [0.26, 0.24, 0.22, 0.28]]\n",
      "Agent 2 : Proba distribution on the 0 state : \n",
      " [0.25 0.26 0.24 0.25]\n",
      "Agent 3 : Predicted Values and True Return : \n",
      " [-7.0, -11.0, -7.0, -13.0, -16.0, -4.0, -12.0, -13.0, -7.0, -16.0] \n",
      " [-24.0, -28.0, -20.0, -42.0, -38.0, -33.0, -29.0, -19.0, -38.0, -26.0]\n",
      "Agent 3 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.34, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.34, 0.33, 0.34], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.33, 0.33], [0.31, 0.34, 0.35]]\n",
      "Agent 3 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 4 : Predicted Values and True Return : \n",
      " [-7.0, -25.0, -36.0, -13.0, -25.0, -45.0, -30.0, -20.0, -19.0, -7.0] \n",
      " [-30.0, -16.0, -64.0, -20.0, -15.0, -34.0, -18.0, -19.0, -5.0, -8.0]\n",
      "Agent 4 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.34, 0.34, 0.31], [0.33, 0.34, 0.33], [0.34, 0.33, 0.33], [0.35, 0.33, 0.31], [0.34, 0.35, 0.31], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.33, 0.33]]\n",
      "Agent 4 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 5 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -3.0, -1.0] \n",
      " [-5.0, -3.0, -0.0, -1.0, -2.0, -1.0, -1.0, -2.0, -3.0, -2.0]\n",
      "Agent 5 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 5 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 6 : Predicted Values and True Return : \n",
      " [-36.0, -3.0, -6.0, -6.0, -19.0, -3.0, -7.0, -13.0, -3.0, -3.0] \n",
      " [-21.0, -8.0, -5.0, -11.0, -12.0, -7.0, -18.0, -13.0, -13.0, -10.0]\n",
      "Agent 6 : Proba distribution on those states : \n",
      " [[0.49, 0.51], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.48, 0.52], [0.5, 0.5], [0.5, 0.5], [0.49, 0.51], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 6 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 7 : Predicted Values and True Return : \n",
      " [-43.0, -8.0, -4.0, -7.0, -12.0, -7.0, -32.0, -43.0, -7.0, -24.0] \n",
      " [-86.0, -66.0, -30.0, -19.0, -59.0, -21.0, -27.0, -65.0, -34.0, -43.0]\n",
      "Agent 7 : Proba distribution on those states : \n",
      " [[0.3, 0.34, 0.36], [0.34, 0.34, 0.33], [0.33, 0.34, 0.34], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.33, 0.34], [0.32, 0.33, 0.35], [0.3, 0.34, 0.36], [0.33, 0.33, 0.34], [0.33, 0.34, 0.34]]\n",
      "Agent 7 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.34]\n",
      "Agent 8 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -3.0, -4.0, -7.0, -3.0, -4.0, -4.0, -3.0, -3.0] \n",
      " [-4.0, -4.0, -3.0, -6.0, -13.0, -6.0, -10.0, -3.0, -11.0, -2.0]\n",
      "Agent 8 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 8 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 9 : Predicted Values and True Return : \n",
      " [-4.0, -3.0, -3.0, -3.0, -4.0, -6.0, -7.0, -3.0, -3.0, -7.0] \n",
      " [-3.0, -5.0, -7.0, -5.0, -11.0, -7.0, -2.0, -3.0, -4.0, -8.0]\n",
      "Agent 9 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 9 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 10 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -3.0, -9.0, -5.0, -18.0, -6.0, -6.0, -3.0, -6.0] \n",
      " [-5.0, -2.0, -3.0, -4.0, -4.0, -10.0, -5.0, -8.0, -4.0, -3.0]\n",
      "Agent 10 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 10 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 11 : Predicted Values and True Return : \n",
      " [-2.0, -2.0, -5.0, -4.0, -3.0, -7.0, -5.0, -2.0, -3.0, -4.0] \n",
      " [-2.0, -5.0, -4.0, -7.0, -11.0, -11.0, -6.0, -3.0, -3.0, -5.0]\n",
      "Agent 11 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 11 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 12 : Predicted Values and True Return : \n",
      " [-15.0, -19.0, -24.0, -15.0, -15.0, -22.0, -71.0, -12.0, -25.0, -59.0] \n",
      " [-16.0, -50.0, -49.0, -16.0, -17.0, -31.0, -61.0, -26.0, -56.0, -39.0]\n",
      "Agent 12 : Proba distribution on those states : \n",
      " [[0.25, 0.25, 0.25, 0.25], [0.24, 0.25, 0.25, 0.25], [0.25, 0.25, 0.26, 0.24], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.27, 0.24, 0.24, 0.25], [0.25, 0.25, 0.24, 0.26], [0.25, 0.25, 0.25, 0.25], [0.24, 0.26, 0.25, 0.24], [0.25, 0.26, 0.25, 0.24]]\n",
      "Agent 12 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Agent 13 : Predicted Values and True Return : \n",
      " [-70.0, -121.0, -102.0, -52.0, -113.0, -90.0, -113.0, -4.0, -137.0, -113.0] \n",
      " [-105.0, -123.0, -99.0, -111.0, -104.0, -99.0, -135.0, -12.0, -126.0, -104.0]\n",
      "Agent 13 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.34], [0.32, 0.32, 0.36], [0.33, 0.32, 0.35], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.32, 0.35], [0.34, 0.32, 0.34], [0.33, 0.33, 0.33], [0.33, 0.31, 0.35], [0.35, 0.34, 0.31]]\n",
      "Agent 13 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 8 : Entropy reduced to 500.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_12  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 12 : Entropy reduced to 500.0 \n",
      "Model: \"model2_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_13  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 13 : Entropy reduced to 500.0 \n",
      "patate\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-56.0, -56.0, -67.0, -22.0, -70.0, -56.0, -22.0, -57.0, -30.0, -34.0] \n",
      " [-93.0, -81.0, -30.0, -5.0, -78.0, -81.0, -61.0, -63.0, -19.0, -21.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.33, 0.35, 0.32], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.33, 0.33], [0.33, 0.33, 0.34], [0.34, 0.34, 0.32], [0.33, 0.33, 0.33], [0.33, 0.35, 0.32], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 1 : Predicted Values and True Return : \n",
      " [-89.0, -134.0, -17.0, -151.0, -32.0, -25.0, -27.0, -79.0, -55.0, -17.0] \n",
      " [-52.0, -95.0, -7.0, -68.0, -100.0, -71.0, -87.0, -71.0, -99.0, -9.0]\n",
      "Agent 1 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.34], [0.34, 0.33, 0.33], [0.33, 0.33, 0.33], [0.34, 0.33, 0.34], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33]]\n",
      "Agent 1 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 2 : Predicted Values and True Return : \n",
      " [-151.0, -189.0, -186.0, -115.0, -189.0, -159.0, -179.0, -104.0, -262.0, -61.0] \n",
      " [-155.0, -149.0, -106.0, -91.0, -104.0, -97.0, -120.0, -96.0, -160.0, -175.0]\n",
      "Agent 2 : Proba distribution on those states : \n",
      " [[0.26, 0.25, 0.25, 0.25], [0.25, 0.26, 0.25, 0.24], [0.25, 0.25, 0.26, 0.24], [0.25, 0.25, 0.25, 0.25], [0.24, 0.25, 0.26, 0.25], [0.26, 0.25, 0.25, 0.24], [0.25, 0.26, 0.25, 0.24], [0.25, 0.24, 0.26, 0.25], [0.24, 0.27, 0.26, 0.23], [0.23, 0.26, 0.25, 0.26]]\n",
      "Agent 2 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Agent 3 : Predicted Values and True Return : \n",
      " [-23.0, -22.0, -29.0, -22.0, -22.0, -22.0, -23.0, -49.0, -29.0, -33.0] \n",
      " [-24.0, -32.0, -14.0, -38.0, -38.0, -18.0, -32.0, -11.0, -28.0, -13.0]\n",
      "Agent 3 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33]]\n",
      "Agent 3 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 4 : Predicted Values and True Return : \n",
      " [-100.0, -12.0, -62.0, -18.0, -43.0, -42.0, -18.0, -22.0, -100.0, -71.0] \n",
      " [-11.0, -31.0, -13.0, -19.0, -36.0, -16.0, -19.0, -30.0, -11.0, -34.0]\n",
      "Agent 4 : Proba distribution on those states : \n",
      " [[0.34, 0.35, 0.31], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.34, 0.35, 0.31], [0.34, 0.35, 0.32]]\n",
      "Agent 4 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 5 : Predicted Values and True Return : \n",
      " [-6.0, -9.0, -17.0, -6.0, -6.0, -6.0, -6.0, -7.0, -7.0, -6.0] \n",
      " [-1.0, -2.0, -2.0, -3.0, -2.0, -3.0, -3.0, -2.0, -2.0, -1.0]\n",
      "Agent 5 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.51, 0.49], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 5 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 6 : Predicted Values and True Return : \n",
      " [-16.0, -19.0, -14.0, -24.0, -14.0, -20.0, -40.0, -26.0, -14.0, -14.0] \n",
      " [-18.0, -33.0, -14.0, -9.0, -9.0, -18.0, -26.0, -36.0, -5.0, -6.0]\n",
      "Agent 6 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.49, 0.51], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 6 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 7 : Predicted Values and True Return : \n",
      " [-55.0, -27.0, -21.0, -16.0, -20.0, -16.0, -27.0, -16.0, -16.0, -49.0] \n",
      " [-37.0, -78.0, -33.0, -27.0, -65.0, -42.0, -20.0, -18.0, -50.0, -21.0]\n",
      "Agent 7 : Proba distribution on those states : \n",
      " [[0.34, 0.34, 0.32], [0.33, 0.34, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.34, 0.33, 0.33]]\n",
      "Agent 7 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 8 : Predicted Values and True Return : \n",
      " [-4.0, -4.0, -4.0, -4.0, -5.0, -4.0, -4.0, -4.0, -5.0, -7.0] \n",
      " [-9.0, -7.0, -6.0, -3.0, -3.0, -8.0, -3.0, -3.0, -9.0, -4.0]\n",
      "Agent 8 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49]]\n",
      "Agent 8 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 9 : Predicted Values and True Return : \n",
      " [-7.0, -4.0, -5.0, -4.0, -4.0, -4.0, -4.0, -4.0, -4.0, -4.0] \n",
      " [-8.0, -11.0, -6.0, -11.0, -3.0, -7.0, -5.0, -4.0, -4.0, -5.0]\n",
      "Agent 9 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 9 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 10 : Predicted Values and True Return : \n",
      " [-7.0, -7.0, -7.0, -8.0, -6.0, -9.0, -10.0, -7.0, -8.0, -7.0] \n",
      " [-8.0, -4.0, -1.0, -4.0, -8.0, -6.0, -2.0, -5.0, -10.0, -7.0]\n",
      "Agent 10 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 10 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 11 : Predicted Values and True Return : \n",
      " [-5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0] \n",
      " [-6.0, -6.0, -10.0, -8.0, -5.0, -10.0, -6.0, -3.0, -2.0, -3.0]\n",
      "Agent 11 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 11 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 12 : Predicted Values and True Return : \n",
      " [-47.0, -25.0, -61.0, -16.0, -37.0, -16.0, -54.0, -49.0, -31.0, -16.0] \n",
      " [-44.0, -19.0, -18.0, -8.0, -39.0, -7.0, -16.0, -19.0, -12.0, -6.0]\n",
      "Agent 12 : Proba distribution on those states : \n",
      " [[0.26, 0.24, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.26, 0.25, 0.24, 0.25], [0.26, 0.25, 0.24, 0.25], [0.26, 0.25, 0.25, 0.25], [0.26, 0.25, 0.24, 0.25], [0.25, 0.24, 0.26, 0.24], [0.26, 0.25, 0.24, 0.25], [0.24, 0.26, 0.25, 0.24], [0.26, 0.25, 0.24, 0.25]]\n",
      "Agent 12 : Proba distribution on the 0 state : \n",
      " [0.26 0.25 0.24 0.25]\n",
      "Agent 13 : Predicted Values and True Return : \n",
      " [-13.0, -86.0, -43.0, -83.0, -26.0, -105.0, -63.0, -13.0, -98.0, -31.0] \n",
      " [-9.0, -105.0, -83.0, -103.0, -66.0, -98.0, -99.0, -54.0, -103.0, -49.0]\n",
      "Agent 13 : Proba distribution on those states : \n",
      " [[0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.35, 0.33, 0.33], [0.35, 0.33, 0.33], [0.34, 0.33, 0.33], [0.35, 0.32, 0.34], [0.35, 0.31, 0.34], [0.33, 0.34, 0.33], [0.35, 0.32, 0.33], [0.34, 0.33, 0.33]]\n",
      "Agent 13 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.33]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ef3781782ae8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_step_size\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Actor_Critic_Class.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[0macts_and_advs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                 \u001b[1;31m# performs a full training step on the collected batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m                 \u001b[1;31m# note: no need to mess around with gradients, Keras API handles it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0macts_and_advs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3510\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3512\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 572\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 445\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    446\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        \n",
    "        \n",
    "        # Only for AC\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "agent_type = \"DuelingDDQN\"\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 48)           432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48)           2352        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            49          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            147         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_5[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,684\n",
      "Trainable params: 7,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 0\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 48)           432         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 48)           2352        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 48)           2352        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 48)           2352        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            49          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 3)            147         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_17[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,684\n",
      "Trainable params: 7,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 1\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 14)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 48)           720         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 48)           2352        dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 48)           2352        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 48)           2352        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            49          dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 4)            196         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 4)            0           dense_29[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,021\n",
      "Trainable params: 8,021\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 2\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 48)           432         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 48)           2352        dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 48)           2352        dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 48)           2352        dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 1)            49          dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 3)            147         dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_41[0][0]                   \n",
      "                                                                 dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,684\n",
      "Trainable params: 7,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 48)           336         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 48)           2352        dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 48)           2352        dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 48)           2352        dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 1)            49          dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 3)            147         dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_53[0][0]                   \n",
      "                                                                 dense_51[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,588\n",
      "Trainable params: 7,588\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 4\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 48)           192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 48)           2352        dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 48)           2352        dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 48)           2352        dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 1)            49          dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 2)            98          dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_65[0][0]                   \n",
      "                                                                 dense_63[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,395\n",
      "Trainable params: 7,395\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 5\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 48)           336         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_73 (Dense)                (None, 48)           2352        dense_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 48)           2352        dense_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_74 (Dense)                (None, 48)           2352        dense_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 1)            49          dense_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 2)            98          dense_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_77[0][0]                   \n",
      "                                                                 dense_75[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,539\n",
      "Trainable params: 7,539\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 6\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 48)           384         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 48)           2352        dense_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 48)           2352        dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 48)           2352        dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 1)            49          dense_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 3)            147         dense_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_89[0][0]                   \n",
      "                                                                 dense_87[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,636\n",
      "Trainable params: 7,636\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (None, 48)           192         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 48)           2352        dense_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 48)           2352        dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 48)           2352        dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 1)            49          dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 2)            98          dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_101[0][0]                  \n",
      "                                                                 dense_99[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,395\n",
      "Trainable params: 7,395\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 8\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 48)           240         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (None, 48)           2352        dense_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_112 (Dense)               (None, 48)           2352        dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_110 (Dense)               (None, 48)           2352        dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_113 (Dense)               (None, 1)            49          dense_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_111 (Dense)               (None, 2)            98          dense_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_113[0][0]                  \n",
      "                                                                 dense_111[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,443\n",
      "Trainable params: 7,443\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 9\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_120 (Dense)               (None, 48)           240         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_121 (Dense)               (None, 48)           2352        dense_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_124 (Dense)               (None, 48)           2352        dense_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_122 (Dense)               (None, 48)           2352        dense_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_125 (Dense)               (None, 1)            49          dense_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_123 (Dense)               (None, 2)            98          dense_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_125[0][0]                  \n",
      "                                                                 dense_123[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,443\n",
      "Trainable params: 7,443\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 10\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_132 (Dense)               (None, 48)           240         input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_133 (Dense)               (None, 48)           2352        dense_132[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, 48)           2352        dense_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_134 (Dense)               (None, 48)           2352        dense_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 1)            49          dense_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_135 (Dense)               (None, 2)            98          dense_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_137[0][0]                  \n",
      "                                                                 dense_135[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,443\n",
      "Trainable params: 7,443\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_144 (Dense)               (None, 48)           384         input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_145 (Dense)               (None, 48)           2352        dense_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_148 (Dense)               (None, 48)           2352        dense_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_146 (Dense)               (None, 48)           2352        dense_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_149 (Dense)               (None, 1)            49          dense_148[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_147 (Dense)               (None, 4)            196         dense_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 4)            0           dense_149[0][0]                  \n",
      "                                                                 dense_147[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,685\n",
      "Trainable params: 7,685\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 12\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_156 (Dense)               (None, 48)           336         input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_157 (Dense)               (None, 48)           2352        dense_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_160 (Dense)               (None, 48)           2352        dense_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_158 (Dense)               (None, 48)           2352        dense_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_161 (Dense)               (None, 1)            49          dense_160[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_159 (Dense)               (None, 3)            147         dense_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_161[0][0]                  \n",
      "                                                                 dense_159[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,588\n",
      "Trainable params: 7,588\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 13\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3600 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 1.11 seconds.\n",
      "\n",
      "After 0 actions taken by the Agents,  Agent 0 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 1 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 2 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 3 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 4 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 5 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 6 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 7 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 8 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 9 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 10 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 11 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 12 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 13 memory is 0.0 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 0 memory is 7.2 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 1 memory is 6.9 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 2 memory is 6.8 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 3 memory is 6.8 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 4 memory is 6.8 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 5 memory is 7.2 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 6 memory is 7.5 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 7 memory is 7.0 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 8 memory is 7.4 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 9 memory is 7.4 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 10 memory is 7.5 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 11 memory is 7.5 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 12 memory is 6.8 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 13 memory is 7.2 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 0 memory is 14.1 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 1 memory is 13.9 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 2 memory is 13.6 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 3 memory is 13.9 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 4 memory is 13.8 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 5 memory is 14.5 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 6 memory is 15.1 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 7 memory is 14.0 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 8 memory is 14.8 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 9 memory is 14.7 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 10 memory is 14.8 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 11 memory is 15.1 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 12 memory is 13.6 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 13 memory is 14.1 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 0 memory is 21.3 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 1 memory is 20.8 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 2 memory is 20.4 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 3 memory is 20.9 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 4 memory is 20.8 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 5 memory is 21.6 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 6 memory is 22.5 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 7 memory is 20.9 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 8 memory is 22.1 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 9 memory is 22.3 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 10 memory is 22.2 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 11 memory is 22.5 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 12 memory is 20.5 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 13 memory is 21.2 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 0 memory is 28.3 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 1 memory is 27.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 2 memory is 27.0 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 3 memory is 27.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 4 memory is 27.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 5 memory is 28.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 6 memory is 30.2 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 7 memory is 28.0 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 8 memory is 29.6 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 9 memory is 29.6 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 10 memory is 29.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 11 memory is 29.9 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 12 memory is 27.2 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 13 memory is 28.2 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 0 memory is 35.2 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 1 memory is 34.8 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 2 memory is 33.9 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 3 memory is 34.8 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 4 memory is 34.9 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 5 memory is 36.2 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 6 memory is 37.9 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 7 memory is 34.7 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 8 memory is 37.1 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 9 memory is 36.9 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 10 memory is 37.0 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 11 memory is 37.3 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 12 memory is 34.1 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 13 memory is 35.2 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 0 memory is 42.2 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 1 memory is 41.8 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 2 memory is 40.7 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 3 memory is 41.9 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 4 memory is 41.8 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 5 memory is 43.7 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 6 memory is 45.5 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 7 memory is 41.3 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 8 memory is 44.5 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 9 memory is 44.3 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 10 memory is 44.2 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 11 memory is 44.8 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 12 memory is 41.0 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 13 memory is 42.3 percent full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 8000 actions taken by the Agents,  Agent 0 memory is 56.2 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 1 memory is 55.8 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 2 memory is 54.4 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 3 memory is 55.9 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 4 memory is 55.4 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 5 memory is 58.6 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 6 memory is 60.7 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 7 memory is 55.3 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 8 memory is 59.1 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 9 memory is 59.3 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 10 memory is 59.1 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 11 memory is 59.6 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 12 memory is 54.6 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 13 memory is 56.0 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 0 memory is 63.2 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 1 memory is 62.7 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 2 memory is 61.3 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 3 memory is 63.0 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 4 memory is 62.6 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 5 memory is 65.9 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 6 memory is 68.1 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 7 memory is 61.9 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 8 memory is 66.6 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 9 memory is 66.7 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 10 memory is 66.6 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 11 memory is 67.1 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 12 memory is 61.3 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 13 memory is 63.0 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 0 memory is 70.1 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 1 memory is 69.7 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 2 memory is 67.9 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 3 memory is 69.9 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 4 memory is 69.7 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 5 memory is 73.4 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 6 memory is 75.6 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 7 memory is 68.9 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 8 memory is 74.0 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 9 memory is 74.1 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 10 memory is 74.1 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 11 memory is 74.6 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 12 memory is 68.1 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 13 memory is 69.9 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 0 memory is 76.9 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 1 memory is 76.7 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 2 memory is 74.8 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 3 memory is 76.7 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 4 memory is 76.6 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 5 memory is 80.7 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 6 memory is 83.2 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 7 memory is 75.9 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 8 memory is 81.8 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 9 memory is 81.4 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 10 memory is 81.4 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 11 memory is 82.0 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 12 memory is 75.1 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 13 memory is 76.8 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 0 memory is 84.0 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 1 memory is 83.5 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 2 memory is 81.6 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 3 memory is 83.6 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 4 memory is 83.6 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 5 memory is 88.1 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 6 memory is 90.7 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 7 memory is 82.8 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 8 memory is 89.2 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 9 memory is 89.1 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 10 memory is 88.8 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 11 memory is 89.1 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 12 memory is 81.9 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 13 memory is 84.0 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 0 memory is 97.9 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 1 memory is 97.7 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 2 memory is 95.3 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 3 memory is 97.7 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 4 memory is 97.8 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 5 memory is 102.8 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 6 memory is 105.4 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 7 memory is 96.9 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 8 memory is 103.9 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 9 memory is 103.5 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 10 memory is 103.4 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 11 memory is 104.2 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 12 memory is 95.5 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 13 memory is 98.0 percent full\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent0_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent1_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent2_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent3_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent4_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent5_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent6_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent7_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent8_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent9_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent10_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent11_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent12_PERPre_1000.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent13_PERPre_1000.p\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 1800 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 1.05 seconds.\n",
      "\n",
      "Episode 1 is finished\n",
      "Average Reward for Agent 0 this episode : -9.14\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.3804\n",
      "Average Reward for Agent 1 this episode : -8.09\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2765\n",
      "Average Reward for Agent 2 this episode : -22.76\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 283.3306\n",
      "Average Reward for Agent 3 this episode : -4.65\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3318\n",
      "Average Reward for Agent 4 this episode : -4.03\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.7298\n",
      "Average Reward for Agent 5 this episode : -0.61\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3412\n",
      "Average Reward for Agent 6 this episode : -2.96\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.6568\n",
      "Average Reward for Agent 7 this episode : -4.0\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4407\n",
      "Average Reward for Agent 8 this episode : -1.0\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8942\n",
      "Average Reward for Agent 9 this episode : -0.91\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4833\n",
      "Average Reward for Agent 10 this episode : -0.83\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3785\n",
      "Average Reward for Agent 11 this episode : -0.64\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3695\n",
      "Average Reward for Agent 12 this episode : -8.11\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.6909\n",
      "Average Reward for Agent 13 this episode : -9.47\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 539.1490\n",
      "Reducing exploration for all agents to 0.9828\n",
      "Episode 2 is finished\n",
      "Average Reward for Agent 0 this episode : -7.94\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.7246\n",
      "Average Reward for Agent 1 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.9159\n",
      "Average Reward for Agent 2 this episode : -21.65\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.8548\n",
      "Average Reward for Agent 3 this episode : -4.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7310\n",
      "Average Reward for Agent 4 this episode : -4.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0268\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9154\n",
      "Average Reward for Agent 6 this episode : -4.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.0959\n",
      "Average Reward for Agent 7 this episode : -4.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.7624\n",
      "Average Reward for Agent 8 this episode : -0.5\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3741\n",
      "Average Reward for Agent 9 this episode : -0.51\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3777\n",
      "Average Reward for Agent 10 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4060\n",
      "Average Reward for Agent 11 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8977\n",
      "Average Reward for Agent 12 this episode : -10.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8198\n",
      "Average Reward for Agent 13 this episode : -15.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 266.3109\n",
      "Reducing exploration for all agents to 0.966\n",
      "Episode 3 is finished\n",
      "Average Reward for Agent 0 this episode : -7.87\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.3141\n",
      "Average Reward for Agent 1 this episode : -9.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.7916\n",
      "Average Reward for Agent 2 this episode : -23.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 143.2689\n",
      "Average Reward for Agent 3 this episode : -3.05\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2439\n",
      "Average Reward for Agent 4 this episode : -4.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2109\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5564\n",
      "Average Reward for Agent 6 this episode : -1.6\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.9247\n",
      "Average Reward for Agent 7 this episode : -3.49\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3447\n",
      "Average Reward for Agent 8 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6219\n",
      "Average Reward for Agent 9 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7330\n",
      "Average Reward for Agent 10 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1871\n",
      "Average Reward for Agent 11 this episode : -0.58\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6408\n",
      "Average Reward for Agent 12 this episode : -5.2\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7444\n",
      "Average Reward for Agent 13 this episode : -10.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.6539\n",
      "Reducing exploration for all agents to 0.9494\n",
      "Episode 4 is finished\n",
      "Average Reward for Agent 0 this episode : -6.55\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.8731\n",
      "Average Reward for Agent 1 this episode : -10.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9808\n",
      "Average Reward for Agent 2 this episode : -23.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8398\n",
      "Average Reward for Agent 3 this episode : -3.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8426\n",
      "Average Reward for Agent 4 this episode : -3.21\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2364\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5233\n",
      "Average Reward for Agent 6 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9227\n",
      "Average Reward for Agent 7 this episode : -3.25\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0759\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6392\n",
      "Average Reward for Agent 9 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6643\n",
      "Average Reward for Agent 10 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8338\n",
      "Average Reward for Agent 11 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7722\n",
      "Average Reward for Agent 12 this episode : -4.08\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6141\n",
      "Average Reward for Agent 13 this episode : -14.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0274\n",
      "Reducing exploration for all agents to 0.9331\n",
      "Episode 5 is finished\n",
      "Average Reward for Agent 0 this episode : -7.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.4908\n",
      "Average Reward for Agent 1 this episode : -8.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1416\n",
      "Average Reward for Agent 2 this episode : -21.6\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7274\n",
      "Average Reward for Agent 3 this episode : -4.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4158\n",
      "Average Reward for Agent 4 this episode : -2.82\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4203\n",
      "Average Reward for Agent 6 this episode : -2.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8617\n",
      "Average Reward for Agent 7 this episode : -5.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7148\n",
      "Average Reward for Agent 8 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8013\n",
      "Average Reward for Agent 9 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6257\n",
      "Average Reward for Agent 10 this episode : -0.53\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4954\n",
      "Average Reward for Agent 11 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8114\n",
      "Average Reward for Agent 12 this episode : -4.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5877\n",
      "Average Reward for Agent 13 this episode : -5.28\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.5047\n",
      "Reducing exploration for all agents to 0.9171\n",
      "Episode 6 is finished\n",
      "Average Reward for Agent 0 this episode : -6.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3908\n",
      "Average Reward for Agent 1 this episode : -6.83\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3953\n",
      "Average Reward for Agent 2 this episode : -19.68\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.2103\n",
      "Average Reward for Agent 3 this episode : -3.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9408\n",
      "Average Reward for Agent 4 this episode : -3.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7336\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5326\n",
      "Average Reward for Agent 6 this episode : -2.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3889\n",
      "Average Reward for Agent 7 this episode : -3.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6101\n",
      "Average Reward for Agent 8 this episode : -0.35\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0716\n",
      "Average Reward for Agent 9 this episode : -0.46\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3643\n",
      "Average Reward for Agent 10 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9440\n",
      "Average Reward for Agent 11 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0864\n",
      "Average Reward for Agent 12 this episode : -3.6\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5372\n",
      "Average Reward for Agent 13 this episode : -5.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.9860\n",
      "Reducing exploration for all agents to 0.9013\n",
      "Episode 7 is finished\n",
      "Average Reward for Agent 0 this episode : -8.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3728\n",
      "Average Reward for Agent 1 this episode : -8.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7771\n",
      "Average Reward for Agent 2 this episode : -21.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.6391\n",
      "Average Reward for Agent 3 this episode : -3.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2971\n",
      "Average Reward for Agent 4 this episode : -3.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0203\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6555\n",
      "Average Reward for Agent 6 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3738\n",
      "Average Reward for Agent 7 this episode : -3.1\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7322\n",
      "Average Reward for Agent 8 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6496\n",
      "Average Reward for Agent 9 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6364\n",
      "Average Reward for Agent 10 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5316\n",
      "Average Reward for Agent 11 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9854\n",
      "Average Reward for Agent 12 this episode : -4.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9965\n",
      "Average Reward for Agent 13 this episode : -7.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0670\n",
      "Reducing exploration for all agents to 0.8859\n",
      "Episode 8 is finished\n",
      "Average Reward for Agent 0 this episode : -5.7\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7536\n",
      "Average Reward for Agent 1 this episode : -8.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0148\n",
      "Average Reward for Agent 2 this episode : -15.57\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1962\n",
      "Average Reward for Agent 3 this episode : -4.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2435\n",
      "Average Reward for Agent 4 this episode : -2.78\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3268\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5013\n",
      "Average Reward for Agent 6 this episode : -2.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9313\n",
      "Average Reward for Agent 7 this episode : -3.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2173\n",
      "Average Reward for Agent 8 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8844\n",
      "Average Reward for Agent 9 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5761\n",
      "Average Reward for Agent 10 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0010\n",
      "Average Reward for Agent 11 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7597\n",
      "Average Reward for Agent 12 this episode : -3.44\n",
      "Saving architecture, weights, optimizer state for best agent-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9708\n",
      "Average Reward for Agent 13 this episode : -14.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7990\n",
      "Reducing exploration for all agents to 0.8707\n",
      "Episode 9 is finished\n",
      "Average Reward for Agent 0 this episode : -4.79\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6407\n",
      "Average Reward for Agent 1 this episode : -7.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0892\n",
      "Average Reward for Agent 2 this episode : -17.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8566\n",
      "Average Reward for Agent 3 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4184\n",
      "Average Reward for Agent 4 this episode : -4.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9068\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4772\n",
      "Average Reward for Agent 6 this episode : -2.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9600\n",
      "Average Reward for Agent 7 this episode : -3.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9764\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6040\n",
      "Average Reward for Agent 9 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4217\n",
      "Average Reward for Agent 10 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8022\n",
      "Average Reward for Agent 11 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6485\n",
      "Average Reward for Agent 12 this episode : -5.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6304\n",
      "Average Reward for Agent 13 this episode : -5.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7976\n",
      "Reducing exploration for all agents to 0.8557\n",
      "Episode 10 is finished\n",
      "Average Reward for Agent 0 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3396\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -6.75\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.0779\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -21.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8456\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.43\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6868\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -2.69\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7365\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3016\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2974\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -2.86\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7857\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7469\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.38\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5698\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -1.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1898\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5066\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -4.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1315\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -7.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7770\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.841\n",
      "Episode 11 is finished\n",
      "Average Reward for Agent 0 this episode : -7.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6552\n",
      "Average Reward for Agent 1 this episode : -7.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8250\n",
      "Average Reward for Agent 2 this episode : -19.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.8954\n",
      "Average Reward for Agent 3 this episode : -2.3\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2309\n",
      "Average Reward for Agent 4 this episode : -2.31\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0035\n",
      "Average Reward for Agent 5 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7205\n",
      "Average Reward for Agent 6 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4604\n",
      "Average Reward for Agent 7 this episode : -2.65\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6959\n",
      "Average Reward for Agent 8 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7498\n",
      "Average Reward for Agent 9 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7644\n",
      "Average Reward for Agent 10 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 11 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7866\n",
      "Average Reward for Agent 12 this episode : -3.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0895\n",
      "Average Reward for Agent 13 this episode : -11.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.0518\n",
      "Reducing exploration for all agents to 0.8266\n",
      "Episode 12 is finished\n",
      "Average Reward for Agent 0 this episode : -5.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2512\n",
      "Average Reward for Agent 1 this episode : -8.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.3611\n",
      "Average Reward for Agent 2 this episode : -18.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.4201\n",
      "Average Reward for Agent 3 this episode : -3.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3003\n",
      "Average Reward for Agent 4 this episode : -3.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7058\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5809\n",
      "Average Reward for Agent 6 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3478\n",
      "Average Reward for Agent 7 this episode : -3.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.2255\n",
      "Average Reward for Agent 8 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9016\n",
      "Average Reward for Agent 9 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7350\n",
      "Average Reward for Agent 10 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6570\n",
      "Average Reward for Agent 11 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0553\n",
      "Average Reward for Agent 12 this episode : -3.11\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6261\n",
      "Average Reward for Agent 13 this episode : -4.52\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7353\n",
      "Reducing exploration for all agents to 0.8124\n",
      "Episode 13 is finished\n",
      "Average Reward for Agent 0 this episode : -5.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6281\n",
      "Average Reward for Agent 1 this episode : -8.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4011\n",
      "Average Reward for Agent 2 this episode : -23.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1234\n",
      "Average Reward for Agent 3 this episode : -3.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9800\n",
      "Average Reward for Agent 4 this episode : -3.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6972\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5910\n",
      "Average Reward for Agent 6 this episode : -3.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.8368\n",
      "Average Reward for Agent 7 this episode : -3.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7051\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9657\n",
      "Average Reward for Agent 9 this episode : -0.36\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6837\n",
      "Average Reward for Agent 10 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0510\n",
      "Average Reward for Agent 11 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1597\n",
      "Average Reward for Agent 12 this episode : -4.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0018\n",
      "Average Reward for Agent 13 this episode : -6.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0034\n",
      "Reducing exploration for all agents to 0.7985\n",
      "Episode 14 is finished\n",
      "Average Reward for Agent 0 this episode : -7.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2812\n",
      "Average Reward for Agent 1 this episode : -8.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5375\n",
      "Average Reward for Agent 2 this episode : -15.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0130\n",
      "Average Reward for Agent 3 this episode : -2.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0031\n",
      "Average Reward for Agent 4 this episode : -4.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8746\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7721\n",
      "Average Reward for Agent 6 this episode : -2.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.3094\n",
      "Average Reward for Agent 7 this episode : -2.44\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2483\n",
      "Average Reward for Agent 8 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8653\n",
      "Average Reward for Agent 9 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6918\n",
      "Average Reward for Agent 10 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1700\n",
      "Average Reward for Agent 11 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2560\n",
      "Average Reward for Agent 12 this episode : -3.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3593\n",
      "Average Reward for Agent 13 this episode : -4.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.3877\n",
      "Reducing exploration for all agents to 0.7848\n",
      "Episode 15 is finished\n",
      "Average Reward for Agent 0 this episode : -7.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0974\n",
      "Average Reward for Agent 1 this episode : -7.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.8728\n",
      "Average Reward for Agent 2 this episode : -18.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.2482\n",
      "Average Reward for Agent 3 this episode : -3.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9252\n",
      "Average Reward for Agent 4 this episode : -4.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0581\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6308\n",
      "Average Reward for Agent 6 this episode : -2.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2170\n",
      "Average Reward for Agent 7 this episode : -2.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0746\n",
      "Average Reward for Agent 8 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0691\n",
      "Average Reward for Agent 9 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2154\n",
      "Average Reward for Agent 10 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8831\n",
      "Average Reward for Agent 11 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4695\n",
      "Average Reward for Agent 12 this episode : -4.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0623\n",
      "Average Reward for Agent 13 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8584\n",
      "Reducing exploration for all agents to 0.7713\n",
      "Episode 16 is finished\n",
      "Average Reward for Agent 0 this episode : -5.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.7233\n",
      "Average Reward for Agent 1 this episode : -6.72\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3880\n",
      "Average Reward for Agent 2 this episode : -20.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3533\n",
      "Average Reward for Agent 3 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 4 this episode : -4.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4681\n",
      "Average Reward for Agent 5 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8428\n",
      "Average Reward for Agent 6 this episode : -3.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9218\n",
      "Average Reward for Agent 7 this episode : -3.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1881\n",
      "Average Reward for Agent 8 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3325\n",
      "Average Reward for Agent 9 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7765\n",
      "Average Reward for Agent 10 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5976\n",
      "Average Reward for Agent 11 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4145\n",
      "Average Reward for Agent 12 this episode : -3.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.1477\n",
      "Average Reward for Agent 13 this episode : -5.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8134\n",
      "Reducing exploration for all agents to 0.7581\n",
      "Episode 17 is finished\n",
      "Average Reward for Agent 0 this episode : -7.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6016\n",
      "Average Reward for Agent 1 this episode : -7.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.6510\n",
      "Average Reward for Agent 2 this episode : -17.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.6198\n",
      "Average Reward for Agent 3 this episode : -3.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0590\n",
      "Average Reward for Agent 4 this episode : -3.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8954\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6985\n",
      "Average Reward for Agent 6 this episode : -4.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.3096\n",
      "Average Reward for Agent 7 this episode : -2.16\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5888\n",
      "Average Reward for Agent 8 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6930\n",
      "Average Reward for Agent 9 this episode : -0.34\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8289\n",
      "Average Reward for Agent 10 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1076\n",
      "Average Reward for Agent 11 this episode : -0.54\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1634\n",
      "Average Reward for Agent 12 this episode : -4.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9608\n",
      "Average Reward for Agent 13 this episode : -5.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3906\n",
      "Reducing exploration for all agents to 0.745\n",
      "Episode 18 is finished\n",
      "Average Reward for Agent 0 this episode : -7.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0170\n",
      "Average Reward for Agent 1 this episode : -8.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3071\n",
      "Average Reward for Agent 2 this episode : -18.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.7434\n",
      "Average Reward for Agent 3 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4113\n",
      "Average Reward for Agent 4 this episode : -2.28\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7519\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4865\n",
      "Average Reward for Agent 6 this episode : -3.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.1401\n",
      "Average Reward for Agent 7 this episode : -3.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7052\n",
      "Average Reward for Agent 8 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3680\n",
      "Average Reward for Agent 9 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6964\n",
      "Average Reward for Agent 10 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5797\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9100\n",
      "Average Reward for Agent 12 this episode : -4.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8242\n",
      "Average Reward for Agent 13 this episode : -5.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.6782\n",
      "Reducing exploration for all agents to 0.7323\n",
      "Episode 19 is finished\n",
      "Average Reward for Agent 0 this episode : -5.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3277\n",
      "Average Reward for Agent 1 this episode : -11.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.6795\n",
      "Average Reward for Agent 2 this episode : -16.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.0112\n",
      "Average Reward for Agent 3 this episode : -2.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2811\n",
      "Average Reward for Agent 4 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.1842\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5985\n",
      "Average Reward for Agent 6 this episode : -3.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.2858\n",
      "Average Reward for Agent 7 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2930\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2313\n",
      "Average Reward for Agent 9 this episode : -0.33\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7480\n",
      "Average Reward for Agent 10 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9731\n",
      "Average Reward for Agent 11 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7025\n",
      "Average Reward for Agent 12 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9493\n",
      "Average Reward for Agent 13 this episode : -9.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5125\n",
      "Reducing exploration for all agents to 0.7197\n",
      "Episode 20 is finished\n",
      "Average Reward for Agent 0 this episode : -7.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6412\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -10.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.6091\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -17.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5545\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3822\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -2.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8374\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.3\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4729\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -3.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.0463\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -2.14\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9924\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0436\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7507\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4121\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6791\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -4.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7508\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -6.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2099\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.7073\n",
      "Episode 21 is finished\n",
      "Average Reward for Agent 0 this episode : -7.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9397\n",
      "Average Reward for Agent 1 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0225\n",
      "Average Reward for Agent 2 this episode : -19.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.8253\n",
      "Average Reward for Agent 3 this episode : -2.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5244\n",
      "Average Reward for Agent 4 this episode : -2.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3973\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4445\n",
      "Average Reward for Agent 6 this episode : -2.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.0217\n",
      "Average Reward for Agent 7 this episode : -1.8\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6379\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2107\n",
      "Average Reward for Agent 9 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4391\n",
      "Average Reward for Agent 10 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6209\n",
      "Average Reward for Agent 11 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4131\n",
      "Average Reward for Agent 12 this episode : -4.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9530\n",
      "Average Reward for Agent 13 this episode : -6.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7550\n",
      "Reducing exploration for all agents to 0.6952\n",
      "Episode 22 is finished\n",
      "Average Reward for Agent 0 this episode : -7.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.6989\n",
      "Average Reward for Agent 1 this episode : -10.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.2131\n",
      "Average Reward for Agent 2 this episode : -20.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.7664\n",
      "Average Reward for Agent 3 this episode : -2.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3829\n",
      "Average Reward for Agent 4 this episode : -2.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6168\n",
      "Average Reward for Agent 5 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5455\n",
      "Average Reward for Agent 6 this episode : -3.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.6375\n",
      "Average Reward for Agent 7 this episode : -2.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3847\n",
      "Average Reward for Agent 8 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1919\n",
      "Average Reward for Agent 9 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4515\n",
      "Average Reward for Agent 10 this episode : -1.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5717\n",
      "Average Reward for Agent 11 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2098\n",
      "Average Reward for Agent 12 this episode : -4.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0554\n",
      "Average Reward for Agent 13 this episode : -9.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8011\n",
      "Reducing exploration for all agents to 0.6833\n",
      "Episode 23 is finished\n",
      "Average Reward for Agent 0 this episode : -9.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8240\n",
      "Average Reward for Agent 1 this episode : -8.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0282\n",
      "Average Reward for Agent 2 this episode : -22.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.7644\n",
      "Average Reward for Agent 3 this episode : -2.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0919\n",
      "Average Reward for Agent 4 this episode : -2.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9734\n",
      "Average Reward for Agent 5 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5274\n",
      "Average Reward for Agent 6 this episode : -2.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0797\n",
      "Average Reward for Agent 7 this episode : -3.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0839\n",
      "Average Reward for Agent 8 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9686\n",
      "Average Reward for Agent 9 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6701\n",
      "Average Reward for Agent 10 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5664\n",
      "Average Reward for Agent 11 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9913\n",
      "Average Reward for Agent 12 this episode : -4.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5137\n",
      "Average Reward for Agent 13 this episode : -6.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.3607\n",
      "Reducing exploration for all agents to 0.6715\n",
      "Episode 24 is finished\n",
      "Average Reward for Agent 0 this episode : -8.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.0954\n",
      "Average Reward for Agent 1 this episode : -11.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.9897\n",
      "Average Reward for Agent 2 this episode : -20.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.4642\n",
      "Average Reward for Agent 3 this episode : -2.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6027\n",
      "Average Reward for Agent 4 this episode : -1.94\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0057\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5715\n",
      "Average Reward for Agent 6 this episode : -2.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.4818\n",
      "Average Reward for Agent 7 this episode : -3.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1027\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9997\n",
      "Average Reward for Agent 9 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5199\n",
      "Average Reward for Agent 10 this episode : -1.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1608\n",
      "Average Reward for Agent 11 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9349\n",
      "Average Reward for Agent 12 this episode : -3.06\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9426\n",
      "Average Reward for Agent 13 this episode : -8.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.7212\n",
      "Reducing exploration for all agents to 0.66\n",
      "Episode 25 is finished\n",
      "Average Reward for Agent 0 this episode : -11.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7270\n",
      "Average Reward for Agent 1 this episode : -8.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.0765\n",
      "Average Reward for Agent 2 this episode : -19.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.1987\n",
      "Average Reward for Agent 3 this episode : -2.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4057\n",
      "Average Reward for Agent 4 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3803\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.2347\n",
      "Average Reward for Agent 7 this episode : -3.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9547\n",
      "Average Reward for Agent 8 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7967\n",
      "Average Reward for Agent 9 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7947\n",
      "Average Reward for Agent 10 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8897\n",
      "Average Reward for Agent 11 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9934\n",
      "Average Reward for Agent 12 this episode : -2.95\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6725\n",
      "Average Reward for Agent 13 this episode : -4.48\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9128\n",
      "Reducing exploration for all agents to 0.6487\n",
      "Episode 26 is finished\n",
      "Average Reward for Agent 0 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5855\n",
      "Average Reward for Agent 1 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2192\n",
      "Average Reward for Agent 2 this episode : -21.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.8108\n",
      "Average Reward for Agent 3 this episode : -3.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0948\n",
      "Average Reward for Agent 4 this episode : -2.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8743\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4815\n",
      "Average Reward for Agent 6 this episode : -1.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7033\n",
      "Average Reward for Agent 7 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3617\n",
      "Average Reward for Agent 8 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1712\n",
      "Average Reward for Agent 9 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8028\n",
      "Average Reward for Agent 10 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8707\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0632\n",
      "Average Reward for Agent 12 this episode : -3.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2790\n",
      "Average Reward for Agent 13 this episode : -14.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.1295\n",
      "Reducing exploration for all agents to 0.6375\n",
      "Episode 27 is finished\n",
      "Average Reward for Agent 0 this episode : -8.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3125\n",
      "Average Reward for Agent 1 this episode : -9.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1460\n",
      "Average Reward for Agent 2 this episode : -21.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.8520\n",
      "Average Reward for Agent 3 this episode : -3.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6467\n",
      "Average Reward for Agent 4 this episode : -2.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4452\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4844\n",
      "Average Reward for Agent 6 this episode : -2.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2849\n",
      "Average Reward for Agent 7 this episode : -2.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9879\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9570\n",
      "Average Reward for Agent 9 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6786\n",
      "Average Reward for Agent 10 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9087\n",
      "Average Reward for Agent 11 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7717\n",
      "Average Reward for Agent 12 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4114\n",
      "Average Reward for Agent 13 this episode : -5.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.9749\n",
      "Reducing exploration for all agents to 0.6266\n",
      "Episode 28 is finished\n",
      "Average Reward for Agent 0 this episode : -10.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.0491\n",
      "Average Reward for Agent 1 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.8454\n",
      "Average Reward for Agent 2 this episode : -22.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.4178\n",
      "Average Reward for Agent 3 this episode : -4.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4464\n",
      "Average Reward for Agent 4 this episode : -2.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6178\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5064\n",
      "Average Reward for Agent 6 this episode : -2.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2854\n",
      "Average Reward for Agent 7 this episode : -3.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5079\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8142\n",
      "Average Reward for Agent 9 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8348\n",
      "Average Reward for Agent 10 this episode : -0.52\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2563\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9022\n",
      "Average Reward for Agent 12 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2063\n",
      "Average Reward for Agent 13 this episode : -11.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.7990\n",
      "Reducing exploration for all agents to 0.6158\n",
      "Episode 29 is finished\n",
      "Average Reward for Agent 0 this episode : -9.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.4440\n",
      "Average Reward for Agent 1 this episode : -11.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.4590\n",
      "Average Reward for Agent 2 this episode : -21.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.4348\n",
      "Average Reward for Agent 3 this episode : -3.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4899\n",
      "Average Reward for Agent 4 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1776\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3975\n",
      "Average Reward for Agent 6 this episode : -3.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.5243\n",
      "Average Reward for Agent 7 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6608\n",
      "Average Reward for Agent 8 this episode : -0.25\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7501\n",
      "Average Reward for Agent 9 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8105\n",
      "Average Reward for Agent 10 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4389\n",
      "Average Reward for Agent 11 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7781\n",
      "Average Reward for Agent 12 this episode : -2.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6724\n",
      "Average Reward for Agent 13 this episode : -16.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2276\n",
      "Reducing exploration for all agents to 0.6053\n",
      "Episode 30 is finished\n",
      "Average Reward for Agent 0 this episode : -9.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.1379\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -11.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -21.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.2260\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3163\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -2.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5809\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3864\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -4.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.7459\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -3.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7252\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6291\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6829\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1430\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7496\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -2.74\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9822\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -20.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.8526\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.5949\n",
      "Episode 31 is finished\n",
      "Average Reward for Agent 0 this episode : -11.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.4031\n",
      "Average Reward for Agent 1 this episode : -10.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.2446\n",
      "Average Reward for Agent 2 this episode : -23.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.7383\n",
      "Average Reward for Agent 3 this episode : -4.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1570\n",
      "Average Reward for Agent 4 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1085\n",
      "Average Reward for Agent 5 this episode : -0.21\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1948\n",
      "Average Reward for Agent 6 this episode : -5.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9678\n",
      "Average Reward for Agent 7 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8553\n",
      "Average Reward for Agent 8 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4591\n",
      "Average Reward for Agent 9 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8869\n",
      "Average Reward for Agent 10 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2749\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1544\n",
      "Average Reward for Agent 12 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6630\n",
      "Average Reward for Agent 13 this episode : -22.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.3936\n",
      "Reducing exploration for all agents to 0.5847\n",
      "Episode 32 is finished\n",
      "Average Reward for Agent 0 this episode : -9.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4792\n",
      "Average Reward for Agent 1 this episode : -12.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8293\n",
      "Average Reward for Agent 2 this episode : -23.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.1553\n",
      "Average Reward for Agent 3 this episode : -4.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0344\n",
      "Average Reward for Agent 4 this episode : -2.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5205\n",
      "Average Reward for Agent 5 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3357\n",
      "Average Reward for Agent 6 this episode : -5.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8027\n",
      "Average Reward for Agent 7 this episode : -2.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3956\n",
      "Average Reward for Agent 8 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5014\n",
      "Average Reward for Agent 9 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8254\n",
      "Average Reward for Agent 10 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7420\n",
      "Average Reward for Agent 11 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4624\n",
      "Average Reward for Agent 12 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0875\n",
      "Average Reward for Agent 13 this episode : -19.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.5244\n",
      "Reducing exploration for all agents to 0.5746\n",
      "Episode 33 is finished\n",
      "Average Reward for Agent 0 this episode : -12.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.8971\n",
      "Average Reward for Agent 1 this episode : -12.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8968\n",
      "Average Reward for Agent 2 this episode : -18.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.1715\n",
      "Average Reward for Agent 3 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.3184\n",
      "Average Reward for Agent 4 this episode : -3.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5587\n",
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2227\n",
      "Average Reward for Agent 6 this episode : -9.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.5251\n",
      "Average Reward for Agent 7 this episode : -3.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7537\n",
      "Average Reward for Agent 8 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4141\n",
      "Average Reward for Agent 9 this episode : -0.31\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8470\n",
      "Average Reward for Agent 10 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3224\n",
      "Average Reward for Agent 11 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1044\n",
      "Average Reward for Agent 12 this episode : -3.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6563\n",
      "Average Reward for Agent 13 this episode : -3.18\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.6588\n",
      "Reducing exploration for all agents to 0.5648\n",
      "Episode 34 is finished\n",
      "Average Reward for Agent 0 this episode : -11.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.0457\n",
      "Average Reward for Agent 1 this episode : -8.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0831\n",
      "Average Reward for Agent 2 this episode : -21.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.8781\n",
      "Average Reward for Agent 3 this episode : -4.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8394\n",
      "Average Reward for Agent 4 this episode : -2.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1442\n",
      "Average Reward for Agent 6 this episode : -6.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.6092\n",
      "Average Reward for Agent 7 this episode : -2.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2311\n",
      "Average Reward for Agent 8 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4476\n",
      "Average Reward for Agent 9 this episode : -0.3\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6985\n",
      "Average Reward for Agent 10 this episode : -1.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1807\n",
      "Average Reward for Agent 11 this episode : -0.41\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2298\n",
      "Average Reward for Agent 12 this episode : -2.47\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1220\n",
      "Average Reward for Agent 13 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.7930\n",
      "Reducing exploration for all agents to 0.5551\n",
      "Episode 35 is finished\n",
      "Average Reward for Agent 0 this episode : -11.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8109\n",
      "Average Reward for Agent 1 this episode : -9.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1023\n",
      "Average Reward for Agent 2 this episode : -25.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9544\n",
      "Average Reward for Agent 3 this episode : -5.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.5903\n",
      "Average Reward for Agent 4 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2057\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2412\n",
      "Average Reward for Agent 6 this episode : -3.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3705\n",
      "Average Reward for Agent 7 this episode : -1.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2732\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3258\n",
      "Average Reward for Agent 9 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7618\n",
      "Average Reward for Agent 10 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5818\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4662\n",
      "Average Reward for Agent 12 this episode : -2.12\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6572\n",
      "Average Reward for Agent 13 this episode : -9.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.1785\n",
      "Reducing exploration for all agents to 0.5456\n",
      "Episode 36 is finished\n",
      "Average Reward for Agent 0 this episode : -10.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.4092\n",
      "Average Reward for Agent 1 this episode : -10.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5381\n",
      "Average Reward for Agent 2 this episode : -20.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.4355\n",
      "Average Reward for Agent 3 this episode : -9.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5026\n",
      "Average Reward for Agent 4 this episode : -5.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6664\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1712\n",
      "Average Reward for Agent 6 this episode : -2.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.8313\n",
      "Average Reward for Agent 7 this episode : -1.65\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5410\n",
      "Average Reward for Agent 8 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4887\n",
      "Average Reward for Agent 9 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7192\n",
      "Average Reward for Agent 10 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9883\n",
      "Average Reward for Agent 11 this episode : -0.39\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7735\n",
      "Average Reward for Agent 12 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5034\n",
      "Average Reward for Agent 13 this episode : -5.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8340\n",
      "Reducing exploration for all agents to 0.5362\n",
      "Episode 37 is finished\n",
      "Average Reward for Agent 0 this episode : -10.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.0980\n",
      "Average Reward for Agent 1 this episode : -10.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.1506\n",
      "Average Reward for Agent 2 this episode : -20.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.3325\n",
      "Average Reward for Agent 3 this episode : -10.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.6758\n",
      "Average Reward for Agent 4 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7378\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1982\n",
      "Average Reward for Agent 6 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5204\n",
      "Average Reward for Agent 7 this episode : -1.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7681\n",
      "Average Reward for Agent 8 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6077\n",
      "Average Reward for Agent 9 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5236\n",
      "Average Reward for Agent 10 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0248\n",
      "Average Reward for Agent 11 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8449\n",
      "Average Reward for Agent 12 this episode : -2.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3933\n",
      "Average Reward for Agent 13 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9614\n",
      "Reducing exploration for all agents to 0.527\n",
      "Episode 38 is finished\n",
      "Average Reward for Agent 0 this episode : -11.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.6783\n",
      "Average Reward for Agent 1 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.7581\n",
      "Average Reward for Agent 2 this episode : -25.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.9419\n",
      "Average Reward for Agent 3 this episode : -7.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.0075\n",
      "Average Reward for Agent 4 this episode : -2.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7355\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1554\n",
      "Average Reward for Agent 6 this episode : -1.48\n",
      "Saving architecture, weights, optimizer state for best agent-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.1270\n",
      "Average Reward for Agent 7 this episode : -1.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8199\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6326\n",
      "Average Reward for Agent 9 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5748\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1429\n",
      "Average Reward for Agent 11 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9640\n",
      "Average Reward for Agent 12 this episode : -2.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8368\n",
      "Average Reward for Agent 13 this episode : -8.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.6031\n",
      "Reducing exploration for all agents to 0.5179\n",
      "Episode 39 is finished\n",
      "Average Reward for Agent 0 this episode : -10.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2367\n",
      "Average Reward for Agent 1 this episode : -11.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.9042\n",
      "Average Reward for Agent 2 this episode : -22.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.7094\n",
      "Average Reward for Agent 3 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.6020\n",
      "Average Reward for Agent 4 this episode : -3.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1767\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1655\n",
      "Average Reward for Agent 6 this episode : -1.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.4344\n",
      "Average Reward for Agent 7 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0708\n",
      "Average Reward for Agent 8 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5585\n",
      "Average Reward for Agent 9 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8949\n",
      "Average Reward for Agent 10 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2671\n",
      "Average Reward for Agent 11 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9567\n",
      "Average Reward for Agent 12 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4346\n",
      "Average Reward for Agent 13 this episode : -6.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.2085\n",
      "Reducing exploration for all agents to 0.5091\n",
      "Episode 40 is finished\n",
      "Average Reward for Agent 0 this episode : -9.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.1320\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3267\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -20.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.7178\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1254\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -3.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8881\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3144\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7268\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5050\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9526\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8064\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.5\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9302\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8705\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -2.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4991\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -4.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.7483\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.5003\n",
      "Episode 41 is finished\n",
      "Average Reward for Agent 0 this episode : -12.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.7857\n",
      "Average Reward for Agent 1 this episode : -10.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.8763\n",
      "Average Reward for Agent 2 this episode : -19.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.3599\n",
      "Average Reward for Agent 3 this episode : -10.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.4128\n",
      "Average Reward for Agent 4 this episode : -4.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.7377\n",
      "Average Reward for Agent 5 this episode : -0.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4094\n",
      "Average Reward for Agent 6 this episode : -1.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.6943\n",
      "Average Reward for Agent 7 this episode : -2.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6375\n",
      "Average Reward for Agent 8 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6344\n",
      "Average Reward for Agent 9 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4243\n",
      "Average Reward for Agent 10 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3693\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1212\n",
      "Average Reward for Agent 12 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4337\n",
      "Average Reward for Agent 13 this episode : -15.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.9785\n",
      "Reducing exploration for all agents to 0.4917\n",
      "Episode 42 is finished\n",
      "Average Reward for Agent 0 this episode : -12.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9652\n",
      "Average Reward for Agent 1 this episode : -11.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.5608\n",
      "Average Reward for Agent 2 this episode : -19.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.8164\n",
      "Average Reward for Agent 3 this episode : -8.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.9795\n",
      "Average Reward for Agent 4 this episode : -3.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8107\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2267\n",
      "Average Reward for Agent 6 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.5281\n",
      "Average Reward for Agent 7 this episode : -2.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8151\n",
      "Average Reward for Agent 8 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5257\n",
      "Average Reward for Agent 9 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9328\n",
      "Average Reward for Agent 10 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9680\n",
      "Average Reward for Agent 11 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8431\n",
      "Average Reward for Agent 12 this episode : -2.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9056\n",
      "Average Reward for Agent 13 this episode : -4.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.1735\n",
      "Reducing exploration for all agents to 0.4833\n",
      "Episode 43 is finished\n",
      "Average Reward for Agent 0 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.7190\n",
      "Average Reward for Agent 1 this episode : -10.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.8558\n",
      "Average Reward for Agent 2 this episode : -29.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.2884\n",
      "Average Reward for Agent 3 this episode : -8.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.2153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 4 this episode : -5.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.3875\n",
      "Average Reward for Agent 5 this episode : -0.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2412\n",
      "Average Reward for Agent 6 this episode : -2.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9014\n",
      "Average Reward for Agent 7 this episode : -2.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9010\n",
      "Average Reward for Agent 8 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4684\n",
      "Average Reward for Agent 9 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9367\n",
      "Average Reward for Agent 10 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2266\n",
      "Average Reward for Agent 11 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8588\n",
      "Average Reward for Agent 12 this episode : -2.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8959\n",
      "Average Reward for Agent 13 this episode : -16.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8258\n",
      "Reducing exploration for all agents to 0.475\n",
      "Episode 44 is finished\n",
      "Average Reward for Agent 0 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.1554\n",
      "Average Reward for Agent 1 this episode : -10.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.0760\n",
      "Average Reward for Agent 2 this episode : -27.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.3107\n",
      "Average Reward for Agent 3 this episode : -9.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.3299\n",
      "Average Reward for Agent 4 this episode : -6.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5158\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2810\n",
      "Average Reward for Agent 6 this episode : -2.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.8377\n",
      "Average Reward for Agent 7 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3971\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6378\n",
      "Average Reward for Agent 9 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9498\n",
      "Average Reward for Agent 10 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0922\n",
      "Average Reward for Agent 11 this episode : -0.35\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6935\n",
      "Average Reward for Agent 12 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5550\n",
      "Average Reward for Agent 13 this episode : -26.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.7666\n",
      "Reducing exploration for all agents to 0.4668\n",
      "Episode 45 is finished\n",
      "Average Reward for Agent 0 this episode : -9.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7147\n",
      "Average Reward for Agent 1 this episode : -21.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.2642\n",
      "Average Reward for Agent 2 this episode : -37.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 193.5515\n",
      "Average Reward for Agent 3 this episode : -10.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.6606\n",
      "Average Reward for Agent 4 this episode : -3.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.9205\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3391\n",
      "Average Reward for Agent 6 this episode : -3.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.3904\n",
      "Average Reward for Agent 7 this episode : -3.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5628\n",
      "Average Reward for Agent 8 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5547\n",
      "Average Reward for Agent 9 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7830\n",
      "Average Reward for Agent 10 this episode : -0.47\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2074\n",
      "Average Reward for Agent 11 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7431\n",
      "Average Reward for Agent 12 this episode : -3.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6235\n",
      "Average Reward for Agent 13 this episode : -32.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.0271\n",
      "Reducing exploration for all agents to 0.4588\n",
      "Episode 46 is finished\n",
      "Average Reward for Agent 0 this episode : -10.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.2220\n",
      "Average Reward for Agent 1 this episode : -23.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.5991\n",
      "Average Reward for Agent 2 this episode : -41.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.1601\n",
      "Average Reward for Agent 3 this episode : -8.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5576\n",
      "Average Reward for Agent 4 this episode : -1.78\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7936\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2457\n",
      "Average Reward for Agent 6 this episode : -4.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.4021\n",
      "Average Reward for Agent 7 this episode : -5.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6160\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7503\n",
      "Average Reward for Agent 9 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7109\n",
      "Average Reward for Agent 10 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0936\n",
      "Average Reward for Agent 11 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5926\n",
      "Average Reward for Agent 12 this episode : -2.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3191\n",
      "Average Reward for Agent 13 this episode : -28.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.8222\n",
      "Reducing exploration for all agents to 0.451\n",
      "Episode 47 is finished\n",
      "Average Reward for Agent 0 this episode : -11.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.8348\n",
      "Average Reward for Agent 1 this episode : -20.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.3193\n",
      "Average Reward for Agent 2 this episode : -47.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.0925\n",
      "Average Reward for Agent 3 this episode : -8.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.9280\n",
      "Average Reward for Agent 4 this episode : -1.35\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5064\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2148\n",
      "Average Reward for Agent 6 this episode : -14.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 260.6093\n",
      "Average Reward for Agent 7 this episode : -3.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2614\n",
      "Average Reward for Agent 8 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3739\n",
      "Average Reward for Agent 9 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6204\n",
      "Average Reward for Agent 10 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9655\n",
      "Average Reward for Agent 11 this episode : -1.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8933\n",
      "Average Reward for Agent 12 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2576\n",
      "Average Reward for Agent 13 this episode : -33.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.2881\n",
      "Reducing exploration for all agents to 0.4432\n",
      "Episode 48 is finished\n",
      "Average Reward for Agent 0 this episode : -11.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0204\n",
      "Average Reward for Agent 1 this episode : -16.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.5814\n",
      "Average Reward for Agent 2 this episode : -29.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 186.8422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 3 this episode : -10.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.9440\n",
      "Average Reward for Agent 4 this episode : -1.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1943\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2185\n",
      "Average Reward for Agent 6 this episode : -4.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.4624\n",
      "Average Reward for Agent 7 this episode : -2.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6194\n",
      "Average Reward for Agent 8 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4179\n",
      "Average Reward for Agent 9 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3241\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2328\n",
      "Average Reward for Agent 11 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1671\n",
      "Average Reward for Agent 12 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6406\n",
      "Average Reward for Agent 13 this episode : -25.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.2646\n",
      "Reducing exploration for all agents to 0.4356\n",
      "Episode 49 is finished\n",
      "Average Reward for Agent 0 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.9264\n",
      "Average Reward for Agent 1 this episode : -17.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.0487\n",
      "Average Reward for Agent 2 this episode : -29.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.8073\n",
      "Average Reward for Agent 3 this episode : -8.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2823\n",
      "Average Reward for Agent 4 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.2292\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1984\n",
      "Average Reward for Agent 6 this episode : -3.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.8121\n",
      "Average Reward for Agent 7 this episode : -2.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2617\n",
      "Average Reward for Agent 8 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6074\n",
      "Average Reward for Agent 9 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0768\n",
      "Average Reward for Agent 10 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1522\n",
      "Average Reward for Agent 11 this episode : -1.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5257\n",
      "Average Reward for Agent 12 this episode : -2.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4934\n",
      "Average Reward for Agent 13 this episode : -33.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.8524\n",
      "Reducing exploration for all agents to 0.4281\n",
      "Episode 50 is finished\n",
      "Average Reward for Agent 0 this episode : -10.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.2176\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -19.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.8694\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -23.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.7107\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7932\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -7.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3664\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2011\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -2.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0222\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -1.62\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6924\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5200\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8766\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7618\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6266\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -2.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1210\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -13.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.2551\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.4208\n",
      "Episode 51 is finished\n",
      "Average Reward for Agent 0 this episode : -11.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.6255\n",
      "Average Reward for Agent 1 this episode : -13.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 192.3026\n",
      "Average Reward for Agent 2 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 292.6200\n",
      "Average Reward for Agent 3 this episode : -12.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0584\n",
      "Average Reward for Agent 4 this episode : -17.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.5030\n",
      "Average Reward for Agent 5 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4954\n",
      "Average Reward for Agent 6 this episode : -1.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.6249\n",
      "Average Reward for Agent 7 this episode : -1.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9787\n",
      "Average Reward for Agent 8 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4886\n",
      "Average Reward for Agent 9 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9278\n",
      "Average Reward for Agent 10 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9165\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1912\n",
      "Average Reward for Agent 12 this episode : -2.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5767\n",
      "Average Reward for Agent 13 this episode : -3.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.6239\n",
      "Reducing exploration for all agents to 0.4136\n",
      "Episode 52 is finished\n",
      "Average Reward for Agent 0 this episode : -12.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.4650\n",
      "Average Reward for Agent 1 this episode : -10.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.8535\n",
      "Average Reward for Agent 2 this episode : -25.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.3273\n",
      "Average Reward for Agent 3 this episode : -9.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5273\n",
      "Average Reward for Agent 4 this episode : -4.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.3488\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4161\n",
      "Average Reward for Agent 6 this episode : -2.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.8371\n",
      "Average Reward for Agent 7 this episode : -1.59\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6241\n",
      "Average Reward for Agent 8 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5859\n",
      "Average Reward for Agent 9 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1374\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0216\n",
      "Average Reward for Agent 11 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6293\n",
      "Average Reward for Agent 12 this episode : -2.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3675\n",
      "Average Reward for Agent 13 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.7481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.4065\n",
      "Episode 53 is finished\n",
      "Average Reward for Agent 0 this episode : -12.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3458\n",
      "Average Reward for Agent 1 this episode : -9.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.9371\n",
      "Average Reward for Agent 2 this episode : -31.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 149.7209\n",
      "Average Reward for Agent 3 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.4396\n",
      "Average Reward for Agent 4 this episode : -1.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.0562\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3508\n",
      "Average Reward for Agent 6 this episode : -2.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.7345\n",
      "Average Reward for Agent 7 this episode : -1.58\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3836\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4787\n",
      "Average Reward for Agent 9 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4480\n",
      "Average Reward for Agent 10 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8625\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3135\n",
      "Average Reward for Agent 12 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6464\n",
      "Average Reward for Agent 13 this episode : -5.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.9759\n",
      "Reducing exploration for all agents to 0.3995\n",
      "Episode 54 is finished\n",
      "Average Reward for Agent 0 this episode : -10.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.9388\n",
      "Average Reward for Agent 1 this episode : -13.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 223.3953\n",
      "Average Reward for Agent 2 this episode : -24.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.2782\n",
      "Average Reward for Agent 3 this episode : -10.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.1826\n",
      "Average Reward for Agent 4 this episode : -7.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1030\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2349\n",
      "Average Reward for Agent 6 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.1132\n",
      "Average Reward for Agent 7 this episode : -1.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6152\n",
      "Average Reward for Agent 8 this episode : -0.19\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3857\n",
      "Average Reward for Agent 9 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1926\n",
      "Average Reward for Agent 10 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5761\n",
      "Average Reward for Agent 11 this episode : -1.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5393\n",
      "Average Reward for Agent 12 this episode : -4.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0597\n",
      "Average Reward for Agent 13 this episode : -18.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.0030\n",
      "Reducing exploration for all agents to 0.3926\n",
      "Episode 55 is finished\n",
      "Average Reward for Agent 0 this episode : -11.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9425\n",
      "Average Reward for Agent 1 this episode : -11.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.7806\n",
      "Average Reward for Agent 2 this episode : -26.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.6655\n",
      "Average Reward for Agent 3 this episode : -9.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2684\n",
      "Average Reward for Agent 4 this episode : -7.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1861\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2853\n",
      "Average Reward for Agent 6 this episode : -1.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.8400\n",
      "Average Reward for Agent 7 this episode : -1.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1852\n",
      "Average Reward for Agent 8 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5815\n",
      "Average Reward for Agent 9 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8289\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5921\n",
      "Average Reward for Agent 11 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3312\n",
      "Average Reward for Agent 12 this episode : -2.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3510\n",
      "Average Reward for Agent 13 this episode : -4.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.0519\n",
      "Reducing exploration for all agents to 0.3859\n",
      "Episode 56 is finished\n",
      "Average Reward for Agent 0 this episode : -10.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.8503\n",
      "Average Reward for Agent 1 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.7282\n",
      "Average Reward for Agent 2 this episode : -31.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.2217\n",
      "Average Reward for Agent 3 this episode : -8.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.3794\n",
      "Average Reward for Agent 4 this episode : -7.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.8801\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3408\n",
      "Average Reward for Agent 6 this episode : -2.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.0777\n",
      "Average Reward for Agent 7 this episode : -1.35\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6580\n",
      "Average Reward for Agent 8 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6492\n",
      "Average Reward for Agent 9 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8588\n",
      "Average Reward for Agent 10 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6991\n",
      "Average Reward for Agent 11 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9183\n",
      "Average Reward for Agent 12 this episode : -2.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3575\n",
      "Average Reward for Agent 13 this episode : -17.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.7156\n",
      "Reducing exploration for all agents to 0.3793\n",
      "Episode 57 is finished\n",
      "Average Reward for Agent 0 this episode : -11.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.2463\n",
      "Average Reward for Agent 1 this episode : -11.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.7524\n",
      "Average Reward for Agent 2 this episode : -28.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 235.7295\n",
      "Average Reward for Agent 3 this episode : -8.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4242\n",
      "Average Reward for Agent 4 this episode : -15.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1282\n",
      "Average Reward for Agent 5 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2763\n",
      "Average Reward for Agent 6 this episode : -3.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.8419\n",
      "Average Reward for Agent 7 this episode : -2.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8416\n",
      "Average Reward for Agent 8 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4829\n",
      "Average Reward for Agent 9 this episode : -1.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6567\n",
      "Average Reward for Agent 10 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7821\n",
      "Average Reward for Agent 11 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1406\n",
      "Average Reward for Agent 12 this episode : -2.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5468\n",
      "Average Reward for Agent 13 this episode : -15.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.0805\n",
      "Reducing exploration for all agents to 0.3728\n",
      "Episode 58 is finished\n",
      "Average Reward for Agent 0 this episode : -13.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.8895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -12.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.7687\n",
      "Average Reward for Agent 2 this episode : -34.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.8978\n",
      "Average Reward for Agent 3 this episode : -8.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5013\n",
      "Average Reward for Agent 4 this episode : -5.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2722\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3733\n",
      "Average Reward for Agent 6 this episode : -2.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.3083\n",
      "Average Reward for Agent 7 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6963\n",
      "Average Reward for Agent 8 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4693\n",
      "Average Reward for Agent 9 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8937\n",
      "Average Reward for Agent 10 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7791\n",
      "Average Reward for Agent 11 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8163\n",
      "Average Reward for Agent 12 this episode : -3.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6160\n",
      "Average Reward for Agent 13 this episode : -17.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.0950\n",
      "Reducing exploration for all agents to 0.3664\n",
      "Episode 59 is finished\n",
      "Average Reward for Agent 0 this episode : -10.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.6070\n",
      "Average Reward for Agent 1 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.2994\n",
      "Average Reward for Agent 2 this episode : -47.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 260.8953\n",
      "Average Reward for Agent 3 this episode : -8.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0276\n",
      "Average Reward for Agent 4 this episode : -22.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.9815\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1724\n",
      "Average Reward for Agent 6 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.4225\n",
      "Average Reward for Agent 7 this episode : -2.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5798\n",
      "Average Reward for Agent 8 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3823\n",
      "Average Reward for Agent 9 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7093\n",
      "Average Reward for Agent 10 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9742\n",
      "Average Reward for Agent 11 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5009\n",
      "Average Reward for Agent 12 this episode : -3.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0583\n",
      "Average Reward for Agent 13 this episode : -13.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.3905\n",
      "Reducing exploration for all agents to 0.3601\n",
      "Episode 60 is finished\n",
      "Average Reward for Agent 0 this episode : -11.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.3828\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.4705\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -50.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 214.3582\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -8.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.5919\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -3.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1199\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1945\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.3217\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0425\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4508\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9403\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8538\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9432\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4235\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -16.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2455\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.3539\n",
      "Episode 61 is finished\n",
      "Average Reward for Agent 0 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.7195\n",
      "Average Reward for Agent 1 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.5265\n",
      "Average Reward for Agent 2 this episode : -49.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 569.3131\n",
      "Average Reward for Agent 3 this episode : -10.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.6987\n",
      "Average Reward for Agent 4 this episode : -9.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.9542\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2081\n",
      "Average Reward for Agent 6 this episode : -7.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.1836\n",
      "Average Reward for Agent 7 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7030\n",
      "Average Reward for Agent 8 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4023\n",
      "Average Reward for Agent 9 this episode : -0.23\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2839\n",
      "Average Reward for Agent 10 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8149\n",
      "Average Reward for Agent 11 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6254\n",
      "Average Reward for Agent 12 this episode : -4.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0263\n",
      "Average Reward for Agent 13 this episode : -18.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.9789\n",
      "Reducing exploration for all agents to 0.3478\n",
      "Episode 62 is finished\n",
      "Average Reward for Agent 0 this episode : -12.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.8670\n",
      "Average Reward for Agent 1 this episode : -10.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.3025\n",
      "Average Reward for Agent 2 this episode : -47.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.8611\n",
      "Average Reward for Agent 3 this episode : -9.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.6625\n",
      "Average Reward for Agent 4 this episode : -4.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.0326\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1925\n",
      "Average Reward for Agent 6 this episode : -8.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.4963\n",
      "Average Reward for Agent 7 this episode : -3.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0797\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6961\n",
      "Average Reward for Agent 9 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3000\n",
      "Average Reward for Agent 10 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9037\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 12 this episode : -3.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5499\n",
      "Average Reward for Agent 13 this episode : -7.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.2441\n",
      "Reducing exploration for all agents to 0.3418\n",
      "Episode 63 is finished\n",
      "Average Reward for Agent 0 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.3810\n",
      "Average Reward for Agent 1 this episode : -11.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.5265\n",
      "Average Reward for Agent 2 this episode : -24.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 751.3621\n",
      "Average Reward for Agent 3 this episode : -9.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0901\n",
      "Average Reward for Agent 4 this episode : -8.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.3409\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1601\n",
      "Average Reward for Agent 6 this episode : -14.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.5343\n",
      "Average Reward for Agent 7 this episode : -4.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9899\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7477\n",
      "Average Reward for Agent 9 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1034\n",
      "Average Reward for Agent 10 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6270\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1160\n",
      "Average Reward for Agent 12 this episode : -2.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3277\n",
      "Average Reward for Agent 13 this episode : -9.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5996\n",
      "Reducing exploration for all agents to 0.336\n",
      "Episode 64 is finished\n",
      "Average Reward for Agent 0 this episode : -12.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.3977\n",
      "Average Reward for Agent 1 this episode : -10.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.0994\n",
      "Average Reward for Agent 2 this episode : -25.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 466.9989\n",
      "Average Reward for Agent 3 this episode : -9.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.2566\n",
      "Average Reward for Agent 4 this episode : -17.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.2627\n",
      "Average Reward for Agent 5 this episode : -0.08\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2844\n",
      "Average Reward for Agent 6 this episode : -11.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.3979\n",
      "Average Reward for Agent 7 this episode : -7.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1830\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6696\n",
      "Average Reward for Agent 9 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0580\n",
      "Average Reward for Agent 10 this episode : -0.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5552\n",
      "Average Reward for Agent 11 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9682\n",
      "Average Reward for Agent 12 this episode : -2.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7850\n",
      "Average Reward for Agent 13 this episode : -6.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1972\n",
      "Reducing exploration for all agents to 0.3302\n",
      "Episode 65 is finished\n",
      "Average Reward for Agent 0 this episode : -10.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.2585\n",
      "Average Reward for Agent 1 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.8518\n",
      "Average Reward for Agent 2 this episode : -49.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 385.6602\n",
      "Average Reward for Agent 3 this episode : -11.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.7832\n",
      "Average Reward for Agent 4 this episode : -5.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.3598\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1402\n",
      "Average Reward for Agent 6 this episode : -8.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.9142\n",
      "Average Reward for Agent 7 this episode : -8.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1612\n",
      "Average Reward for Agent 8 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4876\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6823\n",
      "Average Reward for Agent 10 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8722\n",
      "Average Reward for Agent 11 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1912\n",
      "Average Reward for Agent 12 this episode : -2.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7589\n",
      "Average Reward for Agent 13 this episode : -22.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.8909\n",
      "Reducing exploration for all agents to 0.3245\n",
      "Episode 66 is finished\n",
      "Average Reward for Agent 0 this episode : -11.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.1849\n",
      "Average Reward for Agent 1 this episode : -10.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.4081\n",
      "Average Reward for Agent 2 this episode : -43.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 444.4622\n",
      "Average Reward for Agent 3 this episode : -9.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6013\n",
      "Average Reward for Agent 4 this episode : -7.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1052\n",
      "Average Reward for Agent 5 this episode : -0.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2401\n",
      "Average Reward for Agent 6 this episode : -4.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.6611\n",
      "Average Reward for Agent 7 this episode : -6.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3394\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5224\n",
      "Average Reward for Agent 9 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5495\n",
      "Average Reward for Agent 10 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9521\n",
      "Average Reward for Agent 11 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3411\n",
      "Average Reward for Agent 12 this episode : -2.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3462\n",
      "Average Reward for Agent 13 this episode : -19.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.3022\n",
      "Reducing exploration for all agents to 0.319\n",
      "Episode 67 is finished\n",
      "Average Reward for Agent 0 this episode : -13.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.1093\n",
      "Average Reward for Agent 1 this episode : -11.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9039\n",
      "Average Reward for Agent 2 this episode : -48.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 494.4794\n",
      "Average Reward for Agent 3 this episode : -9.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.7411\n",
      "Average Reward for Agent 4 this episode : -4.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7509\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2276\n",
      "Average Reward for Agent 6 this episode : -9.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.2350\n",
      "Average Reward for Agent 7 this episode : -10.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.3149\n",
      "Average Reward for Agent 8 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5035\n",
      "Average Reward for Agent 9 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7401\n",
      "Average Reward for Agent 10 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4347\n",
      "Average Reward for Agent 11 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0801\n",
      "Average Reward for Agent 12 this episode : -3.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3178\n",
      "Average Reward for Agent 13 this episode : -26.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4173\n",
      "Reducing exploration for all agents to 0.3135\n",
      "Episode 68 is finished\n",
      "Average Reward for Agent 0 this episode : -15.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.1802\n",
      "Average Reward for Agent 1 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.8314\n",
      "Average Reward for Agent 2 this episode : -45.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 486.4888\n",
      "Average Reward for Agent 3 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.1459\n",
      "Average Reward for Agent 4 this episode : -5.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.1883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1742\n",
      "Average Reward for Agent 6 this episode : -8.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.5002\n",
      "Average Reward for Agent 7 this episode : -10.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2319\n",
      "Average Reward for Agent 8 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6245\n",
      "Average Reward for Agent 9 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3522\n",
      "Average Reward for Agent 10 this episode : -1.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4014\n",
      "Average Reward for Agent 11 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5022\n",
      "Average Reward for Agent 12 this episode : -2.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4792\n",
      "Average Reward for Agent 13 this episode : -22.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.7242\n",
      "Reducing exploration for all agents to 0.3081\n",
      "Episode 69 is finished\n",
      "Average Reward for Agent 0 this episode : -15.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.3858\n",
      "Average Reward for Agent 1 this episode : -10.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.4859\n",
      "Average Reward for Agent 2 this episode : -37.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 340.1132\n",
      "Average Reward for Agent 3 this episode : -11.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.4192\n",
      "Average Reward for Agent 4 this episode : -6.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9041\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1590\n",
      "Average Reward for Agent 6 this episode : -17.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.9056\n",
      "Average Reward for Agent 7 this episode : -12.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6936\n",
      "Average Reward for Agent 8 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6154\n",
      "Average Reward for Agent 9 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9966\n",
      "Average Reward for Agent 10 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7680\n",
      "Average Reward for Agent 11 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3363\n",
      "Average Reward for Agent 12 this episode : -4.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4327\n",
      "Average Reward for Agent 13 this episode : -24.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.5449\n",
      "Reducing exploration for all agents to 0.3028\n",
      "Episode 70 is finished\n",
      "Average Reward for Agent 0 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.1936\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -11.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.7008\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -32.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 478.1192\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7993\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -3.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.3106\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1663\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -13.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.1474\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -12.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.4749\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5504\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4980\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6249\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3379\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3398\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -36.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 215.2323\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.2976\n",
      "Episode 71 is finished\n",
      "Average Reward for Agent 0 this episode : -17.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.9660\n",
      "Average Reward for Agent 1 this episode : -9.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.3465\n",
      "Average Reward for Agent 2 this episode : -27.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 587.5759\n",
      "Average Reward for Agent 3 this episode : -11.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.1749\n",
      "Average Reward for Agent 4 this episode : -6.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.0850\n",
      "Average Reward for Agent 5 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1160\n",
      "Average Reward for Agent 6 this episode : -25.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 251.7942\n",
      "Average Reward for Agent 7 this episode : -11.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.4411\n",
      "Average Reward for Agent 8 this episode : -0.18\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5155\n",
      "Average Reward for Agent 9 this episode : -1.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5381\n",
      "Average Reward for Agent 10 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7321\n",
      "Average Reward for Agent 11 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3295\n",
      "Average Reward for Agent 12 this episode : -4.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3802\n",
      "Average Reward for Agent 13 this episode : -26.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 346.7922\n",
      "Reducing exploration for all agents to 0.2925\n",
      "Episode 72 is finished\n",
      "Average Reward for Agent 0 this episode : -12.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.7832\n",
      "Average Reward for Agent 1 this episode : -7.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.4388\n",
      "Average Reward for Agent 2 this episode : -25.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 286.9981\n",
      "Average Reward for Agent 3 this episode : -9.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1314\n",
      "Average Reward for Agent 4 this episode : -10.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.4251\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0810\n",
      "Average Reward for Agent 6 this episode : -8.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 225.7781\n",
      "Average Reward for Agent 7 this episode : -12.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.0243\n",
      "Average Reward for Agent 8 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3690\n",
      "Average Reward for Agent 9 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8176\n",
      "Average Reward for Agent 10 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1940\n",
      "Average Reward for Agent 11 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9341\n",
      "Average Reward for Agent 12 this episode : -3.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0436\n",
      "Average Reward for Agent 13 this episode : -44.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.4530\n",
      "Reducing exploration for all agents to 0.2875\n",
      "Episode 73 is finished\n",
      "Average Reward for Agent 0 this episode : -10.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.4292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.5686\n",
      "Average Reward for Agent 2 this episode : -23.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 287.3917\n",
      "Average Reward for Agent 3 this episode : -10.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4717\n",
      "Average Reward for Agent 4 this episode : -17.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4581\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1670\n",
      "Average Reward for Agent 6 this episode : -19.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 134.4130\n",
      "Average Reward for Agent 7 this episode : -13.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.2484\n",
      "Average Reward for Agent 8 this episode : -0.14\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4050\n",
      "Average Reward for Agent 9 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9145\n",
      "Average Reward for Agent 10 this episode : -0.44\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0036\n",
      "Average Reward for Agent 11 this episode : -4.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2672\n",
      "Average Reward for Agent 12 this episode : -5.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7670\n",
      "Average Reward for Agent 13 this episode : -47.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 345.2883\n",
      "Reducing exploration for all agents to 0.2826\n",
      "Episode 74 is finished\n",
      "Average Reward for Agent 0 this episode : -9.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.2621\n",
      "Average Reward for Agent 1 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3752\n",
      "Average Reward for Agent 2 this episode : -21.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 380.2667\n",
      "Average Reward for Agent 3 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.3448\n",
      "Average Reward for Agent 4 this episode : -14.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.4292\n",
      "Average Reward for Agent 5 this episode : -0.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2780\n",
      "Average Reward for Agent 6 this episode : -5.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.9109\n",
      "Average Reward for Agent 7 this episode : -13.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.3942\n",
      "Average Reward for Agent 8 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4551\n",
      "Average Reward for Agent 9 this episode : -1.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8515\n",
      "Average Reward for Agent 10 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0761\n",
      "Average Reward for Agent 11 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3276\n",
      "Average Reward for Agent 12 this episode : -4.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4890\n",
      "Average Reward for Agent 13 this episode : -22.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.4071\n",
      "Reducing exploration for all agents to 0.2777\n",
      "Episode 75 is finished\n",
      "Average Reward for Agent 0 this episode : -9.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9799\n",
      "Average Reward for Agent 1 this episode : -9.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.9323\n",
      "Average Reward for Agent 2 this episode : -25.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 504.1471\n",
      "Average Reward for Agent 3 this episode : -10.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.8908\n",
      "Average Reward for Agent 4 this episode : -16.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.4792\n",
      "Average Reward for Agent 5 this episode : -0.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0793\n",
      "Average Reward for Agent 6 this episode : -3.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.0819\n",
      "Average Reward for Agent 7 this episode : -13.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.4536\n",
      "Average Reward for Agent 8 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4013\n",
      "Average Reward for Agent 9 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6644\n",
      "Average Reward for Agent 10 this episode : -1.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8497\n",
      "Average Reward for Agent 11 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0660\n",
      "Average Reward for Agent 12 this episode : -3.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0755\n",
      "Average Reward for Agent 13 this episode : -49.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 187.4326\n",
      "Reducing exploration for all agents to 0.273\n",
      "Episode 76 is finished\n",
      "Average Reward for Agent 0 this episode : -11.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.5614\n",
      "Average Reward for Agent 1 this episode : -14.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.4486\n",
      "Average Reward for Agent 2 this episode : -20.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 484.0334\n",
      "Average Reward for Agent 3 this episode : -11.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3033\n",
      "Average Reward for Agent 4 this episode : -13.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.7358\n",
      "Average Reward for Agent 5 this episode : -0.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2675\n",
      "Average Reward for Agent 6 this episode : -2.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 186.0912\n",
      "Average Reward for Agent 7 this episode : -13.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.8910\n",
      "Average Reward for Agent 8 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4734\n",
      "Average Reward for Agent 9 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1678\n",
      "Average Reward for Agent 10 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7327\n",
      "Average Reward for Agent 11 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0652\n",
      "Average Reward for Agent 12 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9050\n",
      "Average Reward for Agent 13 this episode : -3.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.1291\n",
      "Reducing exploration for all agents to 0.2683\n",
      "Episode 77 is finished\n",
      "Average Reward for Agent 0 this episode : -14.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.1865\n",
      "Average Reward for Agent 1 this episode : -11.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.5381\n",
      "Average Reward for Agent 2 this episode : -24.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 231.6999\n",
      "Average Reward for Agent 3 this episode : -9.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7882\n",
      "Average Reward for Agent 4 this episode : -16.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4861\n",
      "Average Reward for Agent 5 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3322\n",
      "Average Reward for Agent 6 this episode : -3.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.6278\n",
      "Average Reward for Agent 7 this episode : -13.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.9315\n",
      "Average Reward for Agent 8 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0331\n",
      "Average Reward for Agent 9 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1577\n",
      "Average Reward for Agent 10 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1994\n",
      "Average Reward for Agent 11 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7393\n",
      "Average Reward for Agent 12 this episode : -6.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4429\n",
      "Average Reward for Agent 13 this episode : -7.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.7340\n",
      "Reducing exploration for all agents to 0.2637\n",
      "Episode 78 is finished\n",
      "Average Reward for Agent 0 this episode : -12.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.7397\n",
      "Average Reward for Agent 1 this episode : -15.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.4030\n",
      "Average Reward for Agent 2 this episode : -27.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 283.7814\n",
      "Average Reward for Agent 3 this episode : -12.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5238\n",
      "Average Reward for Agent 4 this episode : -12.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.9158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3859\n",
      "Average Reward for Agent 6 this episode : -2.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.0384\n",
      "Average Reward for Agent 7 this episode : -16.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9905\n",
      "Average Reward for Agent 8 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3073\n",
      "Average Reward for Agent 9 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8874\n",
      "Average Reward for Agent 10 this episode : -1.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1786\n",
      "Average Reward for Agent 11 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8557\n",
      "Average Reward for Agent 12 this episode : -4.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9646\n",
      "Average Reward for Agent 13 this episode : -4.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.8458\n",
      "Reducing exploration for all agents to 0.2591\n",
      "Episode 79 is finished\n",
      "Average Reward for Agent 0 this episode : -12.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.3620\n",
      "Average Reward for Agent 1 this episode : -15.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.6531\n",
      "Average Reward for Agent 2 this episode : -28.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 411.0974\n",
      "Average Reward for Agent 3 this episode : -8.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.1810\n",
      "Average Reward for Agent 4 this episode : -12.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.6900\n",
      "Average Reward for Agent 5 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4950\n",
      "Average Reward for Agent 6 this episode : -1.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.5056\n",
      "Average Reward for Agent 7 this episode : -12.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.3351\n",
      "Average Reward for Agent 8 this episode : -1.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2758\n",
      "Average Reward for Agent 9 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3020\n",
      "Average Reward for Agent 10 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5109\n",
      "Average Reward for Agent 11 this episode : -6.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3046\n",
      "Average Reward for Agent 12 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6144\n",
      "Average Reward for Agent 13 this episode : -16.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.5518\n",
      "Reducing exploration for all agents to 0.2547\n",
      "Episode 80 is finished\n",
      "Average Reward for Agent 0 this episode : -13.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.2135\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -14.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.2108\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -30.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 404.5280\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -12.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.0288\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -10.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.8867\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4370\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -9.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5497\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.7048\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3002\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4666\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3435\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4728\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -1.44\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8614\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -5.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.2917\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.2503\n",
      "Episode 81 is finished\n",
      "Average Reward for Agent 0 this episode : -8.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.1631\n",
      "Average Reward for Agent 1 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 143.4521\n",
      "Average Reward for Agent 2 this episode : -30.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 383.3384\n",
      "Average Reward for Agent 3 this episode : -14.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1440\n",
      "Average Reward for Agent 4 this episode : -11.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.7410\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5775\n",
      "Average Reward for Agent 6 this episode : -2.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.2277\n",
      "Average Reward for Agent 7 this episode : -12.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.1752\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9588\n",
      "Average Reward for Agent 9 this episode : -1.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3070\n",
      "Average Reward for Agent 10 this episode : -3.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8458\n",
      "Average Reward for Agent 11 this episode : -0.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0776\n",
      "Average Reward for Agent 12 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4399\n",
      "Average Reward for Agent 13 this episode : -13.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.2104\n",
      "Reducing exploration for all agents to 0.246\n",
      "Episode 82 is finished\n",
      "Average Reward for Agent 0 this episode : -7.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.4452\n",
      "Average Reward for Agent 1 this episode : -15.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 128.7930\n",
      "Average Reward for Agent 2 this episode : -33.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 426.4365\n",
      "Average Reward for Agent 3 this episode : -9.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2873\n",
      "Average Reward for Agent 4 this episode : -7.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9785\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3802\n",
      "Average Reward for Agent 6 this episode : -2.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.2110\n",
      "Average Reward for Agent 7 this episode : -13.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4370\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1664\n",
      "Average Reward for Agent 9 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1499\n",
      "Average Reward for Agent 10 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7793\n",
      "Average Reward for Agent 11 this episode : -0.32\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0316\n",
      "Average Reward for Agent 12 this episode : -26.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.2223\n",
      "Average Reward for Agent 13 this episode : -18.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 150.2686\n",
      "Reducing exploration for all agents to 0.2418\n",
      "Episode 83 is finished\n",
      "Average Reward for Agent 0 this episode : -7.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.8490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -18.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.1514\n",
      "Average Reward for Agent 2 this episode : -24.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 319.1600\n",
      "Average Reward for Agent 3 this episode : -10.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.7151\n",
      "Average Reward for Agent 4 this episode : -13.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.0120\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3179\n",
      "Average Reward for Agent 6 this episode : -6.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.8354\n",
      "Average Reward for Agent 7 this episode : -14.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.8872\n",
      "Average Reward for Agent 8 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1155\n",
      "Average Reward for Agent 9 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8776\n",
      "Average Reward for Agent 10 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5165\n",
      "Average Reward for Agent 11 this episode : -1.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3639\n",
      "Average Reward for Agent 12 this episode : -9.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.1882\n",
      "Average Reward for Agent 13 this episode : -6.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.9806\n",
      "Reducing exploration for all agents to 0.2377\n",
      "Episode 84 is finished\n",
      "Average Reward for Agent 0 this episode : -7.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7563\n",
      "Average Reward for Agent 1 this episode : -15.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.9687\n",
      "Average Reward for Agent 2 this episode : -24.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 294.6349\n",
      "Average Reward for Agent 3 this episode : -11.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.0566\n",
      "Average Reward for Agent 4 this episode : -8.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.8019\n",
      "Average Reward for Agent 5 this episode : -0.05\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2202\n",
      "Average Reward for Agent 6 this episode : -4.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.6499\n",
      "Average Reward for Agent 7 this episode : -13.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.2226\n",
      "Average Reward for Agent 8 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0150\n",
      "Average Reward for Agent 9 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1488\n",
      "Average Reward for Agent 10 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9188\n",
      "Average Reward for Agent 11 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5594\n",
      "Average Reward for Agent 12 this episode : -29.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0401\n",
      "Average Reward for Agent 13 this episode : -9.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.9416\n",
      "Reducing exploration for all agents to 0.2336\n",
      "Episode 85 is finished\n",
      "Average Reward for Agent 0 this episode : -8.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.2679\n",
      "Average Reward for Agent 1 this episode : -21.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.1210\n",
      "Average Reward for Agent 2 this episode : -22.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.4385\n",
      "Average Reward for Agent 3 this episode : -9.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2673\n",
      "Average Reward for Agent 4 this episode : -10.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.6238\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2755\n",
      "Average Reward for Agent 6 this episode : -8.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.9383\n",
      "Average Reward for Agent 7 this episode : -13.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.7247\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1726\n",
      "Average Reward for Agent 9 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5775\n",
      "Average Reward for Agent 10 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4064\n",
      "Average Reward for Agent 11 this episode : -3.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1969\n",
      "Average Reward for Agent 12 this episode : -18.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8015\n",
      "Average Reward for Agent 13 this episode : -10.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 128.7266\n",
      "Reducing exploration for all agents to 0.2296\n",
      "Episode 86 is finished\n",
      "Average Reward for Agent 0 this episode : -7.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.3523\n",
      "Average Reward for Agent 1 this episode : -22.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.1621\n",
      "Average Reward for Agent 2 this episode : -26.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.9471\n",
      "Average Reward for Agent 3 this episode : -6.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.1475\n",
      "Average Reward for Agent 4 this episode : -18.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2602\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1502\n",
      "Average Reward for Agent 6 this episode : -20.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.9628\n",
      "Average Reward for Agent 7 this episode : -13.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.6767\n",
      "Average Reward for Agent 8 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7360\n",
      "Average Reward for Agent 9 this episode : -1.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1762\n",
      "Average Reward for Agent 10 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6063\n",
      "Average Reward for Agent 11 this episode : -9.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7834\n",
      "Average Reward for Agent 12 this episode : -20.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.6897\n",
      "Average Reward for Agent 13 this episode : -15.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 143.3687\n",
      "Reducing exploration for all agents to 0.2256\n",
      "Episode 87 is finished\n",
      "Average Reward for Agent 0 this episode : -8.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.6064\n",
      "Average Reward for Agent 1 this episode : -24.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.6003\n",
      "Average Reward for Agent 2 this episode : -31.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 313.8816\n",
      "Average Reward for Agent 3 this episode : -9.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.5581\n",
      "Average Reward for Agent 4 this episode : -18.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.0427\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1473\n",
      "Average Reward for Agent 6 this episode : -25.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 149.6140\n",
      "Average Reward for Agent 7 this episode : -13.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.8418\n",
      "Average Reward for Agent 8 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1956\n",
      "Average Reward for Agent 9 this episode : -1.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6230\n",
      "Average Reward for Agent 10 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3710\n",
      "Average Reward for Agent 11 this episode : -7.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8871\n",
      "Average Reward for Agent 12 this episode : -21.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6010\n",
      "Average Reward for Agent 13 this episode : -7.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.7257\n",
      "Reducing exploration for all agents to 0.2218\n",
      "Episode 88 is finished\n",
      "Average Reward for Agent 0 this episode : -8.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0731\n",
      "Average Reward for Agent 1 this episode : -15.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.5801\n",
      "Average Reward for Agent 2 this episode : -33.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.3640\n",
      "Average Reward for Agent 3 this episode : -10.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.1989\n",
      "Average Reward for Agent 4 this episode : -18.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1663\n",
      "Average Reward for Agent 6 this episode : -9.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.8065\n",
      "Average Reward for Agent 7 this episode : -12.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.8809\n",
      "Average Reward for Agent 8 this episode : -2.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7962\n",
      "Average Reward for Agent 9 this episode : -1.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4988\n",
      "Average Reward for Agent 10 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1673\n",
      "Average Reward for Agent 11 this episode : -15.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.7426\n",
      "Average Reward for Agent 12 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.5888\n",
      "Average Reward for Agent 13 this episode : -2.24\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.4924\n",
      "Reducing exploration for all agents to 0.2179\n",
      "Episode 89 is finished\n",
      "Average Reward for Agent 0 this episode : -8.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.2050\n",
      "Average Reward for Agent 1 this episode : -11.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.0080\n",
      "Average Reward for Agent 2 this episode : -62.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 404.8332\n",
      "Average Reward for Agent 3 this episode : -9.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.1800\n",
      "Average Reward for Agent 4 this episode : -16.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.7205\n",
      "Average Reward for Agent 5 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2104\n",
      "Average Reward for Agent 6 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 187.8987\n",
      "Average Reward for Agent 7 this episode : -11.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.8792\n",
      "Average Reward for Agent 8 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2347\n",
      "Average Reward for Agent 9 this episode : -1.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1419\n",
      "Average Reward for Agent 10 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2871\n",
      "Average Reward for Agent 11 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6692\n",
      "Average Reward for Agent 12 this episode : -24.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.2642\n",
      "Average Reward for Agent 13 this episode : -12.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.3811\n",
      "Reducing exploration for all agents to 0.2142\n",
      "Episode 90 is finished\n",
      "Average Reward for Agent 0 this episode : -9.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.9404\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 169.9436\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -44.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 443.1603\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -9.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.5214\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -37.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.8644\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -4.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4101\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -10.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.0966\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -12.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.4106\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6620\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2599\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3085\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -31.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6639\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -7.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4210\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -4.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8341\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.2105\n",
      "Episode 91 is finished\n",
      "Average Reward for Agent 0 this episode : -8.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.8430\n",
      "Average Reward for Agent 1 this episode : -11.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.1935\n",
      "Average Reward for Agent 2 this episode : -44.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 605.3844\n",
      "Average Reward for Agent 3 this episode : -11.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.2007\n",
      "Average Reward for Agent 4 this episode : -19.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 179.1351\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9526\n",
      "Average Reward for Agent 6 this episode : -8.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.4241\n",
      "Average Reward for Agent 7 this episode : -13.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.6549\n",
      "Average Reward for Agent 8 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.8928\n",
      "Average Reward for Agent 9 this episode : -1.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1233\n",
      "Average Reward for Agent 10 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5144\n",
      "Average Reward for Agent 11 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.7889\n",
      "Average Reward for Agent 12 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.9381\n",
      "Average Reward for Agent 13 this episode : -32.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.4135\n",
      "Reducing exploration for all agents to 0.2069\n",
      "Episode 92 is finished\n",
      "Average Reward for Agent 0 this episode : -8.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.7230\n",
      "Average Reward for Agent 1 this episode : -9.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.1082\n",
      "Average Reward for Agent 2 this episode : -48.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 500.8323\n",
      "Average Reward for Agent 3 this episode : -11.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.6890\n",
      "Average Reward for Agent 4 this episode : -11.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1078\n",
      "Average Reward for Agent 5 this episode : -0.05\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2092\n",
      "Average Reward for Agent 6 this episode : -7.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.0928\n",
      "Average Reward for Agent 7 this episode : -12.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5719\n",
      "Average Reward for Agent 8 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0927\n",
      "Average Reward for Agent 9 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6058\n",
      "Average Reward for Agent 10 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8212\n",
      "Average Reward for Agent 11 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2163\n",
      "Average Reward for Agent 12 this episode : -14.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.3142\n",
      "Average Reward for Agent 13 this episode : -15.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.3223\n",
      "Reducing exploration for all agents to 0.2034\n",
      "Episode 93 is finished\n",
      "Average Reward for Agent 0 this episode : -9.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.1799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -9.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.8898\n",
      "Average Reward for Agent 2 this episode : -29.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 772.3987\n",
      "Average Reward for Agent 3 this episode : -14.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.9298\n",
      "Average Reward for Agent 4 this episode : -10.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.7427\n",
      "Average Reward for Agent 5 this episode : -3.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0408\n",
      "Average Reward for Agent 6 this episode : -9.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 149.3531\n",
      "Average Reward for Agent 7 this episode : -13.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.0032\n",
      "Average Reward for Agent 8 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2177\n",
      "Average Reward for Agent 9 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8104\n",
      "Average Reward for Agent 10 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4813\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.4915\n",
      "Average Reward for Agent 12 this episode : -27.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.3917\n",
      "Average Reward for Agent 13 this episode : -14.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.3860\n",
      "Reducing exploration for all agents to 0.1999\n",
      "Episode 94 is finished\n",
      "Average Reward for Agent 0 this episode : -9.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.6055\n",
      "Average Reward for Agent 1 this episode : -11.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.3887\n",
      "Average Reward for Agent 2 this episode : -29.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 445.8951\n",
      "Average Reward for Agent 3 this episode : -9.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.9861\n",
      "Average Reward for Agent 4 this episode : -8.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.3563\n",
      "Average Reward for Agent 5 this episode : -5.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9299\n",
      "Average Reward for Agent 6 this episode : -3.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.1188\n",
      "Average Reward for Agent 7 this episode : -12.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8512\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5438\n",
      "Average Reward for Agent 9 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1635\n",
      "Average Reward for Agent 10 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8124\n",
      "Average Reward for Agent 11 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.0046\n",
      "Average Reward for Agent 12 this episode : -31.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.1260\n",
      "Average Reward for Agent 13 this episode : -40.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.4155\n",
      "Reducing exploration for all agents to 0.1964\n",
      "Episode 95 is finished\n",
      "Average Reward for Agent 0 this episode : -8.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.8574\n",
      "Average Reward for Agent 1 this episode : -10.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.5822\n",
      "Average Reward for Agent 2 this episode : -33.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 414.8715\n",
      "Average Reward for Agent 3 this episode : -12.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.7488\n",
      "Average Reward for Agent 4 this episode : -12.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.0741\n",
      "Average Reward for Agent 5 this episode : -6.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6600\n",
      "Average Reward for Agent 6 this episode : -9.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.9325\n",
      "Average Reward for Agent 7 this episode : -15.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.9197\n",
      "Average Reward for Agent 8 this episode : -1.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3171\n",
      "Average Reward for Agent 9 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3736\n",
      "Average Reward for Agent 10 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1001\n",
      "Average Reward for Agent 11 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.9214\n",
      "Average Reward for Agent 12 this episode : -8.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.8355\n",
      "Average Reward for Agent 13 this episode : -34.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.2525\n",
      "Reducing exploration for all agents to 0.1931\n",
      "Episode 96 is finished\n",
      "Average Reward for Agent 0 this episode : -7.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.0062\n",
      "Average Reward for Agent 1 this episode : -41.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.2047\n",
      "Average Reward for Agent 2 this episode : -47.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 290.9095\n",
      "Average Reward for Agent 3 this episode : -9.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.8829\n",
      "Average Reward for Agent 4 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.6754\n",
      "Average Reward for Agent 5 this episode : -1.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6101\n",
      "Average Reward for Agent 6 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.5864\n",
      "Average Reward for Agent 7 this episode : -13.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.1112\n",
      "Average Reward for Agent 8 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2047\n",
      "Average Reward for Agent 9 this episode : -0.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2394\n",
      "Average Reward for Agent 10 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3076\n",
      "Average Reward for Agent 11 this episode : -2.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9920\n",
      "Average Reward for Agent 12 this episode : -17.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.1905\n",
      "Average Reward for Agent 13 this episode : -38.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.8929\n",
      "Reducing exploration for all agents to 0.1898\n",
      "Episode 97 is finished\n",
      "Average Reward for Agent 0 this episode : -9.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9846\n",
      "Average Reward for Agent 1 this episode : -42.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.8383\n",
      "Average Reward for Agent 2 this episode : -46.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 411.3826\n",
      "Average Reward for Agent 3 this episode : -11.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.4444\n",
      "Average Reward for Agent 4 this episode : -10.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3659\n",
      "Average Reward for Agent 5 this episode : -5.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3206\n",
      "Average Reward for Agent 6 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.3493\n",
      "Average Reward for Agent 7 this episode : -9.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4288\n",
      "Average Reward for Agent 8 this episode : -2.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1707\n",
      "Average Reward for Agent 9 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2253\n",
      "Average Reward for Agent 10 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7305\n",
      "Average Reward for Agent 11 this episode : -5.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.9407\n",
      "Average Reward for Agent 12 this episode : -21.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.2315\n",
      "Average Reward for Agent 13 this episode : -42.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.8273\n",
      "Reducing exploration for all agents to 0.1865\n",
      "Episode 98 is finished\n",
      "Average Reward for Agent 0 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.9213\n",
      "Average Reward for Agent 1 this episode : -17.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.4556\n",
      "Average Reward for Agent 2 this episode : -46.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 489.9577\n",
      "Average Reward for Agent 3 this episode : -10.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.5560\n",
      "Average Reward for Agent 4 this episode : -7.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.6664\n",
      "Average Reward for Agent 5 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6155\n",
      "Average Reward for Agent 6 this episode : -1.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.3955\n",
      "Average Reward for Agent 7 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.8884\n",
      "Average Reward for Agent 8 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3546\n",
      "Average Reward for Agent 9 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2150\n",
      "Average Reward for Agent 10 this episode : -12.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.2846\n",
      "Average Reward for Agent 11 this episode : -13.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.8341\n",
      "Average Reward for Agent 12 this episode : -19.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9239\n",
      "Average Reward for Agent 13 this episode : -11.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.7499\n",
      "Reducing exploration for all agents to 0.1833\n",
      "Episode 99 is finished\n",
      "Average Reward for Agent 0 this episode : -9.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.1495\n",
      "Average Reward for Agent 1 this episode : -39.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.9156\n",
      "Average Reward for Agent 2 this episode : -38.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 582.2767\n",
      "Average Reward for Agent 3 this episode : -12.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.3068\n",
      "Average Reward for Agent 4 this episode : -5.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.8274\n",
      "Average Reward for Agent 5 this episode : -1.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5338\n",
      "Average Reward for Agent 6 this episode : -1.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.2735\n",
      "Average Reward for Agent 7 this episode : -13.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4608\n",
      "Average Reward for Agent 8 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7032\n",
      "Average Reward for Agent 9 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3166\n",
      "Average Reward for Agent 10 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.1616\n",
      "Average Reward for Agent 11 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0405\n",
      "Average Reward for Agent 12 this episode : -22.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.7938\n",
      "Average Reward for Agent 13 this episode : -1.27\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8470\n",
      "Reducing exploration for all agents to 0.1802\n",
      "Episode 100 is finished\n",
      "Average Reward for Agent 0 this episode : -8.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.7292\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -36.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 256.7587\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -31.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 273.6862\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.8458\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.2152\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6584\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0457\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.9797\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6822\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7531\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -5.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2748\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -2.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.7641\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -15.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9198\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -14.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.5185\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1771\n",
      "Episode 101 is finished\n",
      "Average Reward for Agent 0 this episode : -9.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.7247\n",
      "Average Reward for Agent 1 this episode : -46.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 443.9505\n",
      "Average Reward for Agent 2 this episode : -36.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 515.9100\n",
      "Average Reward for Agent 3 this episode : -9.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.1766\n",
      "Average Reward for Agent 4 this episode : -8.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.1306\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.9220\n",
      "Average Reward for Agent 6 this episode : -1.27\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5525\n",
      "Average Reward for Agent 7 this episode : -12.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.3380\n",
      "Average Reward for Agent 8 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9940\n",
      "Average Reward for Agent 9 this episode : -0.21\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4272\n",
      "Average Reward for Agent 10 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4934\n",
      "Average Reward for Agent 11 this episode : -3.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.7974\n",
      "Average Reward for Agent 12 this episode : -24.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.5033\n",
      "Average Reward for Agent 13 this episode : -0.61\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.8967\n",
      "Reducing exploration for all agents to 0.174\n",
      "Episode 102 is finished\n",
      "Average Reward for Agent 0 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3274\n",
      "Average Reward for Agent 1 this episode : -14.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 777.5656\n",
      "Average Reward for Agent 2 this episode : -38.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 305.9509\n",
      "Average Reward for Agent 3 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.6860\n",
      "Average Reward for Agent 4 this episode : -13.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1919\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7176\n",
      "Average Reward for Agent 6 this episode : -2.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.4406\n",
      "Average Reward for Agent 7 this episode : -16.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8078\n",
      "Average Reward for Agent 8 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3792\n",
      "Average Reward for Agent 9 this episode : -3.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4623\n",
      "Average Reward for Agent 10 this episode : -4.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.4414\n",
      "Average Reward for Agent 11 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.4078\n",
      "Average Reward for Agent 12 this episode : -21.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.4602\n",
      "Average Reward for Agent 13 this episode : -11.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.7989\n",
      "Reducing exploration for all agents to 0.171\n",
      "Episode 103 is finished\n",
      "Average Reward for Agent 0 this episode : -7.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.8873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -20.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 275.6100\n",
      "Average Reward for Agent 2 this episode : -34.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 657.7899\n",
      "Average Reward for Agent 3 this episode : -11.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.4869\n",
      "Average Reward for Agent 4 this episode : -9.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.8808\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4436\n",
      "Average Reward for Agent 6 this episode : -2.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.4187\n",
      "Average Reward for Agent 7 this episode : -17.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.9753\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2627\n",
      "Average Reward for Agent 9 this episode : -13.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6926\n",
      "Average Reward for Agent 10 this episode : -3.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.9212\n",
      "Average Reward for Agent 11 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.8341\n",
      "Average Reward for Agent 12 this episode : -21.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.4547\n",
      "Average Reward for Agent 13 this episode : -7.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4933\n",
      "Reducing exploration for all agents to 0.1681\n",
      "Episode 104 is finished\n",
      "Average Reward for Agent 0 this episode : -8.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2878\n",
      "Average Reward for Agent 1 this episode : -28.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.7563\n",
      "Average Reward for Agent 2 this episode : -27.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 626.7217\n",
      "Average Reward for Agent 3 this episode : -10.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.1975\n",
      "Average Reward for Agent 4 this episode : -11.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.5786\n",
      "Average Reward for Agent 5 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8596\n",
      "Average Reward for Agent 6 this episode : -2.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.2059\n",
      "Average Reward for Agent 7 this episode : -17.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.3499\n",
      "Average Reward for Agent 8 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6637\n",
      "Average Reward for Agent 9 this episode : -0.2\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8247\n",
      "Average Reward for Agent 10 this episode : -31.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6633\n",
      "Average Reward for Agent 11 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6593\n",
      "Average Reward for Agent 12 this episode : -6.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.1335\n",
      "Average Reward for Agent 13 this episode : -3.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9439\n",
      "Reducing exploration for all agents to 0.1652\n",
      "Episode 105 is finished\n",
      "Average Reward for Agent 0 this episode : -8.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1367\n",
      "Average Reward for Agent 1 this episode : -24.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 314.7083\n",
      "Average Reward for Agent 2 this episode : -29.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 411.4998\n",
      "Average Reward for Agent 3 this episode : -13.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.8435\n",
      "Average Reward for Agent 4 this episode : -12.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.3437\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6808\n",
      "Average Reward for Agent 6 this episode : -4.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.8489\n",
      "Average Reward for Agent 7 this episode : -13.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.2312\n",
      "Average Reward for Agent 8 this episode : -11.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6494\n",
      "Average Reward for Agent 9 this episode : -1.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.7301\n",
      "Average Reward for Agent 10 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.5310\n",
      "Average Reward for Agent 11 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4304\n",
      "Average Reward for Agent 12 this episode : -15.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.8421\n",
      "Average Reward for Agent 13 this episode : -3.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.0696\n",
      "Reducing exploration for all agents to 0.1624\n",
      "Episode 106 is finished\n",
      "Average Reward for Agent 0 this episode : -8.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5531\n",
      "Average Reward for Agent 1 this episode : -27.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 281.9355\n",
      "Average Reward for Agent 2 this episode : -45.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 454.7802\n",
      "Average Reward for Agent 3 this episode : -12.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.4284\n",
      "Average Reward for Agent 4 this episode : -12.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8942\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1956\n",
      "Average Reward for Agent 6 this episode : -2.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.4248\n",
      "Average Reward for Agent 7 this episode : -17.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.7350\n",
      "Average Reward for Agent 8 this episode : -5.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9568\n",
      "Average Reward for Agent 9 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.1736\n",
      "Average Reward for Agent 10 this episode : -6.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.4526\n",
      "Average Reward for Agent 11 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3566\n",
      "Average Reward for Agent 12 this episode : -10.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.0267\n",
      "Average Reward for Agent 13 this episode : -4.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2761\n",
      "Reducing exploration for all agents to 0.1596\n",
      "Episode 107 is finished\n",
      "Average Reward for Agent 0 this episode : -9.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7504\n",
      "Average Reward for Agent 1 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 248.6133\n",
      "Average Reward for Agent 2 this episode : -47.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 478.6785\n",
      "Average Reward for Agent 3 this episode : -13.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5196\n",
      "Average Reward for Agent 4 this episode : -11.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.3845\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9064\n",
      "Average Reward for Agent 6 this episode : -2.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.4644\n",
      "Average Reward for Agent 7 this episode : -14.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.0920\n",
      "Average Reward for Agent 8 this episode : -19.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.8822\n",
      "Average Reward for Agent 9 this episode : -0.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5503\n",
      "Average Reward for Agent 10 this episode : -7.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.7024\n",
      "Average Reward for Agent 11 this episode : -0.26\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1211\n",
      "Average Reward for Agent 12 this episode : -22.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.5996\n",
      "Average Reward for Agent 13 this episode : -9.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0825\n",
      "Reducing exploration for all agents to 0.1569\n",
      "Episode 108 is finished\n",
      "Average Reward for Agent 0 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.4089\n",
      "Average Reward for Agent 1 this episode : -17.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.1480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 2 this episode : -25.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 496.1916\n",
      "Average Reward for Agent 3 this episode : -11.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.0211\n",
      "Average Reward for Agent 4 this episode : -9.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.9175\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4250\n",
      "Average Reward for Agent 6 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.7786\n",
      "Average Reward for Agent 7 this episode : -19.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.2845\n",
      "Average Reward for Agent 8 this episode : -13.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.9949\n",
      "Average Reward for Agent 9 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0371\n",
      "Average Reward for Agent 10 this episode : -7.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9510\n",
      "Average Reward for Agent 11 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1047\n",
      "Average Reward for Agent 12 this episode : -20.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.3482\n",
      "Average Reward for Agent 13 this episode : -10.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.5930\n",
      "Reducing exploration for all agents to 0.1542\n",
      "Episode 109 is finished\n",
      "Average Reward for Agent 0 this episode : -9.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.9739\n",
      "Average Reward for Agent 1 this episode : -11.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 345.4435\n",
      "Average Reward for Agent 2 this episode : -28.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 451.2758\n",
      "Average Reward for Agent 3 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.0856\n",
      "Average Reward for Agent 4 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.5045\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1914\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.7945\n",
      "Average Reward for Agent 7 this episode : -15.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4867\n",
      "Average Reward for Agent 8 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4285\n",
      "Average Reward for Agent 9 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2762\n",
      "Average Reward for Agent 10 this episode : -5.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.8089\n",
      "Average Reward for Agent 11 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8675\n",
      "Average Reward for Agent 12 this episode : -17.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.2831\n",
      "Average Reward for Agent 13 this episode : -10.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.0885\n",
      "Reducing exploration for all agents to 0.1515\n",
      "Episode 110 is finished\n",
      "Average Reward for Agent 0 this episode : -8.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.1596\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.0621\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -28.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 331.2689\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7776\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -9.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.2120\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8063\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.4255\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -16.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.5046\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -2.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.3937\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3018\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -10.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.3476\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2793\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -9.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.2993\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -11.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.2213\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1489\n",
      "Episode 111 is finished\n",
      "Average Reward for Agent 0 this episode : -9.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.1971\n",
      "Average Reward for Agent 1 this episode : -12.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 300.1627\n",
      "Average Reward for Agent 2 this episode : -24.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 480.3204\n",
      "Average Reward for Agent 3 this episode : -12.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1811\n",
      "Average Reward for Agent 4 this episode : -11.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.2712\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7234\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.4114\n",
      "Average Reward for Agent 7 this episode : -13.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.8745\n",
      "Average Reward for Agent 8 this episode : -5.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.0515\n",
      "Average Reward for Agent 9 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3240\n",
      "Average Reward for Agent 10 this episode : -27.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.6723\n",
      "Average Reward for Agent 11 this episode : -0.26\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5939\n",
      "Average Reward for Agent 12 this episode : -24.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.4932\n",
      "Average Reward for Agent 13 this episode : -13.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.9725\n",
      "Reducing exploration for all agents to 0.1464\n",
      "Episode 112 is finished\n",
      "Average Reward for Agent 0 this episode : -9.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5919\n",
      "Average Reward for Agent 1 this episode : -14.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.1122\n",
      "Average Reward for Agent 2 this episode : -24.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 344.7686\n",
      "Average Reward for Agent 3 this episode : -12.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.5210\n",
      "Average Reward for Agent 4 this episode : -7.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.6640\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4429\n",
      "Average Reward for Agent 6 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.4908\n",
      "Average Reward for Agent 7 this episode : -17.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.6770\n",
      "Average Reward for Agent 8 this episode : -7.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.3301\n",
      "Average Reward for Agent 9 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 10 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.7953\n",
      "Average Reward for Agent 11 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1295\n",
      "Average Reward for Agent 12 this episode : -26.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5867\n",
      "Average Reward for Agent 13 this episode : -19.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.4882\n",
      "Reducing exploration for all agents to 0.1438\n",
      "Episode 113 is finished\n",
      "Average Reward for Agent 0 this episode : -9.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.4275\n",
      "Average Reward for Agent 1 this episode : -12.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.5827\n",
      "Average Reward for Agent 2 this episode : -20.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.2021\n",
      "Average Reward for Agent 3 this episode : -13.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8288\n",
      "Average Reward for Agent 4 this episode : -9.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.0076\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2766\n",
      "Average Reward for Agent 6 this episode : -5.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.5862\n",
      "Average Reward for Agent 7 this episode : -15.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.2084\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8843\n",
      "Average Reward for Agent 9 this episode : -0.19\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2039\n",
      "Average Reward for Agent 10 this episode : -8.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.8773\n",
      "Average Reward for Agent 11 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6805\n",
      "Average Reward for Agent 12 this episode : -8.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.3472\n",
      "Average Reward for Agent 13 this episode : -12.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.9238\n",
      "Reducing exploration for all agents to 0.1414\n",
      "Episode 114 is finished\n",
      "Average Reward for Agent 0 this episode : -10.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.7989\n",
      "Average Reward for Agent 1 this episode : -13.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.4087\n",
      "Average Reward for Agent 2 this episode : -18.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 450.7555\n",
      "Average Reward for Agent 3 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7651\n",
      "Average Reward for Agent 4 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.8821\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5424\n",
      "Average Reward for Agent 6 this episode : -16.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 138.2957\n",
      "Average Reward for Agent 7 this episode : -19.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.2563\n",
      "Average Reward for Agent 8 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1542\n",
      "Average Reward for Agent 9 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1737\n",
      "Average Reward for Agent 10 this episode : -9.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.9257\n",
      "Average Reward for Agent 11 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4096\n",
      "Average Reward for Agent 12 this episode : -16.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.9460\n",
      "Average Reward for Agent 13 this episode : -14.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.9304\n",
      "Reducing exploration for all agents to 0.1389\n",
      "Episode 115 is finished\n",
      "Average Reward for Agent 0 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.9924\n",
      "Average Reward for Agent 1 this episode : -11.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.7609\n",
      "Average Reward for Agent 2 this episode : -21.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 427.4049\n",
      "Average Reward for Agent 3 this episode : -10.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9349\n",
      "Average Reward for Agent 4 this episode : -10.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.8487\n",
      "Average Reward for Agent 5 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4105\n",
      "Average Reward for Agent 6 this episode : -13.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.3986\n",
      "Average Reward for Agent 7 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.4377\n",
      "Average Reward for Agent 8 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9995\n",
      "Average Reward for Agent 9 this episode : -10.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.1518\n",
      "Average Reward for Agent 10 this episode : -5.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.7222\n",
      "Average Reward for Agent 11 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1731\n",
      "Average Reward for Agent 12 this episode : -6.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.8069\n",
      "Average Reward for Agent 13 this episode : -25.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.5088\n",
      "Reducing exploration for all agents to 0.1366\n",
      "Episode 116 is finished\n",
      "Average Reward for Agent 0 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.1498\n",
      "Average Reward for Agent 1 this episode : -14.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1288\n",
      "Average Reward for Agent 2 this episode : -19.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 327.5681\n",
      "Average Reward for Agent 3 this episode : -11.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.6469\n",
      "Average Reward for Agent 4 this episode : -19.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.0099\n",
      "Average Reward for Agent 5 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5957\n",
      "Average Reward for Agent 6 this episode : -4.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5341\n",
      "Average Reward for Agent 7 this episode : -16.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.8053\n",
      "Average Reward for Agent 8 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6691\n",
      "Average Reward for Agent 9 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5767\n",
      "Average Reward for Agent 10 this episode : -26.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0004\n",
      "Average Reward for Agent 11 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3973\n",
      "Average Reward for Agent 12 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.1276\n",
      "Average Reward for Agent 13 this episode : -11.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.8553\n",
      "Reducing exploration for all agents to 0.1342\n",
      "Episode 117 is finished\n",
      "Average Reward for Agent 0 this episode : -9.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.2757\n",
      "Average Reward for Agent 1 this episode : -14.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 154.6555\n",
      "Average Reward for Agent 2 this episode : -27.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.7051\n",
      "Average Reward for Agent 3 this episode : -11.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.8511\n",
      "Average Reward for Agent 4 this episode : -10.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.6099\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6932\n",
      "Average Reward for Agent 6 this episode : -19.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.2683\n",
      "Average Reward for Agent 7 this episode : -14.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.9581\n",
      "Average Reward for Agent 8 this episode : -2.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8256\n",
      "Average Reward for Agent 9 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.6914\n",
      "Average Reward for Agent 10 this episode : -13.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.2145\n",
      "Average Reward for Agent 11 this episode : -0.22\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5702\n",
      "Average Reward for Agent 12 this episode : -22.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9177\n",
      "Average Reward for Agent 13 this episode : -4.71\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 0s - loss: 111.0916\n",
      "Reducing exploration for all agents to 0.1319\n",
      "Episode 118 is finished\n",
      "Average Reward for Agent 0 this episode : -9.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4038\n",
      "Average Reward for Agent 1 this episode : -14.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 240.4838\n",
      "Average Reward for Agent 2 this episode : -22.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 338.6398\n",
      "Average Reward for Agent 3 this episode : -9.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.9163\n",
      "Average Reward for Agent 4 this episode : -10.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.6828\n",
      "Average Reward for Agent 5 this episode : -0.05\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8418\n",
      "Average Reward for Agent 6 this episode : -8.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.3615\n",
      "Average Reward for Agent 7 this episode : -18.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.5600\n",
      "Average Reward for Agent 8 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4481\n",
      "Average Reward for Agent 9 this episode : -2.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6753\n",
      "Average Reward for Agent 10 this episode : -19.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.9623\n",
      "Average Reward for Agent 11 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6743\n",
      "Average Reward for Agent 12 this episode : -34.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.0684\n",
      "Average Reward for Agent 13 this episode : -10.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.4311\n",
      "Reducing exploration for all agents to 0.1297\n",
      "Episode 119 is finished\n",
      "Average Reward for Agent 0 this episode : -10.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.0500\n",
      "Average Reward for Agent 1 this episode : -11.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 134.5589\n",
      "Average Reward for Agent 2 this episode : -24.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 355.4579\n",
      "Average Reward for Agent 3 this episode : -11.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.0218\n",
      "Average Reward for Agent 4 this episode : -13.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.8388\n",
      "Average Reward for Agent 5 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3705\n",
      "Average Reward for Agent 6 this episode : -4.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.0537\n",
      "Average Reward for Agent 7 this episode : -21.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.6479\n",
      "Average Reward for Agent 8 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5148\n",
      "Average Reward for Agent 9 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4797\n",
      "Average Reward for Agent 10 this episode : -33.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7569\n",
      "Average Reward for Agent 11 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2289\n",
      "Average Reward for Agent 12 this episode : -8.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.4214\n",
      "Average Reward for Agent 13 this episode : -5.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.6924\n",
      "Reducing exploration for all agents to 0.1274\n",
      "Episode 120 is finished\n",
      "Average Reward for Agent 0 this episode : -9.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.5277\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.7924\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -20.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 252.5289\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3449\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -13.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0953\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6480\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -17.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.3422\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -16.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.4857\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3621\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0891\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -18.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.1189\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5859\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -8.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.9498\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.6649\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1252\n",
      "Episode 121 is finished\n",
      "Average Reward for Agent 0 this episode : -9.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7842\n",
      "Average Reward for Agent 1 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 272.7328\n",
      "Average Reward for Agent 2 this episode : -25.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 259.7020\n",
      "Average Reward for Agent 3 this episode : -11.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.5329\n",
      "Average Reward for Agent 4 this episode : -13.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.1075\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3409\n",
      "Average Reward for Agent 6 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 159.8240\n",
      "Average Reward for Agent 7 this episode : -19.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.2504\n",
      "Average Reward for Agent 8 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2346\n",
      "Average Reward for Agent 9 this episode : -2.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7198\n",
      "Average Reward for Agent 10 this episode : -10.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.8700\n",
      "Average Reward for Agent 11 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7142\n",
      "Average Reward for Agent 12 this episode : -14.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.4628\n",
      "Average Reward for Agent 13 this episode : -24.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.3158\n",
      "Reducing exploration for all agents to 0.1231\n",
      "Episode 122 is finished\n",
      "Average Reward for Agent 0 this episode : -9.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.7221\n",
      "Average Reward for Agent 1 this episode : -9.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.0439\n",
      "Average Reward for Agent 2 this episode : -26.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 254.0466\n",
      "Average Reward for Agent 3 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.9907\n",
      "Average Reward for Agent 4 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.5651\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8094\n",
      "Average Reward for Agent 6 this episode : -3.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.2324\n",
      "Average Reward for Agent 7 this episode : -12.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.4580\n",
      "Average Reward for Agent 8 this episode : -3.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5474\n",
      "Average Reward for Agent 9 this episode : -6.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6637\n",
      "Average Reward for Agent 10 this episode : -37.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.2553\n",
      "Average Reward for Agent 11 this episode : -0.21\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8047\n",
      "Average Reward for Agent 12 this episode : -23.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.4569\n",
      "Average Reward for Agent 13 this episode : -33.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.8428\n",
      "Reducing exploration for all agents to 0.121\n",
      "Episode 123 is finished\n",
      "Average Reward for Agent 0 this episode : -12.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.0586\n",
      "Average Reward for Agent 1 this episode : -13.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.4013\n",
      "Average Reward for Agent 2 this episode : -31.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.4885\n",
      "Average Reward for Agent 3 this episode : -8.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2137\n",
      "Average Reward for Agent 4 this episode : -10.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.0285\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0058\n",
      "Average Reward for Agent 6 this episode : -17.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.1698\n",
      "Average Reward for Agent 7 this episode : -16.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.5508\n",
      "Average Reward for Agent 8 this episode : -2.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3001\n",
      "Average Reward for Agent 9 this episode : -1.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.0965\n",
      "Average Reward for Agent 10 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.5509\n",
      "Average Reward for Agent 11 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1929\n",
      "Average Reward for Agent 12 this episode : -14.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.4543\n",
      "Average Reward for Agent 13 this episode : -15.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.1019\n",
      "Reducing exploration for all agents to 0.1189\n",
      "Episode 124 is finished\n",
      "Average Reward for Agent 0 this episode : -15.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.9922\n",
      "Average Reward for Agent 1 this episode : -10.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.6081\n",
      "Average Reward for Agent 2 this episode : -36.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 253.5570\n",
      "Average Reward for Agent 3 this episode : -11.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.2434\n",
      "Average Reward for Agent 4 this episode : -14.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.7141\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8258\n",
      "Average Reward for Agent 6 this episode : -31.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.4911\n",
      "Average Reward for Agent 7 this episode : -13.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.1202\n",
      "Average Reward for Agent 8 this episode : -3.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6540\n",
      "Average Reward for Agent 9 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6133\n",
      "Average Reward for Agent 10 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.5572\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1185\n",
      "Average Reward for Agent 12 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1118\n",
      "Average Reward for Agent 13 this episode : -26.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.4500\n",
      "Reducing exploration for all agents to 0.1169\n",
      "Episode 125 is finished\n",
      "Average Reward for Agent 0 this episode : -11.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.2149\n",
      "Average Reward for Agent 1 this episode : -8.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.7382\n",
      "Average Reward for Agent 2 this episode : -39.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.5215\n",
      "Average Reward for Agent 3 this episode : -10.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.8299\n",
      "Average Reward for Agent 4 this episode : -19.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 177.1325\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7100\n",
      "Average Reward for Agent 6 this episode : -21.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.6462\n",
      "Average Reward for Agent 7 this episode : -14.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0597\n",
      "Average Reward for Agent 8 this episode : -1.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2115\n",
      "Average Reward for Agent 9 this episode : -3.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6614\n",
      "Average Reward for Agent 10 this episode : -4.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.6152\n",
      "Average Reward for Agent 11 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6843\n",
      "Average Reward for Agent 12 this episode : -10.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.1967\n",
      "Average Reward for Agent 13 this episode : -46.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 297.6099\n",
      "Reducing exploration for all agents to 0.1149\n",
      "Episode 126 is finished\n",
      "Average Reward for Agent 0 this episode : -10.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.7406\n",
      "Average Reward for Agent 1 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.0051\n",
      "Average Reward for Agent 2 this episode : -34.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.1263\n",
      "Average Reward for Agent 3 this episode : -12.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.5994\n",
      "Average Reward for Agent 4 this episode : -15.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.8290\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4724\n",
      "Average Reward for Agent 6 this episode : -18.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.9194\n",
      "Average Reward for Agent 7 this episode : -18.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.3099\n",
      "Average Reward for Agent 8 this episode : -1.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3559\n",
      "Average Reward for Agent 9 this episode : -4.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9007\n",
      "Average Reward for Agent 10 this episode : -14.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.1657\n",
      "Average Reward for Agent 11 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3718\n",
      "Average Reward for Agent 12 this episode : -9.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.3736\n",
      "Average Reward for Agent 13 this episode : -36.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.0430\n",
      "Reducing exploration for all agents to 0.1129\n",
      "Episode 127 is finished\n",
      "Average Reward for Agent 0 this episode : -11.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.5913\n",
      "Average Reward for Agent 1 this episode : -10.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.3027\n",
      "Average Reward for Agent 2 this episode : -36.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.9022\n",
      "Average Reward for Agent 3 this episode : -12.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.4746\n",
      "Average Reward for Agent 4 this episode : -14.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.8215\n",
      "Average Reward for Agent 5 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6509\n",
      "Average Reward for Agent 6 this episode : -13.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.6254\n",
      "Average Reward for Agent 7 this episode : -16.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.5834\n",
      "Average Reward for Agent 8 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6961\n",
      "Average Reward for Agent 9 this episode : -1.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7261\n",
      "Average Reward for Agent 10 this episode : -16.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.9430\n",
      "Average Reward for Agent 11 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2000\n",
      "Average Reward for Agent 12 this episode : -13.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.5738\n",
      "Average Reward for Agent 13 this episode : -63.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 338.8625\n",
      "Reducing exploration for all agents to 0.1109\n",
      "Episode 128 is finished\n",
      "Average Reward for Agent 0 this episode : -12.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.0928\n",
      "Average Reward for Agent 1 this episode : -12.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.8273\n",
      "Average Reward for Agent 2 this episode : -40.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 364.6086\n",
      "Average Reward for Agent 3 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.7457\n",
      "Average Reward for Agent 4 this episode : -15.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.3601\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.3181\n",
      "Average Reward for Agent 7 this episode : -16.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.4150\n",
      "Average Reward for Agent 8 this episode : -2.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6721\n",
      "Average Reward for Agent 9 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9562\n",
      "Average Reward for Agent 10 this episode : -22.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.8817\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6961\n",
      "Average Reward for Agent 12 this episode : -20.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.9244\n",
      "Average Reward for Agent 13 this episode : -25.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.5449\n",
      "Reducing exploration for all agents to 0.109\n",
      "Episode 129 is finished\n",
      "Average Reward for Agent 0 this episode : -12.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.3337\n",
      "Average Reward for Agent 1 this episode : -14.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.3675\n",
      "Average Reward for Agent 2 this episode : -30.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 539.2883\n",
      "Average Reward for Agent 3 this episode : -13.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.4700\n",
      "Average Reward for Agent 4 this episode : -10.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.7285\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7786\n",
      "Average Reward for Agent 6 this episode : -43.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.6520\n",
      "Average Reward for Agent 7 this episode : -17.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.4130\n",
      "Average Reward for Agent 8 this episode : -4.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0947\n",
      "Average Reward for Agent 9 this episode : -1.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5173\n",
      "Average Reward for Agent 10 this episode : -25.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 187.5630\n",
      "Average Reward for Agent 11 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7494\n",
      "Average Reward for Agent 12 this episode : -12.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.3526\n",
      "Average Reward for Agent 13 this episode : -11.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.9781\n",
      "Reducing exploration for all agents to 0.1072\n",
      "Episode 130 is finished\n",
      "Average Reward for Agent 0 this episode : -13.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.8640\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.0603\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -45.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 554.7793\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -11.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4765\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -10.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.8462\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0096\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -25.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.5352\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -19.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.8876\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0420\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -2.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3624\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -8.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3658\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5223\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -16.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.0676\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -21.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.7944\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1053\n",
      "Episode 131 is finished\n",
      "Average Reward for Agent 0 this episode : -15.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2003\n",
      "Average Reward for Agent 1 this episode : -13.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.7044\n",
      "Average Reward for Agent 2 this episode : -40.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 816.4365\n",
      "Average Reward for Agent 3 this episode : -10.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.6545\n",
      "Average Reward for Agent 4 this episode : -12.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.1606\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5856\n",
      "Average Reward for Agent 6 this episode : -11.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 347.2599\n",
      "Average Reward for Agent 7 this episode : -13.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.7036\n",
      "Average Reward for Agent 8 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8147\n",
      "Average Reward for Agent 9 this episode : -3.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5860\n",
      "Average Reward for Agent 10 this episode : -22.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 242.1912\n",
      "Average Reward for Agent 11 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5818\n",
      "Average Reward for Agent 12 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.8863\n",
      "Average Reward for Agent 13 this episode : -36.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 361.5804\n",
      "Reducing exploration for all agents to 0.1035\n",
      "Episode 132 is finished\n",
      "Average Reward for Agent 0 this episode : -17.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.4874\n",
      "Average Reward for Agent 1 this episode : -16.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.5872\n",
      "Average Reward for Agent 2 this episode : -35.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 454.0334\n",
      "Average Reward for Agent 3 this episode : -11.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.9429\n",
      "Average Reward for Agent 4 this episode : -11.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.6877\n",
      "Average Reward for Agent 5 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3403\n",
      "Average Reward for Agent 6 this episode : -14.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.0648\n",
      "Average Reward for Agent 7 this episode : -14.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.4116\n",
      "Average Reward for Agent 8 this episode : -3.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1464\n",
      "Average Reward for Agent 9 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2419\n",
      "Average Reward for Agent 10 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.9790\n",
      "Average Reward for Agent 11 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2775\n",
      "Average Reward for Agent 12 this episode : -15.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.3260\n",
      "Average Reward for Agent 13 this episode : -19.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.5580\n",
      "Reducing exploration for all agents to 0.1017\n",
      "Episode 133 is finished\n",
      "Average Reward for Agent 0 this episode : -20.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.0838\n",
      "Average Reward for Agent 1 this episode : -20.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.4754\n",
      "Average Reward for Agent 2 this episode : -45.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 607.2479\n",
      "Average Reward for Agent 3 this episode : -9.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0577\n",
      "Average Reward for Agent 4 this episode : -12.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.6930\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9115\n",
      "Average Reward for Agent 6 this episode : -20.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.3904\n",
      "Average Reward for Agent 7 this episode : -18.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -2.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5067\n",
      "Average Reward for Agent 9 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.2159\n",
      "Average Reward for Agent 10 this episode : -9.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.8507\n",
      "Average Reward for Agent 11 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1381\n",
      "Average Reward for Agent 12 this episode : -15.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.1842\n",
      "Average Reward for Agent 13 this episode : -8.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 186.1469\n",
      "Reducing exploration for all agents to 0.1\n",
      "Episode 134 is finished\n",
      "Average Reward for Agent 0 this episode : -34.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.7716\n",
      "Average Reward for Agent 1 this episode : -18.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 269.9113\n",
      "Average Reward for Agent 2 this episode : -39.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 555.9478\n",
      "Average Reward for Agent 3 this episode : -9.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.1607\n",
      "Average Reward for Agent 4 this episode : -9.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.6542\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5934\n",
      "Average Reward for Agent 6 this episode : -27.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.9645\n",
      "Average Reward for Agent 7 this episode : -23.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.8542\n",
      "Average Reward for Agent 8 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.9918\n",
      "Average Reward for Agent 9 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8317\n",
      "Average Reward for Agent 10 this episode : -3.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 226.5485\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7960\n",
      "Average Reward for Agent 12 this episode : -17.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.1319\n",
      "Average Reward for Agent 13 this episode : -3.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 271.0791\n",
      "Reducing exploration for all agents to 0.0983\n",
      "Episode 135 is finished\n",
      "Average Reward for Agent 0 this episode : -25.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.5821\n",
      "Average Reward for Agent 1 this episode : -19.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.3840\n",
      "Average Reward for Agent 2 this episode : -56.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 440.4575\n",
      "Average Reward for Agent 3 this episode : -9.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4623\n",
      "Average Reward for Agent 4 this episode : -13.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.2531\n",
      "Average Reward for Agent 5 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7556\n",
      "Average Reward for Agent 6 this episode : -5.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.1618\n",
      "Average Reward for Agent 7 this episode : -13.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.0922\n",
      "Average Reward for Agent 8 this episode : -1.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3633\n",
      "Average Reward for Agent 9 this episode : -2.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9084\n",
      "Average Reward for Agent 10 this episode : -14.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.6552\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5592\n",
      "Average Reward for Agent 12 this episode : -23.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.7058\n",
      "Average Reward for Agent 13 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.3850\n",
      "Reducing exploration for all agents to 0.0966\n",
      "Episode 136 is finished\n",
      "Average Reward for Agent 0 this episode : -27.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2780\n",
      "Average Reward for Agent 1 this episode : -17.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 279.2607\n",
      "Average Reward for Agent 2 this episode : -32.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 618.7347\n",
      "Average Reward for Agent 3 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.7784\n",
      "Average Reward for Agent 4 this episode : -14.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2576\n",
      "Average Reward for Agent 5 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4208\n",
      "Average Reward for Agent 6 this episode : -3.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.6112\n",
      "Average Reward for Agent 7 this episode : -20.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.3927\n",
      "Average Reward for Agent 8 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0568\n",
      "Average Reward for Agent 9 this episode : -3.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3507\n",
      "Average Reward for Agent 10 this episode : -26.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.8879\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2059\n",
      "Average Reward for Agent 12 this episode : -45.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.8862\n",
      "Average Reward for Agent 13 this episode : -10.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2828\n",
      "Reducing exploration for all agents to 0.0949\n",
      "Episode 137 is finished\n",
      "Average Reward for Agent 0 this episode : -40.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.7119\n",
      "Average Reward for Agent 1 this episode : -20.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.8420\n",
      "Average Reward for Agent 2 this episode : -32.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 660.7380\n",
      "Average Reward for Agent 3 this episode : -9.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.0016\n",
      "Average Reward for Agent 4 this episode : -12.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.8732\n",
      "Average Reward for Agent 5 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7130\n",
      "Average Reward for Agent 6 this episode : -4.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.6939\n",
      "Average Reward for Agent 7 this episode : -16.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0918\n",
      "Average Reward for Agent 8 this episode : -6.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9398\n",
      "Average Reward for Agent 9 this episode : -5.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2458\n",
      "Average Reward for Agent 10 this episode : -0.33\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.8619\n",
      "Average Reward for Agent 11 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5647\n",
      "Average Reward for Agent 12 this episode : -19.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.2724\n",
      "Average Reward for Agent 13 this episode : -34.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.6410\n",
      "Reducing exploration for all agents to 0.0933\n",
      "Episode 138 is finished\n",
      "Average Reward for Agent 0 this episode : -32.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.3810\n",
      "Average Reward for Agent 1 this episode : -23.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.4537\n",
      "Average Reward for Agent 2 this episode : -40.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 594.4152\n",
      "Average Reward for Agent 3 this episode : -8.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.0068\n",
      "Average Reward for Agent 4 this episode : -22.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.4168\n",
      "Average Reward for Agent 5 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9277\n",
      "Average Reward for Agent 6 this episode : -6.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.0925\n",
      "Average Reward for Agent 7 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.2283\n",
      "Average Reward for Agent 8 this episode : -4.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8164\n",
      "Average Reward for Agent 9 this episode : -2.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4612\n",
      "Average Reward for Agent 10 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.5463\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0411\n",
      "Average Reward for Agent 12 this episode : -26.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2586\n",
      "Average Reward for Agent 13 this episode : -7.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.0818\n",
      "Reducing exploration for all agents to 0.0917\n",
      "Episode 139 is finished\n",
      "Average Reward for Agent 0 this episode : -16.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.0996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -22.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 220.4361\n",
      "Average Reward for Agent 2 this episode : -40.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 514.3062\n",
      "Average Reward for Agent 3 this episode : -9.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.8752\n",
      "Average Reward for Agent 4 this episode : -24.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.9813\n",
      "Average Reward for Agent 5 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9278\n",
      "Average Reward for Agent 6 this episode : -10.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.8856\n",
      "Average Reward for Agent 7 this episode : -13.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.2487\n",
      "Average Reward for Agent 8 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7879\n",
      "Average Reward for Agent 9 this episode : -5.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7489\n",
      "Average Reward for Agent 10 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.5300\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1720\n",
      "Average Reward for Agent 12 this episode : -16.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.0182\n",
      "Average Reward for Agent 13 this episode : -4.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.7816\n",
      "Reducing exploration for all agents to 0.0901\n",
      "Episode 140 is finished\n",
      "Average Reward for Agent 0 this episode : -13.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2213\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -22.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 261.8577\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -39.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.5575\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -9.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.4563\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.9321\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3554\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.9827\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2933\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5718\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -2.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.0217\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.5684\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7742\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -16.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8135\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -7.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.1974\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0886\n",
      "Episode 141 is finished\n",
      "Average Reward for Agent 0 this episode : -21.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.4820\n",
      "Average Reward for Agent 1 this episode : -20.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.6506\n",
      "Average Reward for Agent 2 this episode : -43.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 635.2786\n",
      "Average Reward for Agent 3 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.0836\n",
      "Average Reward for Agent 4 this episode : -17.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.4916\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1328\n",
      "Average Reward for Agent 6 this episode : -19.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.0569\n",
      "Average Reward for Agent 7 this episode : -12.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.3497\n",
      "Average Reward for Agent 8 this episode : -2.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.5164\n",
      "Average Reward for Agent 9 this episode : -3.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4454\n",
      "Average Reward for Agent 10 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.5669\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8526\n",
      "Average Reward for Agent 12 this episode : -14.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.2121\n",
      "Average Reward for Agent 13 this episode : -12.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.1836\n",
      "Reducing exploration for all agents to 0.0871\n",
      "Episode 142 is finished\n",
      "Average Reward for Agent 0 this episode : -15.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.7914\n",
      "Average Reward for Agent 1 this episode : -15.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 302.6029\n",
      "Average Reward for Agent 2 this episode : -20.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 270.8188\n",
      "Average Reward for Agent 3 this episode : -10.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.2847\n",
      "Average Reward for Agent 4 this episode : -21.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.6452\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9087\n",
      "Average Reward for Agent 6 this episode : -3.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.7460\n",
      "Average Reward for Agent 7 this episode : -12.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.5089\n",
      "Average Reward for Agent 8 this episode : -1.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8219\n",
      "Average Reward for Agent 9 this episode : -1.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9639\n",
      "Average Reward for Agent 10 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.9628\n",
      "Average Reward for Agent 11 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9264\n",
      "Average Reward for Agent 12 this episode : -13.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.0753\n",
      "Average Reward for Agent 13 this episode : -44.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8513\n",
      "Reducing exploration for all agents to 0.0856\n",
      "Episode 143 is finished\n",
      "Average Reward for Agent 0 this episode : -10.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1525\n",
      "Average Reward for Agent 1 this episode : -22.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 262.0148\n",
      "Average Reward for Agent 2 this episode : -21.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 257.1731\n",
      "Average Reward for Agent 3 this episode : -8.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.0769\n",
      "Average Reward for Agent 4 this episode : -19.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.1788\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2429\n",
      "Average Reward for Agent 6 this episode : -5.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.4044\n",
      "Average Reward for Agent 7 this episode : -14.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.5975\n",
      "Average Reward for Agent 8 this episode : -4.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0891\n",
      "Average Reward for Agent 9 this episode : -3.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4222\n",
      "Average Reward for Agent 10 this episode : -1.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.3415\n",
      "Average Reward for Agent 11 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9361\n",
      "Average Reward for Agent 12 this episode : -14.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.5725\n",
      "Average Reward for Agent 13 this episode : -59.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.6206\n",
      "Reducing exploration for all agents to 0.0841\n",
      "Episode 144 is finished\n",
      "Average Reward for Agent 0 this episode : -14.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.5361\n",
      "Average Reward for Agent 1 this episode : -17.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.5840\n",
      "Average Reward for Agent 2 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 423.0189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 3 this episode : -7.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.1420\n",
      "Average Reward for Agent 4 this episode : -19.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.0154\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7133\n",
      "Average Reward for Agent 6 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.0339\n",
      "Average Reward for Agent 7 this episode : -14.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.9850\n",
      "Average Reward for Agent 8 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3791\n",
      "Average Reward for Agent 9 this episode : -5.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7992\n",
      "Average Reward for Agent 10 this episode : -1.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5067\n",
      "Average Reward for Agent 11 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2290\n",
      "Average Reward for Agent 12 this episode : -25.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.9158\n",
      "Average Reward for Agent 13 this episode : -47.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 208.8654\n",
      "Reducing exploration for all agents to 0.0827\n",
      "Episode 145 is finished\n",
      "Average Reward for Agent 0 this episode : -11.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1024\n",
      "Average Reward for Agent 1 this episode : -22.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.8244\n",
      "Average Reward for Agent 2 this episode : -10.66\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 471.1725\n",
      "Average Reward for Agent 3 this episode : -8.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0931\n",
      "Average Reward for Agent 4 this episode : -19.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.3636\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2761\n",
      "Average Reward for Agent 6 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.8703\n",
      "Average Reward for Agent 7 this episode : -13.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.2753\n",
      "Average Reward for Agent 8 this episode : -1.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9413\n",
      "Average Reward for Agent 9 this episode : -11.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6409\n",
      "Average Reward for Agent 10 this episode : -5.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.7689\n",
      "Average Reward for Agent 11 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7949\n",
      "Average Reward for Agent 12 this episode : -14.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.1470\n",
      "Average Reward for Agent 13 this episode : -55.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.0208\n",
      "Reducing exploration for all agents to 0.0812\n",
      "Episode 146 is finished\n",
      "Average Reward for Agent 0 this episode : -12.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.4557\n",
      "Average Reward for Agent 1 this episode : -10.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 298.1434\n",
      "Average Reward for Agent 2 this episode : -15.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 177.4566\n",
      "Average Reward for Agent 3 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.3265\n",
      "Average Reward for Agent 4 this episode : -19.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.9553\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3102\n",
      "Average Reward for Agent 6 this episode : -3.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.2385\n",
      "Average Reward for Agent 7 this episode : -11.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.0478\n",
      "Average Reward for Agent 8 this episode : -3.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5721\n",
      "Average Reward for Agent 9 this episode : -3.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.1878\n",
      "Average Reward for Agent 10 this episode : -2.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2629\n",
      "Average Reward for Agent 11 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5526\n",
      "Average Reward for Agent 12 this episode : -24.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.6683\n",
      "Average Reward for Agent 13 this episode : -65.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.8778\n",
      "Reducing exploration for all agents to 0.0798\n",
      "Episode 147 is finished\n",
      "Average Reward for Agent 0 this episode : -10.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.6707\n",
      "Average Reward for Agent 1 this episode : -24.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 242.1986\n",
      "Average Reward for Agent 2 this episode : -28.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.5282\n",
      "Average Reward for Agent 3 this episode : -12.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.2065\n",
      "Average Reward for Agent 4 this episode : -25.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8296\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0961\n",
      "Average Reward for Agent 6 this episode : -0.78\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0793\n",
      "Average Reward for Agent 7 this episode : -14.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.8969\n",
      "Average Reward for Agent 8 this episode : -2.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0417\n",
      "Average Reward for Agent 9 this episode : -6.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.7146\n",
      "Average Reward for Agent 10 this episode : -7.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.2151\n",
      "Average Reward for Agent 11 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1174\n",
      "Average Reward for Agent 12 this episode : -30.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.3932\n",
      "Average Reward for Agent 13 this episode : -34.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 523.2546\n",
      "Reducing exploration for all agents to 0.0785\n",
      "Episode 148 is finished\n",
      "Average Reward for Agent 0 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.0088\n",
      "Average Reward for Agent 1 this episode : -16.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 267.1527\n",
      "Average Reward for Agent 2 this episode : -32.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 297.4489\n",
      "Average Reward for Agent 3 this episode : -8.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.9499\n",
      "Average Reward for Agent 4 this episode : -17.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.9422\n",
      "Average Reward for Agent 5 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7995\n",
      "Average Reward for Agent 6 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.6523\n",
      "Average Reward for Agent 7 this episode : -12.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.0842\n",
      "Average Reward for Agent 8 this episode : -5.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6367\n",
      "Average Reward for Agent 9 this episode : -7.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1622\n",
      "Average Reward for Agent 10 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.5175\n",
      "Average Reward for Agent 11 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4048\n",
      "Average Reward for Agent 12 this episode : -15.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.2436\n",
      "Average Reward for Agent 13 this episode : -53.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.1347\n",
      "Reducing exploration for all agents to 0.0771\n",
      "Episode 149 is finished\n",
      "Average Reward for Agent 0 this episode : -10.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0119\n",
      "Average Reward for Agent 1 this episode : -22.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.9028\n",
      "Average Reward for Agent 2 this episode : -9.69\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.0949\n",
      "Average Reward for Agent 3 this episode : -3.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 4 this episode : -16.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.8605\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9480\n",
      "Average Reward for Agent 6 this episode : -2.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.8341\n",
      "Average Reward for Agent 7 this episode : -13.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.7591\n",
      "Average Reward for Agent 8 this episode : -3.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3872\n",
      "Average Reward for Agent 9 this episode : -2.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2377\n",
      "Average Reward for Agent 10 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.0617\n",
      "Average Reward for Agent 11 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0040\n",
      "Average Reward for Agent 12 this episode : -17.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.3013\n",
      "Average Reward for Agent 13 this episode : -46.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 230.5553\n",
      "Reducing exploration for all agents to 0.0758\n",
      "Episode 150 is finished\n",
      "Average Reward for Agent 0 this episode : -18.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.4191\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -19.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.3064\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -30.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.7897\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -8.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.3049\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.1511\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4550\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -4.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1582\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 147.5610\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7861\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -5.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8103\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -1.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6799\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -1.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6262\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -28.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.9004\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -36.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 204.4188\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0745\n",
      "Episode 151 is finished\n",
      "Average Reward for Agent 0 this episode : -13.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.7882\n",
      "Average Reward for Agent 1 this episode : -17.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.6155\n",
      "Average Reward for Agent 2 this episode : -22.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.6378\n",
      "Average Reward for Agent 3 this episode : -7.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.6192\n",
      "Average Reward for Agent 4 this episode : -24.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.7366\n",
      "Average Reward for Agent 5 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6443\n",
      "Average Reward for Agent 6 this episode : -4.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.8668\n",
      "Average Reward for Agent 7 this episode : -14.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.3525\n",
      "Average Reward for Agent 8 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0070\n",
      "Average Reward for Agent 9 this episode : -4.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.3176\n",
      "Average Reward for Agent 10 this episode : -1.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7573\n",
      "Average Reward for Agent 11 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3918\n",
      "Average Reward for Agent 12 this episode : -21.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.3505\n",
      "Average Reward for Agent 13 this episode : -48.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 301.9410\n",
      "Reducing exploration for all agents to 0.0732\n",
      "Episode 152 is finished\n",
      "Average Reward for Agent 0 this episode : -11.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3719\n",
      "Average Reward for Agent 1 this episode : -18.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.8160\n",
      "Average Reward for Agent 2 this episode : -17.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.4957\n",
      "Average Reward for Agent 3 this episode : -6.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.5246\n",
      "Average Reward for Agent 4 this episode : -19.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 145.9533\n",
      "Average Reward for Agent 5 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7475\n",
      "Average Reward for Agent 6 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.6287\n",
      "Average Reward for Agent 7 this episode : -12.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.4382\n",
      "Average Reward for Agent 8 this episode : -6.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2464\n",
      "Average Reward for Agent 9 this episode : -3.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.8582\n",
      "Average Reward for Agent 10 this episode : -1.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2978\n",
      "Average Reward for Agent 11 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7372\n",
      "Average Reward for Agent 12 this episode : -16.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.6357\n",
      "Average Reward for Agent 13 this episode : -37.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.6906\n",
      "Reducing exploration for all agents to 0.072\n",
      "Episode 153 is finished\n",
      "Average Reward for Agent 0 this episode : -13.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5772\n",
      "Average Reward for Agent 1 this episode : -19.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.0427\n",
      "Average Reward for Agent 2 this episode : -21.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 235.1276\n",
      "Average Reward for Agent 3 this episode : -8.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.7796\n",
      "Average Reward for Agent 4 this episode : -18.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.3777\n",
      "Average Reward for Agent 5 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1758\n",
      "Average Reward for Agent 6 this episode : -4.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.7727\n",
      "Average Reward for Agent 7 this episode : -14.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.7111\n",
      "Average Reward for Agent 8 this episode : -2.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3060\n",
      "Average Reward for Agent 9 this episode : -5.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.0643\n",
      "Average Reward for Agent 10 this episode : -7.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5473\n",
      "Average Reward for Agent 11 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1328\n",
      "Average Reward for Agent 12 this episode : -19.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.4607\n",
      "Average Reward for Agent 13 this episode : -40.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 372.9155\n",
      "Reducing exploration for all agents to 0.0707\n",
      "Episode 154 is finished\n",
      "Average Reward for Agent 0 this episode : -12.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.1171\n",
      "Average Reward for Agent 1 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.8139\n",
      "Average Reward for Agent 2 this episode : -21.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.5393\n",
      "Average Reward for Agent 3 this episode : -12.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1346\n",
      "Average Reward for Agent 4 this episode : -15.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.4592\n",
      "Average Reward for Agent 5 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9083\n",
      "Average Reward for Agent 7 this episode : -14.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.7201\n",
      "Average Reward for Agent 8 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3901\n",
      "Average Reward for Agent 9 this episode : -3.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5950\n",
      "Average Reward for Agent 10 this episode : -11.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.8272\n",
      "Average Reward for Agent 11 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3611\n",
      "Average Reward for Agent 12 this episode : -14.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.6425\n",
      "Average Reward for Agent 13 this episode : -31.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.0499\n",
      "Reducing exploration for all agents to 0.0695\n",
      "Episode 155 is finished\n",
      "Average Reward for Agent 0 this episode : -11.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.8922\n",
      "Average Reward for Agent 1 this episode : -15.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.0343\n",
      "Average Reward for Agent 2 this episode : -18.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.2493\n",
      "Average Reward for Agent 3 this episode : -12.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.4069\n",
      "Average Reward for Agent 4 this episode : -14.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4139\n",
      "Average Reward for Agent 5 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1418\n",
      "Average Reward for Agent 6 this episode : -1.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.5084\n",
      "Average Reward for Agent 7 this episode : -13.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.1065\n",
      "Average Reward for Agent 8 this episode : -3.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9020\n",
      "Average Reward for Agent 9 this episode : -4.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.0342\n",
      "Average Reward for Agent 10 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.0968\n",
      "Average Reward for Agent 11 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8442\n",
      "Average Reward for Agent 12 this episode : -44.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.0829\n",
      "Average Reward for Agent 13 this episode : -46.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 347.8455\n",
      "Reducing exploration for all agents to 0.0683\n",
      "Episode 156 is finished\n",
      "Average Reward for Agent 0 this episode : -14.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.9713\n",
      "Average Reward for Agent 1 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.3898\n",
      "Average Reward for Agent 2 this episode : -21.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.0093\n",
      "Average Reward for Agent 3 this episode : -10.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.6360\n",
      "Average Reward for Agent 4 this episode : -18.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.9543\n",
      "Average Reward for Agent 5 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2292\n",
      "Average Reward for Agent 6 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.7725\n",
      "Average Reward for Agent 7 this episode : -11.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.5262\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.3973\n",
      "Average Reward for Agent 9 this episode : -9.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.0326\n",
      "Average Reward for Agent 10 this episode : -7.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8693\n",
      "Average Reward for Agent 11 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8657\n",
      "Average Reward for Agent 12 this episode : -30.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.0555\n",
      "Average Reward for Agent 13 this episode : -35.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.4673\n",
      "Reducing exploration for all agents to 0.0672\n",
      "Episode 157 is finished\n",
      "Average Reward for Agent 0 this episode : -14.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7041\n",
      "Average Reward for Agent 1 this episode : -21.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.4648\n",
      "Average Reward for Agent 2 this episode : -18.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.6715\n",
      "Average Reward for Agent 3 this episode : -10.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6954\n",
      "Average Reward for Agent 4 this episode : -16.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.8211\n",
      "Average Reward for Agent 5 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0353\n",
      "Average Reward for Agent 6 this episode : -0.68\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.7375\n",
      "Average Reward for Agent 7 this episode : -14.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.1600\n",
      "Average Reward for Agent 8 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9931\n",
      "Average Reward for Agent 9 this episode : -8.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1438\n",
      "Average Reward for Agent 10 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.4315\n",
      "Average Reward for Agent 11 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7171\n",
      "Average Reward for Agent 12 this episode : -20.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7137\n",
      "Average Reward for Agent 13 this episode : -45.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7361\n",
      "Reducing exploration for all agents to 0.066\n",
      "Episode 158 is finished\n",
      "Average Reward for Agent 0 this episode : -15.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.4995\n",
      "Average Reward for Agent 1 this episode : -18.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 208.7967\n",
      "Average Reward for Agent 2 this episode : -19.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 224.1400\n",
      "Average Reward for Agent 3 this episode : -5.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.7118\n",
      "Average Reward for Agent 4 this episode : -14.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.3945\n",
      "Average Reward for Agent 5 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1034\n",
      "Average Reward for Agent 6 this episode : -0.67\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.9970\n",
      "Average Reward for Agent 7 this episode : -12.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.2799\n",
      "Average Reward for Agent 8 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.8106\n",
      "Average Reward for Agent 9 this episode : -7.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.0651\n",
      "Average Reward for Agent 10 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.2903\n",
      "Average Reward for Agent 11 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4380\n",
      "Average Reward for Agent 12 this episode : -22.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.9547\n",
      "Average Reward for Agent 13 this episode : -36.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 297.8316\n",
      "Reducing exploration for all agents to 0.0649\n",
      "Episode 159 is finished\n",
      "Average Reward for Agent 0 this episode : -9.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.4759\n",
      "Average Reward for Agent 1 this episode : -25.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.2592\n",
      "Average Reward for Agent 2 this episode : -20.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.7230\n",
      "Average Reward for Agent 3 this episode : -9.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8025\n",
      "Average Reward for Agent 4 this episode : -11.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.4353\n",
      "Average Reward for Agent 5 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0673\n",
      "Average Reward for Agent 6 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.2995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 7 this episode : -12.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.6643\n",
      "Average Reward for Agent 8 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.3488\n",
      "Average Reward for Agent 9 this episode : -9.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.4438\n",
      "Average Reward for Agent 10 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.3214\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1138\n",
      "Average Reward for Agent 12 this episode : -32.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9238\n",
      "Average Reward for Agent 13 this episode : -43.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.5245\n",
      "Reducing exploration for all agents to 0.0638\n",
      "Episode 160 is finished\n",
      "Average Reward for Agent 0 this episode : -24.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0161\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -18.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.5363\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 273.5960\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -7.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.9788\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -16.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4980\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -1.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5229\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.2716\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.5329\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5125\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -9.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.5673\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -1.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9654\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0623\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -11.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.2126\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -43.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 257.5831\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0627\n",
      "Episode 161 is finished\n",
      "Average Reward for Agent 0 this episode : -19.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.3044\n",
      "Average Reward for Agent 1 this episode : -16.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.0058\n",
      "Average Reward for Agent 2 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.4330\n",
      "Average Reward for Agent 3 this episode : -10.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.2144\n",
      "Average Reward for Agent 4 this episode : -17.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.6621\n",
      "Average Reward for Agent 5 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7129\n",
      "Average Reward for Agent 6 this episode : -0.67\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.7206\n",
      "Average Reward for Agent 7 this episode : -14.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0071\n",
      "Average Reward for Agent 8 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.7095\n",
      "Average Reward for Agent 9 this episode : -3.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7551\n",
      "Average Reward for Agent 10 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.8116\n",
      "Average Reward for Agent 11 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9826\n",
      "Average Reward for Agent 12 this episode : -23.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.5495\n",
      "Average Reward for Agent 13 this episode : -44.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 247.7288\n",
      "Reducing exploration for all agents to 0.0616\n",
      "Episode 162 is finished\n",
      "Average Reward for Agent 0 this episode : -12.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.7374\n",
      "Average Reward for Agent 1 this episode : -21.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.2500\n",
      "Average Reward for Agent 2 this episode : -22.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 256.8298\n",
      "Average Reward for Agent 3 this episode : -13.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.3157\n",
      "Average Reward for Agent 4 this episode : -22.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.4479\n",
      "Average Reward for Agent 5 this episode : -1.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5298\n",
      "Average Reward for Agent 6 this episode : -2.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.7659\n",
      "Average Reward for Agent 7 this episode : -14.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.4471\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.4683\n",
      "Average Reward for Agent 9 this episode : -13.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.1627\n",
      "Average Reward for Agent 10 this episode : -5.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0218\n",
      "Average Reward for Agent 11 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3774\n",
      "Average Reward for Agent 12 this episode : -11.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.4791\n",
      "Average Reward for Agent 13 this episode : -21.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.7527\n",
      "Reducing exploration for all agents to 0.0605\n",
      "Episode 163 is finished\n",
      "Average Reward for Agent 0 this episode : -14.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.6764\n",
      "Average Reward for Agent 1 this episode : -22.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 226.1348\n",
      "Average Reward for Agent 2 this episode : -33.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 226.5347\n",
      "Average Reward for Agent 3 this episode : -7.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.9846\n",
      "Average Reward for Agent 4 this episode : -20.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 269.7645\n",
      "Average Reward for Agent 5 this episode : -2.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0539\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9576\n",
      "Average Reward for Agent 7 this episode : -14.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2540\n",
      "Average Reward for Agent 8 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0157\n",
      "Average Reward for Agent 9 this episode : -2.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.5302\n",
      "Average Reward for Agent 10 this episode : -20.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.3114\n",
      "Average Reward for Agent 11 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3207\n",
      "Average Reward for Agent 12 this episode : -15.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.4496\n",
      "Average Reward for Agent 13 this episode : -13.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 367.4000\n",
      "Reducing exploration for all agents to 0.0595\n",
      "Episode 164 is finished\n",
      "Average Reward for Agent 0 this episode : -10.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1096\n",
      "Average Reward for Agent 1 this episode : -20.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 284.4751\n",
      "Average Reward for Agent 2 this episode : -32.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 192.7763\n",
      "Average Reward for Agent 3 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.0298\n",
      "Average Reward for Agent 4 this episode : -16.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.2794\n",
      "Average Reward for Agent 5 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -1.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.9934\n",
      "Average Reward for Agent 7 this episode : -14.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.8340\n",
      "Average Reward for Agent 8 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6809\n",
      "Average Reward for Agent 9 this episode : -6.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7720\n",
      "Average Reward for Agent 10 this episode : -3.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.8811\n",
      "Average Reward for Agent 11 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6563\n",
      "Average Reward for Agent 12 this episode : -8.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.1973\n",
      "Average Reward for Agent 13 this episode : -8.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.8685\n",
      "Reducing exploration for all agents to 0.0585\n",
      "Episode 165 is finished\n",
      "Average Reward for Agent 0 this episode : -13.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.3380\n",
      "Average Reward for Agent 1 this episode : -18.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.6125\n",
      "Average Reward for Agent 2 this episode : -31.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 218.5067\n",
      "Average Reward for Agent 3 this episode : -13.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6870\n",
      "Average Reward for Agent 4 this episode : -16.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 256.2220\n",
      "Average Reward for Agent 5 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8982\n",
      "Average Reward for Agent 6 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1689\n",
      "Average Reward for Agent 7 this episode : -14.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.4838\n",
      "Average Reward for Agent 8 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4678\n",
      "Average Reward for Agent 9 this episode : -2.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.7635\n",
      "Average Reward for Agent 10 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5179\n",
      "Average Reward for Agent 11 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6129\n",
      "Average Reward for Agent 12 this episode : -11.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.6220\n",
      "Average Reward for Agent 13 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.0608\n",
      "Reducing exploration for all agents to 0.0575\n",
      "Episode 166 is finished\n",
      "Average Reward for Agent 0 this episode : -14.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.8073\n",
      "Average Reward for Agent 1 this episode : -17.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.0593\n",
      "Average Reward for Agent 2 this episode : -29.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 267.1552\n",
      "Average Reward for Agent 3 this episode : -10.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.1807\n",
      "Average Reward for Agent 4 this episode : -15.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.2004\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8631\n",
      "Average Reward for Agent 6 this episode : -0.41\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8980\n",
      "Average Reward for Agent 7 this episode : -11.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0004\n",
      "Average Reward for Agent 8 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2776\n",
      "Average Reward for Agent 9 this episode : -14.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.9617\n",
      "Average Reward for Agent 10 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4254\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9825\n",
      "Average Reward for Agent 12 this episode : -21.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9092\n",
      "Average Reward for Agent 13 this episode : -20.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.0814\n",
      "Reducing exploration for all agents to 0.0565\n",
      "Episode 167 is finished\n",
      "Average Reward for Agent 0 this episode : -13.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.7587\n",
      "Average Reward for Agent 1 this episode : -18.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.4170\n",
      "Average Reward for Agent 2 this episode : -35.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.6166\n",
      "Average Reward for Agent 3 this episode : -6.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.4279\n",
      "Average Reward for Agent 4 this episode : -20.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.0239\n",
      "Average Reward for Agent 5 this episode : -0.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3488\n",
      "Average Reward for Agent 6 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.6418\n",
      "Average Reward for Agent 7 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.0373\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.7184\n",
      "Average Reward for Agent 9 this episode : -8.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.0114\n",
      "Average Reward for Agent 10 this episode : -32.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.1726\n",
      "Average Reward for Agent 11 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8493\n",
      "Average Reward for Agent 12 this episode : -14.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.6958\n",
      "Average Reward for Agent 13 this episode : -6.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.4492\n",
      "Reducing exploration for all agents to 0.0555\n",
      "Episode 168 is finished\n",
      "Average Reward for Agent 0 this episode : -31.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 143.8265\n",
      "Average Reward for Agent 1 this episode : -13.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 320.0261\n",
      "Average Reward for Agent 2 this episode : -21.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.5485\n",
      "Average Reward for Agent 3 this episode : -15.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.7792\n",
      "Average Reward for Agent 4 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.7163\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7235\n",
      "Average Reward for Agent 6 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.5986\n",
      "Average Reward for Agent 7 this episode : -13.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.1584\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9314\n",
      "Average Reward for Agent 9 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.3461\n",
      "Average Reward for Agent 10 this episode : -54.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.4147\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6974\n",
      "Average Reward for Agent 12 this episode : -10.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.1064\n",
      "Average Reward for Agent 13 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.6132\n",
      "Reducing exploration for all agents to 0.0546\n",
      "Episode 169 is finished\n",
      "Average Reward for Agent 0 this episode : -14.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.7532\n",
      "Average Reward for Agent 1 this episode : -18.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.2719\n",
      "Average Reward for Agent 2 this episode : -36.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.1575\n",
      "Average Reward for Agent 3 this episode : -6.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.8636\n",
      "Average Reward for Agent 4 this episode : -22.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.9236\n",
      "Average Reward for Agent 5 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8253\n",
      "Average Reward for Agent 6 this episode : -1.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.7563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 7 this episode : -14.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.1190\n",
      "Average Reward for Agent 8 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1885\n",
      "Average Reward for Agent 9 this episode : -10.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9754\n",
      "Average Reward for Agent 10 this episode : -40.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.0654\n",
      "Average Reward for Agent 11 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0707\n",
      "Average Reward for Agent 12 this episode : -14.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.4226\n",
      "Average Reward for Agent 13 this episode : -13.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.9390\n",
      "Reducing exploration for all agents to 0.0536\n",
      "Episode 170 is finished\n",
      "Average Reward for Agent 0 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.0571\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -20.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.0252\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -38.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.5517\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -9.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.6797\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -19.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.0223\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8022\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.8272\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9476\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5357\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -4.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.2573\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -36.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.2162\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8725\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -14.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5088\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -17.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.5017\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0527\n",
      "Episode 171 is finished\n",
      "Average Reward for Agent 0 this episode : -21.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.5906\n",
      "Average Reward for Agent 1 this episode : -13.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.1079\n",
      "Average Reward for Agent 2 this episode : -24.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 275.8481\n",
      "Average Reward for Agent 3 this episode : -12.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5230\n",
      "Average Reward for Agent 4 this episode : -15.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.9356\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4476\n",
      "Average Reward for Agent 6 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0084\n",
      "Average Reward for Agent 7 this episode : -13.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.6243\n",
      "Average Reward for Agent 8 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1737\n",
      "Average Reward for Agent 9 this episode : -6.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4718\n",
      "Average Reward for Agent 10 this episode : -34.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 596.2598\n",
      "Average Reward for Agent 11 this episode : -0.15\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7479\n",
      "Average Reward for Agent 12 this episode : -9.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.8027\n",
      "Average Reward for Agent 13 this episode : -4.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.7684\n",
      "Reducing exploration for all agents to 0.0518\n",
      "Episode 172 is finished\n",
      "Average Reward for Agent 0 this episode : -11.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.5995\n",
      "Average Reward for Agent 1 this episode : -18.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.6346\n",
      "Average Reward for Agent 2 this episode : -20.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 295.8344\n",
      "Average Reward for Agent 3 this episode : -8.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.0504\n",
      "Average Reward for Agent 4 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.9189\n",
      "Average Reward for Agent 5 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4487\n",
      "Average Reward for Agent 6 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.5913\n",
      "Average Reward for Agent 7 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9544\n",
      "Average Reward for Agent 8 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1403\n",
      "Average Reward for Agent 9 this episode : -11.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.6278\n",
      "Average Reward for Agent 10 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 379.2169\n",
      "Average Reward for Agent 11 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4966\n",
      "Average Reward for Agent 12 this episode : -28.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.6022\n",
      "Average Reward for Agent 13 this episode : -38.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.8040\n",
      "Reducing exploration for all agents to 0.0509\n",
      "Episode 173 is finished\n",
      "Average Reward for Agent 0 this episode : -12.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.9709\n",
      "Average Reward for Agent 1 this episode : -17.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3614\n",
      "Average Reward for Agent 2 this episode : -16.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.4277\n",
      "Average Reward for Agent 3 this episode : -9.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1828\n",
      "Average Reward for Agent 4 this episode : -17.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.1792\n",
      "Average Reward for Agent 5 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2069\n",
      "Average Reward for Agent 6 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.1955\n",
      "Average Reward for Agent 7 this episode : -14.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.8451\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4129\n",
      "Average Reward for Agent 9 this episode : -13.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.6345\n",
      "Average Reward for Agent 10 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 442.5150\n",
      "Average Reward for Agent 11 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5160\n",
      "Average Reward for Agent 12 this episode : -27.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.3033\n",
      "Average Reward for Agent 13 this episode : -44.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.1304\n",
      "Reducing exploration for all agents to 0.05\n",
      "Episode 174 is finished\n",
      "Average Reward for Agent 0 this episode : -10.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.1753\n",
      "Average Reward for Agent 1 this episode : -22.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 198.2453\n",
      "Average Reward for Agent 2 this episode : -12.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.8840\n",
      "Average Reward for Agent 3 this episode : -14.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.0371\n",
      "Average Reward for Agent 4 this episode : -19.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.3631\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.1684\n",
      "Average Reward for Agent 7 this episode : -14.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.3760\n",
      "Average Reward for Agent 8 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0159\n",
      "Average Reward for Agent 9 this episode : -7.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.3573\n",
      "Average Reward for Agent 10 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.1870\n",
      "Average Reward for Agent 11 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3828\n",
      "Average Reward for Agent 12 this episode : -15.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.3222\n",
      "Average Reward for Agent 13 this episode : -48.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.2286\n",
      "Reducing exploration for all agents to 0.0492\n",
      "Episode 175 is finished\n",
      "Average Reward for Agent 0 this episode : -11.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.2595\n",
      "Average Reward for Agent 1 this episode : -19.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.2941\n",
      "Average Reward for Agent 2 this episode : -21.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.6896\n",
      "Average Reward for Agent 3 this episode : -6.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.9402\n",
      "Average Reward for Agent 4 this episode : -20.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 354.5342\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8661\n",
      "Average Reward for Agent 6 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0187\n",
      "Average Reward for Agent 7 this episode : -14.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.0674\n",
      "Average Reward for Agent 8 this episode : -1.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9879\n",
      "Average Reward for Agent 9 this episode : -7.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8533\n",
      "Average Reward for Agent 10 this episode : -46.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.3234\n",
      "Average Reward for Agent 11 this episode : -4.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7229\n",
      "Average Reward for Agent 12 this episode : -9.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0220\n",
      "Average Reward for Agent 13 this episode : -42.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.5154\n",
      "Reducing exploration for all agents to 0.0483\n",
      "Episode 176 is finished\n",
      "Average Reward for Agent 0 this episode : -14.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6658\n",
      "Average Reward for Agent 1 this episode : -15.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.1068\n",
      "Average Reward for Agent 2 this episode : -15.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.5509\n",
      "Average Reward for Agent 3 this episode : -10.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.0830\n",
      "Average Reward for Agent 4 this episode : -20.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 324.9373\n",
      "Average Reward for Agent 5 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7636\n",
      "Average Reward for Agent 6 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.3096\n",
      "Average Reward for Agent 7 this episode : -14.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.5526\n",
      "Average Reward for Agent 8 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9315\n",
      "Average Reward for Agent 9 this episode : -12.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.4717\n",
      "Average Reward for Agent 10 this episode : -26.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.1278\n",
      "Average Reward for Agent 11 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7533\n",
      "Average Reward for Agent 12 this episode : -14.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.8980\n",
      "Average Reward for Agent 13 this episode : -46.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.9827\n",
      "Reducing exploration for all agents to 0.0475\n",
      "Episode 177 is finished\n",
      "Average Reward for Agent 0 this episode : -14.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.0980\n",
      "Average Reward for Agent 1 this episode : -21.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.8571\n",
      "Average Reward for Agent 2 this episode : -16.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.9251\n",
      "Average Reward for Agent 3 this episode : -10.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.1460\n",
      "Average Reward for Agent 4 this episode : -17.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.8062\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5183\n",
      "Average Reward for Agent 6 this episode : -2.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.4435\n",
      "Average Reward for Agent 7 this episode : -14.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.0476\n",
      "Average Reward for Agent 8 this episode : -1.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.5600\n",
      "Average Reward for Agent 9 this episode : -21.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.2603\n",
      "Average Reward for Agent 10 this episode : -51.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 535.8359\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2032\n",
      "Average Reward for Agent 12 this episode : -12.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1014\n",
      "Average Reward for Agent 13 this episode : -37.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.5695\n",
      "Reducing exploration for all agents to 0.0467\n",
      "Episode 178 is finished\n",
      "Average Reward for Agent 0 this episode : -35.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.9946\n",
      "Average Reward for Agent 1 this episode : -16.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.7206\n",
      "Average Reward for Agent 2 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 179.6663\n",
      "Average Reward for Agent 3 this episode : -5.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.9991\n",
      "Average Reward for Agent 4 this episode : -19.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.1597\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0871\n",
      "Average Reward for Agent 6 this episode : -1.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.8709\n",
      "Average Reward for Agent 7 this episode : -15.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9756\n",
      "Average Reward for Agent 8 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1600\n",
      "Average Reward for Agent 9 this episode : -13.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5887\n",
      "Average Reward for Agent 10 this episode : -10.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.0553\n",
      "Average Reward for Agent 11 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3265\n",
      "Average Reward for Agent 12 this episode : -26.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.8986\n",
      "Average Reward for Agent 13 this episode : -53.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.8567\n",
      "Reducing exploration for all agents to 0.0459\n",
      "Episode 179 is finished\n",
      "Average Reward for Agent 0 this episode : -14.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.3389\n",
      "Average Reward for Agent 1 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.0145\n",
      "Average Reward for Agent 2 this episode : -17.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.6721\n",
      "Average Reward for Agent 3 this episode : -3.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.7330\n",
      "Average Reward for Agent 4 this episode : -19.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 252.2169\n",
      "Average Reward for Agent 5 this episode : -1.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7773\n",
      "Average Reward for Agent 6 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.9395\n",
      "Average Reward for Agent 7 this episode : -11.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.3212\n",
      "Average Reward for Agent 8 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0093\n",
      "Average Reward for Agent 9 this episode : -9.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.4662\n",
      "Average Reward for Agent 10 this episode : -51.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 265.9777\n",
      "Average Reward for Agent 11 this episode : -3.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3611\n",
      "Average Reward for Agent 12 this episode : -15.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.0541\n",
      "Average Reward for Agent 13 this episode : -53.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7246\n",
      "Reducing exploration for all agents to 0.0451\n",
      "Episode 180 is finished\n",
      "Average Reward for Agent 0 this episode : -10.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -17.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 240.3597\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -21.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.2568\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -1.31\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2622\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -20.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 169.0627\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5249\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -3.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8646\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.6907\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5889\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -9.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.3163\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -49.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 395.3576\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -11.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1603\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -15.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.6050\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -27.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.2963\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0443\n",
      "Episode 181 is finished\n",
      "Average Reward for Agent 0 this episode : -13.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.9233\n",
      "Average Reward for Agent 1 this episode : -15.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.2955\n",
      "Average Reward for Agent 2 this episode : -19.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.3848\n",
      "Average Reward for Agent 3 this episode : -1.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0221\n",
      "Average Reward for Agent 4 this episode : -18.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.7210\n",
      "Average Reward for Agent 5 this episode : -1.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4591\n",
      "Average Reward for Agent 6 this episode : -2.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.1788\n",
      "Average Reward for Agent 7 this episode : -14.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.7093\n",
      "Average Reward for Agent 8 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2030\n",
      "Average Reward for Agent 9 this episode : -15.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2194\n",
      "Average Reward for Agent 10 this episode : -53.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 645.4186\n",
      "Average Reward for Agent 11 this episode : -22.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.6108\n",
      "Average Reward for Agent 12 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.9207\n",
      "Average Reward for Agent 13 this episode : -44.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.0527\n",
      "Reducing exploration for all agents to 0.0436\n",
      "Episode 182 is finished\n",
      "Average Reward for Agent 0 this episode : -12.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.1458\n",
      "Average Reward for Agent 1 this episode : -13.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.0485\n",
      "Average Reward for Agent 2 this episode : -20.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.9069\n",
      "Average Reward for Agent 3 this episode : -1.28\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.9345\n",
      "Average Reward for Agent 4 this episode : -18.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.6049\n",
      "Average Reward for Agent 5 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1383\n",
      "Average Reward for Agent 6 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.0773\n",
      "Average Reward for Agent 7 this episode : -13.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.9682\n",
      "Average Reward for Agent 8 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1511\n",
      "Average Reward for Agent 9 this episode : -10.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.3617\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.0217\n",
      "Average Reward for Agent 11 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.7757\n",
      "Average Reward for Agent 12 this episode : -7.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.1453\n",
      "Average Reward for Agent 13 this episode : -36.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.4864\n",
      "Reducing exploration for all agents to 0.0428\n",
      "Episode 183 is finished\n",
      "Average Reward for Agent 0 this episode : -9.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.0546\n",
      "Average Reward for Agent 1 this episode : -17.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 237.4065\n",
      "Average Reward for Agent 2 this episode : -16.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 193.8459\n",
      "Average Reward for Agent 3 this episode : -3.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.7748\n",
      "Average Reward for Agent 4 this episode : -22.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.7573\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3417\n",
      "Average Reward for Agent 6 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8884\n",
      "Average Reward for Agent 7 this episode : -9.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1275\n",
      "Average Reward for Agent 8 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1419\n",
      "Average Reward for Agent 9 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.2886\n",
      "Average Reward for Agent 10 this episode : -48.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 452.7743\n",
      "Average Reward for Agent 11 this episode : -1.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.2428\n",
      "Average Reward for Agent 12 this episode : -20.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.7931\n",
      "Average Reward for Agent 13 this episode : -47.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.0860\n",
      "Reducing exploration for all agents to 0.0421\n",
      "Episode 184 is finished\n",
      "Average Reward for Agent 0 this episode : -12.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.1232\n",
      "Average Reward for Agent 1 this episode : -18.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.8662\n",
      "Average Reward for Agent 2 this episode : -20.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 223.0735\n",
      "Average Reward for Agent 3 this episode : -2.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.5786\n",
      "Average Reward for Agent 4 this episode : -19.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 356.1767\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4587\n",
      "Average Reward for Agent 6 this episode : -3.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2453\n",
      "Average Reward for Agent 7 this episode : -12.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.9759\n",
      "Average Reward for Agent 8 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5768\n",
      "Average Reward for Agent 9 this episode : -0.14\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 0s - loss: 75.1766\n",
      "Average Reward for Agent 10 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.5539\n",
      "Average Reward for Agent 11 this episode : -3.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.2287\n",
      "Average Reward for Agent 12 this episode : -14.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.5262\n",
      "Average Reward for Agent 13 this episode : -39.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.6699\n",
      "Reducing exploration for all agents to 0.0414\n",
      "Episode 185 is finished\n",
      "Average Reward for Agent 0 this episode : -9.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 186.2825\n",
      "Average Reward for Agent 1 this episode : -18.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 284.6110\n",
      "Average Reward for Agent 2 this episode : -19.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5940\n",
      "Average Reward for Agent 3 this episode : -1.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.2095\n",
      "Average Reward for Agent 4 this episode : -18.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.1071\n",
      "Average Reward for Agent 5 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2541\n",
      "Average Reward for Agent 6 this episode : -4.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.7521\n",
      "Average Reward for Agent 7 this episode : -15.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.0017\n",
      "Average Reward for Agent 8 this episode : -9.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.9308\n",
      "Average Reward for Agent 9 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.2459\n",
      "Average Reward for Agent 10 this episode : -5.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 128.7447\n",
      "Average Reward for Agent 11 this episode : -33.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.0894\n",
      "Average Reward for Agent 12 this episode : -14.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.5670\n",
      "Average Reward for Agent 13 this episode : -53.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.4496\n",
      "Reducing exploration for all agents to 0.0406\n",
      "Episode 186 is finished\n",
      "Average Reward for Agent 0 this episode : -9.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.2338\n",
      "Average Reward for Agent 1 this episode : -17.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 147.7139\n",
      "Average Reward for Agent 2 this episode : -17.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.7000\n",
      "Average Reward for Agent 3 this episode : -3.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9786\n",
      "Average Reward for Agent 4 this episode : -22.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.5673\n",
      "Average Reward for Agent 5 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0817\n",
      "Average Reward for Agent 6 this episode : -1.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.6007\n",
      "Average Reward for Agent 7 this episode : -14.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2382\n",
      "Average Reward for Agent 8 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5557\n",
      "Average Reward for Agent 9 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6275\n",
      "Average Reward for Agent 10 this episode : -54.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.7727\n",
      "Average Reward for Agent 11 this episode : -17.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.7776\n",
      "Average Reward for Agent 12 this episode : -13.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.9114\n",
      "Average Reward for Agent 13 this episode : -50.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.4625\n",
      "Reducing exploration for all agents to 0.0399\n",
      "Episode 187 is finished\n",
      "Average Reward for Agent 0 this episode : -8.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.8110\n",
      "Average Reward for Agent 1 this episode : -16.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 203.8381\n",
      "Average Reward for Agent 2 this episode : -21.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.6483\n",
      "Average Reward for Agent 3 this episode : -3.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.6874\n",
      "Average Reward for Agent 4 this episode : -17.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.7315\n",
      "Average Reward for Agent 5 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6087\n",
      "Average Reward for Agent 6 this episode : -2.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5023\n",
      "Average Reward for Agent 7 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.1409\n",
      "Average Reward for Agent 8 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3828\n",
      "Average Reward for Agent 9 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.4850\n",
      "Average Reward for Agent 10 this episode : -37.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.9910\n",
      "Average Reward for Agent 11 this episode : -18.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2744\n",
      "Average Reward for Agent 12 this episode : -17.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1275\n",
      "Average Reward for Agent 13 this episode : -45.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.3501\n",
      "Reducing exploration for all agents to 0.0393\n",
      "Episode 188 is finished\n",
      "Average Reward for Agent 0 this episode : -9.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.3599\n",
      "Average Reward for Agent 1 this episode : -17.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 438.3466\n",
      "Average Reward for Agent 2 this episode : -17.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 266.2917\n",
      "Average Reward for Agent 3 this episode : -4.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.4537\n",
      "Average Reward for Agent 4 this episode : -15.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 177.0496\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6960\n",
      "Average Reward for Agent 6 this episode : -7.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5467\n",
      "Average Reward for Agent 7 this episode : -13.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.2320\n",
      "Average Reward for Agent 8 this episode : -4.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0283\n",
      "Average Reward for Agent 9 this episode : -1.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7674\n",
      "Average Reward for Agent 10 this episode : -53.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.8791\n",
      "Average Reward for Agent 11 this episode : -12.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.3295\n",
      "Average Reward for Agent 12 this episode : -11.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.4191\n",
      "Average Reward for Agent 13 this episode : -42.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.9983\n",
      "Reducing exploration for all agents to 0.0386\n",
      "Episode 189 is finished\n",
      "Average Reward for Agent 0 this episode : -9.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.0488\n",
      "Average Reward for Agent 1 this episode : -22.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.1536\n",
      "Average Reward for Agent 2 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.0192\n",
      "Average Reward for Agent 3 this episode : -1.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.8957\n",
      "Average Reward for Agent 4 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 319.4866\n",
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2176\n",
      "Average Reward for Agent 6 this episode : -2.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7525\n",
      "Average Reward for Agent 7 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.6582\n",
      "Average Reward for Agent 8 this episode : -7.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0273\n",
      "Average Reward for Agent 9 this episode : -7.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.2055\n",
      "Average Reward for Agent 10 this episode : -39.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.7159\n",
      "Average Reward for Agent 11 this episode : -19.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6058\n",
      "Average Reward for Agent 12 this episode : -22.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5220\n",
      "Average Reward for Agent 13 this episode : -49.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0130\n",
      "Reducing exploration for all agents to 0.0379\n",
      "Episode 190 is finished\n",
      "Average Reward for Agent 0 this episode : -12.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.4709\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -20.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 202.4282\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -22.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 150.4194\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.4112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -15.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 369.9913\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2704\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -5.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.0296\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.6075\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -9.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.7683\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -10.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.9615\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -48.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.2132\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -19.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9763\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -10.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.4626\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -36.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.0409\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0373\n",
      "Episode 191 is finished\n",
      "Average Reward for Agent 0 this episode : -8.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.2402\n",
      "Average Reward for Agent 1 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 198.0692\n",
      "Average Reward for Agent 2 this episode : -16.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.2850\n",
      "Average Reward for Agent 3 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.3082\n",
      "Average Reward for Agent 4 this episode : -20.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.4149\n",
      "Average Reward for Agent 5 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6582\n",
      "Average Reward for Agent 6 this episode : -3.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.0355\n",
      "Average Reward for Agent 7 this episode : -14.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8433\n",
      "Average Reward for Agent 8 this episode : -3.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.7583\n",
      "Average Reward for Agent 9 this episode : -20.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.4851\n",
      "Average Reward for Agent 10 this episode : -45.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 600.6089\n",
      "Average Reward for Agent 11 this episode : -21.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.5336\n",
      "Average Reward for Agent 12 this episode : -13.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5502\n",
      "Average Reward for Agent 13 this episode : -51.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.9772\n",
      "Reducing exploration for all agents to 0.0366\n",
      "Episode 192 is finished\n",
      "Average Reward for Agent 0 this episode : -10.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.1127\n",
      "Average Reward for Agent 1 this episode : -18.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.8097\n",
      "Average Reward for Agent 2 this episode : -19.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.9912\n",
      "Average Reward for Agent 3 this episode : -2.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5560\n",
      "Average Reward for Agent 4 this episode : -19.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.3270\n",
      "Average Reward for Agent 5 this episode : -0.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0526\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.6952\n",
      "Average Reward for Agent 7 this episode : -14.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.6429\n",
      "Average Reward for Agent 8 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0464\n",
      "Average Reward for Agent 9 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.9368\n",
      "Average Reward for Agent 10 this episode : -42.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.5866\n",
      "Average Reward for Agent 11 this episode : -28.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1865\n",
      "Average Reward for Agent 12 this episode : -13.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.6254\n",
      "Average Reward for Agent 13 this episode : -47.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.4222\n",
      "Reducing exploration for all agents to 0.036\n",
      "Episode 193 is finished\n",
      "Average Reward for Agent 0 this episode : -7.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.7420\n",
      "Average Reward for Agent 1 this episode : -19.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 299.9771\n",
      "Average Reward for Agent 2 this episode : -23.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.9153\n",
      "Average Reward for Agent 3 this episode : -5.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.4067\n",
      "Average Reward for Agent 4 this episode : -17.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.0959\n",
      "Average Reward for Agent 5 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9772\n",
      "Average Reward for Agent 6 this episode : -6.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.7766\n",
      "Average Reward for Agent 7 this episode : -14.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.7042\n",
      "Average Reward for Agent 8 this episode : -1.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.4114\n",
      "Average Reward for Agent 9 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.1068\n",
      "Average Reward for Agent 10 this episode : -43.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 337.5015\n",
      "Average Reward for Agent 11 this episode : -4.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.3803\n",
      "Average Reward for Agent 12 this episode : -18.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8759\n",
      "Average Reward for Agent 13 this episode : -41.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 218.6205\n",
      "Reducing exploration for all agents to 0.0354\n",
      "Episode 194 is finished\n",
      "Average Reward for Agent 0 this episode : -8.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.9746\n",
      "Average Reward for Agent 1 this episode : -25.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 326.2140\n",
      "Average Reward for Agent 2 this episode : -20.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.3810\n",
      "Average Reward for Agent 3 this episode : -8.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.8716\n",
      "Average Reward for Agent 4 this episode : -15.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.1734\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3307\n",
      "Average Reward for Agent 6 this episode : -1.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.1433\n",
      "Average Reward for Agent 7 this episode : -14.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.3888\n",
      "Average Reward for Agent 8 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4961\n",
      "Average Reward for Agent 9 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.8221\n",
      "Average Reward for Agent 10 this episode : -28.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 332.0683\n",
      "Average Reward for Agent 11 this episode : -2.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.9857\n",
      "Average Reward for Agent 12 this episode : -10.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.5086\n",
      "Average Reward for Agent 13 this episode : -56.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.9008\n",
      "Reducing exploration for all agents to 0.0348\n",
      "Episode 195 is finished\n",
      "Average Reward for Agent 0 this episode : -8.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.2451\n",
      "Average Reward for Agent 1 this episode : -26.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.2747\n",
      "Average Reward for Agent 2 this episode : -18.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.7087\n",
      "Average Reward for Agent 3 this episode : -3.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.3292\n",
      "Average Reward for Agent 4 this episode : -18.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.9111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1338\n",
      "Average Reward for Agent 6 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.7051\n",
      "Average Reward for Agent 7 this episode : -15.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.7706\n",
      "Average Reward for Agent 8 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4524\n",
      "Average Reward for Agent 9 this episode : -0.11\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5597\n",
      "Average Reward for Agent 10 this episode : -54.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.2034\n",
      "Average Reward for Agent 11 this episode : -3.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.3981\n",
      "Average Reward for Agent 12 this episode : -16.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.5788\n",
      "Average Reward for Agent 13 this episode : -39.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 218.4546\n",
      "Reducing exploration for all agents to 0.0342\n",
      "Episode 196 is finished\n",
      "Average Reward for Agent 0 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0154\n",
      "Average Reward for Agent 1 this episode : -30.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.4891\n",
      "Average Reward for Agent 2 this episode : -24.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.2927\n",
      "Average Reward for Agent 3 this episode : -1.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.0668\n",
      "Average Reward for Agent 4 this episode : -19.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 208.5209\n",
      "Average Reward for Agent 5 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0550\n",
      "Average Reward for Agent 6 this episode : -15.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.8076\n",
      "Average Reward for Agent 7 this episode : -15.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.2153\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0105\n",
      "Average Reward for Agent 9 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1399\n",
      "Average Reward for Agent 10 this episode : -43.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 254.2433\n",
      "Average Reward for Agent 11 this episode : -8.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6420\n",
      "Average Reward for Agent 12 this episode : -11.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8151\n",
      "Average Reward for Agent 13 this episode : -19.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.4571\n",
      "Reducing exploration for all agents to 0.0336\n",
      "Episode 197 is finished\n",
      "Average Reward for Agent 0 this episode : -8.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.2491\n",
      "Average Reward for Agent 1 this episode : -33.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 197.6069\n",
      "Average Reward for Agent 2 this episode : -22.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.1070\n",
      "Average Reward for Agent 3 this episode : -3.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.2272\n",
      "Average Reward for Agent 4 this episode : -17.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 339.9320\n",
      "Average Reward for Agent 5 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0334\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.3601\n",
      "Average Reward for Agent 7 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1469\n",
      "Average Reward for Agent 8 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6792\n",
      "Average Reward for Agent 9 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7569\n",
      "Average Reward for Agent 10 this episode : -56.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.1094\n",
      "Average Reward for Agent 11 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.4196\n",
      "Average Reward for Agent 12 this episode : -11.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.9279\n",
      "Average Reward for Agent 13 this episode : -7.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.7316\n",
      "Reducing exploration for all agents to 0.033\n",
      "Episode 198 is finished\n",
      "Average Reward for Agent 0 this episode : -7.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.5769\n",
      "Average Reward for Agent 1 this episode : -29.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 193.9994\n",
      "Average Reward for Agent 2 this episode : -19.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.5036\n",
      "Average Reward for Agent 3 this episode : -1.17\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5915\n",
      "Average Reward for Agent 4 this episode : -21.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 198.1483\n",
      "Average Reward for Agent 5 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0874\n",
      "Average Reward for Agent 6 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.6293\n",
      "Average Reward for Agent 7 this episode : -13.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.3109\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8997\n",
      "Average Reward for Agent 9 this episode : -3.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5080\n",
      "Average Reward for Agent 10 this episode : -40.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.8186\n",
      "Average Reward for Agent 11 this episode : -25.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.3805\n",
      "Average Reward for Agent 12 this episode : -20.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.7822\n",
      "Average Reward for Agent 13 this episode : -3.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.2167\n",
      "Reducing exploration for all agents to 0.0325\n",
      "Episode 199 is finished\n",
      "Average Reward for Agent 0 this episode : -7.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.4353\n",
      "Average Reward for Agent 1 this episode : -29.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 205.6276\n",
      "Average Reward for Agent 2 this episode : -25.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.1098\n",
      "Average Reward for Agent 3 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.7613\n",
      "Average Reward for Agent 4 this episode : -17.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.6031\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6524\n",
      "Average Reward for Agent 6 this episode : -4.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.8437\n",
      "Average Reward for Agent 7 this episode : -15.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.4555\n",
      "Average Reward for Agent 8 this episode : -1.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7581\n",
      "Average Reward for Agent 9 this episode : -11.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.9821\n",
      "Average Reward for Agent 10 this episode : -39.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.9647\n",
      "Average Reward for Agent 11 this episode : -18.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.5149\n",
      "Average Reward for Agent 12 this episode : -13.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1892\n",
      "Average Reward for Agent 13 this episode : -7.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0197\n",
      "Reducing exploration for all agents to 0.0319\n",
      "Episode 200 is finished\n",
      "Average Reward for Agent 0 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.7307\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -26.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.8164\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -23.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.3925\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -1.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.8641\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0197\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -2.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -27.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.1873\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.7673\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4721\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -3.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0192\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -33.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 239.8092\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -5.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.6001\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1901\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -14.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.4604\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0314\n",
      "Episode 201 is finished\n",
      "Average Reward for Agent 0 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.5785\n",
      "Average Reward for Agent 1 this episode : -24.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 328.2210\n",
      "Average Reward for Agent 2 this episode : -24.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.3140\n",
      "Average Reward for Agent 3 this episode : -0.69\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9839\n",
      "Average Reward for Agent 4 this episode : -21.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.8764\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9902\n",
      "Average Reward for Agent 6 this episode : -37.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.6131\n",
      "Average Reward for Agent 7 this episode : -14.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9738\n",
      "Average Reward for Agent 8 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9265\n",
      "Average Reward for Agent 9 this episode : -6.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.0773\n",
      "Average Reward for Agent 10 this episode : -47.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 497.6796\n",
      "Average Reward for Agent 11 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.7529\n",
      "Average Reward for Agent 12 this episode : -5.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.3395\n",
      "Average Reward for Agent 13 this episode : -8.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.2264\n",
      "Reducing exploration for all agents to 0.0308\n",
      "Episode 202 is finished\n",
      "Average Reward for Agent 0 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.4280\n",
      "Average Reward for Agent 1 this episode : -21.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.9818\n",
      "Average Reward for Agent 2 this episode : -21.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.3506\n",
      "Average Reward for Agent 3 this episode : -3.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0727\n",
      "Average Reward for Agent 4 this episode : -24.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 248.9380\n",
      "Average Reward for Agent 5 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3716\n",
      "Average Reward for Agent 6 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.5725\n",
      "Average Reward for Agent 7 this episode : -14.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.6996\n",
      "Average Reward for Agent 8 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9311\n",
      "Average Reward for Agent 9 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.9351\n",
      "Average Reward for Agent 10 this episode : -62.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 497.1552\n",
      "Average Reward for Agent 11 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1212\n",
      "Average Reward for Agent 12 this episode : -12.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.9263\n",
      "Average Reward for Agent 13 this episode : -10.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.0965\n",
      "Reducing exploration for all agents to 0.0303\n",
      "Episode 203 is finished\n",
      "Average Reward for Agent 0 this episode : -7.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.7880\n",
      "Average Reward for Agent 1 this episode : -22.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 249.5484\n",
      "Average Reward for Agent 2 this episode : -25.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.7581\n",
      "Average Reward for Agent 3 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.2493\n",
      "Average Reward for Agent 4 this episode : -20.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.1165\n",
      "Average Reward for Agent 5 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6281\n",
      "Average Reward for Agent 6 this episode : -1.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 145.0807\n",
      "Average Reward for Agent 7 this episode : -14.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.5004\n",
      "Average Reward for Agent 8 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7776\n",
      "Average Reward for Agent 9 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.0211\n",
      "Average Reward for Agent 10 this episode : -45.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.9570\n",
      "Average Reward for Agent 11 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.8388\n",
      "Average Reward for Agent 12 this episode : -20.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.3156\n",
      "Average Reward for Agent 13 this episode : -30.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.2917\n",
      "Reducing exploration for all agents to 0.0298\n",
      "Episode 204 is finished\n",
      "Average Reward for Agent 0 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.4291\n",
      "Average Reward for Agent 1 this episode : -19.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.7881\n",
      "Average Reward for Agent 2 this episode : -23.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.3589\n",
      "Average Reward for Agent 3 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1701\n",
      "Average Reward for Agent 4 this episode : -21.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.7247\n",
      "Average Reward for Agent 5 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3481\n",
      "Average Reward for Agent 6 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.5592\n",
      "Average Reward for Agent 7 this episode : -15.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.6791\n",
      "Average Reward for Agent 8 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6915\n",
      "Average Reward for Agent 9 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1864\n",
      "Average Reward for Agent 10 this episode : -17.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 376.8611\n",
      "Average Reward for Agent 11 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.6546\n",
      "Average Reward for Agent 12 this episode : -17.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9122\n",
      "Average Reward for Agent 13 this episode : -14.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.4722\n",
      "Reducing exploration for all agents to 0.0293\n",
      "Episode 205 is finished\n",
      "Average Reward for Agent 0 this episode : -9.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8634\n",
      "Average Reward for Agent 1 this episode : -20.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.3975\n",
      "Average Reward for Agent 2 this episode : -29.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.7956\n",
      "Average Reward for Agent 3 this episode : -1.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.1198\n",
      "Average Reward for Agent 4 this episode : -17.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 239.8712\n",
      "Average Reward for Agent 5 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4281\n",
      "Average Reward for Agent 6 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.0710\n",
      "Average Reward for Agent 7 this episode : -15.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.4073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7848\n",
      "Average Reward for Agent 9 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0328\n",
      "Average Reward for Agent 10 this episode : -18.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 328.4584\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.4008\n",
      "Average Reward for Agent 12 this episode : -16.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.4900\n",
      "Average Reward for Agent 13 this episode : -12.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.7040\n",
      "Reducing exploration for all agents to 0.0288\n",
      "Episode 206 is finished\n",
      "Average Reward for Agent 0 this episode : -9.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.6796\n",
      "Average Reward for Agent 1 this episode : -17.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 231.6143\n",
      "Average Reward for Agent 2 this episode : -27.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.0871\n",
      "Average Reward for Agent 3 this episode : -2.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.8716\n",
      "Average Reward for Agent 4 this episode : -20.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 317.0437\n",
      "Average Reward for Agent 5 this episode : -5.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8524\n",
      "Average Reward for Agent 6 this episode : -1.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7328\n",
      "Average Reward for Agent 7 this episode : -13.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.4099\n",
      "Average Reward for Agent 8 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9870\n",
      "Average Reward for Agent 9 this episode : -5.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3168\n",
      "Average Reward for Agent 10 this episode : -60.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 180.4465\n",
      "Average Reward for Agent 11 this episode : -15.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.0482\n",
      "Average Reward for Agent 12 this episode : -29.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2763\n",
      "Average Reward for Agent 13 this episode : -10.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.0079\n",
      "Reducing exploration for all agents to 0.0283\n",
      "Episode 207 is finished\n",
      "Average Reward for Agent 0 this episode : -8.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.1160\n",
      "Average Reward for Agent 1 this episode : -18.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 267.8620\n",
      "Average Reward for Agent 2 this episode : -33.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.5467\n",
      "Average Reward for Agent 3 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2501\n",
      "Average Reward for Agent 4 this episode : -18.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 343.6183\n",
      "Average Reward for Agent 5 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1494\n",
      "Average Reward for Agent 6 this episode : -1.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.1000\n",
      "Average Reward for Agent 7 this episode : -15.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.4214\n",
      "Average Reward for Agent 8 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4874\n",
      "Average Reward for Agent 9 this episode : -13.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.5724\n",
      "Average Reward for Agent 10 this episode : -45.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.0117\n",
      "Average Reward for Agent 11 this episode : -43.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.4942\n",
      "Average Reward for Agent 12 this episode : -12.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.2221\n",
      "Average Reward for Agent 13 this episode : -19.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.2152\n",
      "Reducing exploration for all agents to 0.0278\n",
      "Episode 208 is finished\n",
      "Average Reward for Agent 0 this episode : -8.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9800\n",
      "Average Reward for Agent 1 this episode : -19.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 177.5907\n",
      "Average Reward for Agent 2 this episode : -53.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 192.3902\n",
      "Average Reward for Agent 3 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.8203\n",
      "Average Reward for Agent 4 this episode : -17.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.3634\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9319\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.5484\n",
      "Average Reward for Agent 7 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.9543\n",
      "Average Reward for Agent 8 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4865\n",
      "Average Reward for Agent 9 this episode : -29.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8119\n",
      "Average Reward for Agent 10 this episode : -42.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 390.7974\n",
      "Average Reward for Agent 11 this episode : -26.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.0507\n",
      "Average Reward for Agent 12 this episode : -8.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9102\n",
      "Average Reward for Agent 13 this episode : -2.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.9931\n",
      "Reducing exploration for all agents to 0.0273\n",
      "Episode 209 is finished\n",
      "Average Reward for Agent 0 this episode : -8.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.0315\n",
      "Average Reward for Agent 1 this episode : -16.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1788\n",
      "Average Reward for Agent 2 this episode : -40.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.4191\n",
      "Average Reward for Agent 3 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2860\n",
      "Average Reward for Agent 4 this episode : -22.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 438.7581\n",
      "Average Reward for Agent 5 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8046\n",
      "Average Reward for Agent 6 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.3046\n",
      "Average Reward for Agent 7 this episode : -14.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.3452\n",
      "Average Reward for Agent 8 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5986\n",
      "Average Reward for Agent 9 this episode : -22.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.1398\n",
      "Average Reward for Agent 10 this episode : -51.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 349.6345\n",
      "Average Reward for Agent 11 this episode : -40.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.3976\n",
      "Average Reward for Agent 12 this episode : -18.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0728\n",
      "Average Reward for Agent 13 this episode : -3.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.9063\n",
      "Reducing exploration for all agents to 0.0268\n",
      "Episode 210 is finished\n",
      "Average Reward for Agent 0 this episode : -8.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4830\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.4792\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -44.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 296.6857\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -6.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.2729\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -27.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.2996\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3970\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.6397\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1472\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5753\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -19.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.2719\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -52.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.8253\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -46.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.9057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -12.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.4574\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -7.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.6389\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0264\n",
      "Episode 211 is finished\n",
      "Average Reward for Agent 0 this episode : -12.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4805\n",
      "Average Reward for Agent 1 this episode : -13.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.5249\n",
      "Average Reward for Agent 2 this episode : -54.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 360.2963\n",
      "Average Reward for Agent 3 this episode : -11.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5575\n",
      "Average Reward for Agent 4 this episode : -18.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.2484\n",
      "Average Reward for Agent 5 this episode : -1.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6361\n",
      "Average Reward for Agent 6 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0094\n",
      "Average Reward for Agent 7 this episode : -13.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.0870\n",
      "Average Reward for Agent 8 this episode : -5.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4561\n",
      "Average Reward for Agent 9 this episode : -20.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.6801\n",
      "Average Reward for Agent 10 this episode : -37.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 567.5945\n",
      "Average Reward for Agent 11 this episode : -35.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 578.3784\n",
      "Average Reward for Agent 12 this episode : -11.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.9668\n",
      "Average Reward for Agent 13 this episode : -14.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.8337\n",
      "Reducing exploration for all agents to 0.0259\n",
      "Episode 212 is finished\n",
      "Average Reward for Agent 0 this episode : -8.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1324\n",
      "Average Reward for Agent 1 this episode : -22.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.2485\n",
      "Average Reward for Agent 2 this episode : -45.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6408\n",
      "Average Reward for Agent 3 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.3756\n",
      "Average Reward for Agent 4 this episode : -19.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 265.2144\n",
      "Average Reward for Agent 5 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3772\n",
      "Average Reward for Agent 6 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.7560\n",
      "Average Reward for Agent 7 this episode : -13.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3211\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3541\n",
      "Average Reward for Agent 9 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.2844\n",
      "Average Reward for Agent 10 this episode : -7.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.1936\n",
      "Average Reward for Agent 11 this episode : -30.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 309.2182\n",
      "Average Reward for Agent 12 this episode : -8.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.2020\n",
      "Average Reward for Agent 13 this episode : -8.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.4894\n",
      "Reducing exploration for all agents to 0.0255\n",
      "Episode 213 is finished\n",
      "Average Reward for Agent 0 this episode : -9.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.2307\n",
      "Average Reward for Agent 1 this episode : -15.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.2594\n",
      "Average Reward for Agent 2 this episode : -54.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 311.6386\n",
      "Average Reward for Agent 3 this episode : -1.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.1216\n",
      "Average Reward for Agent 4 this episode : -23.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 287.5095\n",
      "Average Reward for Agent 5 this episode : -1.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3277\n",
      "Average Reward for Agent 6 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1126\n",
      "Average Reward for Agent 7 this episode : -8.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2936\n",
      "Average Reward for Agent 8 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3063\n",
      "Average Reward for Agent 9 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 246.3578\n",
      "Average Reward for Agent 10 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 731.6037\n",
      "Average Reward for Agent 11 this episode : -7.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 606.0239\n",
      "Average Reward for Agent 12 this episode : -7.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4570\n",
      "Average Reward for Agent 13 this episode : -9.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.0640\n",
      "Reducing exploration for all agents to 0.025\n",
      "Episode 214 is finished\n",
      "Average Reward for Agent 0 this episode : -9.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.8590\n",
      "Average Reward for Agent 1 this episode : -16.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.7177\n",
      "Average Reward for Agent 2 this episode : -42.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.0513\n",
      "Average Reward for Agent 3 this episode : -0.46\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1878\n",
      "Average Reward for Agent 4 this episode : -17.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7453\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2440\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1506\n",
      "Average Reward for Agent 7 this episode : -12.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3945\n",
      "Average Reward for Agent 8 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0969\n",
      "Average Reward for Agent 9 this episode : -1.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.2867\n",
      "Average Reward for Agent 10 this episode : -1.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 452.5707\n",
      "Average Reward for Agent 11 this episode : -50.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.9104\n",
      "Average Reward for Agent 12 this episode : -6.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.3812\n",
      "Average Reward for Agent 13 this episode : -11.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.8837\n",
      "Reducing exploration for all agents to 0.0246\n",
      "Episode 215 is finished\n",
      "Average Reward for Agent 0 this episode : -9.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9286\n",
      "Average Reward for Agent 1 this episode : -18.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.0690\n",
      "Average Reward for Agent 2 this episode : -43.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 237.6936\n",
      "Average Reward for Agent 3 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.0098\n",
      "Average Reward for Agent 4 this episode : -19.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.3274\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1212\n",
      "Average Reward for Agent 6 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5721\n",
      "Average Reward for Agent 7 this episode : -12.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.9413\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6365\n",
      "Average Reward for Agent 9 this episode : -43.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.8349\n",
      "Average Reward for Agent 10 this episode : -2.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 316.0307\n",
      "Average Reward for Agent 11 this episode : -22.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.5587\n",
      "Average Reward for Agent 12 this episode : -12.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8011\n",
      "Average Reward for Agent 13 this episode : -18.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.6140\n",
      "Reducing exploration for all agents to 0.0242\n",
      "Episode 216 is finished\n",
      "Average Reward for Agent 0 this episode : -12.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7496\n",
      "Average Reward for Agent 1 this episode : -16.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.3491\n",
      "Average Reward for Agent 2 this episode : -34.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.7955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 3 this episode : -5.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0268\n",
      "Average Reward for Agent 4 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9781\n",
      "Average Reward for Agent 5 this episode : -1.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0464\n",
      "Average Reward for Agent 6 this episode : -0.31\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2898\n",
      "Average Reward for Agent 7 this episode : -12.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.9782\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6114\n",
      "Average Reward for Agent 9 this episode : -25.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 150.4204\n",
      "Average Reward for Agent 10 this episode : -6.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 180.7321\n",
      "Average Reward for Agent 11 this episode : -33.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 366.1126\n",
      "Average Reward for Agent 12 this episode : -10.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.7096\n",
      "Average Reward for Agent 13 this episode : -6.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.2319\n",
      "Reducing exploration for all agents to 0.0238\n",
      "Episode 217 is finished\n",
      "Average Reward for Agent 0 this episode : -8.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.9965\n",
      "Average Reward for Agent 1 this episode : -16.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 386.7484\n",
      "Average Reward for Agent 2 this episode : -42.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.1961\n",
      "Average Reward for Agent 3 this episode : -5.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7370\n",
      "Average Reward for Agent 4 this episode : -20.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.1753\n",
      "Average Reward for Agent 5 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1831\n",
      "Average Reward for Agent 6 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8814\n",
      "Average Reward for Agent 7 this episode : -12.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.3519\n",
      "Average Reward for Agent 8 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9209\n",
      "Average Reward for Agent 9 this episode : -41.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.6334\n",
      "Average Reward for Agent 10 this episode : -38.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.7073\n",
      "Average Reward for Agent 11 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.7324\n",
      "Average Reward for Agent 12 this episode : -12.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.7112\n",
      "Average Reward for Agent 13 this episode : -19.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8023\n",
      "Reducing exploration for all agents to 0.0234\n",
      "Episode 218 is finished\n",
      "Average Reward for Agent 0 this episode : -9.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.8277\n",
      "Average Reward for Agent 1 this episode : -16.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 169.2017\n",
      "Average Reward for Agent 2 this episode : -45.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.2338\n",
      "Average Reward for Agent 3 this episode : -7.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1145\n",
      "Average Reward for Agent 4 this episode : -21.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.9596\n",
      "Average Reward for Agent 5 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1018\n",
      "Average Reward for Agent 6 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3274\n",
      "Average Reward for Agent 7 this episode : -12.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9892\n",
      "Average Reward for Agent 8 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0860\n",
      "Average Reward for Agent 9 this episode : -10.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 224.4409\n",
      "Average Reward for Agent 10 this episode : -39.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.2445\n",
      "Average Reward for Agent 11 this episode : -42.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0090\n",
      "Average Reward for Agent 12 this episode : -6.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.3180\n",
      "Average Reward for Agent 13 this episode : -3.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.4501\n",
      "Reducing exploration for all agents to 0.023\n",
      "Episode 219 is finished\n",
      "Average Reward for Agent 0 this episode : -10.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.6308\n",
      "Average Reward for Agent 1 this episode : -14.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.7032\n",
      "Average Reward for Agent 2 this episode : -47.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.6621\n",
      "Average Reward for Agent 3 this episode : -2.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0930\n",
      "Average Reward for Agent 4 this episode : -19.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.6625\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6572\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4155\n",
      "Average Reward for Agent 7 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.3048\n",
      "Average Reward for Agent 8 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0944\n",
      "Average Reward for Agent 9 this episode : -4.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.5535\n",
      "Average Reward for Agent 10 this episode : -42.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 273.2279\n",
      "Average Reward for Agent 11 this episode : -15.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.0097\n",
      "Average Reward for Agent 12 this episode : -10.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.6132\n",
      "Average Reward for Agent 13 this episode : -1.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5629\n",
      "Reducing exploration for all agents to 0.0226\n",
      "Episode 220 is finished\n",
      "Average Reward for Agent 0 this episode : -8.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.3520\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.6096\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -48.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 375.0239\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1429\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -23.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.9559\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -2.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6491\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7207\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0614\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3107\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -47.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.1671\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -57.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 324.6512\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.7221\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -10.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9982\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -11.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.2555\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0222\n",
      "Episode 221 is finished\n",
      "Average Reward for Agent 0 this episode : -9.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.9696\n",
      "Average Reward for Agent 1 this episode : -15.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 2 this episode : -47.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 341.5912\n",
      "Average Reward for Agent 3 this episode : -5.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3930\n",
      "Average Reward for Agent 4 this episode : -19.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.0712\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7918\n",
      "Average Reward for Agent 6 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3452\n",
      "Average Reward for Agent 7 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1502\n",
      "Average Reward for Agent 8 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3327\n",
      "Average Reward for Agent 9 this episode : -42.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 413.9709\n",
      "Average Reward for Agent 10 this episode : -48.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 508.8001\n",
      "Average Reward for Agent 11 this episode : -0.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.2827\n",
      "Average Reward for Agent 12 this episode : -11.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2113\n",
      "Average Reward for Agent 13 this episode : -15.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.8890\n",
      "Reducing exploration for all agents to 0.0218\n",
      "Episode 222 is finished\n",
      "Average Reward for Agent 0 this episode : -10.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.9360\n",
      "Average Reward for Agent 1 this episode : -16.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.8279\n",
      "Average Reward for Agent 2 this episode : -53.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.0067\n",
      "Average Reward for Agent 3 this episode : -3.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.3300\n",
      "Average Reward for Agent 4 this episode : -20.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.7631\n",
      "Average Reward for Agent 5 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3830\n",
      "Average Reward for Agent 6 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3852\n",
      "Average Reward for Agent 7 this episode : -13.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.4374\n",
      "Average Reward for Agent 8 this episode : -2.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8540\n",
      "Average Reward for Agent 9 this episode : -13.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.3306\n",
      "Average Reward for Agent 10 this episode : -1.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.6933\n",
      "Average Reward for Agent 11 this episode : -1.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.7959\n",
      "Average Reward for Agent 12 this episode : -8.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5024\n",
      "Average Reward for Agent 13 this episode : -10.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.5245\n",
      "Reducing exploration for all agents to 0.0214\n",
      "Episode 223 is finished\n",
      "Average Reward for Agent 0 this episode : -9.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.9261\n",
      "Average Reward for Agent 1 this episode : -17.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.9800\n",
      "Average Reward for Agent 2 this episode : -39.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 204.0556\n",
      "Average Reward for Agent 3 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.4907\n",
      "Average Reward for Agent 4 this episode : -26.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.0822\n",
      "Average Reward for Agent 5 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8585\n",
      "Average Reward for Agent 6 this episode : -1.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3525\n",
      "Average Reward for Agent 7 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.0769\n",
      "Average Reward for Agent 8 this episode : -1.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7422\n",
      "Average Reward for Agent 9 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 271.1955\n",
      "Average Reward for Agent 10 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 709.0652\n",
      "Average Reward for Agent 11 this episode : -4.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.3164\n",
      "Average Reward for Agent 12 this episode : -10.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.7207\n",
      "Average Reward for Agent 13 this episode : -6.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.6749\n",
      "Reducing exploration for all agents to 0.0211\n",
      "Episode 224 is finished\n",
      "Average Reward for Agent 0 this episode : -8.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.3645\n",
      "Average Reward for Agent 1 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2592\n",
      "Average Reward for Agent 2 this episode : -50.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 257.9243\n",
      "Average Reward for Agent 3 this episode : -5.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.3853\n",
      "Average Reward for Agent 4 this episode : -21.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 154.5783\n",
      "Average Reward for Agent 5 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7390\n",
      "Average Reward for Agent 6 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5352\n",
      "Average Reward for Agent 7 this episode : -12.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.1352\n",
      "Average Reward for Agent 8 this episode : -3.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2174\n",
      "Average Reward for Agent 9 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 180.8435\n",
      "Average Reward for Agent 10 this episode : -4.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 341.3053\n",
      "Average Reward for Agent 11 this episode : -38.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.4032\n",
      "Average Reward for Agent 12 this episode : -5.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.6903\n",
      "Average Reward for Agent 13 this episode : -7.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.5197\n",
      "Reducing exploration for all agents to 0.0207\n",
      "Episode 225 is finished\n",
      "Average Reward for Agent 0 this episode : -9.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.8483\n",
      "Average Reward for Agent 1 this episode : -16.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.3269\n",
      "Average Reward for Agent 2 this episode : -51.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.3819\n",
      "Average Reward for Agent 3 this episode : -3.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5124\n",
      "Average Reward for Agent 4 this episode : -19.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 181.6111\n",
      "Average Reward for Agent 5 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0802\n",
      "Average Reward for Agent 6 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3447\n",
      "Average Reward for Agent 7 this episode : -13.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.3357\n",
      "Average Reward for Agent 8 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3345\n",
      "Average Reward for Agent 9 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.2295\n",
      "Average Reward for Agent 10 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.6605\n",
      "Average Reward for Agent 11 this episode : -10.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.8302\n",
      "Average Reward for Agent 12 this episode : -10.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.8857\n",
      "Average Reward for Agent 13 this episode : -10.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.6763\n",
      "Reducing exploration for all agents to 0.0203\n",
      "Episode 226 is finished\n",
      "Average Reward for Agent 0 this episode : -9.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.9080\n",
      "Average Reward for Agent 1 this episode : -13.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.9587\n",
      "Average Reward for Agent 2 this episode : -35.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.2992\n",
      "Average Reward for Agent 3 this episode : -9.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7952\n",
      "Average Reward for Agent 4 this episode : -19.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.8338\n",
      "Average Reward for Agent 5 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8610\n",
      "Average Reward for Agent 6 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8020\n",
      "Average Reward for Agent 7 this episode : -13.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0447\n",
      "Average Reward for Agent 8 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4045\n",
      "Average Reward for Agent 9 this episode : -43.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.7776\n",
      "Average Reward for Agent 10 this episode : -24.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1628\n",
      "Average Reward for Agent 11 this episode : -19.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.7305\n",
      "Average Reward for Agent 12 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.3936\n",
      "Average Reward for Agent 13 this episode : -18.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.1558\n",
      "Reducing exploration for all agents to 0.02\n",
      "Episode 227 is finished\n",
      "Average Reward for Agent 0 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.0198\n",
      "Average Reward for Agent 1 this episode : -14.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.7747\n",
      "Average Reward for Agent 2 this episode : -53.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.7529\n",
      "Average Reward for Agent 3 this episode : -2.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7461\n",
      "Average Reward for Agent 4 this episode : -17.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.3967\n",
      "Average Reward for Agent 5 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8694\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3520\n",
      "Average Reward for Agent 7 this episode : -13.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2383\n",
      "Average Reward for Agent 8 this episode : -2.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6148\n",
      "Average Reward for Agent 9 this episode : -45.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 220.6613\n",
      "Average Reward for Agent 10 this episode : -50.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.4401\n",
      "Average Reward for Agent 11 this episode : -22.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.7281\n",
      "Average Reward for Agent 12 this episode : -9.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1792\n",
      "Average Reward for Agent 13 this episode : -8.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.0577\n",
      "Reducing exploration for all agents to 0.0196\n",
      "Episode 228 is finished\n",
      "Average Reward for Agent 0 this episode : -8.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9248\n",
      "Average Reward for Agent 1 this episode : -14.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0163\n",
      "Average Reward for Agent 2 this episode : -55.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.5117\n",
      "Average Reward for Agent 3 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8996\n",
      "Average Reward for Agent 4 this episode : -21.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 182.6252\n",
      "Average Reward for Agent 5 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3484\n",
      "Average Reward for Agent 6 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4471\n",
      "Average Reward for Agent 7 this episode : -9.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7846\n",
      "Average Reward for Agent 8 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6478\n",
      "Average Reward for Agent 9 this episode : -3.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.0130\n",
      "Average Reward for Agent 10 this episode : -49.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 536.6758\n",
      "Average Reward for Agent 11 this episode : -23.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.8239\n",
      "Average Reward for Agent 12 this episode : -6.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1103\n",
      "Average Reward for Agent 13 this episode : -15.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.9998\n",
      "Reducing exploration for all agents to 0.0193\n",
      "Episode 229 is finished\n",
      "Average Reward for Agent 0 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.8231\n",
      "Average Reward for Agent 1 this episode : -15.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.8931\n",
      "Average Reward for Agent 2 this episode : -47.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.1267\n",
      "Average Reward for Agent 3 this episode : -1.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.0633\n",
      "Average Reward for Agent 4 this episode : -17.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.3510\n",
      "Average Reward for Agent 5 this episode : -1.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6840\n",
      "Average Reward for Agent 6 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0070\n",
      "Average Reward for Agent 7 this episode : -15.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1958\n",
      "Average Reward for Agent 8 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5012\n",
      "Average Reward for Agent 9 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.5633\n",
      "Average Reward for Agent 10 this episode : -49.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 371.9687\n",
      "Average Reward for Agent 11 this episode : -7.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.4236\n",
      "Average Reward for Agent 12 this episode : -8.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0309\n",
      "Average Reward for Agent 13 this episode : -55.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.1363\n",
      "Reducing exploration for all agents to 0.019\n",
      "Episode 230 is finished\n",
      "Average Reward for Agent 0 this episode : -9.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6494\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -10.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.7215\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -48.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.1872\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -6.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2266\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -20.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 286.7814\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8935\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4360\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -10.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0950\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1804\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.7786\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -2.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.3312\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -29.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.4270\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8925\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -46.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.5031\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0186\n",
      "Episode 231 is finished\n",
      "Average Reward for Agent 0 this episode : -9.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.8962\n",
      "Average Reward for Agent 1 this episode : -12.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 336.1835\n",
      "Average Reward for Agent 2 this episode : -41.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.4468\n",
      "Average Reward for Agent 3 this episode : -3.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.8808\n",
      "Average Reward for Agent 4 this episode : -19.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.7811\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6293\n",
      "Average Reward for Agent 6 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6321\n",
      "Average Reward for Agent 7 this episode : -13.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.8320\n",
      "Average Reward for Agent 8 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7599\n",
      "Average Reward for Agent 9 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.5053\n",
      "Average Reward for Agent 10 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.0591\n",
      "Average Reward for Agent 11 this episode : -24.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.3792\n",
      "Average Reward for Agent 12 this episode : -5.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4321\n",
      "Average Reward for Agent 13 this episode : -54.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.1963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0183\n",
      "Episode 232 is finished\n",
      "Average Reward for Agent 0 this episode : -9.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3504\n",
      "Average Reward for Agent 1 this episode : -12.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.1900\n",
      "Average Reward for Agent 2 this episode : -41.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.8258\n",
      "Average Reward for Agent 3 this episode : -4.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5377\n",
      "Average Reward for Agent 4 this episode : -21.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.3190\n",
      "Average Reward for Agent 5 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8108\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2184\n",
      "Average Reward for Agent 7 this episode : -14.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0237\n",
      "Average Reward for Agent 8 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4572\n",
      "Average Reward for Agent 9 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.0567\n",
      "Average Reward for Agent 10 this episode : -21.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 170.9859\n",
      "Average Reward for Agent 11 this episode : -18.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.1103\n",
      "Average Reward for Agent 12 this episode : -5.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7900\n",
      "Average Reward for Agent 13 this episode : -52.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.2095\n",
      "Reducing exploration for all agents to 0.018\n",
      "Episode 233 is finished\n",
      "Average Reward for Agent 0 this episode : -9.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.2491\n",
      "Average Reward for Agent 1 this episode : -14.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 215.0080\n",
      "Average Reward for Agent 2 this episode : -55.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.4415\n",
      "Average Reward for Agent 3 this episode : -5.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.9840\n",
      "Average Reward for Agent 4 this episode : -21.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5046\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9163\n",
      "Average Reward for Agent 6 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3816\n",
      "Average Reward for Agent 7 this episode : -14.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5990\n",
      "Average Reward for Agent 8 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3133\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4650\n",
      "Average Reward for Agent 10 this episode : -51.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.3175\n",
      "Average Reward for Agent 11 this episode : -5.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.9455\n",
      "Average Reward for Agent 12 this episode : -7.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3579\n",
      "Average Reward for Agent 13 this episode : -6.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.0999\n",
      "Reducing exploration for all agents to 0.0177\n",
      "Episode 234 is finished\n",
      "Average Reward for Agent 0 this episode : -8.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.7462\n",
      "Average Reward for Agent 1 this episode : -8.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.5176\n",
      "Average Reward for Agent 2 this episode : -40.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8708\n",
      "Average Reward for Agent 3 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.7206\n",
      "Average Reward for Agent 4 this episode : -16.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.5917\n",
      "Average Reward for Agent 5 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0961\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8198\n",
      "Average Reward for Agent 7 this episode : -15.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0214\n",
      "Average Reward for Agent 8 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0256\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0025\n",
      "Average Reward for Agent 10 this episode : -38.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.6053\n",
      "Average Reward for Agent 11 this episode : -14.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3660\n",
      "Average Reward for Agent 12 this episode : -11.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8827\n",
      "Average Reward for Agent 13 this episode : -54.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 170.0919\n",
      "Reducing exploration for all agents to 0.0174\n",
      "Episode 235 is finished\n",
      "Average Reward for Agent 0 this episode : -8.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.8288\n",
      "Average Reward for Agent 1 this episode : -12.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5294\n",
      "Average Reward for Agent 2 this episode : -47.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.4557\n",
      "Average Reward for Agent 3 this episode : -5.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8516\n",
      "Average Reward for Agent 4 this episode : -18.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 203.6836\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4843\n",
      "Average Reward for Agent 6 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6768\n",
      "Average Reward for Agent 7 this episode : -11.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.1892\n",
      "Average Reward for Agent 8 this episode : -1.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3654\n",
      "Average Reward for Agent 9 this episode : -1.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5500\n",
      "Average Reward for Agent 10 this episode : -41.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.4855\n",
      "Average Reward for Agent 11 this episode : -7.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1295\n",
      "Average Reward for Agent 12 this episode : -9.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3555\n",
      "Average Reward for Agent 13 this episode : -44.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.5380\n",
      "Reducing exploration for all agents to 0.0171\n",
      "Episode 236 is finished\n",
      "Average Reward for Agent 0 this episode : -9.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.3318\n",
      "Average Reward for Agent 1 this episode : -14.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.2355\n",
      "Average Reward for Agent 2 this episode : -46.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.3905\n",
      "Average Reward for Agent 3 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2891\n",
      "Average Reward for Agent 4 this episode : -22.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 254.7280\n",
      "Average Reward for Agent 5 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1486\n",
      "Average Reward for Agent 6 this episode : -1.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7275\n",
      "Average Reward for Agent 7 this episode : -13.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8566\n",
      "Average Reward for Agent 8 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4518\n",
      "Average Reward for Agent 9 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.8784\n",
      "Average Reward for Agent 10 this episode : -52.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.1165\n",
      "Average Reward for Agent 11 this episode : -11.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6031\n",
      "Average Reward for Agent 12 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4253\n",
      "Average Reward for Agent 13 this episode : -37.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.6023\n",
      "Reducing exploration for all agents to 0.0168\n",
      "Episode 237 is finished\n",
      "Average Reward for Agent 0 this episode : -11.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.8003\n",
      "Average Reward for Agent 1 this episode : -14.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.8807\n",
      "Average Reward for Agent 2 this episode : -49.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.6241\n",
      "Average Reward for Agent 3 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.1162\n",
      "Average Reward for Agent 4 this episode : -21.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.3474\n",
      "Average Reward for Agent 5 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7708\n",
      "Average Reward for Agent 7 this episode : -9.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5520\n",
      "Average Reward for Agent 8 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3993\n",
      "Average Reward for Agent 9 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.6957\n",
      "Average Reward for Agent 10 this episode : -50.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.8532\n",
      "Average Reward for Agent 11 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.4922\n",
      "Average Reward for Agent 12 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3206\n",
      "Average Reward for Agent 13 this episode : -12.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.4745\n",
      "Reducing exploration for all agents to 0.0165\n",
      "Episode 238 is finished\n",
      "Average Reward for Agent 0 this episode : -9.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.7184\n",
      "Average Reward for Agent 1 this episode : -14.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4387\n",
      "Average Reward for Agent 2 this episode : -55.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.4872\n",
      "Average Reward for Agent 3 this episode : -5.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.2482\n",
      "Average Reward for Agent 4 this episode : -20.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.9767\n",
      "Average Reward for Agent 5 this episode : -2.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6356\n",
      "Average Reward for Agent 6 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2255\n",
      "Average Reward for Agent 7 this episode : -11.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1769\n",
      "Average Reward for Agent 8 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3199\n",
      "Average Reward for Agent 9 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.7351\n",
      "Average Reward for Agent 10 this episode : -49.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.3168\n",
      "Average Reward for Agent 11 this episode : -7.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.2847\n",
      "Average Reward for Agent 12 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6205\n",
      "Average Reward for Agent 13 this episode : -9.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 253.5626\n",
      "Reducing exploration for all agents to 0.0162\n",
      "Episode 239 is finished\n",
      "Average Reward for Agent 0 this episode : -10.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.8039\n",
      "Average Reward for Agent 1 this episode : -15.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8451\n",
      "Average Reward for Agent 2 this episode : -47.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.4266\n",
      "Average Reward for Agent 3 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9011\n",
      "Average Reward for Agent 4 this episode : -19.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.3401\n",
      "Average Reward for Agent 5 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1837\n",
      "Average Reward for Agent 6 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2973\n",
      "Average Reward for Agent 7 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.2890\n",
      "Average Reward for Agent 8 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4891\n",
      "Average Reward for Agent 9 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.0829\n",
      "Average Reward for Agent 10 this episode : -47.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 454.8944\n",
      "Average Reward for Agent 11 this episode : -13.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5809\n",
      "Average Reward for Agent 12 this episode : -12.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.8800\n",
      "Average Reward for Agent 13 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.3201\n",
      "Reducing exploration for all agents to 0.016\n",
      "Episode 240 is finished\n",
      "Average Reward for Agent 0 this episode : -8.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.4825\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -14.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.2416\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -51.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.9908\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6936\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.2632\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1040\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9946\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -15.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.6507\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5816\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.4742\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -48.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.2827\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7196\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -8.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2311\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -5.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.5878\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0157\n",
      "Episode 241 is finished\n",
      "Average Reward for Agent 0 this episode : -13.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.8316\n",
      "Average Reward for Agent 1 this episode : -14.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.2332\n",
      "Average Reward for Agent 2 this episode : -39.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 376.9191\n",
      "Average Reward for Agent 3 this episode : -1.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5911\n",
      "Average Reward for Agent 4 this episode : -18.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.0793\n",
      "Average Reward for Agent 5 this episode : -0.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6311\n",
      "Average Reward for Agent 6 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2998\n",
      "Average Reward for Agent 7 this episode : -10.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9445\n",
      "Average Reward for Agent 8 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8885\n",
      "Average Reward for Agent 9 this episode : -3.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.1191\n",
      "Average Reward for Agent 10 this episode : -42.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 249.1583\n",
      "Average Reward for Agent 11 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5803\n",
      "Average Reward for Agent 12 this episode : -8.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.6321\n",
      "Average Reward for Agent 13 this episode : -4.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.5836\n",
      "Reducing exploration for all agents to 0.0154\n",
      "Episode 242 is finished\n",
      "Average Reward for Agent 0 this episode : -13.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.5748\n",
      "Average Reward for Agent 1 this episode : -13.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.2939\n",
      "Average Reward for Agent 2 this episode : -25.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.2666\n",
      "Average Reward for Agent 3 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.0011\n",
      "Average Reward for Agent 4 this episode : -19.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.1880\n",
      "Average Reward for Agent 5 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5067\n",
      "Average Reward for Agent 6 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6000\n",
      "Average Reward for Agent 7 this episode : -14.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -2.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2421\n",
      "Average Reward for Agent 9 this episode : -3.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2902\n",
      "Average Reward for Agent 10 this episode : -29.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1071.1663\n",
      "Average Reward for Agent 11 this episode : -11.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3002\n",
      "Average Reward for Agent 12 this episode : -8.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.6385\n",
      "Average Reward for Agent 13 this episode : -18.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.6833\n",
      "Reducing exploration for all agents to 0.0152\n",
      "Episode 243 is finished\n",
      "Average Reward for Agent 0 this episode : -60.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.1904\n",
      "Average Reward for Agent 1 this episode : -14.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.3122\n",
      "Average Reward for Agent 2 this episode : -37.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 308.2233\n",
      "Average Reward for Agent 3 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2113\n",
      "Average Reward for Agent 4 this episode : -18.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 288.6175\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1509\n",
      "Average Reward for Agent 6 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8023\n",
      "Average Reward for Agent 7 this episode : -12.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5328\n",
      "Average Reward for Agent 8 this episode : -1.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7136\n",
      "Average Reward for Agent 9 this episode : -3.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6349\n",
      "Average Reward for Agent 10 this episode : -50.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 451.4456\n",
      "Average Reward for Agent 11 this episode : -7.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.4246\n",
      "Average Reward for Agent 12 this episode : -9.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5650\n",
      "Average Reward for Agent 13 this episode : -15.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.4121\n",
      "Reducing exploration for all agents to 0.0149\n",
      "Episode 244 is finished\n",
      "Average Reward for Agent 0 this episode : -35.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 561.8657\n",
      "Average Reward for Agent 1 this episode : -10.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.3141\n",
      "Average Reward for Agent 2 this episode : -55.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.1829\n",
      "Average Reward for Agent 3 this episode : -12.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0955\n",
      "Average Reward for Agent 4 this episode : -16.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.7302\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2392\n",
      "Average Reward for Agent 6 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2184\n",
      "Average Reward for Agent 7 this episode : -10.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.4355\n",
      "Average Reward for Agent 8 this episode : -1.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8260\n",
      "Average Reward for Agent 9 this episode : -3.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2395\n",
      "Average Reward for Agent 10 this episode : -30.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 333.7205\n",
      "Average Reward for Agent 11 this episode : -4.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.3137\n",
      "Average Reward for Agent 12 this episode : -8.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4277\n",
      "Average Reward for Agent 13 this episode : -15.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.5859\n",
      "Reducing exploration for all agents to 0.0146\n",
      "Episode 245 is finished\n",
      "Average Reward for Agent 0 this episode : -12.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 228.1839\n",
      "Average Reward for Agent 1 this episode : -12.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6369\n",
      "Average Reward for Agent 2 this episode : -53.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2693\n",
      "Average Reward for Agent 3 this episode : -5.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2783\n",
      "Average Reward for Agent 4 this episode : -19.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.4614\n",
      "Average Reward for Agent 5 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0421\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5473\n",
      "Average Reward for Agent 7 this episode : -11.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0435\n",
      "Average Reward for Agent 8 this episode : -2.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1740\n",
      "Average Reward for Agent 9 this episode : -2.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2736\n",
      "Average Reward for Agent 10 this episode : -56.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 968.0775\n",
      "Average Reward for Agent 11 this episode : -5.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.6239\n",
      "Average Reward for Agent 12 this episode : -7.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5012\n",
      "Average Reward for Agent 13 this episode : -17.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.2475\n",
      "Reducing exploration for all agents to 0.0144\n",
      "Episode 246 is finished\n",
      "Average Reward for Agent 0 this episode : -14.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 388.3299\n",
      "Average Reward for Agent 1 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.2983\n",
      "Average Reward for Agent 2 this episode : -30.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.6987\n",
      "Average Reward for Agent 3 this episode : -10.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0443\n",
      "Average Reward for Agent 4 this episode : -19.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.6033\n",
      "Average Reward for Agent 5 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2561\n",
      "Average Reward for Agent 6 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7969\n",
      "Average Reward for Agent 7 this episode : -13.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.3123\n",
      "Average Reward for Agent 8 this episode : -1.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1363\n",
      "Average Reward for Agent 9 this episode : -1.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.9990\n",
      "Average Reward for Agent 10 this episode : -52.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 248.1852\n",
      "Average Reward for Agent 11 this episode : -7.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.9198\n",
      "Average Reward for Agent 12 this episode : -8.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0313\n",
      "Average Reward for Agent 13 this episode : -16.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.9546\n",
      "Reducing exploration for all agents to 0.0141\n",
      "Episode 247 is finished\n",
      "Average Reward for Agent 0 this episode : -12.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.6044\n",
      "Average Reward for Agent 1 this episode : -9.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5185\n",
      "Average Reward for Agent 2 this episode : -46.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 218.0683\n",
      "Average Reward for Agent 3 this episode : -3.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2846\n",
      "Average Reward for Agent 4 this episode : -23.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1526\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5456\n",
      "Average Reward for Agent 6 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1955\n",
      "Average Reward for Agent 7 this episode : -11.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2178\n",
      "Average Reward for Agent 8 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0213\n",
      "Average Reward for Agent 9 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.2060\n",
      "Average Reward for Agent 10 this episode : -45.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 810.0087\n",
      "Average Reward for Agent 11 this episode : -7.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.2166\n",
      "Average Reward for Agent 12 this episode : -5.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3321\n",
      "Average Reward for Agent 13 this episode : -42.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.3813\n",
      "Reducing exploration for all agents to 0.0139\n",
      "Episode 248 is finished\n",
      "Average Reward for Agent 0 this episode : -38.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.2249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.6469\n",
      "Average Reward for Agent 2 this episode : -32.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9407\n",
      "Average Reward for Agent 3 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6847\n",
      "Average Reward for Agent 4 this episode : -21.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.2477\n",
      "Average Reward for Agent 5 this episode : -1.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8400\n",
      "Average Reward for Agent 6 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8635\n",
      "Average Reward for Agent 7 this episode : -14.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.6467\n",
      "Average Reward for Agent 8 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8162\n",
      "Average Reward for Agent 9 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9427\n",
      "Average Reward for Agent 10 this episode : -53.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 500.8222\n",
      "Average Reward for Agent 11 this episode : -11.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.1606\n",
      "Average Reward for Agent 12 this episode : -11.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2759\n",
      "Average Reward for Agent 13 this episode : -51.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.4107\n",
      "Reducing exploration for all agents to 0.0137\n",
      "Episode 249 is finished\n",
      "Average Reward for Agent 0 this episode : -11.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1015\n",
      "Average Reward for Agent 1 this episode : -9.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.6219\n",
      "Average Reward for Agent 2 this episode : -47.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 138.9208\n",
      "Average Reward for Agent 3 this episode : -1.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0202\n",
      "Average Reward for Agent 4 this episode : -18.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.2247\n",
      "Average Reward for Agent 5 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0137\n",
      "Average Reward for Agent 6 this episode : -0.24\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9638\n",
      "Average Reward for Agent 7 this episode : -12.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.7354\n",
      "Average Reward for Agent 8 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9253\n",
      "Average Reward for Agent 9 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2651\n",
      "Average Reward for Agent 10 this episode : -48.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 284.6881\n",
      "Average Reward for Agent 11 this episode : -8.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.2638\n",
      "Average Reward for Agent 12 this episode : -5.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.4856\n",
      "Average Reward for Agent 13 this episode : -43.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.9302\n",
      "Reducing exploration for all agents to 0.0134\n",
      "Episode 250 is finished\n",
      "Average Reward for Agent 0 this episode : -36.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.2470\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.1107\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -28.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7119\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2261\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -19.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.1970\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0541\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6835\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -10.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4749\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3226\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.7809\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -38.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 663.4043\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -11.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9302\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -13.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5183\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -70.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.6771\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0132\n",
      "Episode 251 is finished\n",
      "Average Reward for Agent 0 this episode : -35.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.2364\n",
      "Average Reward for Agent 1 this episode : -6.2\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.8023\n",
      "Average Reward for Agent 2 this episode : -39.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.9049\n",
      "Average Reward for Agent 3 this episode : -6.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9488\n",
      "Average Reward for Agent 4 this episode : -20.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 261.5122\n",
      "Average Reward for Agent 5 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8259\n",
      "Average Reward for Agent 6 this episode : -1.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.4622\n",
      "Average Reward for Agent 7 this episode : -15.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4666\n",
      "Average Reward for Agent 8 this episode : -1.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.9698\n",
      "Average Reward for Agent 9 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.9841\n",
      "Average Reward for Agent 10 this episode : -46.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 286.7099\n",
      "Average Reward for Agent 11 this episode : -3.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.2652\n",
      "Average Reward for Agent 12 this episode : -5.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4334\n",
      "Average Reward for Agent 13 this episode : -53.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.4230\n",
      "Reducing exploration for all agents to 0.013\n",
      "Episode 252 is finished\n",
      "Average Reward for Agent 0 this episode : -42.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9918\n",
      "Average Reward for Agent 1 this episode : -10.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.1439\n",
      "Average Reward for Agent 2 this episode : -14.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.8770\n",
      "Average Reward for Agent 3 this episode : -5.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6800\n",
      "Average Reward for Agent 4 this episode : -17.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.4574\n",
      "Average Reward for Agent 5 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6212\n",
      "Average Reward for Agent 6 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4871\n",
      "Average Reward for Agent 7 this episode : -16.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1332\n",
      "Average Reward for Agent 8 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9667\n",
      "Average Reward for Agent 9 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4492\n",
      "Average Reward for Agent 10 this episode : -61.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 440.1700\n",
      "Average Reward for Agent 11 this episode : -3.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 12 this episode : -9.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4221\n",
      "Average Reward for Agent 13 this episode : -46.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 272.1072\n",
      "Reducing exploration for all agents to 0.0127\n",
      "Episode 253 is finished\n",
      "Average Reward for Agent 0 this episode : -14.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.1384\n",
      "Average Reward for Agent 1 this episode : -11.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.6347\n",
      "Average Reward for Agent 2 this episode : -41.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.9723\n",
      "Average Reward for Agent 3 this episode : -2.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7332\n",
      "Average Reward for Agent 4 this episode : -16.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 154.9458\n",
      "Average Reward for Agent 5 this episode : -2.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2403\n",
      "Average Reward for Agent 6 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.4922\n",
      "Average Reward for Agent 7 this episode : -14.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1879\n",
      "Average Reward for Agent 8 this episode : -3.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.3517\n",
      "Average Reward for Agent 9 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.0553\n",
      "Average Reward for Agent 10 this episode : -52.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 560.5237\n",
      "Average Reward for Agent 11 this episode : -4.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5854\n",
      "Average Reward for Agent 12 this episode : -5.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0060\n",
      "Average Reward for Agent 13 this episode : -54.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.3508\n",
      "Reducing exploration for all agents to 0.0125\n",
      "Episode 254 is finished\n",
      "Average Reward for Agent 0 this episode : -16.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.2107\n",
      "Average Reward for Agent 1 this episode : -12.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3964\n",
      "Average Reward for Agent 2 this episode : -10.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 615.1497\n",
      "Average Reward for Agent 3 this episode : -7.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.6412\n",
      "Average Reward for Agent 4 this episode : -18.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.6954\n",
      "Average Reward for Agent 5 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0631\n",
      "Average Reward for Agent 6 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.4020\n",
      "Average Reward for Agent 7 this episode : -15.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.4686\n",
      "Average Reward for Agent 8 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4567\n",
      "Average Reward for Agent 9 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8517\n",
      "Average Reward for Agent 10 this episode : -51.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.9057\n",
      "Average Reward for Agent 11 this episode : -7.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8402\n",
      "Average Reward for Agent 12 this episode : -11.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7996\n",
      "Average Reward for Agent 13 this episode : -47.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.7764\n",
      "Reducing exploration for all agents to 0.0123\n",
      "Episode 255 is finished\n",
      "Average Reward for Agent 0 this episode : -16.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.5912\n",
      "Average Reward for Agent 1 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.8686\n",
      "Average Reward for Agent 2 this episode : -14.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 346.5079\n",
      "Average Reward for Agent 3 this episode : -4.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2869\n",
      "Average Reward for Agent 4 this episode : -19.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.4226\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7211\n",
      "Average Reward for Agent 6 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1357\n",
      "Average Reward for Agent 7 this episode : -14.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.1396\n",
      "Average Reward for Agent 8 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5213\n",
      "Average Reward for Agent 9 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.1060\n",
      "Average Reward for Agent 10 this episode : -51.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 557.8795\n",
      "Average Reward for Agent 11 this episode : -5.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.3427\n",
      "Average Reward for Agent 12 this episode : -11.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.6021\n",
      "Average Reward for Agent 13 this episode : -60.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.9605\n",
      "Reducing exploration for all agents to 0.0121\n",
      "Episode 256 is finished\n",
      "Average Reward for Agent 0 this episode : -41.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.7310\n",
      "Average Reward for Agent 1 this episode : -13.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.7920\n",
      "Average Reward for Agent 2 this episode : -33.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 468.2490\n",
      "Average Reward for Agent 3 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.6025\n",
      "Average Reward for Agent 4 this episode : -18.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.8793\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0914\n",
      "Average Reward for Agent 6 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9529\n",
      "Average Reward for Agent 7 this episode : -13.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.5698\n",
      "Average Reward for Agent 8 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1210\n",
      "Average Reward for Agent 9 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0952\n",
      "Average Reward for Agent 10 this episode : -62.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.9900\n",
      "Average Reward for Agent 11 this episode : -3.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5807\n",
      "Average Reward for Agent 12 this episode : -4.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.7275\n",
      "Average Reward for Agent 13 this episode : -38.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.5748\n",
      "Reducing exploration for all agents to 0.0119\n",
      "Episode 257 is finished\n",
      "Average Reward for Agent 0 this episode : -40.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.2295\n",
      "Average Reward for Agent 1 this episode : -9.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.7505\n",
      "Average Reward for Agent 2 this episode : -42.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 329.1619\n",
      "Average Reward for Agent 3 this episode : -3.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9823\n",
      "Average Reward for Agent 4 this episode : -16.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5276\n",
      "Average Reward for Agent 5 this episode : -1.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9805\n",
      "Average Reward for Agent 6 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.5722\n",
      "Average Reward for Agent 7 this episode : -16.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.3822\n",
      "Average Reward for Agent 8 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8373\n",
      "Average Reward for Agent 9 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.8874\n",
      "Average Reward for Agent 10 this episode : -46.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 373.4002\n",
      "Average Reward for Agent 11 this episode : -3.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.2352\n",
      "Average Reward for Agent 12 this episode : -12.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0303\n",
      "Average Reward for Agent 13 this episode : -44.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.6820\n",
      "Reducing exploration for all agents to 0.0117\n",
      "Episode 258 is finished\n",
      "Average Reward for Agent 0 this episode : -20.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5360\n",
      "Average Reward for Agent 1 this episode : -12.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.9079\n",
      "Average Reward for Agent 2 this episode : -35.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 283.3174\n",
      "Average Reward for Agent 3 this episode : -3.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0300\n",
      "Average Reward for Agent 4 this episode : -24.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 169.9229\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1936\n",
      "Average Reward for Agent 6 this episode : -2.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.9344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 7 this episode : -14.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.1640\n",
      "Average Reward for Agent 8 this episode : -2.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1185\n",
      "Average Reward for Agent 9 this episode : -2.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.7159\n",
      "Average Reward for Agent 10 this episode : -40.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 558.4656\n",
      "Average Reward for Agent 11 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3137\n",
      "Average Reward for Agent 12 this episode : -7.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8486\n",
      "Average Reward for Agent 13 this episode : -46.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.1461\n",
      "Reducing exploration for all agents to 0.0115\n",
      "Episode 259 is finished\n",
      "Average Reward for Agent 0 this episode : -43.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.2942\n",
      "Average Reward for Agent 1 this episode : -6.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.2371\n",
      "Average Reward for Agent 2 this episode : -39.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 540.8256\n",
      "Average Reward for Agent 3 this episode : -14.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9140\n",
      "Average Reward for Agent 4 this episode : -18.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.4831\n",
      "Average Reward for Agent 5 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7301\n",
      "Average Reward for Agent 6 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7288\n",
      "Average Reward for Agent 7 this episode : -15.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.1745\n",
      "Average Reward for Agent 8 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2412\n",
      "Average Reward for Agent 9 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3491\n",
      "Average Reward for Agent 10 this episode : -51.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.6829\n",
      "Average Reward for Agent 11 this episode : -2.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.2249\n",
      "Average Reward for Agent 12 this episode : -2.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.9009\n",
      "Average Reward for Agent 13 this episode : -43.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.7518\n",
      "Reducing exploration for all agents to 0.0113\n",
      "Episode 260 is finished\n",
      "Average Reward for Agent 0 this episode : -16.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0306\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -8.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.8813\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -21.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.9526\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5339\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.8924\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3112\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.0842\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.8044\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6373\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -4.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.8465\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -44.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 314.2818\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.7567\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -10.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8544\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -54.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.5363\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0111\n",
      "Episode 261 is finished\n",
      "Average Reward for Agent 0 this episode : -14.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.0284\n",
      "Average Reward for Agent 1 this episode : -13.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.9678\n",
      "Average Reward for Agent 2 this episode : -29.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.5964\n",
      "Average Reward for Agent 3 this episode : -2.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1725\n",
      "Average Reward for Agent 4 this episode : -19.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.1442\n",
      "Average Reward for Agent 5 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5717\n",
      "Average Reward for Agent 6 this episode : -7.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.7348\n",
      "Average Reward for Agent 7 this episode : -14.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.1768\n",
      "Average Reward for Agent 8 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6954\n",
      "Average Reward for Agent 9 this episode : -3.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.1963\n",
      "Average Reward for Agent 10 this episode : -38.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 239.8520\n",
      "Average Reward for Agent 11 this episode : -3.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.0539\n",
      "Average Reward for Agent 12 this episode : -2.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.5435\n",
      "Average Reward for Agent 13 this episode : -37.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.2711\n",
      "Reducing exploration for all agents to 0.0109\n",
      "Episode 262 is finished\n",
      "Average Reward for Agent 0 this episode : -43.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1652\n",
      "Average Reward for Agent 1 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.8413\n",
      "Average Reward for Agent 2 this episode : -20.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 271.2299\n",
      "Average Reward for Agent 3 this episode : -11.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9900\n",
      "Average Reward for Agent 4 this episode : -18.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.3225\n",
      "Average Reward for Agent 5 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4578\n",
      "Average Reward for Agent 6 this episode : -15.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.2356\n",
      "Average Reward for Agent 7 this episode : -14.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7507\n",
      "Average Reward for Agent 8 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1092\n",
      "Average Reward for Agent 9 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.8893\n",
      "Average Reward for Agent 10 this episode : -52.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.1281\n",
      "Average Reward for Agent 11 this episode : -2.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3724\n",
      "Average Reward for Agent 12 this episode : -10.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9200\n",
      "Average Reward for Agent 13 this episode : -55.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.9902\n",
      "Reducing exploration for all agents to 0.0107\n",
      "Episode 263 is finished\n",
      "Average Reward for Agent 0 this episode : -12.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.5413\n",
      "Average Reward for Agent 1 this episode : -12.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8866\n",
      "Average Reward for Agent 2 this episode : -28.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.5504\n",
      "Average Reward for Agent 3 this episode : -1.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2070\n",
      "Average Reward for Agent 4 this episode : -14.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 197.0028\n",
      "Average Reward for Agent 5 this episode : -1.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2639\n",
      "Average Reward for Agent 6 this episode : -5.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.1853\n",
      "Average Reward for Agent 7 this episode : -14.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.0277\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 9 this episode : -3.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.7796\n",
      "Average Reward for Agent 10 this episode : -58.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 287.8488\n",
      "Average Reward for Agent 11 this episode : -3.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.0077\n",
      "Average Reward for Agent 12 this episode : -3.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.6454\n",
      "Average Reward for Agent 13 this episode : -42.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.8498\n",
      "Reducing exploration for all agents to 0.0105\n",
      "Episode 264 is finished\n",
      "Average Reward for Agent 0 this episode : -37.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9815\n",
      "Average Reward for Agent 1 this episode : -12.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.8025\n",
      "Average Reward for Agent 2 this episode : -12.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.4982\n",
      "Average Reward for Agent 3 this episode : -1.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.0405\n",
      "Average Reward for Agent 4 this episode : -20.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3768\n",
      "Average Reward for Agent 5 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8893\n",
      "Average Reward for Agent 6 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.4634\n",
      "Average Reward for Agent 7 this episode : -15.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.3177\n",
      "Average Reward for Agent 8 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6677\n",
      "Average Reward for Agent 9 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.7628\n",
      "Average Reward for Agent 10 this episode : -61.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.6435\n",
      "Average Reward for Agent 11 this episode : -3.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.9000\n",
      "Average Reward for Agent 12 this episode : -11.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7854\n",
      "Average Reward for Agent 13 this episode : -53.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.5686\n",
      "Reducing exploration for all agents to 0.0104\n",
      "Episode 265 is finished\n",
      "Average Reward for Agent 0 this episode : -38.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.3866\n",
      "Average Reward for Agent 1 this episode : -10.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1873\n",
      "Average Reward for Agent 2 this episode : -41.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.1026\n",
      "Average Reward for Agent 3 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.3812\n",
      "Average Reward for Agent 4 this episode : -18.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.6394\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2131\n",
      "Average Reward for Agent 6 this episode : -7.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1897\n",
      "Average Reward for Agent 7 this episode : -12.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2130\n",
      "Average Reward for Agent 8 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3344\n",
      "Average Reward for Agent 9 this episode : -2.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.2853\n",
      "Average Reward for Agent 10 this episode : -47.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.6381\n",
      "Average Reward for Agent 11 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.3873\n",
      "Average Reward for Agent 12 this episode : -5.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7085\n",
      "Average Reward for Agent 13 this episode : -53.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.8872\n",
      "Reducing exploration for all agents to 0.0102\n",
      "Episode 266 is finished\n",
      "Average Reward for Agent 0 this episode : -37.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.3958\n",
      "Average Reward for Agent 1 this episode : -8.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6913\n",
      "Average Reward for Agent 2 this episode : -40.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.1890\n",
      "Average Reward for Agent 3 this episode : -1.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0301\n",
      "Average Reward for Agent 4 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.2054\n",
      "Average Reward for Agent 5 this episode : -1.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7094\n",
      "Average Reward for Agent 6 this episode : -15.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.6310\n",
      "Average Reward for Agent 7 this episode : -13.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2479\n",
      "Average Reward for Agent 8 this episode : -2.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9084\n",
      "Average Reward for Agent 9 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.8907\n",
      "Average Reward for Agent 10 this episode : -45.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 221.0627\n",
      "Average Reward for Agent 11 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0414\n",
      "Average Reward for Agent 12 this episode : -8.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1774\n",
      "Average Reward for Agent 13 this episode : -50.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.6132\n",
      "Reducing exploration for all agents to 0.01\n",
      "Episode 267 is finished\n",
      "Average Reward for Agent 0 this episode : -39.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.3390\n",
      "Average Reward for Agent 1 this episode : -11.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.7646\n",
      "Average Reward for Agent 2 this episode : -40.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 182.4724\n",
      "Average Reward for Agent 3 this episode : -3.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3999\n",
      "Average Reward for Agent 4 this episode : -14.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.6567\n",
      "Average Reward for Agent 5 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6284\n",
      "Average Reward for Agent 6 this episode : -14.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.0904\n",
      "Average Reward for Agent 7 this episode : -13.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1597\n",
      "Average Reward for Agent 8 this episode : -3.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2525\n",
      "Average Reward for Agent 9 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6337\n",
      "Average Reward for Agent 10 this episode : -53.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.6167\n",
      "Average Reward for Agent 11 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.8788\n",
      "Average Reward for Agent 12 this episode : -6.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4035\n",
      "Average Reward for Agent 13 this episode : -43.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.2318\n",
      "Reducing exploration for all agents to 0.0098\n",
      "Episode 268 is finished\n",
      "Average Reward for Agent 0 this episode : -36.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.3908\n",
      "Average Reward for Agent 1 this episode : -13.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6884\n",
      "Average Reward for Agent 2 this episode : -33.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.1738\n",
      "Average Reward for Agent 3 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3134\n",
      "Average Reward for Agent 4 this episode : -20.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.5731\n",
      "Average Reward for Agent 5 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8034\n",
      "Average Reward for Agent 6 this episode : -44.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6959\n",
      "Average Reward for Agent 7 this episode : -15.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.1243\n",
      "Average Reward for Agent 8 this episode : -4.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3163\n",
      "Average Reward for Agent 9 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.8378\n",
      "Average Reward for Agent 10 this episode : -46.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.4634\n",
      "Average Reward for Agent 11 this episode : -3.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.4753\n",
      "Average Reward for Agent 12 this episode : -4.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6170\n",
      "Average Reward for Agent 13 this episode : -43.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5531\n",
      "Reducing exploration for all agents to 0.0097\n",
      "Episode 269 is finished\n",
      "Average Reward for Agent 0 this episode : -21.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.0566\n",
      "Average Reward for Agent 1 this episode : -11.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.6854\n",
      "Average Reward for Agent 2 this episode : -38.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.1048\n",
      "Average Reward for Agent 3 this episode : -4.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 4 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.6140\n",
      "Average Reward for Agent 5 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0638\n",
      "Average Reward for Agent 6 this episode : -15.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.3005\n",
      "Average Reward for Agent 7 this episode : -13.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1124\n",
      "Average Reward for Agent 8 this episode : -2.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.8188\n",
      "Average Reward for Agent 9 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2265\n",
      "Average Reward for Agent 10 this episode : -49.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.4629\n",
      "Average Reward for Agent 11 this episode : -3.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.3968\n",
      "Average Reward for Agent 12 this episode : -4.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8041\n",
      "Average Reward for Agent 13 this episode : -49.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.0861\n",
      "Reducing exploration for all agents to 0.0095\n",
      "Episode 270 is finished\n",
      "Average Reward for Agent 0 this episode : -32.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4906\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5043\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -41.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 231.4845\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6865\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -15.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.5241\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3269\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -4.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.1093\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -12.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.2726\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.2407\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6345\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -52.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.6386\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -2.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.9530\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -11.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7223\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -43.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.2152\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0093\n",
      "Episode 271 is finished\n",
      "Average Reward for Agent 0 this episode : -19.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.5859\n",
      "Average Reward for Agent 1 this episode : -5.25\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7082\n",
      "Average Reward for Agent 2 this episode : -39.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.3694\n",
      "Average Reward for Agent 3 this episode : -2.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6919\n",
      "Average Reward for Agent 4 this episode : -18.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.7188\n",
      "Average Reward for Agent 5 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2601\n",
      "Average Reward for Agent 6 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.5365\n",
      "Average Reward for Agent 7 this episode : -11.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.1533\n",
      "Average Reward for Agent 8 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3591\n",
      "Average Reward for Agent 9 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7586\n",
      "Average Reward for Agent 10 this episode : -44.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 261.1198\n",
      "Average Reward for Agent 11 this episode : -2.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8898\n",
      "Average Reward for Agent 12 this episode : -3.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6039\n",
      "Average Reward for Agent 13 this episode : -48.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.6765\n",
      "Reducing exploration for all agents to 0.0092\n",
      "Episode 272 is finished\n",
      "Average Reward for Agent 0 this episode : -41.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.3811\n",
      "Average Reward for Agent 1 this episode : -8.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5332\n",
      "Average Reward for Agent 2 this episode : -20.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.1481\n",
      "Average Reward for Agent 3 this episode : -2.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7135\n",
      "Average Reward for Agent 4 this episode : -18.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8256\n",
      "Average Reward for Agent 5 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3019\n",
      "Average Reward for Agent 6 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.5266\n",
      "Average Reward for Agent 7 this episode : -13.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5241\n",
      "Average Reward for Agent 8 this episode : -0.11\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6082\n",
      "Average Reward for Agent 9 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6349\n",
      "Average Reward for Agent 10 this episode : -45.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.7140\n",
      "Average Reward for Agent 11 this episode : -3.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.3209\n",
      "Average Reward for Agent 12 this episode : -9.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0793\n",
      "Average Reward for Agent 13 this episode : -43.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.3965\n",
      "Reducing exploration for all agents to 0.009\n",
      "Episode 273 is finished\n",
      "Average Reward for Agent 0 this episode : -12.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6954\n",
      "Average Reward for Agent 1 this episode : -14.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7295\n",
      "Average Reward for Agent 2 this episode : -38.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.9856\n",
      "Average Reward for Agent 3 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0931\n",
      "Average Reward for Agent 4 this episode : -19.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.5420\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8724\n",
      "Average Reward for Agent 6 this episode : -5.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.7302\n",
      "Average Reward for Agent 7 this episode : -13.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.1941\n",
      "Average Reward for Agent 8 this episode : -0.07\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3875\n",
      "Average Reward for Agent 9 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6489\n",
      "Average Reward for Agent 10 this episode : -48.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.3137\n",
      "Average Reward for Agent 11 this episode : -2.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.1185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 12 this episode : -6.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5884\n",
      "Average Reward for Agent 13 this episode : -42.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.7175\n",
      "Reducing exploration for all agents to 0.0089\n",
      "Episode 274 is finished\n",
      "Average Reward for Agent 0 this episode : -41.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.7459\n",
      "Average Reward for Agent 1 this episode : -10.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4512\n",
      "Average Reward for Agent 2 this episode : -45.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3257\n",
      "Average Reward for Agent 3 this episode : -8.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5765\n",
      "Average Reward for Agent 4 this episode : -16.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.0493\n",
      "Average Reward for Agent 5 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5710\n",
      "Average Reward for Agent 6 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1755\n",
      "Average Reward for Agent 7 this episode : -10.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9219\n",
      "Average Reward for Agent 8 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1159\n",
      "Average Reward for Agent 9 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.2309\n",
      "Average Reward for Agent 10 this episode : -50.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.1733\n",
      "Average Reward for Agent 11 this episode : -1.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.3595\n",
      "Average Reward for Agent 12 this episode : -4.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7947\n",
      "Average Reward for Agent 13 this episode : -52.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.3681\n",
      "Reducing exploration for all agents to 0.0087\n",
      "Episode 275 is finished\n",
      "Average Reward for Agent 0 this episode : -41.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.1227\n",
      "Average Reward for Agent 1 this episode : -9.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.5051\n",
      "Average Reward for Agent 2 this episode : -48.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.0125\n",
      "Average Reward for Agent 3 this episode : -2.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3113\n",
      "Average Reward for Agent 4 this episode : -23.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1885\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7761\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6585\n",
      "Average Reward for Agent 7 this episode : -8.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6407\n",
      "Average Reward for Agent 8 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2031\n",
      "Average Reward for Agent 9 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0309\n",
      "Average Reward for Agent 10 this episode : -44.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9145\n",
      "Average Reward for Agent 11 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.5172\n",
      "Average Reward for Agent 12 this episode : -6.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2634\n",
      "Average Reward for Agent 13 this episode : -45.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.7181\n",
      "Reducing exploration for all agents to 0.0086\n",
      "Episode 276 is finished\n",
      "Average Reward for Agent 0 this episode : -43.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.6453\n",
      "Average Reward for Agent 1 this episode : -3.49\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.7414\n",
      "Average Reward for Agent 2 this episode : -49.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.3472\n",
      "Average Reward for Agent 3 this episode : -5.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8033\n",
      "Average Reward for Agent 4 this episode : -22.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.6833\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7321\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.3990\n",
      "Average Reward for Agent 7 this episode : -11.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9945\n",
      "Average Reward for Agent 8 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3495\n",
      "Average Reward for Agent 9 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.4816\n",
      "Average Reward for Agent 10 this episode : -53.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.4693\n",
      "Average Reward for Agent 11 this episode : -2.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2060\n",
      "Average Reward for Agent 12 this episode : -5.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1213\n",
      "Average Reward for Agent 13 this episode : -50.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.8620\n",
      "Reducing exploration for all agents to 0.0084\n",
      "Episode 277 is finished\n",
      "Average Reward for Agent 0 this episode : -40.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.8240\n",
      "Average Reward for Agent 1 this episode : -5.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.4533\n",
      "Average Reward for Agent 2 this episode : -40.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.4223\n",
      "Average Reward for Agent 3 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1719\n",
      "Average Reward for Agent 4 this episode : -17.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.2588\n",
      "Average Reward for Agent 5 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3964\n",
      "Average Reward for Agent 6 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9123\n",
      "Average Reward for Agent 7 this episode : -10.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2570\n",
      "Average Reward for Agent 8 this episode : -0.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6451\n",
      "Average Reward for Agent 9 this episode : -1.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2941\n",
      "Average Reward for Agent 10 this episode : -44.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.1036\n",
      "Average Reward for Agent 11 this episode : -2.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.8483\n",
      "Average Reward for Agent 12 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3348\n",
      "Average Reward for Agent 13 this episode : -46.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.5774\n",
      "Reducing exploration for all agents to 0.0083\n",
      "Episode 278 is finished\n",
      "Average Reward for Agent 0 this episode : -17.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.7351\n",
      "Average Reward for Agent 1 this episode : -10.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5261\n",
      "Average Reward for Agent 2 this episode : -43.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5444\n",
      "Average Reward for Agent 3 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1258\n",
      "Average Reward for Agent 4 this episode : -19.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.9064\n",
      "Average Reward for Agent 5 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5284\n",
      "Average Reward for Agent 6 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4612\n",
      "Average Reward for Agent 7 this episode : -11.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3657\n",
      "Average Reward for Agent 8 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2951\n",
      "Average Reward for Agent 9 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.6184\n",
      "Average Reward for Agent 10 this episode : -48.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.2492\n",
      "Average Reward for Agent 11 this episode : -2.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0983\n",
      "Average Reward for Agent 12 this episode : -8.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2110\n",
      "Average Reward for Agent 13 this episode : -42.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.7144\n",
      "Reducing exploration for all agents to 0.0081\n",
      "Episode 279 is finished\n",
      "Average Reward for Agent 0 this episode : -20.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2309\n",
      "Average Reward for Agent 1 this episode : -10.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 2 this episode : -45.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.7545\n",
      "Average Reward for Agent 3 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9086\n",
      "Average Reward for Agent 4 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.6767\n",
      "Average Reward for Agent 5 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8953\n",
      "Average Reward for Agent 6 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.5079\n",
      "Average Reward for Agent 7 this episode : -13.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0343\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7665\n",
      "Average Reward for Agent 9 this episode : -9.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.4961\n",
      "Average Reward for Agent 10 this episode : -52.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.4500\n",
      "Average Reward for Agent 11 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.2399\n",
      "Average Reward for Agent 12 this episode : -4.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.7985\n",
      "Average Reward for Agent 13 this episode : -39.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.8966\n",
      "Reducing exploration for all agents to 0.008\n",
      "Episode 280 is finished\n",
      "Average Reward for Agent 0 this episode : -40.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2081\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -10.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.3045\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -19.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 244.1553\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4915\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1067\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0653\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4747\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0017\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8827\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -3.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.3928\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -55.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.3193\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -2.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.8148\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -9.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5967\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -49.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.3641\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0078\n",
      "Episode 281 is finished\n",
      "Average Reward for Agent 0 this episode : -10.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.0472\n",
      "Average Reward for Agent 1 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.3755\n",
      "Average Reward for Agent 2 this episode : -47.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 269.3832\n",
      "Average Reward for Agent 3 this episode : -4.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.3463\n",
      "Average Reward for Agent 4 this episode : -21.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.1559\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6995\n",
      "Average Reward for Agent 6 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9264\n",
      "Average Reward for Agent 7 this episode : -12.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7651\n",
      "Average Reward for Agent 8 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2616\n",
      "Average Reward for Agent 9 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.4121\n",
      "Average Reward for Agent 10 this episode : -45.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 259.2610\n",
      "Average Reward for Agent 11 this episode : -4.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7357\n",
      "Average Reward for Agent 12 this episode : -5.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.1876\n",
      "Average Reward for Agent 13 this episode : -46.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.9128\n",
      "Reducing exploration for all agents to 0.0077\n",
      "Episode 282 is finished\n",
      "Average Reward for Agent 0 this episode : -40.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.1893\n",
      "Average Reward for Agent 1 this episode : -11.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.1763\n",
      "Average Reward for Agent 2 this episode : -49.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.9262\n",
      "Average Reward for Agent 3 this episode : -2.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7876\n",
      "Average Reward for Agent 4 this episode : -18.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.8189\n",
      "Average Reward for Agent 5 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5673\n",
      "Average Reward for Agent 6 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0875\n",
      "Average Reward for Agent 7 this episode : -9.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0792\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5935\n",
      "Average Reward for Agent 9 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9234\n",
      "Average Reward for Agent 10 this episode : -34.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.0023\n",
      "Average Reward for Agent 11 this episode : -3.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.7399\n",
      "Average Reward for Agent 12 this episode : -6.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1779\n",
      "Average Reward for Agent 13 this episode : -13.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.4149\n",
      "Reducing exploration for all agents to 0.0076\n",
      "Episode 283 is finished\n",
      "Average Reward for Agent 0 this episode : -42.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.5159\n",
      "Average Reward for Agent 1 this episode : -12.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9921\n",
      "Average Reward for Agent 2 this episode : -44.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.1109\n",
      "Average Reward for Agent 3 this episode : -7.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5196\n",
      "Average Reward for Agent 4 this episode : -16.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.2697\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0129\n",
      "Average Reward for Agent 6 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6512\n",
      "Average Reward for Agent 7 this episode : -11.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.4729\n",
      "Average Reward for Agent 8 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8834\n",
      "Average Reward for Agent 9 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.5401\n",
      "Average Reward for Agent 10 this episode : -48.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.5015\n",
      "Average Reward for Agent 11 this episode : -4.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5336\n",
      "Average Reward for Agent 12 this episode : -3.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.0747\n",
      "Average Reward for Agent 13 this episode : -5.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 290.7378\n",
      "Reducing exploration for all agents to 0.0075\n",
      "Episode 284 is finished\n",
      "Average Reward for Agent 0 this episode : -40.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.3215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -7.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.8509\n",
      "Average Reward for Agent 2 this episode : -50.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.0714\n",
      "Average Reward for Agent 3 this episode : -4.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1730\n",
      "Average Reward for Agent 4 this episode : -21.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4465\n",
      "Average Reward for Agent 5 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9947\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5826\n",
      "Average Reward for Agent 7 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.9675\n",
      "Average Reward for Agent 8 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1500\n",
      "Average Reward for Agent 9 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.5930\n",
      "Average Reward for Agent 10 this episode : -61.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.2793\n",
      "Average Reward for Agent 11 this episode : -6.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.0352\n",
      "Average Reward for Agent 12 this episode : -5.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9971\n",
      "Average Reward for Agent 13 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.2901\n",
      "Reducing exploration for all agents to 0.0073\n",
      "Episode 285 is finished\n",
      "Average Reward for Agent 0 this episode : -44.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.8557\n",
      "Average Reward for Agent 1 this episode : -10.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7169\n",
      "Average Reward for Agent 2 this episode : -43.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2816\n",
      "Average Reward for Agent 3 this episode : -6.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7003\n",
      "Average Reward for Agent 4 this episode : -16.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.4241\n",
      "Average Reward for Agent 5 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1644\n",
      "Average Reward for Agent 6 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2054\n",
      "Average Reward for Agent 7 this episode : -14.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.4286\n",
      "Average Reward for Agent 8 this episode : -1.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1455\n",
      "Average Reward for Agent 9 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.3784\n",
      "Average Reward for Agent 10 this episode : -50.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.3949\n",
      "Average Reward for Agent 11 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.4688\n",
      "Average Reward for Agent 12 this episode : -17.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0250\n",
      "Average Reward for Agent 13 this episode : -48.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 181.3405\n",
      "Reducing exploration for all agents to 0.0072\n",
      "Episode 286 is finished\n",
      "Average Reward for Agent 0 this episode : -20.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.8835\n",
      "Average Reward for Agent 1 this episode : -9.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8603\n",
      "Average Reward for Agent 2 this episode : -45.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.7511\n",
      "Average Reward for Agent 3 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6031\n",
      "Average Reward for Agent 4 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.4632\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9167\n",
      "Average Reward for Agent 6 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0846\n",
      "Average Reward for Agent 7 this episode : -14.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3056\n",
      "Average Reward for Agent 8 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2652\n",
      "Average Reward for Agent 9 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7661\n",
      "Average Reward for Agent 10 this episode : -54.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.8334\n",
      "Average Reward for Agent 11 this episode : -4.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.2924\n",
      "Average Reward for Agent 12 this episode : -5.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6439\n",
      "Average Reward for Agent 13 this episode : -41.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 262.7529\n",
      "Reducing exploration for all agents to 0.0071\n",
      "Episode 287 is finished\n",
      "Average Reward for Agent 0 this episode : -44.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.0521\n",
      "Average Reward for Agent 1 this episode : -14.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.4728\n",
      "Average Reward for Agent 2 this episode : -46.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.8099\n",
      "Average Reward for Agent 3 this episode : -4.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3357\n",
      "Average Reward for Agent 4 this episode : -19.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.6014\n",
      "Average Reward for Agent 5 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8733\n",
      "Average Reward for Agent 6 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1189\n",
      "Average Reward for Agent 7 this episode : -12.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7002\n",
      "Average Reward for Agent 8 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7261\n",
      "Average Reward for Agent 9 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.6292\n",
      "Average Reward for Agent 10 this episode : -64.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 211.6123\n",
      "Average Reward for Agent 11 this episode : -3.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5276\n",
      "Average Reward for Agent 12 this episode : -7.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1970\n",
      "Average Reward for Agent 13 this episode : -38.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.0575\n",
      "Reducing exploration for all agents to 0.007\n",
      "Episode 288 is finished\n",
      "Average Reward for Agent 0 this episode : -37.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4263\n",
      "Average Reward for Agent 1 this episode : -9.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4006\n",
      "Average Reward for Agent 2 this episode : -39.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.6650\n",
      "Average Reward for Agent 3 this episode : -5.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2371\n",
      "Average Reward for Agent 4 this episode : -16.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.8620\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7785\n",
      "Average Reward for Agent 6 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9654\n",
      "Average Reward for Agent 7 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.1132\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0606\n",
      "Average Reward for Agent 9 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.3895\n",
      "Average Reward for Agent 10 this episode : -58.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.0520\n",
      "Average Reward for Agent 11 this episode : -2.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.2323\n",
      "Average Reward for Agent 12 this episode : -10.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8360\n",
      "Average Reward for Agent 13 this episode : -7.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.4870\n",
      "Reducing exploration for all agents to 0.0068\n",
      "Episode 289 is finished\n",
      "Average Reward for Agent 0 this episode : -40.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0372\n",
      "Average Reward for Agent 1 this episode : -9.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6829\n",
      "Average Reward for Agent 2 this episode : -35.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.8223\n",
      "Average Reward for Agent 3 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.8555\n",
      "Average Reward for Agent 4 this episode : -16.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7623\n",
      "Average Reward for Agent 5 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9433\n",
      "Average Reward for Agent 6 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9754\n",
      "Average Reward for Agent 7 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5133\n",
      "Average Reward for Agent 8 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0018\n",
      "Average Reward for Agent 9 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.0379\n",
      "Average Reward for Agent 10 this episode : -49.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 181.4820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 11 this episode : -3.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2110\n",
      "Average Reward for Agent 12 this episode : -7.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1326\n",
      "Average Reward for Agent 13 this episode : -7.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.5316\n",
      "Reducing exploration for all agents to 0.0067\n",
      "Episode 290 is finished\n",
      "Average Reward for Agent 0 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2746\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -14.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8239\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -54.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.4288\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8674\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8029\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9067\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -12.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.1896\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -15.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0820\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9876\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.2698\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -36.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.5226\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.4879\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -7.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9285\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -22.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.1586\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0066\n",
      "Episode 291 is finished\n",
      "Average Reward for Agent 0 this episode : -42.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.2551\n",
      "Average Reward for Agent 1 this episode : -7.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3925\n",
      "Average Reward for Agent 2 this episode : -49.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.6993\n",
      "Average Reward for Agent 3 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0282\n",
      "Average Reward for Agent 4 this episode : -16.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.7335\n",
      "Average Reward for Agent 5 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3695\n",
      "Average Reward for Agent 6 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9260\n",
      "Average Reward for Agent 7 this episode : -12.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.9335\n",
      "Average Reward for Agent 8 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2869\n",
      "Average Reward for Agent 9 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.6488\n",
      "Average Reward for Agent 10 this episode : -49.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 289.0737\n",
      "Average Reward for Agent 11 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5151\n",
      "Average Reward for Agent 12 this episode : -6.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4406\n",
      "Average Reward for Agent 13 this episode : -49.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.6643\n",
      "Reducing exploration for all agents to 0.0065\n",
      "Episode 292 is finished\n",
      "Average Reward for Agent 0 this episode : -43.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.3495\n",
      "Average Reward for Agent 1 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 179.0519\n",
      "Average Reward for Agent 2 this episode : -50.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0737\n",
      "Average Reward for Agent 3 this episode : -5.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4046\n",
      "Average Reward for Agent 4 this episode : -16.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2765\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0968\n",
      "Average Reward for Agent 6 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.8467\n",
      "Average Reward for Agent 7 this episode : -13.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5347\n",
      "Average Reward for Agent 8 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4617\n",
      "Average Reward for Agent 9 this episode : -7.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.5677\n",
      "Average Reward for Agent 10 this episode : -52.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.0750\n",
      "Average Reward for Agent 11 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6791\n",
      "Average Reward for Agent 12 this episode : -4.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.3167\n",
      "Average Reward for Agent 13 this episode : -7.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.3475\n",
      "Reducing exploration for all agents to 0.0064\n",
      "Episode 293 is finished\n",
      "Average Reward for Agent 0 this episode : -13.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.2262\n",
      "Average Reward for Agent 1 this episode : -12.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.1084\n",
      "Average Reward for Agent 2 this episode : -47.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.6092\n",
      "Average Reward for Agent 3 this episode : -11.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6579\n",
      "Average Reward for Agent 4 this episode : -18.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4625\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4576\n",
      "Average Reward for Agent 6 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.8144\n",
      "Average Reward for Agent 7 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6398\n",
      "Average Reward for Agent 8 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3425\n",
      "Average Reward for Agent 9 this episode : -16.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.5130\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.5798\n",
      "Average Reward for Agent 11 this episode : -2.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0984\n",
      "Average Reward for Agent 12 this episode : -7.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.1256\n",
      "Average Reward for Agent 13 this episode : -8.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.2377\n",
      "Reducing exploration for all agents to 0.0063\n",
      "Episode 294 is finished\n",
      "Average Reward for Agent 0 this episode : -34.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.3145\n",
      "Average Reward for Agent 1 this episode : -13.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.4534\n",
      "Average Reward for Agent 2 this episode : -51.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.8021\n",
      "Average Reward for Agent 3 this episode : -3.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5166\n",
      "Average Reward for Agent 4 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0845\n",
      "Average Reward for Agent 5 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9836\n",
      "Average Reward for Agent 6 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2938\n",
      "Average Reward for Agent 7 this episode : -12.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1425\n",
      "Average Reward for Agent 8 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3183\n",
      "Average Reward for Agent 9 this episode : -38.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.3453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 10 this episode : -34.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 245.2687\n",
      "Average Reward for Agent 11 this episode : -2.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.5684\n",
      "Average Reward for Agent 12 this episode : -3.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5878\n",
      "Average Reward for Agent 13 this episode : -16.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.5106\n",
      "Reducing exploration for all agents to 0.0062\n",
      "Episode 295 is finished\n",
      "Average Reward for Agent 0 this episode : -39.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.9156\n",
      "Average Reward for Agent 1 this episode : -11.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.4404\n",
      "Average Reward for Agent 2 this episode : -46.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.7757\n",
      "Average Reward for Agent 3 this episode : -4.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5755\n",
      "Average Reward for Agent 4 this episode : -14.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6701\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3974\n",
      "Average Reward for Agent 6 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4791\n",
      "Average Reward for Agent 7 this episode : -12.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8770\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3372\n",
      "Average Reward for Agent 9 this episode : -0.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 245.2676\n",
      "Average Reward for Agent 10 this episode : -35.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.8001\n",
      "Average Reward for Agent 11 this episode : -2.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.4170\n",
      "Average Reward for Agent 12 this episode : -4.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4576\n",
      "Average Reward for Agent 13 this episode : -7.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.2908\n",
      "Reducing exploration for all agents to 0.0061\n",
      "Episode 296 is finished\n",
      "Average Reward for Agent 0 this episode : -30.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.7039\n",
      "Average Reward for Agent 1 this episode : -12.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8746\n",
      "Average Reward for Agent 2 this episode : -52.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.1270\n",
      "Average Reward for Agent 3 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8314\n",
      "Average Reward for Agent 4 this episode : -17.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8637\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3626\n",
      "Average Reward for Agent 6 this episode : -0.11\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4823\n",
      "Average Reward for Agent 7 this episode : -14.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.9720\n",
      "Average Reward for Agent 8 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.3116\n",
      "Average Reward for Agent 9 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.9822\n",
      "Average Reward for Agent 10 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.1281\n",
      "Average Reward for Agent 11 this episode : -2.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7607\n",
      "Average Reward for Agent 12 this episode : -4.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1751\n",
      "Average Reward for Agent 13 this episode : -16.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.6222\n",
      "Reducing exploration for all agents to 0.0059\n",
      "Episode 297 is finished\n",
      "Average Reward for Agent 0 this episode : -41.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.4533\n",
      "Average Reward for Agent 1 this episode : -7.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.0531\n",
      "Average Reward for Agent 2 this episode : -38.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.1060\n",
      "Average Reward for Agent 3 this episode : -3.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8338\n",
      "Average Reward for Agent 4 this episode : -18.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.7088\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8598\n",
      "Average Reward for Agent 6 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.8965\n",
      "Average Reward for Agent 7 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4911\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7996\n",
      "Average Reward for Agent 9 this episode : -15.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.8106\n",
      "Average Reward for Agent 10 this episode : -60.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 256.3701\n",
      "Average Reward for Agent 11 this episode : -3.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5481\n",
      "Average Reward for Agent 12 this episode : -4.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1885\n",
      "Average Reward for Agent 13 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.6268\n",
      "Reducing exploration for all agents to 0.0058\n",
      "Episode 298 is finished\n",
      "Average Reward for Agent 0 this episode : -42.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.9333\n",
      "Average Reward for Agent 1 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.9347\n",
      "Average Reward for Agent 2 this episode : -48.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.2869\n",
      "Average Reward for Agent 3 this episode : -3.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0953\n",
      "Average Reward for Agent 4 this episode : -18.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1586\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6860\n",
      "Average Reward for Agent 6 this episode : -14.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.2489\n",
      "Average Reward for Agent 7 this episode : -12.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8108\n",
      "Average Reward for Agent 8 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.3589\n",
      "Average Reward for Agent 9 this episode : -16.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1945\n",
      "Average Reward for Agent 10 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.4465\n",
      "Average Reward for Agent 11 this episode : -3.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.5247\n",
      "Average Reward for Agent 12 this episode : -6.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4633\n",
      "Average Reward for Agent 13 this episode : -6.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.6419\n",
      "Reducing exploration for all agents to 0.0057\n",
      "Episode 299 is finished\n",
      "Average Reward for Agent 0 this episode : -33.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.3256\n",
      "Average Reward for Agent 1 this episode : -13.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.4260\n",
      "Average Reward for Agent 2 this episode : -49.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.3520\n",
      "Average Reward for Agent 3 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2313\n",
      "Average Reward for Agent 4 this episode : -16.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7150\n",
      "Average Reward for Agent 5 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1118\n",
      "Average Reward for Agent 6 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.8023\n",
      "Average Reward for Agent 7 this episode : -14.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5618\n",
      "Average Reward for Agent 8 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3624\n",
      "Average Reward for Agent 9 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9312\n",
      "Average Reward for Agent 10 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.4282\n",
      "Average Reward for Agent 11 this episode : -3.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 194.2527\n",
      "Average Reward for Agent 12 this episode : -6.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9669\n",
      "Average Reward for Agent 13 this episode : -10.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.4627\n",
      "Reducing exploration for all agents to 0.0056\n",
      "Episode 300 is finished\n",
      "Average Reward for Agent 0 this episode : -40.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.9223\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -10.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -57.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1066\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8286\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.1445\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7115\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.7083\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -10.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0154\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1617\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -22.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7131\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -15.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.2854\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -4.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.9307\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -8.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7057\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -14.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.2005\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0056\n",
      "Episode 301 is finished\n",
      "Average Reward for Agent 0 this episode : -32.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.0463\n",
      "Average Reward for Agent 1 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6563\n",
      "Average Reward for Agent 2 this episode : -55.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.8301\n",
      "Average Reward for Agent 3 this episode : -5.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3208\n",
      "Average Reward for Agent 4 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3813\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7985\n",
      "Average Reward for Agent 6 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6088\n",
      "Average Reward for Agent 7 this episode : -11.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6371\n",
      "Average Reward for Agent 8 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2229\n",
      "Average Reward for Agent 9 this episode : -21.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.8897\n",
      "Average Reward for Agent 10 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 240.3017\n",
      "Average Reward for Agent 11 this episode : -8.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.3064\n",
      "Average Reward for Agent 12 this episode : -8.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5879\n",
      "Average Reward for Agent 13 this episode : -9.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.8308\n",
      "Reducing exploration for all agents to 0.0055\n",
      "Episode 302 is finished\n",
      "Average Reward for Agent 0 this episode : -41.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.3830\n",
      "Average Reward for Agent 1 this episode : -12.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3291\n",
      "Average Reward for Agent 2 this episode : -48.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.9471\n",
      "Average Reward for Agent 3 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1237\n",
      "Average Reward for Agent 4 this episode : -17.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5945\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3966\n",
      "Average Reward for Agent 6 this episode : -15.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.0063\n",
      "Average Reward for Agent 7 this episode : -13.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4428\n",
      "Average Reward for Agent 8 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0657\n",
      "Average Reward for Agent 9 this episode : -20.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 150.4942\n",
      "Average Reward for Agent 10 this episode : -6.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.5482\n",
      "Average Reward for Agent 11 this episode : -6.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.1925\n",
      "Average Reward for Agent 12 this episode : -3.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2224\n",
      "Average Reward for Agent 13 this episode : -14.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.6758\n",
      "Reducing exploration for all agents to 0.0054\n",
      "Episode 303 is finished\n",
      "Average Reward for Agent 0 this episode : -11.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 138.7246\n",
      "Average Reward for Agent 1 this episode : -15.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.8289\n",
      "Average Reward for Agent 2 this episode : -49.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.9274\n",
      "Average Reward for Agent 3 this episode : -6.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.5426\n",
      "Average Reward for Agent 4 this episode : -15.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.4027\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5971\n",
      "Average Reward for Agent 6 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.4932\n",
      "Average Reward for Agent 7 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1025\n",
      "Average Reward for Agent 8 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2073\n",
      "Average Reward for Agent 9 this episode : -10.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.9494\n",
      "Average Reward for Agent 10 this episode : -6.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 453.3342\n",
      "Average Reward for Agent 11 this episode : -7.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 225.5532\n",
      "Average Reward for Agent 12 this episode : -3.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.3995\n",
      "Average Reward for Agent 13 this episode : -6.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.9869\n",
      "Reducing exploration for all agents to 0.0053\n",
      "Episode 304 is finished\n",
      "Average Reward for Agent 0 this episode : -11.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5323\n",
      "Average Reward for Agent 1 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.1879\n",
      "Average Reward for Agent 2 this episode : -50.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.1098\n",
      "Average Reward for Agent 3 this episode : -5.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6973\n",
      "Average Reward for Agent 4 this episode : -18.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1542\n",
      "Average Reward for Agent 5 this episode : -0.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0626\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7109\n",
      "Average Reward for Agent 7 this episode : -9.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6524\n",
      "Average Reward for Agent 8 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2508\n",
      "Average Reward for Agent 9 this episode : -21.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5651\n",
      "Average Reward for Agent 10 this episode : -55.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 369.2569\n",
      "Average Reward for Agent 11 this episode : -6.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 278.9003\n",
      "Average Reward for Agent 12 this episode : -5.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2744\n",
      "Average Reward for Agent 13 this episode : -8.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.0072\n",
      "Reducing exploration for all agents to 0.0052\n",
      "Episode 305 is finished\n",
      "Average Reward for Agent 0 this episode : -11.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.1279\n",
      "Average Reward for Agent 1 this episode : -7.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.7831\n",
      "Average Reward for Agent 2 this episode : -51.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.3032\n",
      "Average Reward for Agent 3 this episode : -2.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0755\n",
      "Average Reward for Agent 4 this episode : -16.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2362\n",
      "Average Reward for Agent 5 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8703\n",
      "Average Reward for Agent 6 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8639\n",
      "Average Reward for Agent 7 this episode : -10.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6683\n",
      "Average Reward for Agent 8 this episode : -2.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0387\n",
      "Average Reward for Agent 9 this episode : -17.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.3944\n",
      "Average Reward for Agent 10 this episode : -46.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 244.6613\n",
      "Average Reward for Agent 11 this episode : -7.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 223.7248\n",
      "Average Reward for Agent 12 this episode : -7.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7638\n",
      "Average Reward for Agent 13 this episode : -42.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.8698\n",
      "Reducing exploration for all agents to 0.0051\n",
      "Episode 306 is finished\n",
      "Average Reward for Agent 0 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1355\n",
      "Average Reward for Agent 1 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7079\n",
      "Average Reward for Agent 2 this episode : -49.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.4704\n",
      "Average Reward for Agent 3 this episode : -8.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.4784\n",
      "Average Reward for Agent 4 this episode : -15.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.4985\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5868\n",
      "Average Reward for Agent 6 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0124\n",
      "Average Reward for Agent 7 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3670\n",
      "Average Reward for Agent 8 this episode : -3.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.6867\n",
      "Average Reward for Agent 9 this episode : -13.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.8952\n",
      "Average Reward for Agent 10 this episode : -59.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.3329\n",
      "Average Reward for Agent 11 this episode : -6.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.8077\n",
      "Average Reward for Agent 12 this episode : -4.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2878\n",
      "Average Reward for Agent 13 this episode : -51.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.1956\n",
      "Reducing exploration for all agents to 0.005\n",
      "Episode 307 is finished\n",
      "Average Reward for Agent 0 this episode : -11.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.6466\n",
      "Average Reward for Agent 1 this episode : -11.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.5491\n",
      "Average Reward for Agent 2 this episode : -44.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.2886\n",
      "Average Reward for Agent 3 this episode : -0.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1074\n",
      "Average Reward for Agent 4 this episode : -17.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.5403\n",
      "Average Reward for Agent 5 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2271\n",
      "Average Reward for Agent 6 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7545\n",
      "Average Reward for Agent 7 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0541\n",
      "Average Reward for Agent 8 this episode : -1.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5930\n",
      "Average Reward for Agent 9 this episode : -17.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.5740\n",
      "Average Reward for Agent 10 this episode : -60.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.8698\n",
      "Average Reward for Agent 11 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.2153\n",
      "Average Reward for Agent 12 this episode : -3.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6083\n",
      "Average Reward for Agent 13 this episode : -9.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.2189\n",
      "Reducing exploration for all agents to 0.0049\n",
      "Episode 308 is finished\n",
      "Average Reward for Agent 0 this episode : -11.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1201\n",
      "Average Reward for Agent 1 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6073\n",
      "Average Reward for Agent 2 this episode : -44.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.6192\n",
      "Average Reward for Agent 3 this episode : -3.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0500\n",
      "Average Reward for Agent 4 this episode : -18.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7734\n",
      "Average Reward for Agent 5 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8641\n",
      "Average Reward for Agent 6 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2916\n",
      "Average Reward for Agent 7 this episode : -16.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8618\n",
      "Average Reward for Agent 8 this episode : -1.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.8538\n",
      "Average Reward for Agent 9 this episode : -17.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.6306\n",
      "Average Reward for Agent 10 this episode : -64.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 227.9055\n",
      "Average Reward for Agent 11 this episode : -3.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.0137\n",
      "Average Reward for Agent 12 this episode : -5.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4359\n",
      "Average Reward for Agent 13 this episode : -27.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.8088\n",
      "Reducing exploration for all agents to 0.0048\n",
      "Episode 309 is finished\n",
      "Average Reward for Agent 0 this episode : -10.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.7698\n",
      "Average Reward for Agent 1 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8874\n",
      "Average Reward for Agent 2 this episode : -43.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.0096\n",
      "Average Reward for Agent 3 this episode : -2.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7230\n",
      "Average Reward for Agent 4 this episode : -18.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0300\n",
      "Average Reward for Agent 5 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7842\n",
      "Average Reward for Agent 6 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1730\n",
      "Average Reward for Agent 7 this episode : -14.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4480\n",
      "Average Reward for Agent 8 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9072\n",
      "Average Reward for Agent 9 this episode : -19.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9168\n",
      "Average Reward for Agent 10 this episode : -56.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.7975\n",
      "Average Reward for Agent 11 this episode : -4.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.2609\n",
      "Average Reward for Agent 12 this episode : -4.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2184\n",
      "Average Reward for Agent 13 this episode : -48.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.1508\n",
      "Reducing exploration for all agents to 0.0047\n",
      "Episode 310 is finished\n",
      "Average Reward for Agent 0 this episode : -11.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.6179\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -4.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8001\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -39.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.0516\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -8.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -16.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9620\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3521\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4743\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8656\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6580\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -20.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2396\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -50.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.5336\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -3.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.3076\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0261\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -54.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.9520\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0047\n",
      "Episode 311 is finished\n",
      "Average Reward for Agent 0 this episode : -12.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.2159\n",
      "Average Reward for Agent 1 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.8644\n",
      "Average Reward for Agent 2 this episode : -36.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 300.3206\n",
      "Average Reward for Agent 3 this episode : -4.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7504\n",
      "Average Reward for Agent 4 this episode : -18.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7391\n",
      "Average Reward for Agent 5 this episode : -0.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5010\n",
      "Average Reward for Agent 6 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5213\n",
      "Average Reward for Agent 7 this episode : -12.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3026\n",
      "Average Reward for Agent 8 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2221\n",
      "Average Reward for Agent 9 this episode : -16.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.0694\n",
      "Average Reward for Agent 10 this episode : -48.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 311.1097\n",
      "Average Reward for Agent 11 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.6927\n",
      "Average Reward for Agent 12 this episode : -7.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4149\n",
      "Average Reward for Agent 13 this episode : -49.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.9414\n",
      "Reducing exploration for all agents to 0.0046\n",
      "Episode 312 is finished\n",
      "Average Reward for Agent 0 this episode : -24.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.5051\n",
      "Average Reward for Agent 1 this episode : -11.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9724\n",
      "Average Reward for Agent 2 this episode : -37.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.6003\n",
      "Average Reward for Agent 3 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8956\n",
      "Average Reward for Agent 4 this episode : -18.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3692\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0452\n",
      "Average Reward for Agent 6 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1950\n",
      "Average Reward for Agent 7 this episode : -15.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6481\n",
      "Average Reward for Agent 8 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0636\n",
      "Average Reward for Agent 9 this episode : -19.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.0671\n",
      "Average Reward for Agent 10 this episode : -29.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 673.4749\n",
      "Average Reward for Agent 11 this episode : -4.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.3149\n",
      "Average Reward for Agent 12 this episode : -4.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2048\n",
      "Average Reward for Agent 13 this episode : -56.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.9960\n",
      "Reducing exploration for all agents to 0.0045\n",
      "Episode 313 is finished\n",
      "Average Reward for Agent 0 this episode : -15.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.2768\n",
      "Average Reward for Agent 1 this episode : -13.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4199\n",
      "Average Reward for Agent 2 this episode : -39.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.5567\n",
      "Average Reward for Agent 3 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6694\n",
      "Average Reward for Agent 4 this episode : -19.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3120\n",
      "Average Reward for Agent 5 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7544\n",
      "Average Reward for Agent 6 this episode : -1.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9970\n",
      "Average Reward for Agent 7 this episode : -18.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6466\n",
      "Average Reward for Agent 8 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8006\n",
      "Average Reward for Agent 9 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0995\n",
      "Average Reward for Agent 10 this episode : -56.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 339.2770\n",
      "Average Reward for Agent 11 this episode : -3.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.1675\n",
      "Average Reward for Agent 12 this episode : -5.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7389\n",
      "Average Reward for Agent 13 this episode : -55.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3039\n",
      "Reducing exploration for all agents to 0.0044\n",
      "Episode 314 is finished\n",
      "Average Reward for Agent 0 this episode : -13.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9919\n",
      "Average Reward for Agent 1 this episode : -12.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1441\n",
      "Average Reward for Agent 2 this episode : -43.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7263\n",
      "Average Reward for Agent 3 this episode : -5.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5680\n",
      "Average Reward for Agent 4 this episode : -18.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1176\n",
      "Average Reward for Agent 5 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9030\n",
      "Average Reward for Agent 6 this episode : -1.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8605\n",
      "Average Reward for Agent 7 this episode : -13.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2532\n",
      "Average Reward for Agent 8 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8120\n",
      "Average Reward for Agent 9 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.9764\n",
      "Average Reward for Agent 10 this episode : -61.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 410.7097\n",
      "Average Reward for Agent 11 this episode : -4.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.0845\n",
      "Average Reward for Agent 12 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.8110\n",
      "Average Reward for Agent 13 this episode : -47.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.9228\n",
      "Reducing exploration for all agents to 0.0044\n",
      "Episode 315 is finished\n",
      "Average Reward for Agent 0 this episode : -13.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8810\n",
      "Average Reward for Agent 1 this episode : -7.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9345\n",
      "Average Reward for Agent 2 this episode : -43.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.3791\n",
      "Average Reward for Agent 3 this episode : -4.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9215\n",
      "Average Reward for Agent 4 this episode : -18.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7825\n",
      "Average Reward for Agent 5 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5238\n",
      "Average Reward for Agent 6 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.4435\n",
      "Average Reward for Agent 7 this episode : -10.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1288\n",
      "Average Reward for Agent 9 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.7192\n",
      "Average Reward for Agent 10 this episode : -65.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 723.8567\n",
      "Average Reward for Agent 11 this episode : -4.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.2397\n",
      "Average Reward for Agent 12 this episode : -7.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6661\n",
      "Average Reward for Agent 13 this episode : -44.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4700\n",
      "Reducing exploration for all agents to 0.0043\n",
      "Episode 316 is finished\n",
      "Average Reward for Agent 0 this episode : -13.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.7002\n",
      "Average Reward for Agent 1 this episode : -7.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6099\n",
      "Average Reward for Agent 2 this episode : -44.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.2907\n",
      "Average Reward for Agent 3 this episode : -5.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0213\n",
      "Average Reward for Agent 4 this episode : -17.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3981\n",
      "Average Reward for Agent 5 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1917\n",
      "Average Reward for Agent 6 this episode : -1.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6535\n",
      "Average Reward for Agent 7 this episode : -13.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2290\n",
      "Average Reward for Agent 8 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9663\n",
      "Average Reward for Agent 9 this episode : -17.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.9369\n",
      "Average Reward for Agent 10 this episode : -56.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 330.3774\n",
      "Average Reward for Agent 11 this episode : -5.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.2120\n",
      "Average Reward for Agent 12 this episode : -6.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1364\n",
      "Average Reward for Agent 13 this episode : -46.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.9089\n",
      "Reducing exploration for all agents to 0.0042\n",
      "Episode 317 is finished\n",
      "Average Reward for Agent 0 this episode : -13.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.2928\n",
      "Average Reward for Agent 1 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6849\n",
      "Average Reward for Agent 2 this episode : -43.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.7112\n",
      "Average Reward for Agent 3 this episode : -1.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5667\n",
      "Average Reward for Agent 4 this episode : -21.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.4668\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1535\n",
      "Average Reward for Agent 6 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6291\n",
      "Average Reward for Agent 7 this episode : -11.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2949\n",
      "Average Reward for Agent 8 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3746\n",
      "Average Reward for Agent 9 this episode : -21.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.1595\n",
      "Average Reward for Agent 10 this episode : -48.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 426.9586\n",
      "Average Reward for Agent 11 this episode : -7.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.8644\n",
      "Average Reward for Agent 12 this episode : -6.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7655\n",
      "Average Reward for Agent 13 this episode : -40.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0003\n",
      "Reducing exploration for all agents to 0.0041\n",
      "Episode 318 is finished\n",
      "Average Reward for Agent 0 this episode : -14.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.1291\n",
      "Average Reward for Agent 1 this episode : -7.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9264\n",
      "Average Reward for Agent 2 this episode : -40.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.5451\n",
      "Average Reward for Agent 3 this episode : -1.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4184\n",
      "Average Reward for Agent 4 this episode : -17.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0235\n",
      "Average Reward for Agent 5 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5328\n",
      "Average Reward for Agent 6 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1324\n",
      "Average Reward for Agent 7 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.8796\n",
      "Average Reward for Agent 8 this episode : -1.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8127\n",
      "Average Reward for Agent 9 this episode : -13.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.0400\n",
      "Average Reward for Agent 10 this episode : -56.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.3979\n",
      "Average Reward for Agent 11 this episode : -10.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.7101\n",
      "Average Reward for Agent 12 this episode : -8.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3990\n",
      "Average Reward for Agent 13 this episode : -45.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.9826\n",
      "Reducing exploration for all agents to 0.0041\n",
      "Episode 319 is finished\n",
      "Average Reward for Agent 0 this episode : -13.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1379\n",
      "Average Reward for Agent 1 this episode : -7.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.2152\n",
      "Average Reward for Agent 2 this episode : -46.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.8465\n",
      "Average Reward for Agent 3 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9504\n",
      "Average Reward for Agent 4 this episode : -16.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0575\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4684\n",
      "Average Reward for Agent 6 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3602\n",
      "Average Reward for Agent 7 this episode : -11.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9191\n",
      "Average Reward for Agent 8 this episode : -0.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1609\n",
      "Average Reward for Agent 9 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6576\n",
      "Average Reward for Agent 10 this episode : -53.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 340.1475\n",
      "Average Reward for Agent 11 this episode : -11.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 187.8068\n",
      "Average Reward for Agent 12 this episode : -5.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.2512\n",
      "Average Reward for Agent 13 this episode : -48.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.9665\n",
      "Reducing exploration for all agents to 0.004\n",
      "Episode 320 is finished\n",
      "Average Reward for Agent 0 this episode : -15.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.8574\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -4.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5825\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -38.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.6579\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.7660\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4057\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1188\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7335\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -9.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9710\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8825\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -14.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.1913\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -43.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.0964\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -6.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.8837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -6.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7631\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -41.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.9395\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0039\n",
      "Episode 321 is finished\n",
      "Average Reward for Agent 0 this episode : -11.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9506\n",
      "Average Reward for Agent 1 this episode : -10.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7442\n",
      "Average Reward for Agent 2 this episode : -40.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.3392\n",
      "Average Reward for Agent 3 this episode : -4.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8531\n",
      "Average Reward for Agent 4 this episode : -19.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0411\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6704\n",
      "Average Reward for Agent 6 this episode : -1.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2533\n",
      "Average Reward for Agent 7 this episode : -9.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2594\n",
      "Average Reward for Agent 8 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3918\n",
      "Average Reward for Agent 9 this episode : -17.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.4697\n",
      "Average Reward for Agent 10 this episode : -45.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.1531\n",
      "Average Reward for Agent 11 this episode : -1.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.0941\n",
      "Average Reward for Agent 12 this episode : -5.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7124\n",
      "Average Reward for Agent 13 this episode : -48.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.2291\n",
      "Reducing exploration for all agents to 0.0039\n",
      "Episode 322 is finished\n",
      "Average Reward for Agent 0 this episode : -17.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4813\n",
      "Average Reward for Agent 1 this episode : -4.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0673\n",
      "Average Reward for Agent 2 this episode : -44.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.2849\n",
      "Average Reward for Agent 3 this episode : -5.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5356\n",
      "Average Reward for Agent 4 this episode : -14.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3445\n",
      "Average Reward for Agent 5 this episode : -0.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2230\n",
      "Average Reward for Agent 6 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7028\n",
      "Average Reward for Agent 7 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6728\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5980\n",
      "Average Reward for Agent 9 this episode : -13.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.9457\n",
      "Average Reward for Agent 10 this episode : -34.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 633.5563\n",
      "Average Reward for Agent 11 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.4438\n",
      "Average Reward for Agent 12 this episode : -4.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8841\n",
      "Average Reward for Agent 13 this episode : -39.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2902\n",
      "Reducing exploration for all agents to 0.0038\n",
      "Episode 323 is finished\n",
      "Average Reward for Agent 0 this episode : -22.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.1942\n",
      "Average Reward for Agent 1 this episode : -11.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7749\n",
      "Average Reward for Agent 2 this episode : -40.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9249\n",
      "Average Reward for Agent 3 this episode : -3.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2107\n",
      "Average Reward for Agent 4 this episode : -13.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4643\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9562\n",
      "Average Reward for Agent 6 this episode : -1.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5473\n",
      "Average Reward for Agent 7 this episode : -13.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9488\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1189\n",
      "Average Reward for Agent 9 this episode : -3.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4613\n",
      "Average Reward for Agent 10 this episode : -35.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 413.8347\n",
      "Average Reward for Agent 11 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.8063\n",
      "Average Reward for Agent 12 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9943\n",
      "Average Reward for Agent 13 this episode : -44.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.3489\n",
      "Reducing exploration for all agents to 0.0037\n",
      "Episode 324 is finished\n",
      "Average Reward for Agent 0 this episode : -18.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9257\n",
      "Average Reward for Agent 1 this episode : -10.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6437\n",
      "Average Reward for Agent 2 this episode : -41.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.2708\n",
      "Average Reward for Agent 3 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1771\n",
      "Average Reward for Agent 4 this episode : -18.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3343\n",
      "Average Reward for Agent 5 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9256\n",
      "Average Reward for Agent 6 this episode : -4.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9249\n",
      "Average Reward for Agent 7 this episode : -11.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4101\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4682\n",
      "Average Reward for Agent 9 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.9630\n",
      "Average Reward for Agent 10 this episode : -38.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 227.3518\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.9447\n",
      "Average Reward for Agent 12 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.9292\n",
      "Average Reward for Agent 13 this episode : -42.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.7579\n",
      "Reducing exploration for all agents to 0.0037\n",
      "Episode 325 is finished\n",
      "Average Reward for Agent 0 this episode : -15.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.7985\n",
      "Average Reward for Agent 1 this episode : -5.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2968\n",
      "Average Reward for Agent 2 this episode : -53.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6891\n",
      "Average Reward for Agent 3 this episode : -5.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1065\n",
      "Average Reward for Agent 4 this episode : -23.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3020\n",
      "Average Reward for Agent 5 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9221\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0670\n",
      "Average Reward for Agent 7 this episode : -12.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2175\n",
      "Average Reward for Agent 8 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5177\n",
      "Average Reward for Agent 9 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.8501\n",
      "Average Reward for Agent 10 this episode : -45.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 398.2901\n",
      "Average Reward for Agent 11 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6207\n",
      "Average Reward for Agent 12 this episode : -5.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7988\n",
      "Average Reward for Agent 13 this episode : -44.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.3125\n",
      "Reducing exploration for all agents to 0.0036\n",
      "Episode 326 is finished\n",
      "Average Reward for Agent 0 this episode : -13.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.4284\n",
      "Average Reward for Agent 1 this episode : -6.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8600\n",
      "Average Reward for Agent 2 this episode : -44.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.5228\n",
      "Average Reward for Agent 3 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8696\n",
      "Average Reward for Agent 4 this episode : -21.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2258\n",
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -1.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5271\n",
      "Average Reward for Agent 7 this episode : -10.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8187\n",
      "Average Reward for Agent 8 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7257\n",
      "Average Reward for Agent 9 this episode : -2.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.0019\n",
      "Average Reward for Agent 10 this episode : -51.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 523.4622\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.3468\n",
      "Average Reward for Agent 12 this episode : -5.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5680\n",
      "Average Reward for Agent 13 this episode : -45.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.9653\n",
      "Reducing exploration for all agents to 0.0035\n",
      "Episode 327 is finished\n",
      "Average Reward for Agent 0 this episode : -12.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7771\n",
      "Average Reward for Agent 1 this episode : -13.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.1408\n",
      "Average Reward for Agent 2 this episode : -46.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.6689\n",
      "Average Reward for Agent 3 this episode : -3.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.8458\n",
      "Average Reward for Agent 4 this episode : -19.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.7410\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4822\n",
      "Average Reward for Agent 6 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0828\n",
      "Average Reward for Agent 7 this episode : -14.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7132\n",
      "Average Reward for Agent 8 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8951\n",
      "Average Reward for Agent 9 this episode : -3.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9915\n",
      "Average Reward for Agent 10 this episode : -56.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.3598\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2121\n",
      "Average Reward for Agent 12 this episode : -7.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.7976\n",
      "Average Reward for Agent 13 this episode : -44.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.4113\n",
      "Reducing exploration for all agents to 0.0035\n",
      "Episode 328 is finished\n",
      "Average Reward for Agent 0 this episode : -14.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7576\n",
      "Average Reward for Agent 1 this episode : -8.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9708\n",
      "Average Reward for Agent 2 this episode : -39.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.9596\n",
      "Average Reward for Agent 3 this episode : -9.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1691\n",
      "Average Reward for Agent 4 this episode : -20.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.5649\n",
      "Average Reward for Agent 5 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2743\n",
      "Average Reward for Agent 6 this episode : -1.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8316\n",
      "Average Reward for Agent 7 this episode : -12.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3717\n",
      "Average Reward for Agent 8 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8803\n",
      "Average Reward for Agent 9 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.9178\n",
      "Average Reward for Agent 10 this episode : -53.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 583.2089\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5122\n",
      "Average Reward for Agent 12 this episode : -4.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4792\n",
      "Average Reward for Agent 13 this episode : -53.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3783\n",
      "Reducing exploration for all agents to 0.0034\n",
      "Episode 329 is finished\n",
      "Average Reward for Agent 0 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2196\n",
      "Average Reward for Agent 1 this episode : -9.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.5981\n",
      "Average Reward for Agent 2 this episode : -42.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.3924\n",
      "Average Reward for Agent 3 this episode : -3.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5522\n",
      "Average Reward for Agent 4 this episode : -17.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2242\n",
      "Average Reward for Agent 5 this episode : -0.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6243\n",
      "Average Reward for Agent 6 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6315\n",
      "Average Reward for Agent 7 this episode : -12.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1177\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6327\n",
      "Average Reward for Agent 9 this episode : -17.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6787\n",
      "Average Reward for Agent 10 this episode : -55.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 331.2067\n",
      "Average Reward for Agent 11 this episode : -63.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.4646\n",
      "Average Reward for Agent 12 this episode : -6.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8012\n",
      "Average Reward for Agent 13 this episode : -46.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.2258\n",
      "Reducing exploration for all agents to 0.0034\n",
      "Episode 330 is finished\n",
      "Average Reward for Agent 0 this episode : -13.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1826\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0323\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -40.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9949\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4230\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -19.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9202\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4470\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7746\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0798\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9276\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -23.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6517\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -52.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 472.8186\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -2.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.4824\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -5.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1559\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -58.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.4941\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0033\n",
      "Episode 331 is finished\n",
      "Average Reward for Agent 0 this episode : -15.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.7188\n",
      "Average Reward for Agent 1 this episode : -10.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1609\n",
      "Average Reward for Agent 2 this episode : -49.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.7600\n",
      "Average Reward for Agent 3 this episode : -2.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6346\n",
      "Average Reward for Agent 4 this episode : -17.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.6045\n",
      "Average Reward for Agent 5 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8191\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3344\n",
      "Average Reward for Agent 7 this episode : -10.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -1.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4871\n",
      "Average Reward for Agent 9 this episode : -20.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1022\n",
      "Average Reward for Agent 10 this episode : -37.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 325.9182\n",
      "Average Reward for Agent 11 this episode : -17.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 330.0278\n",
      "Average Reward for Agent 12 this episode : -11.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0952\n",
      "Average Reward for Agent 13 this episode : -48.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.9236\n",
      "Reducing exploration for all agents to 0.0032\n",
      "Episode 332 is finished\n",
      "Average Reward for Agent 0 this episode : -17.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3880\n",
      "Average Reward for Agent 1 this episode : -8.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2519\n",
      "Average Reward for Agent 2 this episode : -49.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.4855\n",
      "Average Reward for Agent 3 this episode : -3.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4884\n",
      "Average Reward for Agent 4 this episode : -20.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2954\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2936\n",
      "Average Reward for Agent 6 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4216\n",
      "Average Reward for Agent 7 this episode : -11.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8128\n",
      "Average Reward for Agent 8 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2693\n",
      "Average Reward for Agent 9 this episode : -12.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.5810\n",
      "Average Reward for Agent 10 this episode : -48.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 237.9923\n",
      "Average Reward for Agent 11 this episode : -34.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 403.5765\n",
      "Average Reward for Agent 12 this episode : -5.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.2110\n",
      "Average Reward for Agent 13 this episode : -15.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.5057\n",
      "Reducing exploration for all agents to 0.0032\n",
      "Episode 333 is finished\n",
      "Average Reward for Agent 0 this episode : -12.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.6150\n",
      "Average Reward for Agent 1 this episode : -13.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.1645\n",
      "Average Reward for Agent 2 this episode : -58.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.5477\n",
      "Average Reward for Agent 3 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1351\n",
      "Average Reward for Agent 4 this episode : -14.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.5084\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9131\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8713\n",
      "Average Reward for Agent 7 this episode : -11.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5694\n",
      "Average Reward for Agent 8 this episode : -1.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7224\n",
      "Average Reward for Agent 9 this episode : -2.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.3312\n",
      "Average Reward for Agent 10 this episode : -41.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 154.4376\n",
      "Average Reward for Agent 11 this episode : -34.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.3830\n",
      "Average Reward for Agent 12 this episode : -8.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8866\n",
      "Average Reward for Agent 13 this episode : -11.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.9487\n",
      "Reducing exploration for all agents to 0.0031\n",
      "Episode 334 is finished\n",
      "Average Reward for Agent 0 this episode : -19.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0419\n",
      "Average Reward for Agent 1 this episode : -10.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0225\n",
      "Average Reward for Agent 2 this episode : -48.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.1037\n",
      "Average Reward for Agent 3 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6353\n",
      "Average Reward for Agent 4 this episode : -18.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.3970\n",
      "Average Reward for Agent 5 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4217\n",
      "Average Reward for Agent 6 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5548\n",
      "Average Reward for Agent 7 this episode : -11.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7274\n",
      "Average Reward for Agent 8 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3034\n",
      "Average Reward for Agent 9 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.8197\n",
      "Average Reward for Agent 10 this episode : -36.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 255.3861\n",
      "Average Reward for Agent 11 this episode : -32.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.7810\n",
      "Average Reward for Agent 12 this episode : -3.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9929\n",
      "Average Reward for Agent 13 this episode : -34.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.8830\n",
      "Reducing exploration for all agents to 0.0031\n",
      "Episode 335 is finished\n",
      "Average Reward for Agent 0 this episode : -19.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5330\n",
      "Average Reward for Agent 1 this episode : -5.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9823\n",
      "Average Reward for Agent 2 this episode : -42.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2589\n",
      "Average Reward for Agent 3 this episode : -2.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7528\n",
      "Average Reward for Agent 4 this episode : -14.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4908\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4836\n",
      "Average Reward for Agent 6 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0253\n",
      "Average Reward for Agent 7 this episode : -12.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2147\n",
      "Average Reward for Agent 8 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8811\n",
      "Average Reward for Agent 9 this episode : -7.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.4386\n",
      "Average Reward for Agent 10 this episode : -48.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 235.7023\n",
      "Average Reward for Agent 11 this episode : -26.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.9962\n",
      "Average Reward for Agent 12 this episode : -3.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8120\n",
      "Average Reward for Agent 13 this episode : -44.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.5870\n",
      "Reducing exploration for all agents to 0.003\n",
      "Episode 336 is finished\n",
      "Average Reward for Agent 0 this episode : -15.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2107\n",
      "Average Reward for Agent 1 this episode : -12.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.4050\n",
      "Average Reward for Agent 2 this episode : -48.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.7326\n",
      "Average Reward for Agent 3 this episode : -3.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3233\n",
      "Average Reward for Agent 4 this episode : -17.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0684\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0623\n",
      "Average Reward for Agent 6 this episode : -12.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.5019\n",
      "Average Reward for Agent 7 this episode : -13.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4099\n",
      "Average Reward for Agent 8 this episode : -1.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6574\n",
      "Average Reward for Agent 9 this episode : -13.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5490\n",
      "Average Reward for Agent 10 this episode : -45.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 363.1840\n",
      "Average Reward for Agent 11 this episode : -26.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.2942\n",
      "Average Reward for Agent 12 this episode : -4.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8672\n",
      "Average Reward for Agent 13 this episode : -31.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.1014\n",
      "Reducing exploration for all agents to 0.003\n",
      "Episode 337 is finished\n",
      "Average Reward for Agent 0 this episode : -13.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3876\n",
      "Average Reward for Agent 1 this episode : -11.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4693\n",
      "Average Reward for Agent 2 this episode : -48.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.4769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 3 this episode : -2.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2123\n",
      "Average Reward for Agent 4 this episode : -16.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2919\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8828\n",
      "Average Reward for Agent 6 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9427\n",
      "Average Reward for Agent 7 this episode : -11.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6767\n",
      "Average Reward for Agent 8 this episode : -1.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9053\n",
      "Average Reward for Agent 9 this episode : -18.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0566\n",
      "Average Reward for Agent 10 this episode : -38.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 299.6945\n",
      "Average Reward for Agent 11 this episode : -27.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.7649\n",
      "Average Reward for Agent 12 this episode : -4.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9986\n",
      "Average Reward for Agent 13 this episode : -10.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.7655\n",
      "Reducing exploration for all agents to 0.0029\n",
      "Episode 338 is finished\n",
      "Average Reward for Agent 0 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2311\n",
      "Average Reward for Agent 1 this episode : -13.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5689\n",
      "Average Reward for Agent 2 this episode : -46.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.2754\n",
      "Average Reward for Agent 3 this episode : -2.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5648\n",
      "Average Reward for Agent 4 this episode : -16.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8830\n",
      "Average Reward for Agent 5 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7420\n",
      "Average Reward for Agent 6 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.5395\n",
      "Average Reward for Agent 7 this episode : -9.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7614\n",
      "Average Reward for Agent 8 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7521\n",
      "Average Reward for Agent 9 this episode : -21.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.2212\n",
      "Average Reward for Agent 10 this episode : -39.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 149.2630\n",
      "Average Reward for Agent 11 this episode : -30.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4637\n",
      "Average Reward for Agent 12 this episode : -4.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7488\n",
      "Average Reward for Agent 13 this episode : -5.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.9646\n",
      "Reducing exploration for all agents to 0.0029\n",
      "Episode 339 is finished\n",
      "Average Reward for Agent 0 this episode : -27.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.5065\n",
      "Average Reward for Agent 1 this episode : -10.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7095\n",
      "Average Reward for Agent 2 this episode : -51.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.1306\n",
      "Average Reward for Agent 3 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2240\n",
      "Average Reward for Agent 4 this episode : -13.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3595\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6438\n",
      "Average Reward for Agent 6 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9424\n",
      "Average Reward for Agent 7 this episode : -11.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9014\n",
      "Average Reward for Agent 8 this episode : -1.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1872\n",
      "Average Reward for Agent 9 this episode : -20.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7854\n",
      "Average Reward for Agent 10 this episode : -27.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 354.8549\n",
      "Average Reward for Agent 11 this episode : -28.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.5489\n",
      "Average Reward for Agent 12 this episode : -3.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2541\n",
      "Average Reward for Agent 13 this episode : -8.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.8435\n",
      "Reducing exploration for all agents to 0.0028\n",
      "Episode 340 is finished\n",
      "Average Reward for Agent 0 this episode : -22.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.6620\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -7.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.4515\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -47.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.8919\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7391\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -20.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5917\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4275\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9312\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0260\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2585\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -13.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5678\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -36.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 319.7881\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -27.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.8925\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -6.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1838\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -10.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.4698\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0028\n",
      "Episode 341 is finished\n",
      "Average Reward for Agent 0 this episode : -24.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.1677\n",
      "Average Reward for Agent 1 this episode : -10.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6933\n",
      "Average Reward for Agent 2 this episode : -46.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.3611\n",
      "Average Reward for Agent 3 this episode : -4.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4099\n",
      "Average Reward for Agent 4 this episode : -20.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1787\n",
      "Average Reward for Agent 5 this episode : -0.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1009\n",
      "Average Reward for Agent 6 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5666\n",
      "Average Reward for Agent 7 this episode : -12.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1117\n",
      "Average Reward for Agent 8 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1317\n",
      "Average Reward for Agent 9 this episode : -5.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.3004\n",
      "Average Reward for Agent 10 this episode : -23.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 390.8648\n",
      "Average Reward for Agent 11 this episode : -22.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.8942\n",
      "Average Reward for Agent 12 this episode : -6.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3703\n",
      "Average Reward for Agent 13 this episode : -33.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.1199\n",
      "Reducing exploration for all agents to 0.0027\n",
      "Episode 342 is finished\n",
      "Average Reward for Agent 0 this episode : -26.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3745\n",
      "Average Reward for Agent 1 this episode : -9.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6324\n",
      "Average Reward for Agent 2 this episode : -59.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7747\n",
      "Average Reward for Agent 3 this episode : -3.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4893\n",
      "Average Reward for Agent 4 this episode : -19.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.6549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6744\n",
      "Average Reward for Agent 6 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2126\n",
      "Average Reward for Agent 7 this episode : -10.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2357\n",
      "Average Reward for Agent 8 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4894\n",
      "Average Reward for Agent 9 this episode : -1.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.3794\n",
      "Average Reward for Agent 10 this episode : -21.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.5049\n",
      "Average Reward for Agent 11 this episode : -23.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 460.1569\n",
      "Average Reward for Agent 12 this episode : -5.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5318\n",
      "Average Reward for Agent 13 this episode : -35.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2573\n",
      "Reducing exploration for all agents to 0.0027\n",
      "Episode 343 is finished\n",
      "Average Reward for Agent 0 this episode : -31.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5477\n",
      "Average Reward for Agent 1 this episode : -10.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4711\n",
      "Average Reward for Agent 2 this episode : -46.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.0444\n",
      "Average Reward for Agent 3 this episode : -3.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.3256\n",
      "Average Reward for Agent 4 this episode : -17.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9263\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9110\n",
      "Average Reward for Agent 6 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2803\n",
      "Average Reward for Agent 7 this episode : -12.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7651\n",
      "Average Reward for Agent 8 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4349\n",
      "Average Reward for Agent 9 this episode : -1.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.8598\n",
      "Average Reward for Agent 10 this episode : -15.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.7771\n",
      "Average Reward for Agent 11 this episode : -28.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.9460\n",
      "Average Reward for Agent 12 this episode : -7.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6710\n",
      "Average Reward for Agent 13 this episode : -46.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.3405\n",
      "Reducing exploration for all agents to 0.0026\n",
      "Episode 344 is finished\n",
      "Average Reward for Agent 0 this episode : -38.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4542\n",
      "Average Reward for Agent 1 this episode : -10.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4901\n",
      "Average Reward for Agent 2 this episode : -49.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.1073\n",
      "Average Reward for Agent 3 this episode : -1.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1194\n",
      "Average Reward for Agent 4 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.7320\n",
      "Average Reward for Agent 5 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8238\n",
      "Average Reward for Agent 6 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0534\n",
      "Average Reward for Agent 7 this episode : -14.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1618\n",
      "Average Reward for Agent 8 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6834\n",
      "Average Reward for Agent 9 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.6930\n",
      "Average Reward for Agent 10 this episode : -14.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 205.8251\n",
      "Average Reward for Agent 11 this episode : -37.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 374.2917\n",
      "Average Reward for Agent 12 this episode : -7.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3949\n",
      "Average Reward for Agent 13 this episode : -8.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.5915\n",
      "Reducing exploration for all agents to 0.0026\n",
      "Episode 345 is finished\n",
      "Average Reward for Agent 0 this episode : -39.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.7972\n",
      "Average Reward for Agent 1 this episode : -10.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4600\n",
      "Average Reward for Agent 2 this episode : -50.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.1807\n",
      "Average Reward for Agent 3 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6201\n",
      "Average Reward for Agent 4 this episode : -17.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7352\n",
      "Average Reward for Agent 5 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9546\n",
      "Average Reward for Agent 6 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4383\n",
      "Average Reward for Agent 7 this episode : -13.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.9347\n",
      "Average Reward for Agent 8 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4135\n",
      "Average Reward for Agent 9 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5258\n",
      "Average Reward for Agent 10 this episode : -35.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.3351\n",
      "Average Reward for Agent 11 this episode : -13.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.4160\n",
      "Average Reward for Agent 12 this episode : -7.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0855\n",
      "Average Reward for Agent 13 this episode : -17.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.5698\n",
      "Reducing exploration for all agents to 0.0025\n",
      "Episode 346 is finished\n",
      "Average Reward for Agent 0 this episode : -39.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2647\n",
      "Average Reward for Agent 1 this episode : -11.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9006\n",
      "Average Reward for Agent 2 this episode : -52.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6444\n",
      "Average Reward for Agent 3 this episode : -2.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0043\n",
      "Average Reward for Agent 4 this episode : -15.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7962\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1054\n",
      "Average Reward for Agent 6 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1287\n",
      "Average Reward for Agent 7 this episode : -12.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3112\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5887\n",
      "Average Reward for Agent 9 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.7243\n",
      "Average Reward for Agent 10 this episode : -41.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.1075\n",
      "Average Reward for Agent 11 this episode : -11.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.5894\n",
      "Average Reward for Agent 12 this episode : -8.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2773\n",
      "Average Reward for Agent 13 this episode : -10.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.5100\n",
      "Reducing exploration for all agents to 0.0025\n",
      "Episode 347 is finished\n",
      "Average Reward for Agent 0 this episode : -34.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.7517\n",
      "Average Reward for Agent 1 this episode : -12.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6009\n",
      "Average Reward for Agent 2 this episode : -46.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.7611\n",
      "Average Reward for Agent 3 this episode : -4.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5946\n",
      "Average Reward for Agent 4 this episode : -15.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.1012\n",
      "Average Reward for Agent 5 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4640\n",
      "Average Reward for Agent 6 this episode : -1.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4968\n",
      "Average Reward for Agent 7 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4900\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0022\n",
      "Average Reward for Agent 9 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.7976\n",
      "Average Reward for Agent 10 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.8887\n",
      "Average Reward for Agent 11 this episode : -32.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.3300\n",
      "Average Reward for Agent 12 this episode : -7.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0378\n",
      "Average Reward for Agent 13 this episode : -38.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.8022\n",
      "Reducing exploration for all agents to 0.0025\n",
      "Episode 348 is finished\n",
      "Average Reward for Agent 0 this episode : -38.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.3355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -6.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.2063\n",
      "Average Reward for Agent 2 this episode : -45.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2195\n",
      "Average Reward for Agent 3 this episode : -2.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4995\n",
      "Average Reward for Agent 4 this episode : -17.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2809\n",
      "Average Reward for Agent 5 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5360\n",
      "Average Reward for Agent 6 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7351\n",
      "Average Reward for Agent 7 this episode : -11.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2113\n",
      "Average Reward for Agent 8 this episode : -1.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4788\n",
      "Average Reward for Agent 9 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.3604\n",
      "Average Reward for Agent 10 this episode : -9.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 277.5350\n",
      "Average Reward for Agent 11 this episode : -35.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.1480\n",
      "Average Reward for Agent 12 this episode : -3.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7239\n",
      "Average Reward for Agent 13 this episode : -38.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.4522\n",
      "Reducing exploration for all agents to 0.0024\n",
      "Episode 349 is finished\n",
      "Average Reward for Agent 0 this episode : -33.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.5829\n",
      "Average Reward for Agent 1 this episode : -10.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.4243\n",
      "Average Reward for Agent 2 this episode : -50.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.3518\n",
      "Average Reward for Agent 3 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5507\n",
      "Average Reward for Agent 4 this episode : -18.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.4105\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0766\n",
      "Average Reward for Agent 6 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1517\n",
      "Average Reward for Agent 7 this episode : -10.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3821\n",
      "Average Reward for Agent 8 this episode : -1.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0613\n",
      "Average Reward for Agent 9 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.7738\n",
      "Average Reward for Agent 10 this episode : -45.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 211.2412\n",
      "Average Reward for Agent 11 this episode : -6.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 194.0010\n",
      "Average Reward for Agent 12 this episode : -5.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8899\n",
      "Average Reward for Agent 13 this episode : -6.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8044\n",
      "Reducing exploration for all agents to 0.0024\n",
      "Episode 350 is finished\n",
      "Average Reward for Agent 0 this episode : -17.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.9521\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -13.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9011\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -46.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.0909\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -1.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0723\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -22.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.2577\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4083\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0169\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3048\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7967\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.8057\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -45.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 220.4250\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -9.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.3866\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8209\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -14.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8379\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0023\n",
      "Episode 351 is finished\n",
      "Average Reward for Agent 0 this episode : -58.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 202.9419\n",
      "Average Reward for Agent 1 this episode : -8.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5515\n",
      "Average Reward for Agent 2 this episode : -53.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.9428\n",
      "Average Reward for Agent 3 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6814\n",
      "Average Reward for Agent 4 this episode : -16.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2770\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9802\n",
      "Average Reward for Agent 6 this episode : -1.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4700\n",
      "Average Reward for Agent 7 this episode : -13.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9716\n",
      "Average Reward for Agent 8 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1291\n",
      "Average Reward for Agent 9 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.4597\n",
      "Average Reward for Agent 10 this episode : -59.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 228.1279\n",
      "Average Reward for Agent 11 this episode : -9.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.2109\n",
      "Average Reward for Agent 12 this episode : -5.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.7695\n",
      "Average Reward for Agent 13 this episode : -3.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.2466\n",
      "Reducing exploration for all agents to 0.0023\n",
      "Episode 352 is finished\n",
      "Average Reward for Agent 0 this episode : -12.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0276\n",
      "Average Reward for Agent 1 this episode : -13.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4965\n",
      "Average Reward for Agent 2 this episode : -45.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9619\n",
      "Average Reward for Agent 3 this episode : -4.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2682\n",
      "Average Reward for Agent 4 this episode : -17.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.5848\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4814\n",
      "Average Reward for Agent 6 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7873\n",
      "Average Reward for Agent 7 this episode : -13.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9588\n",
      "Average Reward for Agent 8 this episode : -1.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4537\n",
      "Average Reward for Agent 9 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.0760\n",
      "Average Reward for Agent 10 this episode : -48.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.4806\n",
      "Average Reward for Agent 11 this episode : -8.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.6453\n",
      "Average Reward for Agent 12 this episode : -2.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6926\n",
      "Average Reward for Agent 13 this episode : -14.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.8350\n",
      "Reducing exploration for all agents to 0.0023\n",
      "Episode 353 is finished\n",
      "Average Reward for Agent 0 this episode : -43.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -9.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0857\n",
      "Average Reward for Agent 2 this episode : -52.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.0227\n",
      "Average Reward for Agent 3 this episode : -3.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9014\n",
      "Average Reward for Agent 4 this episode : -21.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6269\n",
      "Average Reward for Agent 5 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0868\n",
      "Average Reward for Agent 6 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3977\n",
      "Average Reward for Agent 7 this episode : -12.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1747\n",
      "Average Reward for Agent 8 this episode : -1.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3078\n",
      "Average Reward for Agent 9 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.1390\n",
      "Average Reward for Agent 10 this episode : -32.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.7808\n",
      "Average Reward for Agent 11 this episode : -6.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.8948\n",
      "Average Reward for Agent 12 this episode : -6.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2207\n",
      "Average Reward for Agent 13 this episode : -14.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.9190\n",
      "Reducing exploration for all agents to 0.0022\n",
      "Episode 354 is finished\n",
      "Average Reward for Agent 0 this episode : -45.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.1952\n",
      "Average Reward for Agent 1 this episode : -9.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.4715\n",
      "Average Reward for Agent 2 this episode : -51.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.4396\n",
      "Average Reward for Agent 3 this episode : -3.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1844\n",
      "Average Reward for Agent 4 this episode : -15.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0781\n",
      "Average Reward for Agent 5 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6465\n",
      "Average Reward for Agent 6 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1687\n",
      "Average Reward for Agent 7 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3699\n",
      "Average Reward for Agent 8 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4438\n",
      "Average Reward for Agent 9 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.5455\n",
      "Average Reward for Agent 10 this episode : -11.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 145.2071\n",
      "Average Reward for Agent 11 this episode : -3.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.7514\n",
      "Average Reward for Agent 12 this episode : -5.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1138\n",
      "Average Reward for Agent 13 this episode : -10.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.8466\n",
      "Reducing exploration for all agents to 0.0022\n",
      "Episode 355 is finished\n",
      "Average Reward for Agent 0 this episode : -17.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9248\n",
      "Average Reward for Agent 1 this episode : -13.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3700\n",
      "Average Reward for Agent 2 this episode : -50.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.1077\n",
      "Average Reward for Agent 3 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2168\n",
      "Average Reward for Agent 4 this episode : -20.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0140\n",
      "Average Reward for Agent 5 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9986\n",
      "Average Reward for Agent 6 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4972\n",
      "Average Reward for Agent 7 this episode : -11.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7591\n",
      "Average Reward for Agent 8 this episode : -1.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0389\n",
      "Average Reward for Agent 9 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.1188\n",
      "Average Reward for Agent 10 this episode : -12.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 375.0826\n",
      "Average Reward for Agent 11 this episode : -3.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 332.0146\n",
      "Average Reward for Agent 12 this episode : -6.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0750\n",
      "Average Reward for Agent 13 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4106\n",
      "Reducing exploration for all agents to 0.0021\n",
      "Episode 356 is finished\n",
      "Average Reward for Agent 0 this episode : -38.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.2430\n",
      "Average Reward for Agent 1 this episode : -13.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5202\n",
      "Average Reward for Agent 2 this episode : -47.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.3347\n",
      "Average Reward for Agent 3 this episode : -4.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5733\n",
      "Average Reward for Agent 4 this episode : -18.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.4379\n",
      "Average Reward for Agent 5 this episode : -0.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1510\n",
      "Average Reward for Agent 6 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8773\n",
      "Average Reward for Agent 7 this episode : -13.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2416\n",
      "Average Reward for Agent 8 this episode : -1.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5690\n",
      "Average Reward for Agent 9 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.3549\n",
      "Average Reward for Agent 10 this episode : -25.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.4846\n",
      "Average Reward for Agent 11 this episode : -3.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.9115\n",
      "Average Reward for Agent 12 this episode : -6.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7628\n",
      "Average Reward for Agent 13 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.2972\n",
      "Reducing exploration for all agents to 0.0021\n",
      "Episode 357 is finished\n",
      "Average Reward for Agent 0 this episode : -30.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.3237\n",
      "Average Reward for Agent 1 this episode : -12.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0800\n",
      "Average Reward for Agent 2 this episode : -46.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.2608\n",
      "Average Reward for Agent 3 this episode : -3.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4121\n",
      "Average Reward for Agent 4 this episode : -19.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.3238\n",
      "Average Reward for Agent 5 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2923\n",
      "Average Reward for Agent 6 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3872\n",
      "Average Reward for Agent 7 this episode : -13.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2789\n",
      "Average Reward for Agent 8 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6664\n",
      "Average Reward for Agent 9 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.8474\n",
      "Average Reward for Agent 10 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 496.2447\n",
      "Average Reward for Agent 11 this episode : -5.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.5925\n",
      "Average Reward for Agent 12 this episode : -4.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1958\n",
      "Average Reward for Agent 13 this episode : -16.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.4460\n",
      "Reducing exploration for all agents to 0.0021\n",
      "Episode 358 is finished\n",
      "Average Reward for Agent 0 this episode : -40.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.8398\n",
      "Average Reward for Agent 1 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0641\n",
      "Average Reward for Agent 2 this episode : -50.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2378\n",
      "Average Reward for Agent 3 this episode : -2.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6899\n",
      "Average Reward for Agent 4 this episode : -16.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0951\n",
      "Average Reward for Agent 5 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4433\n",
      "Average Reward for Agent 6 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1598\n",
      "Average Reward for Agent 7 this episode : -13.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1438\n",
      "Average Reward for Agent 8 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1318\n",
      "Average Reward for Agent 9 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.0043\n",
      "Average Reward for Agent 10 this episode : -27.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 591.1052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 11 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 354.6861\n",
      "Average Reward for Agent 12 this episode : -4.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7497\n",
      "Average Reward for Agent 13 this episode : -7.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.4694\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 359 is finished\n",
      "Average Reward for Agent 0 this episode : -38.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.7813\n",
      "Average Reward for Agent 1 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2314\n",
      "Average Reward for Agent 2 this episode : -54.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.0127\n",
      "Average Reward for Agent 3 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5630\n",
      "Average Reward for Agent 4 this episode : -18.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.0885\n",
      "Average Reward for Agent 5 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0756\n",
      "Average Reward for Agent 6 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1049\n",
      "Average Reward for Agent 7 this episode : -11.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0669\n",
      "Average Reward for Agent 8 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6633\n",
      "Average Reward for Agent 9 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0628\n",
      "Average Reward for Agent 10 this episode : -14.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 313.2361\n",
      "Average Reward for Agent 11 this episode : -29.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.8424\n",
      "Average Reward for Agent 12 this episode : -5.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5689\n",
      "Average Reward for Agent 13 this episode : -13.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.1154\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 360 is finished\n",
      "Average Reward for Agent 0 this episode : -40.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.3619\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -11.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9936\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -54.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.7229\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1219\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2375\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3295\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3836\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -11.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4055\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5251\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.1579\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -5.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 419.4131\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -32.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.1554\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7815\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -7.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.6904\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 361 is finished\n",
      "Average Reward for Agent 0 this episode : -40.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.6920\n",
      "Average Reward for Agent 1 this episode : -10.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.7434\n",
      "Average Reward for Agent 2 this episode : -50.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.5685\n",
      "Average Reward for Agent 3 this episode : -1.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6348\n",
      "Average Reward for Agent 4 this episode : -20.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4365\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4341\n",
      "Average Reward for Agent 6 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5530\n",
      "Average Reward for Agent 7 this episode : -10.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9554\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4090\n",
      "Average Reward for Agent 9 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9520\n",
      "Average Reward for Agent 10 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 503.5298\n",
      "Average Reward for Agent 11 this episode : -32.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.7474\n",
      "Average Reward for Agent 12 this episode : -4.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9344\n",
      "Average Reward for Agent 13 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0904\n",
      "Reducing exploration for all agents to 0.0019\n",
      "Episode 362 is finished\n",
      "Average Reward for Agent 0 this episode : -37.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.8000\n",
      "Average Reward for Agent 1 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1122\n",
      "Average Reward for Agent 2 this episode : -51.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.6673\n",
      "Average Reward for Agent 3 this episode : -3.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2008\n",
      "Average Reward for Agent 4 this episode : -20.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.9051\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4069\n",
      "Average Reward for Agent 6 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7031\n",
      "Average Reward for Agent 7 this episode : -8.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5581\n",
      "Average Reward for Agent 8 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8889\n",
      "Average Reward for Agent 9 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.9916\n",
      "Average Reward for Agent 10 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.7114\n",
      "Average Reward for Agent 11 this episode : -38.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 247.4493\n",
      "Average Reward for Agent 12 this episode : -3.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3566\n",
      "Average Reward for Agent 13 this episode : -9.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.8736\n",
      "Reducing exploration for all agents to 0.0019\n",
      "Episode 363 is finished\n",
      "Average Reward for Agent 0 this episode : -57.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.8691\n",
      "Average Reward for Agent 1 this episode : -8.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.0743\n",
      "Average Reward for Agent 2 this episode : -47.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9481\n",
      "Average Reward for Agent 3 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6461\n",
      "Average Reward for Agent 4 this episode : -16.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2141\n",
      "Average Reward for Agent 5 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0569\n",
      "Average Reward for Agent 6 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6474\n",
      "Average Reward for Agent 7 this episode : -11.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3284\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0405\n",
      "Average Reward for Agent 9 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1929\n",
      "Average Reward for Agent 10 this episode : -7.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.3666\n",
      "Average Reward for Agent 11 this episode : -35.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.1316\n",
      "Average Reward for Agent 12 this episode : -4.96\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 0s - loss: 2.5395\n",
      "Average Reward for Agent 13 this episode : -13.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0575\n",
      "Reducing exploration for all agents to 0.0019\n",
      "Episode 364 is finished\n",
      "Average Reward for Agent 0 this episode : -63.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.8124\n",
      "Average Reward for Agent 1 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.8051\n",
      "Average Reward for Agent 2 this episode : -48.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.4533\n",
      "Average Reward for Agent 3 this episode : -1.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5701\n",
      "Average Reward for Agent 4 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3543\n",
      "Average Reward for Agent 5 this episode : -0.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4038\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0103\n",
      "Average Reward for Agent 7 this episode : -12.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4869\n",
      "Average Reward for Agent 8 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8536\n",
      "Average Reward for Agent 9 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.8542\n",
      "Average Reward for Agent 10 this episode : -6.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.1196\n",
      "Average Reward for Agent 11 this episode : -37.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 197.7617\n",
      "Average Reward for Agent 12 this episode : -4.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9932\n",
      "Average Reward for Agent 13 this episode : -11.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.0160\n",
      "Reducing exploration for all agents to 0.0018\n",
      "Episode 365 is finished\n",
      "Average Reward for Agent 0 this episode : -32.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.1588\n",
      "Average Reward for Agent 1 this episode : -12.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.8593\n",
      "Average Reward for Agent 2 this episode : -49.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.2536\n",
      "Average Reward for Agent 3 this episode : -2.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0284\n",
      "Average Reward for Agent 4 this episode : -16.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3046\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4405\n",
      "Average Reward for Agent 6 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5129\n",
      "Average Reward for Agent 7 this episode : -14.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0681\n",
      "Average Reward for Agent 8 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6068\n",
      "Average Reward for Agent 9 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.1254\n",
      "Average Reward for Agent 10 this episode : -7.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.7295\n",
      "Average Reward for Agent 11 this episode : -35.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 208.3745\n",
      "Average Reward for Agent 12 this episode : -4.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9584\n",
      "Average Reward for Agent 13 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4256\n",
      "Reducing exploration for all agents to 0.0018\n",
      "Episode 366 is finished\n",
      "Average Reward for Agent 0 this episode : -17.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.8426\n",
      "Average Reward for Agent 1 this episode : -15.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.1681\n",
      "Average Reward for Agent 2 this episode : -44.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7140\n",
      "Average Reward for Agent 3 this episode : -2.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0481\n",
      "Average Reward for Agent 4 this episode : -19.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9244\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3666\n",
      "Average Reward for Agent 6 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.6828\n",
      "Average Reward for Agent 7 this episode : -14.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6280\n",
      "Average Reward for Agent 8 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7039\n",
      "Average Reward for Agent 9 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3515\n",
      "Average Reward for Agent 10 this episode : -2.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.4622\n",
      "Average Reward for Agent 11 this episode : -30.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.4897\n",
      "Average Reward for Agent 12 this episode : -4.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6124\n",
      "Average Reward for Agent 13 this episode : -13.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.1230\n",
      "Reducing exploration for all agents to 0.0018\n",
      "Episode 367 is finished\n",
      "Average Reward for Agent 0 this episode : -34.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.9731\n",
      "Average Reward for Agent 1 this episode : -15.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.7819\n",
      "Average Reward for Agent 2 this episode : -49.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.8139\n",
      "Average Reward for Agent 3 this episode : -4.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8931\n",
      "Average Reward for Agent 4 this episode : -19.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8519\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0358\n",
      "Average Reward for Agent 6 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0930\n",
      "Average Reward for Agent 7 this episode : -13.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8954\n",
      "Average Reward for Agent 8 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0710\n",
      "Average Reward for Agent 9 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1857\n",
      "Average Reward for Agent 10 this episode : -3.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.1106\n",
      "Average Reward for Agent 11 this episode : -33.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.4917\n",
      "Average Reward for Agent 12 this episode : -4.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9745\n",
      "Average Reward for Agent 13 this episode : -10.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9156\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 368 is finished\n",
      "Average Reward for Agent 0 this episode : -46.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4230\n",
      "Average Reward for Agent 1 this episode : -11.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2524\n",
      "Average Reward for Agent 2 this episode : -46.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6055\n",
      "Average Reward for Agent 3 this episode : -2.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7980\n",
      "Average Reward for Agent 4 this episode : -15.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3007\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0485\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1598\n",
      "Average Reward for Agent 7 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6439\n",
      "Average Reward for Agent 8 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2611\n",
      "Average Reward for Agent 9 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.3734\n",
      "Average Reward for Agent 10 this episode : -12.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 138.6330\n",
      "Average Reward for Agent 11 this episode : -29.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.2864\n",
      "Average Reward for Agent 12 this episode : -4.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3514\n",
      "Average Reward for Agent 13 this episode : -48.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.0517\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 369 is finished\n",
      "Average Reward for Agent 0 this episode : -39.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.1832\n",
      "Average Reward for Agent 1 this episode : -6.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.2068\n",
      "Average Reward for Agent 2 this episode : -42.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.8761\n",
      "Average Reward for Agent 3 this episode : -6.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2356\n",
      "Average Reward for Agent 4 this episode : -16.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4641\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7517\n",
      "Average Reward for Agent 6 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5301\n",
      "Average Reward for Agent 7 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -1.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5082\n",
      "Average Reward for Agent 9 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.0898\n",
      "Average Reward for Agent 10 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.1566\n",
      "Average Reward for Agent 11 this episode : -32.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.6363\n",
      "Average Reward for Agent 12 this episode : -5.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3032\n",
      "Average Reward for Agent 13 this episode : -40.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.4130\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 370 is finished\n",
      "Average Reward for Agent 0 this episode : -31.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.2573\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1808\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -41.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2346\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4255\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -15.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2916\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4974\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7332\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9904\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0224\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.2230\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -6.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 230.1795\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -36.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.1361\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -4.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5647\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -47.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.6861\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 371 is finished\n",
      "Average Reward for Agent 0 this episode : -36.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.9824\n",
      "Average Reward for Agent 1 this episode : -8.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1237\n",
      "Average Reward for Agent 2 this episode : -45.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.3912\n",
      "Average Reward for Agent 3 this episode : -7.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5024\n",
      "Average Reward for Agent 4 this episode : -15.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7594\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7445\n",
      "Average Reward for Agent 6 this episode : -0.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1478\n",
      "Average Reward for Agent 7 this episode : -11.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4450\n",
      "Average Reward for Agent 8 this episode : -0.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2971\n",
      "Average Reward for Agent 9 this episode : -1.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.5475\n",
      "Average Reward for Agent 10 this episode : -4.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 244.1625\n",
      "Average Reward for Agent 11 this episode : -35.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.4022\n",
      "Average Reward for Agent 12 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7240\n",
      "Average Reward for Agent 13 this episode : -48.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.9091\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 372 is finished\n",
      "Average Reward for Agent 0 this episode : -36.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.5057\n",
      "Average Reward for Agent 1 this episode : -14.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.4538\n",
      "Average Reward for Agent 2 this episode : -47.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.3131\n",
      "Average Reward for Agent 3 this episode : -5.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5393\n",
      "Average Reward for Agent 4 this episode : -15.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6237\n",
      "Average Reward for Agent 5 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4175\n",
      "Average Reward for Agent 6 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8789\n",
      "Average Reward for Agent 7 this episode : -12.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8529\n",
      "Average Reward for Agent 8 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9306\n",
      "Average Reward for Agent 9 this episode : -1.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.4296\n",
      "Average Reward for Agent 10 this episode : -5.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.7860\n",
      "Average Reward for Agent 11 this episode : -35.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 393.2792\n",
      "Average Reward for Agent 12 this episode : -6.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6865\n",
      "Average Reward for Agent 13 this episode : -35.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.6055\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 373 is finished\n",
      "Average Reward for Agent 0 this episode : -35.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.5515\n",
      "Average Reward for Agent 1 this episode : -14.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2255\n",
      "Average Reward for Agent 2 this episode : -29.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9182\n",
      "Average Reward for Agent 3 this episode : -7.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9186\n",
      "Average Reward for Agent 4 this episode : -19.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2949\n",
      "Average Reward for Agent 5 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1170\n",
      "Average Reward for Agent 6 this episode : -1.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3981\n",
      "Average Reward for Agent 7 this episode : -17.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1999\n",
      "Average Reward for Agent 8 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5597\n",
      "Average Reward for Agent 9 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.8112\n",
      "Average Reward for Agent 10 this episode : -4.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 189.6779\n",
      "Average Reward for Agent 11 this episode : -34.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.9095\n",
      "Average Reward for Agent 12 this episode : -7.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0105\n",
      "Average Reward for Agent 13 this episode : -51.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.5562\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 374 is finished\n",
      "Average Reward for Agent 0 this episode : -37.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2892\n",
      "Average Reward for Agent 1 this episode : -11.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4167\n",
      "Average Reward for Agent 2 this episode : -40.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.7871\n",
      "Average Reward for Agent 3 this episode : -6.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9283\n",
      "Average Reward for Agent 4 this episode : -18.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6379\n",
      "Average Reward for Agent 5 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8987\n",
      "Average Reward for Agent 6 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.6704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 7 this episode : -10.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6130\n",
      "Average Reward for Agent 8 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5160\n",
      "Average Reward for Agent 9 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.0164\n",
      "Average Reward for Agent 10 this episode : -2.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.1833\n",
      "Average Reward for Agent 11 this episode : -33.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 261.0465\n",
      "Average Reward for Agent 12 this episode : -5.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0881\n",
      "Average Reward for Agent 13 this episode : -53.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.9935\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 375 is finished\n",
      "Average Reward for Agent 0 this episode : -41.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.7902\n",
      "Average Reward for Agent 1 this episode : -10.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6513\n",
      "Average Reward for Agent 2 this episode : -46.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.5100\n",
      "Average Reward for Agent 3 this episode : -4.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9086\n",
      "Average Reward for Agent 4 this episode : -17.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3255\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6776\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6327\n",
      "Average Reward for Agent 7 this episode : -13.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9397\n",
      "Average Reward for Agent 8 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3547\n",
      "Average Reward for Agent 9 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.0780\n",
      "Average Reward for Agent 10 this episode : -6.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.9789\n",
      "Average Reward for Agent 11 this episode : -42.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.7399\n",
      "Average Reward for Agent 12 this episode : -3.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2774\n",
      "Average Reward for Agent 13 this episode : -52.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.1061\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 376 is finished\n",
      "Average Reward for Agent 0 this episode : -43.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.0524\n",
      "Average Reward for Agent 1 this episode : -12.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5908\n",
      "Average Reward for Agent 2 this episode : -43.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.5283\n",
      "Average Reward for Agent 3 this episode : -1.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9301\n",
      "Average Reward for Agent 4 this episode : -18.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7026\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3184\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9779\n",
      "Average Reward for Agent 7 this episode : -13.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1925\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5977\n",
      "Average Reward for Agent 9 this episode : -3.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.2208\n",
      "Average Reward for Agent 10 this episode : -11.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.8744\n",
      "Average Reward for Agent 11 this episode : -36.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 180.4889\n",
      "Average Reward for Agent 12 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4825\n",
      "Average Reward for Agent 13 this episode : -45.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.9765\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 377 is finished\n",
      "Average Reward for Agent 0 this episode : -44.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.3900\n",
      "Average Reward for Agent 1 this episode : -14.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8256\n",
      "Average Reward for Agent 2 this episode : -33.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 249.1891\n",
      "Average Reward for Agent 3 this episode : -3.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0666\n",
      "Average Reward for Agent 4 this episode : -20.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3397\n",
      "Average Reward for Agent 5 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4221\n",
      "Average Reward for Agent 6 this episode : -2.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3228\n",
      "Average Reward for Agent 7 this episode : -15.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2107\n",
      "Average Reward for Agent 8 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0337\n",
      "Average Reward for Agent 9 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.7718\n",
      "Average Reward for Agent 10 this episode : -5.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 202.2209\n",
      "Average Reward for Agent 11 this episode : -35.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.4141\n",
      "Average Reward for Agent 12 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3657\n",
      "Average Reward for Agent 13 this episode : -56.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.7003\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 378 is finished\n",
      "Average Reward for Agent 0 this episode : -43.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1068\n",
      "Average Reward for Agent 1 this episode : -13.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5458\n",
      "Average Reward for Agent 2 this episode : -42.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.5036\n",
      "Average Reward for Agent 3 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4923\n",
      "Average Reward for Agent 4 this episode : -18.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.6205\n",
      "Average Reward for Agent 5 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3932\n",
      "Average Reward for Agent 6 this episode : -1.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8069\n",
      "Average Reward for Agent 7 this episode : -12.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.5805\n",
      "Average Reward for Agent 8 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8820\n",
      "Average Reward for Agent 9 this episode : -3.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5158\n",
      "Average Reward for Agent 10 this episode : -6.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 181.6543\n",
      "Average Reward for Agent 11 this episode : -28.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.4960\n",
      "Average Reward for Agent 12 this episode : -4.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0615\n",
      "Average Reward for Agent 13 this episode : -47.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.8517\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 379 is finished\n",
      "Average Reward for Agent 0 this episode : -44.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8536\n",
      "Average Reward for Agent 1 this episode : -2.5\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9666\n",
      "Average Reward for Agent 2 this episode : -44.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.4491\n",
      "Average Reward for Agent 3 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7634\n",
      "Average Reward for Agent 4 this episode : -16.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1313\n",
      "Average Reward for Agent 5 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0437\n",
      "Average Reward for Agent 6 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.1280\n",
      "Average Reward for Agent 7 this episode : -12.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4054\n",
      "Average Reward for Agent 8 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8988\n",
      "Average Reward for Agent 9 this episode : -4.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.6121\n",
      "Average Reward for Agent 10 this episode : -8.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 145.3011\n",
      "Average Reward for Agent 11 this episode : -37.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.9173\n",
      "Average Reward for Agent 12 this episode : -4.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3255\n",
      "Average Reward for Agent 13 this episode : -52.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.6533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 380 is finished\n",
      "Average Reward for Agent 0 this episode : -41.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9880\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -9.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1378\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -39.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.5188\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3454\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5898\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0535\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.3092\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -8.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9523\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6326\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.7436\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -5.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 189.0476\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -35.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.3051\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -5.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8083\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -51.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.5795\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 381 is finished\n",
      "Average Reward for Agent 0 this episode : -37.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.5394\n",
      "Average Reward for Agent 1 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2833\n",
      "Average Reward for Agent 2 this episode : -41.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6197\n",
      "Average Reward for Agent 3 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7955\n",
      "Average Reward for Agent 4 this episode : -14.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7046\n",
      "Average Reward for Agent 5 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3823\n",
      "Average Reward for Agent 6 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0156\n",
      "Average Reward for Agent 7 this episode : -11.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3141\n",
      "Average Reward for Agent 8 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8253\n",
      "Average Reward for Agent 9 this episode : -4.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7938\n",
      "Average Reward for Agent 10 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.1217\n",
      "Average Reward for Agent 11 this episode : -33.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.6146\n",
      "Average Reward for Agent 12 this episode : -3.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0372\n",
      "Average Reward for Agent 13 this episode : -51.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.1862\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 382 is finished\n",
      "Average Reward for Agent 0 this episode : -36.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 242.8042\n",
      "Average Reward for Agent 1 this episode : -12.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5093\n",
      "Average Reward for Agent 2 this episode : -48.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7407\n",
      "Average Reward for Agent 3 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0797\n",
      "Average Reward for Agent 4 this episode : -15.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0307\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7759\n",
      "Average Reward for Agent 6 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8250\n",
      "Average Reward for Agent 7 this episode : -13.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7745\n",
      "Average Reward for Agent 8 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2011\n",
      "Average Reward for Agent 9 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3148\n",
      "Average Reward for Agent 10 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.3525\n",
      "Average Reward for Agent 11 this episode : -12.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.2238\n",
      "Average Reward for Agent 12 this episode : -5.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2778\n",
      "Average Reward for Agent 13 this episode : -46.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.5758\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 383 is finished\n",
      "Average Reward for Agent 0 this episode : -35.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0356\n",
      "Average Reward for Agent 1 this episode : -12.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7270\n",
      "Average Reward for Agent 2 this episode : -35.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.6615\n",
      "Average Reward for Agent 3 this episode : -4.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9837\n",
      "Average Reward for Agent 4 this episode : -15.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0247\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0939\n",
      "Average Reward for Agent 6 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8827\n",
      "Average Reward for Agent 7 this episode : -13.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9602\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0516\n",
      "Average Reward for Agent 9 this episode : -4.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 202.7490\n",
      "Average Reward for Agent 10 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 171.0812\n",
      "Average Reward for Agent 11 this episode : -9.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 363.8252\n",
      "Average Reward for Agent 12 this episode : -8.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5501\n",
      "Average Reward for Agent 13 this episode : -50.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.5198\n",
      "Reducing exploration for all agents to 0.0013\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147023170, 'Échec de l’appel de procédure distante.', None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4bceb6a6005c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBalance_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\MasterDQN_Agent.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, number_of_episode)\u001b[0m\n\u001b[0;32m     89\u001b[0m                                 \u001b[1;31m# For the saving , monitoring of the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandom_Seed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_episode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;31m# Simulate one step and give the control to COM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147023170, 'Échec de l’appel de procédure distante.', None, None)"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-1\n",
      "Dumping agent-1 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-3\n",
      "Dumping agent-3 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-4\n",
      "Dumping agent-4 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-5\n",
      "Dumping agent-5 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-6\n",
      "Dumping agent-6 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-7\n",
      "Dumping agent-7 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-8\n",
      "Dumping agent-8 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-9\n",
      "Dumping agent-9 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-10\n",
      "Dumping agent-10 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-11\n",
      "Dumping agent-11 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-12\n",
      "Dumping agent-12 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 1, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 2, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 3, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 4, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 5, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 6, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 7, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 8, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 9, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 10, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 11, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 12, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 13, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents.load(best = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single_Cross_Straight AC\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Straigth_AC\"\n",
    "\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 0.5\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  288       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  2352      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2352      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  49        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  288       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  98        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 10,131\n",
      "Trainable params: 10,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes, Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.09 seconds.\n",
      "\n",
      "Episode 1 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -12.0, -9.0, -10.0, -12.0, -2.0, -21.0, -10.0, -3.0, -8.0] \n",
      " [-95.0, -163.0, -110.0, -94.0, -222.0, -176.0, -138.0, -94.0, -208.0, -94.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.8, 0.2], [0.99, 0.01], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.78, 0.22], [0.0, 1.0], [1.0, 0.0], [0.04, 0.96], [0.1, 0.9]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.47 0.53]\n",
      "Average Reward for Agent 0 this episode : -23.44\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_AC\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 2 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-27.0, -37.0, -2.0, -5.0, -7.0, -20.0, -20.0, -59.0, -22.0, -2.0] \n",
      " [-147.0, -92.0, -123.0, -98.0, -115.0, -79.0, -152.0, -288.0, -116.0, -120.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [0.54, 0.46], [0.49, 0.51], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.46, 0.54]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.38 0.62]\n",
      "Average Reward for Agent 0 this episode : -22.22\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_AC\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 3 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-130.0, -64.0, -4.0, -54.0, -17.0, -20.0, -53.0, -2.0, -29.0, -12.0] \n",
      " [-181.0, -101.0, -107.0, -138.0, -101.0, -147.0, -109.0, -56.0, -187.0, -72.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.7, 0.3], [0.0, 1.0], [0.0, 1.0], [0.55, 0.45], [0.81, 0.19], [0.23, 0.77], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.23 0.77]\n",
      "Average Reward for Agent 0 this episode : -20.72\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_AC\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 4 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -7.0, -7.0, -7.0, -7.0, -56.0, -136.0, -19.0, -162.0, -3.0] \n",
      " [-88.0, -138.0, -83.0, -143.0, -147.0, -143.0, -158.0, -200.0, -155.0, -81.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.07, 0.93], [0.74, 0.26], [0.74, 0.26], [0.74, 0.26], [0.74, 0.26], [1.0, 0.0], [0.0, 1.0], [0.02, 0.98], [0.0, 1.0], [0.07, 0.93]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.07 0.93]\n",
      "Average Reward for Agent 0 this episode : -20.98\n",
      "Episode 5 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-74.0, -105.0, -63.0, -88.0, -62.0, -59.0, -41.0, -102.0, -96.0, -4.0] \n",
      " [-137.0, -202.0, -111.0, -98.0, -192.0, -133.0, -147.0, -126.0, -103.0, -202.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.01 0.99]\n",
      "Average Reward for Agent 0 this episode : -20.2\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_AC\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 6 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-60.0, -129.0, -26.0, -108.0, -143.0, -35.0, -70.0, -108.0, -72.0, -35.0] \n",
      " [-96.0, -111.0, -209.0, -98.0, -158.0, -162.0, -137.0, -141.0, -193.0, -148.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.47\n",
      "Episode 7 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-103.0, -22.0, -133.0, -90.0, -13.0, -115.0, -230.0, -83.0, -77.0, -118.0] \n",
      " [-151.0, -142.0, -228.0, -314.0, -131.0, -170.0, -227.0, -89.0, -146.0, -261.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.97, 0.03], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -22.57\n",
      "Episode 8 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7.0, -14.0, -27.0, -47.0, -37.0, -86.0, -89.0, -70.0, -77.0, -82.0] \n",
      " [-125.0, -84.0, -185.0, -153.0, -98.0, -187.0, -150.0, -132.0, -174.0, -112.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.99, 0.01], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -20.55\n",
      "Episode 9 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-178.0, -66.0, -38.0, -24.0, -78.0, -8.0, -22.0, -8.0, -138.0, -48.0] \n",
      " [-73.0, -107.0, -138.0, -172.0, -64.0, -207.0, -128.0, -144.0, -63.0, -156.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.85\n",
      "Episode 10 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-47.0, -127.0, -40.0, -400.0, -18.0, -44.0, -175.0, -37.0, -19.0, -29.0] \n",
      " [-75.0, -275.0, -60.0, -102.0, -72.0, -138.0, -109.0, -201.0, -77.0, -89.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.94, 0.06], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -20.48\n",
      "Episode 11 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-39.0, -15.0, -66.0, -10.0, -24.0, -15.0, -106.0, -10.0, -38.0, -44.0] \n",
      " [-68.0, -129.0, -175.0, -126.0, -139.0, -87.0, -121.0, -114.0, -148.0, -124.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -17.86\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_AC\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 12 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-69.0, -58.0, -75.0, -22.0, -96.0, -115.0, -82.0, -98.0, -79.0, -90.0] \n",
      " [-202.0, -219.0, -119.0, -146.0, -220.0, -418.0, -231.0, -112.0, -110.0, -121.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -20.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-77.0, -28.0, -158.0, -163.0, -167.0, -28.0, -55.0, -137.0, -100.0, -100.0] \n",
      " [-129.0, -174.0, -157.0, -91.0, -226.0, -174.0, -137.0, -141.0, -134.0, -134.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.21, 0.79], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.21, 0.79], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.85\n",
      "Episode 14 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-76.0, -75.0, -54.0, -29.0, -160.0, -64.0, -128.0, -132.0, -141.0, -143.0] \n",
      " [-111.0, -158.0, -138.0, -70.0, -196.0, -104.0, -129.0, -115.0, -120.0, -161.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -20.09\n",
      "Episode 15 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-72.0, -152.0, -17.0, -238.0, -58.0, -50.0, -85.0, -52.0, -91.0, -100.0] \n",
      " [-143.0, -186.0, -78.0, -234.0, -261.0, -159.0, -105.0, -75.0, -159.0, -138.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.74\n",
      "Episode 16 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-39.0, -61.0, -60.0, -167.0, -152.0, -58.0, -123.0, -102.0, -89.0, -210.0] \n",
      " [-224.0, -122.0, -61.0, -117.0, -129.0, -136.0, -145.0, -102.0, -172.0, -182.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.69\n",
      "Episode 17 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-101.0, -134.0, -113.0, -22.0, -25.0, -68.0, -78.0, -161.0, -38.0, -49.0] \n",
      " [-165.0, -223.0, -141.0, -184.0, -53.0, -144.0, -66.0, -138.0, -131.0, -153.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.33\n",
      "Episode 18 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-143.0, -196.0, -76.0, -64.0, -102.0, -189.0, -205.0, -32.0, -69.0, -55.0] \n",
      " [-195.0, -155.0, -151.0, -162.0, -214.0, -119.0, -148.0, -147.0, -126.0, -270.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -23.95\n",
      "Episode 19 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-66.0, -95.0, -126.0, -80.0, -127.0, -30.0, -74.0, -46.0, -89.0, -81.0] \n",
      " [-163.0, -167.0, -116.0, -114.0, -261.0, -178.0, -141.0, -107.0, -109.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.56\n",
      "Episode 20 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-150.0, -47.0, -39.0, -66.0, -124.0, -51.0, -192.0, -124.0, -94.0, -72.0] \n",
      " [-287.0, -57.0, -108.0, -130.0, -183.0, -111.0, -219.0, -183.0, -123.0, -95.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.25\n",
      "Episode 21 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-52.0, -60.0, -53.0, -198.0, -272.0, -54.0, -50.0, -33.0, -125.0, -226.0] \n",
      " [-150.0, -178.0, -85.0, -162.0, -190.0, -75.0, -198.0, -94.0, -175.0, -240.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.23\n",
      "Episode 22 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-143.0, -68.0, -72.0, -45.0, -38.0, -88.0, -74.0, -88.0, -68.0, -115.0] \n",
      " [-232.0, -160.0, -89.0, -195.0, -98.0, -161.0, -93.0, -94.0, -91.0, -185.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -19.26\n",
      "Episode 23 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-70.0, -64.0, -64.0, -76.0, -111.0, -151.0, -82.0, -141.0, -79.0, -123.0] \n",
      " [-124.0, -73.0, -59.0, -123.0, -156.0, -176.0, -113.0, -132.0, -71.0, -63.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -19.62\n",
      "Episode 24 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-52.0, -180.0, -68.0, -116.0, -80.0, -124.0, -153.0, -89.0, -105.0, -47.0] \n",
      " [-106.0, -103.0, -274.0, -146.0, -109.0, -184.0, -126.0, -183.0, -168.0, -91.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -21.89\n",
      "Episode 25 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-82.0, -98.0, -116.0, -72.0, -119.0, -136.0, -81.0, -265.0, -83.0, -71.0] \n",
      " [-127.0, -160.0, -166.0, -147.0, -153.0, -115.0, -95.0, -178.0, -98.0, -188.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -20.48\n",
      "Episode 26 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-148.0, -94.0, -95.0, -148.0, -136.0, -182.0, -78.0, -168.0, -97.0, -94.0] \n",
      " [-266.0, -125.0, -141.0, -161.0, -85.0, -147.0, -185.0, -198.0, -94.0, -79.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -23.36\n",
      "Episode 27 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-167.0, -70.0, -132.0, -384.0, -80.0, -95.0, -92.0, -143.0, -166.0, -148.0] \n",
      " [-161.0, -116.0, -84.0, -300.0, -129.0, -197.0, -147.0, -184.0, -206.0, -111.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -24.94\n",
      "Episode 28 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-187.0, -96.0, -154.0, -88.0, -88.0, -91.0, -122.0, -253.0, -210.0, -149.0] \n",
      " [-136.0, -88.0, -293.0, -258.0, -76.0, -196.0, -184.0, -310.0, -285.0, -167.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -24.36\n",
      "Episode 29 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-94.0, -162.0, -123.0, -54.0, -297.0, -192.0, -203.0, -93.0, -124.0, -152.0] \n",
      " [-116.0, -189.0, -206.0, -208.0, -129.0, -231.0, -296.0, -72.0, -171.0, -79.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 1.]\n",
      "Average Reward for Agent 0 this episode : -24.31\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.train(200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  384       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  252       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,267\n",
      "Trainable params: 11,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.save(401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.load(200, best = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay = Single_Cross_Straight_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        \n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Queue Length')\n",
    "    plt.title('Junction {} Queue length'.format(idx))\n",
    "    plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Accumulated Delay')\n",
    "    plt.title('Junction {} Delay'.format(idx))\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Accumulated Stop Delay')\n",
    "    plt.title('Junction {} Stop Delay'.format(idx))\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.png.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Global accumulated Delay')\n",
    "plt.title('Global accumulated Delay')\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Global accumulated stop Delay')\n",
    "plt.title('Global accumulated stop Delay')\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in Single_Cross_Straight_MultiAC_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Agent{} Loss over training'.format(idx))\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.title('Agent{} average reward over training'.format(idx))\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_averag_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "plt.figure(6)\n",
    "plt.plot(Single_Cross_Straight_MultiAC_Agents.Agents[0].reward_storage)\n",
    "\n",
    "#print(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "#print(np.min(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].reward_storage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-9369e0ec6a8e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-9369e0ec6a8e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    plt.figure(1'patate')\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1561252.0, -1.3989514e-20, 1561252.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.Agents[0].losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes, Inputs.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.25 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Single_Cross_Straigth_DQN\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig' ,\n",
    "         'reward_type' : 'Queues' }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400]}\n",
    "}\n",
    "\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 300\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 48)           288         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48)           2352        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            49          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            98          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_5[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,491\n",
      "Trainable params: 7,491\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 0\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3600 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes, Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.1 seconds.\n",
      "\n",
      "After 0 actions taken by the Agents,  Agent 0 memory is 0.0 percent full\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\Agent0_PERPre_1000.p\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes, Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.08 seconds.\n",
      "\n",
      "Episode 1 is finished\n",
      "Average Reward for Agent 0 this episode : -32.92\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1078.2571\n",
      "Reducing exploration for all agents to 0.9772\n",
      "Episode 2 is finished\n",
      "Average Reward for Agent 0 this episode : -35.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 695.5424\n",
      "Reducing exploration for all agents to 0.9548\n",
      "Episode 3 is finished\n",
      "Average Reward for Agent 0 this episode : -35.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 541.3646\n",
      "Reducing exploration for all agents to 0.933\n",
      "Episode 4 is finished\n",
      "Average Reward for Agent 0 this episode : -37.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 420.6490\n",
      "Reducing exploration for all agents to 0.9117\n",
      "Episode 5 is finished\n",
      "Average Reward for Agent 0 this episode : -34.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 346.4799\n",
      "Reducing exploration for all agents to 0.8909\n",
      "Episode 6 is finished\n",
      "Average Reward for Agent 0 this episode : -35.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 487.6104\n",
      "Reducing exploration for all agents to 0.8706\n",
      "Episode 7 is finished\n",
      "Average Reward for Agent 0 this episode : -34.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 554.4237\n",
      "Reducing exploration for all agents to 0.8507\n",
      "Episode 8 is finished\n",
      "Average Reward for Agent 0 this episode : -31.18\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 393.6122\n",
      "Reducing exploration for all agents to 0.8313\n",
      "Episode 9 is finished\n",
      "Average Reward for Agent 0 this episode : -32.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 434.2157\n",
      "Reducing exploration for all agents to 0.8123\n",
      "Episode 10 is finished\n",
      "Average Reward for Agent 0 this episode : -26.3\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 245.3127\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.7937\n",
      "Episode 11 is finished\n",
      "Average Reward for Agent 0 this episode : -25.17\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1007.4068\n",
      "Reducing exploration for all agents to 0.7756\n",
      "Episode 12 is finished\n",
      "Average Reward for Agent 0 this episode : -28.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 584.3115\n",
      "Reducing exploration for all agents to 0.7579\n",
      "Episode 13 is finished\n",
      "Average Reward for Agent 0 this episode : -27.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 722.7305\n",
      "Reducing exploration for all agents to 0.7406\n",
      "Episode 14 is finished\n",
      "Average Reward for Agent 0 this episode : -28.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 507.8999\n",
      "Reducing exploration for all agents to 0.7237\n",
      "Episode 15 is finished\n",
      "Average Reward for Agent 0 this episode : -25.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 851.7153\n",
      "Reducing exploration for all agents to 0.7071\n",
      "Episode 16 is finished\n",
      "Average Reward for Agent 0 this episode : -31.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 545.4640\n",
      "Reducing exploration for all agents to 0.691\n",
      "Episode 17 is finished\n",
      "Average Reward for Agent 0 this episode : -25.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 647.1102\n",
      "Reducing exploration for all agents to 0.6752\n",
      "Episode 18 is finished\n",
      "Average Reward for Agent 0 this episode : -27.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 427.7350\n",
      "Reducing exploration for all agents to 0.6598\n",
      "Episode 19 is finished\n",
      "Average Reward for Agent 0 this episode : -25.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 499.4966\n",
      "Reducing exploration for all agents to 0.6447\n",
      "Episode 20 is finished\n",
      "Average Reward for Agent 0 this episode : -27.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 367.5102\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.63\n",
      "Episode 21 is finished\n",
      "Average Reward for Agent 0 this episode : -28.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 579.9445\n",
      "Reducing exploration for all agents to 0.6156\n",
      "Episode 22 is finished\n",
      "Average Reward for Agent 0 this episode : -24.91\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 597.1199\n",
      "Reducing exploration for all agents to 0.6015\n",
      "Episode 23 is finished\n",
      "Average Reward for Agent 0 this episode : -25.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 578.6542\n",
      "Reducing exploration for all agents to 0.5878\n",
      "Episode 24 is finished\n",
      "Average Reward for Agent 0 this episode : -26.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 579.5203\n",
      "Reducing exploration for all agents to 0.5744\n",
      "Episode 25 is finished\n",
      "Average Reward for Agent 0 this episode : -24.83\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 507.7702\n",
      "Reducing exploration for all agents to 0.5613\n",
      "Episode 26 is finished\n",
      "Average Reward for Agent 0 this episode : -26.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 488.0643\n",
      "Reducing exploration for all agents to 0.5484\n",
      "Episode 27 is finished\n",
      "Average Reward for Agent 0 this episode : -28.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 794.4094\n",
      "Reducing exploration for all agents to 0.5359\n",
      "Episode 28 is finished\n",
      "Average Reward for Agent 0 this episode : -24.5\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 498.6133\n",
      "Reducing exploration for all agents to 0.5237\n",
      "Episode 29 is finished\n",
      "Average Reward for Agent 0 this episode : -28.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 564.7405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.5117\n",
      "Episode 30 is finished\n",
      "Average Reward for Agent 0 this episode : -26.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 582.7361\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.5\n",
      "Episode 31 is finished\n",
      "Average Reward for Agent 0 this episode : -27.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 572.0840\n",
      "Reducing exploration for all agents to 0.4886\n",
      "Episode 32 is finished\n",
      "Average Reward for Agent 0 this episode : -30.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 505.3847\n",
      "Reducing exploration for all agents to 0.4775\n",
      "Episode 33 is finished\n",
      "Average Reward for Agent 0 this episode : -26.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 485.4490\n",
      "Reducing exploration for all agents to 0.4665\n",
      "Episode 34 is finished\n",
      "Average Reward for Agent 0 this episode : -25.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 604.3898\n",
      "Reducing exploration for all agents to 0.4559\n",
      "Episode 35 is finished\n",
      "Average Reward for Agent 0 this episode : -23.92\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 420.9639\n",
      "Reducing exploration for all agents to 0.4455\n",
      "Episode 36 is finished\n",
      "Average Reward for Agent 0 this episode : -21.48\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 380.5412\n",
      "Reducing exploration for all agents to 0.4353\n",
      "Episode 37 is finished\n",
      "Average Reward for Agent 0 this episode : -28.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 354.4582\n",
      "Reducing exploration for all agents to 0.4254\n",
      "Episode 38 is finished\n",
      "Average Reward for Agent 0 this episode : -24.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 616.8478\n",
      "Reducing exploration for all agents to 0.4157\n",
      "Episode 39 is finished\n",
      "Average Reward for Agent 0 this episode : -29.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 448.6802\n",
      "Reducing exploration for all agents to 0.4062\n",
      "Episode 40 is finished\n",
      "Average Reward for Agent 0 this episode : -21.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 442.0861\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.3969\n",
      "Episode 41 is finished\n",
      "Average Reward for Agent 0 this episode : -23.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 323.8404\n",
      "Reducing exploration for all agents to 0.3878\n",
      "Episode 42 is finished\n",
      "Average Reward for Agent 0 this episode : -25.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 580.4258\n",
      "Reducing exploration for all agents to 0.379\n",
      "Episode 43 is finished\n",
      "Average Reward for Agent 0 this episode : -23.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 472.1230\n",
      "Reducing exploration for all agents to 0.3703\n",
      "Episode 44 is finished\n",
      "Average Reward for Agent 0 this episode : -22.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 402.8380\n",
      "Reducing exploration for all agents to 0.3618\n",
      "Episode 45 is finished\n",
      "Average Reward for Agent 0 this episode : -24.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 355.6806\n",
      "Reducing exploration for all agents to 0.3536\n",
      "Episode 46 is finished\n",
      "Average Reward for Agent 0 this episode : -24.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 562.8169\n",
      "Reducing exploration for all agents to 0.3455\n",
      "Episode 47 is finished\n",
      "Average Reward for Agent 0 this episode : -23.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 649.8277\n",
      "Reducing exploration for all agents to 0.3376\n",
      "Episode 48 is finished\n",
      "Average Reward for Agent 0 this episode : -25.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 504.6038\n",
      "Reducing exploration for all agents to 0.3299\n",
      "Episode 49 is finished\n",
      "Average Reward for Agent 0 this episode : -22.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 693.4068\n",
      "Reducing exploration for all agents to 0.3224\n",
      "Episode 50 is finished\n",
      "Average Reward for Agent 0 this episode : -24.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 538.3532\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.315\n",
      "Episode 51 is finished\n",
      "Average Reward for Agent 0 this episode : -23.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 417.8955\n",
      "Reducing exploration for all agents to 0.3078\n",
      "Episode 52 is finished\n",
      "Average Reward for Agent 0 this episode : -26.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 420.3513\n",
      "Reducing exploration for all agents to 0.3008\n",
      "Episode 53 is finished\n",
      "Average Reward for Agent 0 this episode : -25.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 440.7993\n",
      "Reducing exploration for all agents to 0.2939\n",
      "Episode 54 is finished\n",
      "Average Reward for Agent 0 this episode : -26.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 469.5687\n",
      "Reducing exploration for all agents to 0.2872\n",
      "Episode 55 is finished\n",
      "Average Reward for Agent 0 this episode : -23.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 591.7307\n",
      "Reducing exploration for all agents to 0.2806\n",
      "Episode 56 is finished\n",
      "Average Reward for Agent 0 this episode : -28.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 539.4545\n",
      "Reducing exploration for all agents to 0.2742\n",
      "Episode 57 is finished\n",
      "Average Reward for Agent 0 this episode : -23.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 572.9413\n",
      "Reducing exploration for all agents to 0.268\n",
      "Episode 58 is finished\n",
      "Average Reward for Agent 0 this episode : -22.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 526.3245\n",
      "Reducing exploration for all agents to 0.2619\n",
      "Episode 59 is finished\n",
      "Average Reward for Agent 0 this episode : -23.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 363.1420\n",
      "Reducing exploration for all agents to 0.2559\n",
      "Episode 60 is finished\n",
      "Average Reward for Agent 0 this episode : -23.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 551.4807\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.25\n",
      "Episode 61 is finished\n",
      "Average Reward for Agent 0 this episode : -22.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 576.1160\n",
      "Reducing exploration for all agents to 0.2443\n",
      "Episode 62 is finished\n",
      "Average Reward for Agent 0 this episode : -25.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 622.1095\n",
      "Reducing exploration for all agents to 0.2387\n",
      "Episode 63 is finished\n",
      "Average Reward for Agent 0 this episode : -21.31\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 609.0873\n",
      "Reducing exploration for all agents to 0.2333\n",
      "Episode 64 is finished\n",
      "Average Reward for Agent 0 this episode : -23.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 482.7321\n",
      "Reducing exploration for all agents to 0.228\n",
      "Episode 65 is finished\n",
      "Average Reward for Agent 0 this episode : -23.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 403.9666\n",
      "Reducing exploration for all agents to 0.2228\n",
      "Episode 66 is finished\n",
      "Average Reward for Agent 0 this episode : -24.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 472.1717\n",
      "Reducing exploration for all agents to 0.2177\n",
      "Episode 67 is finished\n",
      "Average Reward for Agent 0 this episode : -25.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 366.5697\n",
      "Reducing exploration for all agents to 0.2127\n",
      "Episode 68 is finished\n",
      "Average Reward for Agent 0 this episode : -24.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 654.7297\n",
      "Reducing exploration for all agents to 0.2078\n",
      "Episode 69 is finished\n",
      "Average Reward for Agent 0 this episode : -21.13\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 337.4377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.2031\n",
      "Episode 70 is finished\n",
      "Average Reward for Agent 0 this episode : -20.35\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 479.2543\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.1985\n",
      "Episode 71 is finished\n",
      "Average Reward for Agent 0 this episode : -22.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 360.0592\n",
      "Reducing exploration for all agents to 0.1939\n",
      "Episode 72 is finished\n",
      "Average Reward for Agent 0 this episode : -20.29\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 393.9892\n",
      "Reducing exploration for all agents to 0.1895\n",
      "Episode 73 is finished\n",
      "Average Reward for Agent 0 this episode : -20.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 364.9051\n",
      "Reducing exploration for all agents to 0.1852\n",
      "Episode 74 is finished\n",
      "Average Reward for Agent 0 this episode : -24.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 518.0034\n",
      "Reducing exploration for all agents to 0.1809\n",
      "Episode 75 is finished\n",
      "Average Reward for Agent 0 this episode : -21.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 547.6928\n",
      "Reducing exploration for all agents to 0.1768\n",
      "Episode 76 is finished\n",
      "Average Reward for Agent 0 this episode : -22.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 332.3582\n",
      "Reducing exploration for all agents to 0.1728\n",
      "Episode 77 is finished\n",
      "Average Reward for Agent 0 this episode : -22.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 342.5206\n",
      "Reducing exploration for all agents to 0.1688\n",
      "Episode 78 is finished\n",
      "Average Reward for Agent 0 this episode : -21.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 323.7590\n",
      "Reducing exploration for all agents to 0.165\n",
      "Episode 79 is finished\n",
      "Average Reward for Agent 0 this episode : -25.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 381.3391\n",
      "Reducing exploration for all agents to 0.1612\n",
      "Episode 80 is finished\n",
      "Average Reward for Agent 0 this episode : -22.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 370.0236\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.1575\n",
      "Episode 81 is finished\n",
      "Average Reward for Agent 0 this episode : -23.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 382.9403\n",
      "Reducing exploration for all agents to 0.1539\n",
      "Episode 82 is finished\n",
      "Average Reward for Agent 0 this episode : -21.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 411.0965\n",
      "Reducing exploration for all agents to 0.1504\n",
      "Episode 83 is finished\n",
      "Average Reward for Agent 0 this episode : -27.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 419.9340\n",
      "Reducing exploration for all agents to 0.147\n",
      "Episode 84 is finished\n",
      "Average Reward for Agent 0 this episode : -21.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 397.1008\n",
      "Reducing exploration for all agents to 0.1436\n",
      "Episode 85 is finished\n",
      "Average Reward for Agent 0 this episode : -22.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 452.6572\n",
      "Reducing exploration for all agents to 0.1403\n",
      "Episode 86 is finished\n",
      "Average Reward for Agent 0 this episode : -21.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 257.5448\n",
      "Reducing exploration for all agents to 0.1371\n",
      "Episode 87 is finished\n",
      "Average Reward for Agent 0 this episode : -21.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 357.6815\n",
      "Reducing exploration for all agents to 0.134\n",
      "Episode 88 is finished\n",
      "Average Reward for Agent 0 this episode : -20.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 355.9137\n",
      "Reducing exploration for all agents to 0.1309\n",
      "Episode 89 is finished\n",
      "Average Reward for Agent 0 this episode : -20.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 303.1206\n",
      "Reducing exploration for all agents to 0.1279\n",
      "Episode 90 is finished\n",
      "Average Reward for Agent 0 this episode : -26.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 382.8725\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.125\n",
      "Episode 91 is finished\n",
      "Average Reward for Agent 0 this episode : -24.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 449.3360\n",
      "Reducing exploration for all agents to 0.1222\n",
      "Episode 92 is finished\n",
      "Average Reward for Agent 0 this episode : -24.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 468.5738\n",
      "Reducing exploration for all agents to 0.1194\n",
      "Episode 93 is finished\n",
      "Average Reward for Agent 0 this episode : -20.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 338.7260\n",
      "Reducing exploration for all agents to 0.1167\n",
      "Episode 94 is finished\n",
      "Average Reward for Agent 0 this episode : -23.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 368.7234\n",
      "Reducing exploration for all agents to 0.114\n",
      "Episode 95 is finished\n",
      "Average Reward for Agent 0 this episode : -22.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 355.1458\n",
      "Reducing exploration for all agents to 0.1114\n",
      "Episode 96 is finished\n",
      "Average Reward for Agent 0 this episode : -22.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 300.0258\n",
      "Reducing exploration for all agents to 0.1088\n",
      "Episode 97 is finished\n",
      "Average Reward for Agent 0 this episode : -24.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 330.2631\n",
      "Reducing exploration for all agents to 0.1064\n",
      "Episode 98 is finished\n",
      "Average Reward for Agent 0 this episode : -23.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 400.7522\n",
      "Reducing exploration for all agents to 0.1039\n",
      "Episode 99 is finished\n",
      "Average Reward for Agent 0 this episode : -23.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 337.1264\n",
      "Reducing exploration for all agents to 0.1016\n",
      "Episode 100 is finished\n",
      "Average Reward for Agent 0 this episode : -21.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 342.5298\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0992\n",
      "Episode 101 is finished\n",
      "Average Reward for Agent 0 this episode : -21.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 335.9615\n",
      "Reducing exploration for all agents to 0.097\n",
      "Episode 102 is finished\n",
      "Average Reward for Agent 0 this episode : -21.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 270.5838\n",
      "Reducing exploration for all agents to 0.0948\n",
      "Episode 103 is finished\n",
      "Average Reward for Agent 0 this episode : -23.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 323.9173\n",
      "Reducing exploration for all agents to 0.0926\n",
      "Episode 104 is finished\n",
      "Average Reward for Agent 0 this episode : -22.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 225.6814\n",
      "Reducing exploration for all agents to 0.0905\n",
      "Episode 105 is finished\n",
      "Average Reward for Agent 0 this episode : -21.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 319.1130\n",
      "Reducing exploration for all agents to 0.0884\n",
      "Episode 106 is finished\n",
      "Average Reward for Agent 0 this episode : -23.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 395.3457\n",
      "Reducing exploration for all agents to 0.0864\n",
      "Episode 107 is finished\n",
      "Average Reward for Agent 0 this episode : -22.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 225.5187\n",
      "Reducing exploration for all agents to 0.0844\n",
      "Episode 108 is finished\n",
      "Average Reward for Agent 0 this episode : -21.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 357.5868\n",
      "Reducing exploration for all agents to 0.0825\n",
      "Episode 109 is finished\n",
      "Average Reward for Agent 0 this episode : -21.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 355.7662\n",
      "Reducing exploration for all agents to 0.0806\n",
      "Episode 110 is finished\n",
      "Average Reward for Agent 0 this episode : -22.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 400.7278\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0788\n",
      "Episode 111 is finished\n",
      "Average Reward for Agent 0 this episode : -24.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 483.0795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.077\n",
      "Episode 112 is finished\n",
      "Average Reward for Agent 0 this episode : -21.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 368.5759\n",
      "Reducing exploration for all agents to 0.0752\n",
      "Episode 113 is finished\n",
      "Average Reward for Agent 0 this episode : -19.78\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 399.8307\n",
      "Reducing exploration for all agents to 0.0735\n",
      "Episode 114 is finished\n",
      "Average Reward for Agent 0 this episode : -23.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 459.4397\n",
      "Reducing exploration for all agents to 0.0718\n",
      "Episode 115 is finished\n",
      "Average Reward for Agent 0 this episode : -24.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 355.8829\n",
      "Reducing exploration for all agents to 0.0702\n",
      "Episode 116 is finished\n",
      "Average Reward for Agent 0 this episode : -21.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 376.6495\n",
      "Reducing exploration for all agents to 0.0686\n",
      "Episode 117 is finished\n",
      "Average Reward for Agent 0 this episode : -27.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 349.9870\n",
      "Reducing exploration for all agents to 0.067\n",
      "Episode 118 is finished\n",
      "Average Reward for Agent 0 this episode : -24.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 336.0348\n",
      "Reducing exploration for all agents to 0.0655\n",
      "Episode 119 is finished\n",
      "Average Reward for Agent 0 this episode : -23.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.3898\n",
      "Reducing exploration for all agents to 0.064\n",
      "Episode 120 is finished\n",
      "Average Reward for Agent 0 this episode : -23.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 329.6828\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0625\n",
      "Episode 121 is finished\n",
      "Average Reward for Agent 0 this episode : -22.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 325.7691\n",
      "Reducing exploration for all agents to 0.0611\n",
      "Episode 122 is finished\n",
      "Average Reward for Agent 0 this episode : -24.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 415.2498\n",
      "Reducing exploration for all agents to 0.0597\n",
      "Episode 123 is finished\n",
      "Average Reward for Agent 0 this episode : -23.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.1797\n",
      "Reducing exploration for all agents to 0.0583\n",
      "Episode 124 is finished\n",
      "Average Reward for Agent 0 this episode : -20.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 326.8557\n",
      "Reducing exploration for all agents to 0.057\n",
      "Episode 125 is finished\n",
      "Average Reward for Agent 0 this episode : -23.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 275.0490\n",
      "Reducing exploration for all agents to 0.0557\n",
      "Episode 126 is finished\n",
      "Average Reward for Agent 0 this episode : -22.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.8805\n",
      "Reducing exploration for all agents to 0.0544\n",
      "Episode 127 is finished\n",
      "Average Reward for Agent 0 this episode : -19.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 291.6914\n",
      "Reducing exploration for all agents to 0.0532\n",
      "Episode 128 is finished\n",
      "Average Reward for Agent 0 this episode : -23.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.2331\n",
      "Reducing exploration for all agents to 0.052\n",
      "Episode 129 is finished\n",
      "Average Reward for Agent 0 this episode : -23.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.6892\n",
      "Reducing exploration for all agents to 0.0508\n",
      "Episode 130 is finished\n",
      "Average Reward for Agent 0 this episode : -23.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 448.1429\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0496\n",
      "Episode 131 is finished\n",
      "Average Reward for Agent 0 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 311.2873\n",
      "Reducing exploration for all agents to 0.0485\n",
      "Episode 132 is finished\n",
      "Average Reward for Agent 0 this episode : -21.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 204.2671\n",
      "Reducing exploration for all agents to 0.0474\n",
      "Episode 133 is finished\n",
      "Average Reward for Agent 0 this episode : -21.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 281.2552\n",
      "Reducing exploration for all agents to 0.0463\n",
      "Episode 134 is finished\n",
      "Average Reward for Agent 0 this episode : -20.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.9089\n",
      "Reducing exploration for all agents to 0.0452\n",
      "Episode 135 is finished\n",
      "Average Reward for Agent 0 this episode : -19.38\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 240.3147\n",
      "Reducing exploration for all agents to 0.0442\n",
      "Episode 136 is finished\n",
      "Average Reward for Agent 0 this episode : -22.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.1702\n",
      "Reducing exploration for all agents to 0.0432\n",
      "Episode 137 is finished\n",
      "Average Reward for Agent 0 this episode : -21.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.7643\n",
      "Reducing exploration for all agents to 0.0422\n",
      "Episode 138 is finished\n",
      "Average Reward for Agent 0 this episode : -22.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 221.9276\n",
      "Reducing exploration for all agents to 0.0412\n",
      "Episode 139 is finished\n",
      "Average Reward for Agent 0 this episode : -22.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 309.6813\n",
      "Reducing exploration for all agents to 0.0403\n",
      "Episode 140 is finished\n",
      "Average Reward for Agent 0 this episode : -21.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 255.7931\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0394\n",
      "Episode 141 is finished\n",
      "Average Reward for Agent 0 this episode : -25.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 285.0308\n",
      "Reducing exploration for all agents to 0.0385\n",
      "Episode 142 is finished\n",
      "Average Reward for Agent 0 this episode : -23.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.4075\n",
      "Reducing exploration for all agents to 0.0376\n",
      "Episode 143 is finished\n",
      "Average Reward for Agent 0 this episode : -24.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 253.4362\n",
      "Reducing exploration for all agents to 0.0367\n",
      "Episode 144 is finished\n",
      "Average Reward for Agent 0 this episode : -22.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 277.2964\n",
      "Reducing exploration for all agents to 0.0359\n",
      "Episode 145 is finished\n",
      "Average Reward for Agent 0 this episode : -24.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 228.3288\n",
      "Reducing exploration for all agents to 0.0351\n",
      "Episode 146 is finished\n",
      "Average Reward for Agent 0 this episode : -19.24\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Straight\\Agents_Results\\Single_Cross_Straigth_DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.5664\n",
      "Reducing exploration for all agents to 0.0343\n",
      "Episode 147 is finished\n",
      "Average Reward for Agent 0 this episode : -24.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 255.7253\n",
      "Reducing exploration for all agents to 0.0335\n",
      "Episode 148 is finished\n",
      "Average Reward for Agent 0 this episode : -23.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.3696\n",
      "Reducing exploration for all agents to 0.0327\n",
      "Episode 149 is finished\n",
      "Average Reward for Agent 0 this episode : -22.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 273.3688\n",
      "Reducing exploration for all agents to 0.032\n",
      "Episode 150 is finished\n",
      "Average Reward for Agent 0 this episode : -22.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 391.4000\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0313\n",
      "Episode 151 is finished\n",
      "Average Reward for Agent 0 this episode : -20.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 436.6184\n",
      "Reducing exploration for all agents to 0.0305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 152 is finished\n",
      "Average Reward for Agent 0 this episode : -23.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 289.5075\n",
      "Reducing exploration for all agents to 0.0298\n",
      "Episode 153 is finished\n",
      "Average Reward for Agent 0 this episode : -22.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 295.4548\n",
      "Reducing exploration for all agents to 0.0292\n",
      "Episode 154 is finished\n",
      "Average Reward for Agent 0 this episode : -21.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.2458\n",
      "Reducing exploration for all agents to 0.0285\n",
      "Episode 155 is finished\n",
      "Average Reward for Agent 0 this episode : -23.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 323.5241\n",
      "Reducing exploration for all agents to 0.0278\n",
      "Episode 156 is finished\n",
      "Average Reward for Agent 0 this episode : -24.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 390.0239\n",
      "Reducing exploration for all agents to 0.0272\n",
      "Episode 157 is finished\n",
      "Average Reward for Agent 0 this episode : -22.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.0830\n",
      "Reducing exploration for all agents to 0.0266\n",
      "Episode 158 is finished\n",
      "Average Reward for Agent 0 this episode : -23.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 254.2218\n",
      "Reducing exploration for all agents to 0.026\n",
      "Episode 159 is finished\n",
      "Average Reward for Agent 0 this episode : -22.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.9909\n",
      "Reducing exploration for all agents to 0.0254\n",
      "Episode 160 is finished\n",
      "Average Reward for Agent 0 this episode : -25.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 308.1826\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0248\n",
      "Episode 161 is finished\n",
      "Average Reward for Agent 0 this episode : -21.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.7675\n",
      "Reducing exploration for all agents to 0.0242\n",
      "Episode 162 is finished\n",
      "Average Reward for Agent 0 this episode : -24.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.6388\n",
      "Reducing exploration for all agents to 0.0237\n",
      "Episode 163 is finished\n",
      "Average Reward for Agent 0 this episode : -21.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 247.4223\n",
      "Reducing exploration for all agents to 0.0231\n",
      "Episode 164 is finished\n",
      "Average Reward for Agent 0 this episode : -22.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 277.4412\n",
      "Reducing exploration for all agents to 0.0226\n",
      "Episode 165 is finished\n",
      "Average Reward for Agent 0 this episode : -26.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 196.2134\n",
      "Reducing exploration for all agents to 0.0221\n",
      "Episode 166 is finished\n",
      "Average Reward for Agent 0 this episode : -22.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 364.7999\n",
      "Reducing exploration for all agents to 0.0216\n",
      "Episode 167 is finished\n",
      "Average Reward for Agent 0 this episode : -25.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 316.1792\n",
      "Reducing exploration for all agents to 0.0211\n",
      "Episode 168 is finished\n",
      "Average Reward for Agent 0 this episode : -27.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 442.5484\n",
      "Reducing exploration for all agents to 0.0206\n",
      "Episode 169 is finished\n",
      "Average Reward for Agent 0 this episode : -23.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 260.2546\n",
      "Reducing exploration for all agents to 0.0202\n",
      "Episode 170 is finished\n",
      "Average Reward for Agent 0 this episode : -23.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 296.2401\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0197\n",
      "Episode 171 is finished\n",
      "Average Reward for Agent 0 this episode : -22.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 384.7092\n",
      "Reducing exploration for all agents to 0.0192\n",
      "Episode 172 is finished\n",
      "Average Reward for Agent 0 this episode : -24.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 283.4623\n",
      "Reducing exploration for all agents to 0.0188\n",
      "Episode 173 is finished\n",
      "Average Reward for Agent 0 this episode : -22.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 293.2926\n",
      "Reducing exploration for all agents to 0.0184\n",
      "Episode 174 is finished\n",
      "Average Reward for Agent 0 this episode : -23.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 280.0843\n",
      "Reducing exploration for all agents to 0.018\n",
      "Episode 175 is finished\n",
      "Average Reward for Agent 0 this episode : -22.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 253.3834\n",
      "Reducing exploration for all agents to 0.0175\n",
      "Episode 176 is finished\n",
      "Average Reward for Agent 0 this episode : -23.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 228.4183\n",
      "Reducing exploration for all agents to 0.0171\n",
      "Episode 177 is finished\n",
      "Average Reward for Agent 0 this episode : -24.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.6270\n",
      "Reducing exploration for all agents to 0.0168\n",
      "Episode 178 is finished\n",
      "Average Reward for Agent 0 this episode : -24.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 353.3512\n",
      "Reducing exploration for all agents to 0.0164\n",
      "Episode 179 is finished\n",
      "Average Reward for Agent 0 this episode : -22.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 292.9828\n",
      "Reducing exploration for all agents to 0.016\n",
      "Episode 180 is finished\n",
      "Average Reward for Agent 0 this episode : -24.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 360.7367\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0156\n",
      "Episode 181 is finished\n",
      "Average Reward for Agent 0 this episode : -21.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.2143\n",
      "Reducing exploration for all agents to 0.0153\n",
      "Episode 182 is finished\n",
      "Average Reward for Agent 0 this episode : -21.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 214.5655\n",
      "Reducing exploration for all agents to 0.0149\n",
      "Episode 183 is finished\n",
      "Average Reward for Agent 0 this episode : -22.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 242.2038\n",
      "Reducing exploration for all agents to 0.0146\n",
      "Episode 184 is finished\n",
      "Average Reward for Agent 0 this episode : -22.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 339.5699\n",
      "Reducing exploration for all agents to 0.0143\n",
      "Episode 185 is finished\n",
      "Average Reward for Agent 0 this episode : -24.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 249.8173\n",
      "Reducing exploration for all agents to 0.0139\n",
      "Episode 186 is finished\n",
      "Average Reward for Agent 0 this episode : -22.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 277.3241\n",
      "Reducing exploration for all agents to 0.0136\n",
      "Episode 187 is finished\n",
      "Average Reward for Agent 0 this episode : -23.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 316.5621\n",
      "Reducing exploration for all agents to 0.0133\n",
      "Episode 188 is finished\n",
      "Average Reward for Agent 0 this episode : -25.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.0269\n",
      "Reducing exploration for all agents to 0.013\n",
      "Episode 189 is finished\n",
      "Average Reward for Agent 0 this episode : -26.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 363.7826\n",
      "Reducing exploration for all agents to 0.0127\n",
      "Episode 190 is finished\n",
      "Average Reward for Agent 0 this episode : -24.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 381.1754\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0124\n",
      "Episode 191 is finished\n",
      "Average Reward for Agent 0 this episode : -25.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 283.9740\n",
      "Reducing exploration for all agents to 0.0121\n",
      "Episode 192 is finished\n",
      "Average Reward for Agent 0 this episode : -21.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 361.0982\n",
      "Reducing exploration for all agents to 0.0118\n",
      "Episode 193 is finished\n",
      "Average Reward for Agent 0 this episode : -23.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.4025\n",
      "Reducing exploration for all agents to 0.0116\n",
      "Episode 194 is finished\n",
      "Average Reward for Agent 0 this episode : -24.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 407.1826\n",
      "Reducing exploration for all agents to 0.0113\n",
      "Episode 195 is finished\n",
      "Average Reward for Agent 0 this episode : -21.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 409.3304\n",
      "Reducing exploration for all agents to 0.0111\n",
      "Episode 196 is finished\n",
      "Average Reward for Agent 0 this episode : -23.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 419.6964\n",
      "Reducing exploration for all agents to 0.0108\n",
      "Episode 197 is finished\n",
      "Average Reward for Agent 0 this episode : -23.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 385.1720\n",
      "Reducing exploration for all agents to 0.0106\n",
      "Episode 198 is finished\n",
      "Average Reward for Agent 0 this episode : -26.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 389.3791\n",
      "Reducing exploration for all agents to 0.0103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 199 is finished\n",
      "Average Reward for Agent 0 this episode : -24.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 351.3962\n",
      "Reducing exploration for all agents to 0.0101\n",
      "Episode 200 is finished\n",
      "Average Reward for Agent 0 this episode : -21.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 496.7721\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0098\n",
      "Episode 201 is finished\n",
      "Average Reward for Agent 0 this episode : -22.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 203.4163\n",
      "Reducing exploration for all agents to 0.0096\n",
      "Episode 202 is finished\n",
      "Average Reward for Agent 0 this episode : -23.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 194.2194\n",
      "Reducing exploration for all agents to 0.0094\n",
      "Episode 203 is finished\n",
      "Average Reward for Agent 0 this episode : -24.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 254.4909\n",
      "Reducing exploration for all agents to 0.0092\n",
      "Episode 204 is finished\n",
      "Average Reward for Agent 0 this episode : -22.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 288.9842\n",
      "Reducing exploration for all agents to 0.009\n",
      "Episode 205 is finished\n",
      "Average Reward for Agent 0 this episode : -22.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 273.1205\n",
      "Reducing exploration for all agents to 0.0088\n",
      "Episode 206 is finished\n",
      "Average Reward for Agent 0 this episode : -28.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 224.0303\n",
      "Reducing exploration for all agents to 0.0086\n",
      "Episode 207 is finished\n",
      "Average Reward for Agent 0 this episode : -26.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 271.3430\n",
      "Reducing exploration for all agents to 0.0084\n",
      "Episode 208 is finished\n",
      "Average Reward for Agent 0 this episode : -24.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.1181\n",
      "Reducing exploration for all agents to 0.0082\n",
      "Episode 209 is finished\n",
      "Average Reward for Agent 0 this episode : -24.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 240.8675\n",
      "Reducing exploration for all agents to 0.008\n",
      "Episode 210 is finished\n",
      "Average Reward for Agent 0 this episode : -22.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 220.5966\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0078\n",
      "Episode 211 is finished\n",
      "Average Reward for Agent 0 this episode : -21.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 251.2813\n",
      "Reducing exploration for all agents to 0.0076\n",
      "Episode 212 is finished\n",
      "Average Reward for Agent 0 this episode : -22.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 267.3118\n",
      "Reducing exploration for all agents to 0.0075\n",
      "Episode 213 is finished\n",
      "Average Reward for Agent 0 this episode : -25.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 304.3717\n",
      "Reducing exploration for all agents to 0.0073\n",
      "Episode 214 is finished\n",
      "Average Reward for Agent 0 this episode : -23.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.1964\n",
      "Reducing exploration for all agents to 0.0071\n",
      "Episode 215 is finished\n",
      "Average Reward for Agent 0 this episode : -25.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 247.5469\n",
      "Reducing exploration for all agents to 0.007\n",
      "Episode 216 is finished\n",
      "Average Reward for Agent 0 this episode : -23.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 255.6825\n",
      "Reducing exploration for all agents to 0.0068\n",
      "Episode 217 is finished\n",
      "Average Reward for Agent 0 this episode : -25.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 203.5121\n",
      "Reducing exploration for all agents to 0.0066\n",
      "Episode 218 is finished\n",
      "Average Reward for Agent 0 this episode : -22.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.5670\n",
      "Reducing exploration for all agents to 0.0065\n",
      "Episode 219 is finished\n",
      "Average Reward for Agent 0 this episode : -25.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 246.2040\n",
      "Reducing exploration for all agents to 0.0063\n",
      "Episode 220 is finished\n",
      "Average Reward for Agent 0 this episode : -25.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.8629\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0062\n",
      "Episode 221 is finished\n",
      "Average Reward for Agent 0 this episode : -23.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 310.4596\n",
      "Reducing exploration for all agents to 0.0061\n",
      "Episode 222 is finished\n",
      "Average Reward for Agent 0 this episode : -27.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 319.9073\n",
      "Reducing exploration for all agents to 0.0059\n",
      "Episode 223 is finished\n",
      "Average Reward for Agent 0 this episode : -23.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 311.0220\n",
      "Reducing exploration for all agents to 0.0058\n",
      "Episode 224 is finished\n",
      "Average Reward for Agent 0 this episode : -22.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.7965\n",
      "Reducing exploration for all agents to 0.0057\n",
      "Episode 225 is finished\n",
      "Average Reward for Agent 0 this episode : -23.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.3072\n",
      "Reducing exploration for all agents to 0.0055\n",
      "Episode 226 is finished\n",
      "Average Reward for Agent 0 this episode : -24.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.0530\n",
      "Reducing exploration for all agents to 0.0054\n",
      "Episode 227 is finished\n",
      "Average Reward for Agent 0 this episode : -23.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.6927\n",
      "Reducing exploration for all agents to 0.0053\n",
      "Episode 228 is finished\n",
      "Average Reward for Agent 0 this episode : -22.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 280.4400\n",
      "Reducing exploration for all agents to 0.0052\n",
      "Episode 229 is finished\n",
      "Average Reward for Agent 0 this episode : -25.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 310.7743\n",
      "Reducing exploration for all agents to 0.005\n",
      "Episode 230 is finished\n",
      "Average Reward for Agent 0 this episode : -22.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.9171\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0049\n",
      "Episode 231 is finished\n",
      "Average Reward for Agent 0 this episode : -25.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 419.0920\n",
      "Reducing exploration for all agents to 0.0048\n",
      "Episode 232 is finished\n",
      "Average Reward for Agent 0 this episode : -25.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 224.5759\n",
      "Reducing exploration for all agents to 0.0047\n",
      "Episode 233 is finished\n",
      "Average Reward for Agent 0 this episode : -24.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.0166\n",
      "Reducing exploration for all agents to 0.0046\n",
      "Episode 234 is finished\n",
      "Average Reward for Agent 0 this episode : -24.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 312.1237\n",
      "Reducing exploration for all agents to 0.0045\n",
      "Episode 235 is finished\n",
      "Average Reward for Agent 0 this episode : -21.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.7706\n",
      "Reducing exploration for all agents to 0.0044\n",
      "Episode 236 is finished\n",
      "Average Reward for Agent 0 this episode : -22.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 346.1973\n",
      "Reducing exploration for all agents to 0.0043\n",
      "Episode 237 is finished\n",
      "Average Reward for Agent 0 this episode : -24.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 265.0867\n",
      "Reducing exploration for all agents to 0.0042\n",
      "Episode 238 is finished\n",
      "Average Reward for Agent 0 this episode : -23.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 277.1458\n",
      "Reducing exploration for all agents to 0.0041\n",
      "Episode 239 is finished\n",
      "Average Reward for Agent 0 this episode : -21.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 284.6180\n",
      "Reducing exploration for all agents to 0.004\n",
      "Episode 240 is finished\n",
      "Average Reward for Agent 0 this episode : -24.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 230.5078\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0039\n",
      "Episode 241 is finished\n",
      "Average Reward for Agent 0 this episode : -24.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 194.0560\n",
      "Reducing exploration for all agents to 0.0038\n",
      "Episode 242 is finished\n",
      "Average Reward for Agent 0 this episode : -22.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 192.6328\n",
      "Reducing exploration for all agents to 0.0037\n",
      "Episode 243 is finished\n",
      "Average Reward for Agent 0 this episode : -23.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.9225\n",
      "Reducing exploration for all agents to 0.0036\n",
      "Episode 244 is finished\n",
      "Average Reward for Agent 0 this episode : -23.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 253.0092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0036\n",
      "Episode 245 is finished\n",
      "Average Reward for Agent 0 this episode : -27.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 266.6054\n",
      "Reducing exploration for all agents to 0.0035\n",
      "Episode 246 is finished\n",
      "Average Reward for Agent 0 this episode : -26.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.3580\n",
      "Reducing exploration for all agents to 0.0034\n",
      "Episode 247 is finished\n",
      "Average Reward for Agent 0 this episode : -25.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.4572\n",
      "Reducing exploration for all agents to 0.0033\n",
      "Episode 248 is finished\n",
      "Average Reward for Agent 0 this episode : -24.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.9077\n",
      "Reducing exploration for all agents to 0.0032\n",
      "Episode 249 is finished\n",
      "Average Reward for Agent 0 this episode : -25.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 253.5419\n",
      "Reducing exploration for all agents to 0.0032\n",
      "Episode 250 is finished\n",
      "Average Reward for Agent 0 this episode : -25.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 328.7245\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0031\n",
      "Episode 251 is finished\n",
      "Average Reward for Agent 0 this episode : -24.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.7642\n",
      "Reducing exploration for all agents to 0.003\n",
      "Episode 252 is finished\n",
      "Average Reward for Agent 0 this episode : -26.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 390.5385\n",
      "Reducing exploration for all agents to 0.003\n",
      "Episode 253 is finished\n",
      "Average Reward for Agent 0 this episode : -25.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 352.6277\n",
      "Reducing exploration for all agents to 0.0029\n",
      "Episode 254 is finished\n",
      "Average Reward for Agent 0 this episode : -24.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 246.5217\n",
      "Reducing exploration for all agents to 0.0028\n",
      "Episode 255 is finished\n",
      "Average Reward for Agent 0 this episode : -25.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 223.5792\n",
      "Reducing exploration for all agents to 0.0028\n",
      "Episode 256 is finished\n",
      "Average Reward for Agent 0 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.3571\n",
      "Reducing exploration for all agents to 0.0027\n",
      "Episode 257 is finished\n",
      "Average Reward for Agent 0 this episode : -22.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.7718\n",
      "Reducing exploration for all agents to 0.0026\n",
      "Episode 258 is finished\n",
      "Average Reward for Agent 0 this episode : -22.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 266.9047\n",
      "Reducing exploration for all agents to 0.0026\n",
      "Episode 259 is finished\n",
      "Average Reward for Agent 0 this episode : -21.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 291.6411\n",
      "Reducing exploration for all agents to 0.0025\n",
      "Episode 260 is finished\n",
      "Average Reward for Agent 0 this episode : -22.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 323.4790\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0025\n",
      "Episode 261 is finished\n",
      "Average Reward for Agent 0 this episode : -22.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 246.4242\n",
      "Reducing exploration for all agents to 0.0024\n",
      "Episode 262 is finished\n",
      "Average Reward for Agent 0 this episode : -23.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 334.1584\n",
      "Reducing exploration for all agents to 0.0024\n",
      "Episode 263 is finished\n",
      "Average Reward for Agent 0 this episode : -22.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 291.6674\n",
      "Reducing exploration for all agents to 0.0023\n",
      "Episode 264 is finished\n",
      "Average Reward for Agent 0 this episode : -25.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 230.5219\n",
      "Reducing exploration for all agents to 0.0022\n",
      "Episode 265 is finished\n",
      "Average Reward for Agent 0 this episode : -26.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 259.5060\n",
      "Reducing exploration for all agents to 0.0022\n",
      "Episode 266 is finished\n",
      "Average Reward for Agent 0 this episode : -23.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 231.7762\n",
      "Reducing exploration for all agents to 0.0021\n",
      "Episode 267 is finished\n",
      "Average Reward for Agent 0 this episode : -24.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 235.7884\n",
      "Reducing exploration for all agents to 0.0021\n",
      "Episode 268 is finished\n",
      "Average Reward for Agent 0 this episode : -25.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 257.5979\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 269 is finished\n",
      "Average Reward for Agent 0 this episode : -23.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 279.6855\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 270 is finished\n",
      "Average Reward for Agent 0 this episode : -25.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.3577\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 271 is finished\n",
      "Average Reward for Agent 0 this episode : -27.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.4219\n",
      "Reducing exploration for all agents to 0.0019\n",
      "Episode 272 is finished\n",
      "Average Reward for Agent 0 this episode : -24.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.0692\n",
      "Reducing exploration for all agents to 0.0019\n",
      "Episode 273 is finished\n",
      "Average Reward for Agent 0 this episode : -25.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 343.9027\n",
      "Reducing exploration for all agents to 0.0018\n",
      "Episode 274 is finished\n",
      "Average Reward for Agent 0 this episode : -23.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 378.8900\n",
      "Reducing exploration for all agents to 0.0018\n",
      "Episode 275 is finished\n",
      "Average Reward for Agent 0 this episode : -22.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 293.6664\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 276 is finished\n",
      "Average Reward for Agent 0 this episode : -24.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 337.9138\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 277 is finished\n",
      "Average Reward for Agent 0 this episode : -23.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 348.0547\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 278 is finished\n",
      "Average Reward for Agent 0 this episode : -22.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 377.0902\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 279 is finished\n",
      "Average Reward for Agent 0 this episode : -21.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 368.9435\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 280 is finished\n",
      "Average Reward for Agent 0 this episode : -21.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 334.4390\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 281 is finished\n",
      "Average Reward for Agent 0 this episode : -24.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.4124\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 282 is finished\n",
      "Average Reward for Agent 0 this episode : -24.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 228.9476\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 283 is finished\n",
      "Average Reward for Agent 0 this episode : -28.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 457.2636\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 284 is finished\n",
      "Average Reward for Agent 0 this episode : -24.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 299.4554\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 285 is finished\n",
      "Average Reward for Agent 0 this episode : -25.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 301.0935\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 286 is finished\n",
      "Average Reward for Agent 0 this episode : -23.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 304.3406\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 287 is finished\n",
      "Average Reward for Agent 0 this episode : -24.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 194.8470\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 288 is finished\n",
      "Average Reward for Agent 0 this episode : -25.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 245.1942\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 289 is finished\n",
      "Average Reward for Agent 0 this episode : -25.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 385.2203\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 290 is finished\n",
      "Average Reward for Agent 0 this episode : -26.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.8098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0012\n",
      "Episode 291 is finished\n",
      "Average Reward for Agent 0 this episode : -28.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 391.6031\n",
      "Reducing exploration for all agents to 0.0012\n",
      "Episode 292 is finished\n",
      "Average Reward for Agent 0 this episode : -28.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 537.2622\n",
      "Reducing exploration for all agents to 0.0012\n",
      "Episode 293 is finished\n",
      "Average Reward for Agent 0 this episode : -27.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 398.0059\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 294 is finished\n",
      "Average Reward for Agent 0 this episode : -24.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 455.0373\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 295 is finished\n",
      "Average Reward for Agent 0 this episode : -28.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 340.5602\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 296 is finished\n",
      "Average Reward for Agent 0 this episode : -23.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 420.5431\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 297 is finished\n",
      "Average Reward for Agent 0 this episode : -25.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 488.2943\n",
      "Reducing exploration for all agents to 0.001\n",
      "Episode 298 is finished\n",
      "Average Reward for Agent 0 this episode : -22.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 496.2303\n",
      "Reducing exploration for all agents to 0.001\n",
      "Episode 299 is finished\n",
      "Average Reward for Agent 0 this episode : -25.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 486.0064\n",
      "Reducing exploration for all agents to 0.001\n",
      "Episode 300 is finished\n",
      "Average Reward for Agent 0 this episode : -24.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 479.7702\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.001\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of tensorflow.python.keras.layers.core failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 244, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 378, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\importlib\\__init__.py\", line 148, in reload\n",
      "    raise ImportError(msg.format(name), name=name)\n",
      "ImportError: module DQNAgents not in sys.modules\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.load(300 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 110\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: test\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.17 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Straight_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d1cba0aac8>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To be arranged for multy agents and add legends...\n",
    "\n",
    "queues = np.array(Episode_Queues[0])\n",
    "queues = queues.T\n",
    "\n",
    "delay = Cumulative_Episode_Delays[0]\n",
    "\n",
    "# Plot the queues\n",
    "plt.figure(1)\n",
    "for queue in queues:\n",
    "    plt.plot(queue)\n",
    "\n",
    "# plot the junctions delays\n",
    "plt.figure(2)\n",
    "plt.plot(delay)\n",
    "\n",
    "#plot the total delays \n",
    "plt.figure(3)\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "\n",
    "# Dont freak out the 2 delays are not the same because the node is not covering the all junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 400\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes, Inputs.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.28 seconds.\n",
      "\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Une exception s’est produite.', (0, None, None, None, 0, -2147467259), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ce80fe176d53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSingle_Cross_Straight_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\MasterDQN_Agent.py\u001b[0m in \u001b[0;36mdemo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                         \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions, green_time)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m                 \u001b[1;31m# increase the update counter by one each step (until reach simulation length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Une exception s’est produite.', (0, None, None, None, 0, -2147467259), None)"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3600\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_TripleAC4test1\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "    },\n",
    "   'demand' : {\"default\" : [400,400,400,400] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 5000\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  672       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  2352      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2352      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  49        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  672       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  196       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 10,997\n",
      "Trainable params: 10,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3600 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.11 seconds.\n",
      "\n",
      "Episode 1 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-344.0, -412.0, -412.0, -252.0, -412.0, -412.0, -412.0, -412.0, -412.0, -412.0] \n",
      " [-13316.0, -15003.0, -15003.0, -9750.0, -15003.0, -15003.0, -15003.0, -15003.0, -15003.0, -15003.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1982.74\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 2 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1222.0, -1222.0, -1222.0, -1222.0, -1222.0, -1222.0, -1222.0, -1222.0, -1222.0, -1222.0] \n",
      " [-15050.0, -15050.0, -15050.0, -15050.0, -15050.0, -15050.0, -15050.0, -15050.0, -15050.0, -15050.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1973.26\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 3 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1639.0, -1564.0, -1636.0, -1622.0, -1576.0, -1304.0, -1663.0, -812.0, -1570.0, -1686.0] \n",
      " [-9909.0, -9616.0, -9733.0, -9733.0, -9603.0, -9349.0, -11568.0, -5580.0, -9495.0, -9525.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.97, 0.03], [0.0, 0.0, 0.92, 0.08], [0.0, 0.0, 0.21, 0.79], [0.0, 0.0, 0.3, 0.7], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.3, 0.7], [0.0, 0.0, 0.1, 0.9], [0.0, 0.0, 1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1264.32\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 4 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-723.0, -3117.0, -3117.0, -2931.0, -3117.0, -1465.0, -3240.0, -444.0, -3189.0, -3187.0] \n",
      " [-3237.0, -9687.0, -9589.0, -9124.0, -9636.0, -5185.0, -9610.0, -2352.0, -9792.0, -9720.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.69, 0.31], [0.0, 0.0, 0.49, 0.51], [0.0, 0.0, 0.53, 0.47], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.53, 0.47], [0.0, 0.0, 0.99, 0.01], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.53, 0.47], [0.0, 0.0, 0.99, 0.01], [0.0, 0.0, 0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1286.1\n",
      "Episode 5 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3934.0, -5423.0, -5437.0, -5392.0, -3224.0, -5393.0, -5625.0, -5439.0, -5366.0, -4683.0] \n",
      " [-7449.0, -9561.0, -9662.0, -9623.0, -5791.0, -9532.0, -9542.0, -9477.0, -9512.0, -8735.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.99, 0.01], [0.0, 0.0, 0.02, 0.98], [0.0, 0.0, 0.01, 0.99], [0.0, 0.0, 0.18, 0.82], [0.0, 0.0, 0.98, 0.02], [0.0, 0.0, 0.14, 0.86], [0.0, 0.0, 0.48, 0.52], [0.0, 0.0, 0.97, 0.03], [0.0, 0.0, 0.5, 0.5], [0.0, 0.0, 0.31, 0.69]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1226.34\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 6 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8627.0, -1657.0, -8481.0, -579.0, -567.0, -8536.0, -8479.0, -6318.0, -8481.0, -8481.0] \n",
      " [-9609.0, -2329.0, -9513.0, -1742.0, -1583.0, -9604.0, -9582.0, -7426.0, -9599.0, -9473.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.67, 0.33], [0.0, 0.0, 0.79, 0.21], [0.0, 0.0, 0.32, 0.68], [0.0, 0.01, 0.84, 0.14], [0.0, 0.01, 0.82, 0.17], [0.0, 0.0, 0.05, 0.95], [0.0, 0.0, 0.28, 0.72], [0.0, 0.0, 0.93, 0.07], [0.0, 0.0, 0.32, 0.68], [0.0, 0.0, 0.32, 0.68]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1234.22\n",
      "Episode 7 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9490.0, -9554.0, -9505.0, -9568.0, -6080.0, -9508.0, -9705.0, -9505.0, -2218.0, -9505.0] \n",
      " [-9605.0, -9564.0, -9555.0, -9513.0, -6356.0, -9611.0, -9579.0, -9577.0, -2790.0, -9612.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.73, 0.27], [0.0, 0.0, 0.32, 0.68], [0.0, 0.0, 0.31, 0.69], [0.0, 0.0, 0.37, 0.63], [0.0, 0.0, 0.63, 0.37], [0.0, 0.0, 0.36, 0.64], [0.0, 0.0, 0.94, 0.06], [0.0, 0.0, 0.31, 0.69], [0.0, 0.0, 0.71, 0.29], [0.0, 0.0, 0.31, 0.69]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1247.2\n",
      "Episode 8 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9581.0, -9488.0, -9534.0, -9729.0, -9491.0, -9712.0, -2905.0, -9491.0, -3035.0, -9649.0] \n",
      " [-9477.0, -9504.0, -9591.0, -9583.0, -9483.0, -9505.0, -3567.0, -9483.0, -3778.0, -9473.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.76, 0.24], [0.0, 0.0, 0.61, 0.39], [0.0, 0.0, 0.66, 0.34], [0.0, 0.0, 0.96, 0.04], [0.0, 0.0, 0.66, 0.34], [0.0, 0.0, 0.96, 0.04], [0.0, 0.0, 0.74, 0.26], [0.0, 0.0, 0.66, 0.34], [0.0, 0.0, 0.85, 0.15], [0.0, 0.0, 0.71, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1234.93\n",
      "Episode 9 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9607.0, -9470.0, -9546.0, -9519.0, -9613.0, -9470.0, -9473.0, -1761.0, -9562.0, -9608.0] \n",
      " [-9575.0, -9484.0, -9521.0, -9462.0, -9625.0, -9506.0, -9472.0, -2794.0, -9477.0, -9506.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.21, 0.79], [0.0, 0.0, 0.14, 0.86], [0.0, 0.0, 0.21, 0.79], [0.0, 0.0, 0.18, 0.82], [0.0, 0.0, 0.34, 0.66], [0.0, 0.0, 0.14, 0.86], [0.0, 0.0, 0.17, 0.83], [0.0, 0.01, 0.77, 0.22], [0.0, 0.0, 0.23, 0.77], [0.0, 0.0, 0.33, 0.67]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1246.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9519.0, -9505.0, -9585.0, -9474.0, -1800.0, -9525.0, -9471.0, -9474.0, -9566.0, -6099.0] \n",
      " [-9491.0, -9539.0, -9516.0, -9502.0, -2402.0, -9501.0, -9581.0, -9530.0, -9556.0, -6438.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.71, 0.29], [0.0, 0.0, 0.7, 0.3], [0.0, 0.0, 0.8, 0.2], [0.0, 0.0, 0.72, 0.28], [0.0, 0.0, 0.54, 0.45], [0.0, 0.0, 0.75, 0.25], [0.0, 0.0, 0.67, 0.33], [0.0, 0.0, 0.72, 0.28], [0.0, 0.0, 0.72, 0.28], [0.0, 0.0, 0.68, 0.32]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -1217.24\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 11 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4075.0, -2826.0, -2576.0, -4311.0, -307.0, -2624.0, -4256.0, -4227.0, -1474.0, -4344.0] \n",
      " [-4094.0, -3591.0, -2777.0, -4398.0, -1287.0, -2498.0, -4253.0, -4411.0, -1735.0, -4156.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.41, 0.26, 0.32], [0.0, 0.35, 0.35, 0.3], [0.0, 0.45, 0.25, 0.3], [0.0, 0.42, 0.22, 0.36], [0.02, 0.28, 0.3, 0.39], [0.0, 0.36, 0.35, 0.29], [0.0, 0.41, 0.26, 0.33], [0.0, 0.41, 0.29, 0.3], [0.0, 0.22, 0.41, 0.37], [0.0, 0.5, 0.23, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -559.01\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 12 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4032.0, -4166.0, -4319.0, -781.0, -4322.0, -4370.0, -4417.0, -1995.0, -4273.0, -4156.0] \n",
      " [-4157.0, -4345.0, -4158.0, -1161.0, -4140.0, -4226.0, -4415.0, -2654.0, -4629.0, -4255.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.39, 0.29, 0.32], [0.0, 0.36, 0.33, 0.31], [0.0, 0.39, 0.32, 0.29], [0.0, 0.14, 0.37, 0.49], [0.0, 0.4, 0.29, 0.31], [0.0, 0.41, 0.28, 0.3], [0.0, 0.38, 0.33, 0.29], [0.0, 0.4, 0.33, 0.27], [0.0, 0.25, 0.29, 0.46], [0.0, 0.42, 0.3, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -576.74\n",
      "Episode 13 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2685.0, -4494.0, -2945.0, -4575.0, -1597.0, -4417.0, -4542.0, -2908.0, -3036.0, -4531.0] \n",
      " [-3083.0, -4459.0, -3154.0, -4392.0, -1861.0, -4454.0, -4418.0, -2833.0, -3332.0, -4369.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.33, 0.33, 0.34], [0.0, 0.38, 0.32, 0.3], [0.0, 0.34, 0.38, 0.28], [0.0, 0.4, 0.29, 0.31], [0.0, 0.37, 0.34, 0.29], [0.0, 0.4, 0.29, 0.32], [0.0, 0.33, 0.34, 0.33], [0.0, 0.39, 0.32, 0.29], [0.0, 0.38, 0.33, 0.3], [0.0, 0.39, 0.31, 0.3]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -565.54\n",
      "Episode 14 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4538.0, -2969.0, -2066.0, -4743.0, -3525.0, -4488.0, -4396.0, -3444.0, -4255.0, -4177.0] \n",
      " [-4288.0, -3389.0, -2515.0, -4310.0, -2987.0, -4354.0, -4338.0, -3660.0, -4275.0, -4265.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.25, 0.37, 0.38], [0.0, 0.36, 0.33, 0.31], [0.0, 0.21, 0.45, 0.34], [0.0, 0.32, 0.34, 0.34], [0.0, 0.07, 0.87, 0.06], [0.0, 0.29, 0.35, 0.36], [0.0, 0.31, 0.36, 0.34], [0.0, 0.27, 0.38, 0.34], [0.0, 0.28, 0.35, 0.37], [0.0, 0.31, 0.36, 0.33]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -586.48\n",
      "Episode 15 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1392.0, -4046.0, -4356.0, -4388.0, -2207.0, -4287.0, -4470.0, -2067.0, -1463.0, -1181.0] \n",
      " [-1890.0, -4199.0, -4445.0, -4105.0, -2343.0, -4235.0, -4416.0, -2384.0, -2135.0, -1807.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.31, 0.36, 0.33], [0.0, 0.19, 0.47, 0.34], [0.0, 0.22, 0.41, 0.36], [0.0, 0.2, 0.39, 0.4], [0.0, 0.12, 0.42, 0.46], [0.0, 0.25, 0.39, 0.35], [0.0, 0.21, 0.42, 0.37], [0.0, 0.2, 0.39, 0.41], [0.0, 0.12, 0.39, 0.49], [0.0, 0.73, 0.16, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -556.43\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 16 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4133.0, -4421.0, -4281.0, -1923.0, -2604.0, -2501.0, -4673.0, -1274.0, -4249.0, -4227.0] \n",
      " [-4396.0, -4357.0, -4241.0, -1471.0, -2923.0, -2875.0, -4283.0, -1740.0, -4191.0, -4232.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.37, 0.38, 0.26], [0.0, 0.39, 0.34, 0.27], [0.0, 0.37, 0.4, 0.23], [0.0, 0.12, 0.4, 0.48], [0.0, 0.38, 0.36, 0.26], [0.0, 0.36, 0.37, 0.26], [0.0, 0.32, 0.41, 0.26], [0.0, 0.3, 0.29, 0.41], [0.0, 0.4, 0.32, 0.28], [0.0, 0.34, 0.39, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -575.71\n",
      "Episode 17 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2507.0, -4286.0, -4649.0, -4649.0, -4620.0, -3540.0, -4352.0, -4466.0, -4253.0, -4588.0] \n",
      " [-2839.0, -4164.0, -4256.0, -4256.0, -4139.0, -3979.0, -4624.0, -4356.0, -4353.0, -4425.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.31, 0.35, 0.34], [0.0, 0.27, 0.36, 0.37], [0.0, 0.25, 0.43, 0.33], [0.0, 0.25, 0.43, 0.33], [0.0, 0.24, 0.43, 0.33], [0.0, 0.34, 0.31, 0.35], [0.0, 0.3, 0.33, 0.37], [0.0, 0.28, 0.33, 0.39], [0.0, 0.3, 0.34, 0.37], [0.0, 0.25, 0.4, 0.35]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -559.74\n",
      "Episode 18 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2361.0, -2654.0, -4172.0, -174.0, -3398.0, -4426.0, -2391.0, -4210.0, -4374.0, -4181.0] \n",
      " [-2568.0, -3215.0, -4247.0, -683.0, -3763.0, -4467.0, -2658.0, -4373.0, -4396.0, -4080.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.33, 0.38, 0.29], [0.0, 0.28, 0.32, 0.4], [0.0, 0.22, 0.33, 0.46], [0.02, 0.33, 0.31, 0.33], [0.0, 0.26, 0.37, 0.38], [0.0, 0.26, 0.35, 0.39], [0.0, 0.25, 0.48, 0.27], [0.0, 0.22, 0.33, 0.45], [0.0, 0.24, 0.36, 0.39], [0.0, 0.27, 0.34, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -575.22\n",
      "Episode 19 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3976.0, -4397.0, -4169.0, -1075.0, -3817.0, -2573.0, -3932.0, -4276.0, -2565.0, -3215.0] \n",
      " [-3875.0, -4334.0, -4282.0, -1262.0, -3876.0, -3009.0, -3946.0, -4443.0, -2935.0, -3072.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.44, 0.27, 0.3], [0.0, 0.46, 0.23, 0.31], [0.0, 0.42, 0.24, 0.34], [0.0, 0.29, 0.4, 0.32], [0.0, 0.45, 0.25, 0.3], [0.0, 0.44, 0.28, 0.27], [0.0, 0.46, 0.26, 0.28], [0.0, 0.48, 0.23, 0.29], [0.0, 0.42, 0.28, 0.3], [0.0, 0.35, 0.27, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -561.38\n",
      "Episode 20 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1147.0, -4398.0, -1576.0, -4863.0, -3754.0, -4745.0, -4726.0, -333.0, -4252.0, -4388.0] \n",
      " [-1071.0, -4638.0, -2296.0, -4605.0, -3682.0, -4682.0, -4829.0, -892.0, -4226.0, -4368.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.2, 0.36, 0.44], [0.0, 0.21, 0.45, 0.33], [0.0, 0.31, 0.36, 0.33], [0.0, 0.27, 0.32, 0.41], [0.0, 0.22, 0.42, 0.36], [0.0, 0.24, 0.48, 0.28], [0.0, 0.21, 0.43, 0.36], [0.0, 0.35, 0.26, 0.39], [0.0, 0.26, 0.38, 0.36], [0.0, 0.23, 0.4, 0.37]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -584.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3761.0, -4501.0, -2706.0, -4435.0, -4493.0, -4176.0, -4163.0, -2728.0, -3550.0, -4284.0] \n",
      " [-3612.0, -4657.0, -2846.0, -4775.0, -4733.0, -4323.0, -4193.0, -2835.0, -3632.0, -4201.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.29, 0.36, 0.35], [0.0, 0.22, 0.33, 0.45], [0.0, 0.25, 0.38, 0.37], [0.0, 0.21, 0.35, 0.44], [0.0, 0.22, 0.37, 0.41], [0.0, 0.23, 0.33, 0.45], [0.0, 0.21, 0.35, 0.44], [0.0, 0.27, 0.37, 0.37], [0.0, 0.23, 0.37, 0.39], [0.0, 0.27, 0.34, 0.4]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -565.87\n",
      "Episode 22 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4357.0, -3573.0, -4338.0, -4313.0, -4534.0, -4598.0, -2467.0, -4475.0, -2832.0, -241.0] \n",
      " [-4212.0, -3878.0, -4249.0, -4340.0, -4551.0, -4288.0, -2536.0, -4436.0, -3303.0, -954.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.45, 0.24, 0.31], [0.0, 0.37, 0.28, 0.35], [0.0, 0.41, 0.25, 0.35], [0.0, 0.44, 0.22, 0.34], [0.0, 0.36, 0.28, 0.36], [0.0, 0.35, 0.28, 0.37], [0.0, 0.41, 0.32, 0.27], [0.0, 0.38, 0.26, 0.36], [0.0, 0.38, 0.28, 0.34], [0.01, 0.35, 0.33, 0.31]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -583.42\n",
      "Episode 23 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4194.0, -250.0, -4512.0, -3659.0, -4214.0, -924.0, -2801.0, -4372.0, -4294.0, -4239.0] \n",
      " [-4523.0, -933.0, -4292.0, -3829.0, -4264.0, -1352.0, -3245.0, -4264.0, -4366.0, -4214.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.24, 0.41, 0.35], [0.0, 0.29, 0.19, 0.52], [0.0, 0.21, 0.4, 0.4], [0.0, 0.28, 0.4, 0.32], [0.0, 0.23, 0.4, 0.37], [0.0, 0.32, 0.36, 0.32], [0.0, 0.29, 0.34, 0.38], [0.0, 0.25, 0.34, 0.41], [0.0, 0.23, 0.39, 0.37], [0.0, 0.24, 0.38, 0.37]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -565.74\n",
      "Episode 24 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4220.0, -4260.0, -1461.0, -329.0, -4344.0, -4266.0, -4288.0, -4245.0, -4377.0, -4238.0] \n",
      " [-4274.0, -4244.0, -1987.0, -953.0, -4282.0, -4293.0, -4209.0, -4252.0, -4528.0, -4347.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.24, 0.37, 0.4], [0.0, 0.26, 0.37, 0.37], [0.0, 0.28, 0.4, 0.33], [0.01, 0.42, 0.3, 0.27], [0.0, 0.27, 0.38, 0.36], [0.0, 0.27, 0.37, 0.36], [0.0, 0.26, 0.35, 0.4], [0.0, 0.26, 0.36, 0.38], [0.0, 0.24, 0.38, 0.37], [0.0, 0.27, 0.39, 0.34]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -561.31\n",
      "Episode 25 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2298.0, -10.0, -4264.0, -4269.0, -2524.0, -2028.0, -4466.0, -4276.0, -4423.0, -4357.0] \n",
      " [-2543.0, -708.0, -4364.0, -4492.0, -2535.0, -2355.0, -4457.0, -4174.0, -4491.0, -4169.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.36, 0.32, 0.32], [0.25, 0.25, 0.25, 0.25], [0.0, 0.39, 0.29, 0.32], [0.0, 0.4, 0.31, 0.29], [0.0, 0.37, 0.27, 0.36], [0.0, 0.33, 0.32, 0.35], [0.0, 0.4, 0.29, 0.31], [0.0, 0.38, 0.3, 0.32], [0.0, 0.4, 0.31, 0.28], [0.0, 0.38, 0.29, 0.32]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -571.7\n",
      "Episode 26 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1602.0, -4601.0, -4138.0, -4171.0, -4367.0, -4431.0, -4409.0, -721.0, -2940.0, -4536.0] \n",
      " [-2059.0, -5065.0, -4181.0, -4255.0, -4388.0, -4204.0, -4335.0, -1312.0, -3601.0, -4940.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.36, 0.3, 0.34], [0.0, 0.35, 0.26, 0.39], [0.0, 0.38, 0.26, 0.36], [0.0, 0.35, 0.26, 0.39], [0.0, 0.37, 0.26, 0.37], [0.0, 0.35, 0.27, 0.38], [0.0, 0.41, 0.27, 0.33], [0.0, 0.34, 0.34, 0.32], [0.0, 0.37, 0.29, 0.34], [0.0, 0.36, 0.27, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -567.9\n",
      "Episode 27 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4410.0, -4373.0, -4218.0, -4469.0, -4346.0, -4196.0, -2946.0, -3640.0, -4525.0, -4463.0] \n",
      " [-4315.0, -4414.0, -4381.0, -4285.0, -4551.0, -4437.0, -3182.0, -3763.0, -4286.0, -4631.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.39, 0.33, 0.29], [0.0, 0.38, 0.32, 0.31], [0.0, 0.38, 0.3, 0.32], [0.0, 0.38, 0.31, 0.32], [0.0, 0.38, 0.3, 0.32], [0.0, 0.38, 0.29, 0.34], [0.0, 0.39, 0.31, 0.3], [0.0, 0.39, 0.32, 0.29], [0.0, 0.38, 0.3, 0.32], [0.0, 0.39, 0.32, 0.28]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -584.35\n",
      "Episode 28 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4391.0, -4461.0, -4164.0, -4515.0, -3773.0, -4866.0, -3974.0, -4350.0, -2326.0, -4443.0] \n",
      " [-4460.0, -4508.0, -4217.0, -4428.0, -4128.0, -4698.0, -4506.0, -4399.0, -2664.0, -4598.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.41, 0.23, 0.36], [0.0, 0.37, 0.25, 0.38], [0.0, 0.39, 0.24, 0.37], [0.0, 0.43, 0.2, 0.37], [0.0, 0.39, 0.24, 0.36], [0.0, 0.45, 0.22, 0.33], [0.0, 0.4, 0.24, 0.36], [0.0, 0.4, 0.24, 0.36], [0.0, 0.39, 0.28, 0.32], [0.0, 0.4, 0.22, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -578.28\n",
      "Episode 29 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10.0, -4547.0, -4460.0, -4547.0, -4460.0, -4303.0, -4548.0, -539.0, -3763.0, -4384.0] \n",
      " [-867.0, -4641.0, -4814.0, -4594.0, -4320.0, -4323.0, -5024.0, -1364.0, -3810.0, -4165.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.25, 0.25, 0.25], [0.0, 0.25, 0.44, 0.31], [0.0, 0.23, 0.42, 0.35], [0.0, 0.26, 0.4, 0.34], [0.0, 0.23, 0.42, 0.35], [0.0, 0.23, 0.42, 0.35], [0.0, 0.22, 0.43, 0.35], [0.0, 0.27, 0.18, 0.54], [0.0, 0.24, 0.42, 0.34], [0.0, 0.23, 0.42, 0.35]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -589.98\n",
      "Episode 30 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4533.0, -4209.0, -4464.0, -1981.0, -4260.0, -4473.0, -4301.0, -4166.0, -4408.0, -4257.0] \n",
      " [-4472.0, -4323.0, -4233.0, -2158.0, -4256.0, -4342.0, -4258.0, -4185.0, -4347.0, -4365.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.48, 0.3, 0.22], [0.0, 0.48, 0.26, 0.25], [0.0, 0.49, 0.28, 0.24], [0.0, 0.42, 0.32, 0.26], [0.0, 0.47, 0.28, 0.25], [0.0, 0.49, 0.27, 0.23], [0.0, 0.49, 0.27, 0.23], [0.0, 0.46, 0.3, 0.23], [0.0, 0.5, 0.27, 0.23], [0.0, 0.48, 0.27, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -561.91\n",
      "Episode 31 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2619.0, -4314.0, -1583.0, -3737.0, -4262.0, -3216.0, -3322.0, -2421.0, -1609.0, -2651.0] \n",
      " [-2811.0, -4509.0, -1711.0, -3862.0, -4146.0, -3340.0, -3576.0, -2639.0, -2127.0, -2911.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.38, 0.29, 0.33], [0.0, 0.35, 0.28, 0.37], [0.0, 0.31, 0.35, 0.34], [0.0, 0.39, 0.26, 0.35], [0.0, 0.41, 0.25, 0.34], [0.0, 0.36, 0.32, 0.32], [0.0, 0.38, 0.29, 0.33], [0.0, 0.36, 0.29, 0.35], [0.0, 0.37, 0.31, 0.32], [0.0, 0.35, 0.31, 0.35]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -568.67\n",
      "Episode 32 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4128.0, -1448.0, -4383.0, -2146.0, -4118.0, -4269.0, -4559.0, -4375.0, -4228.0, -3392.0] \n",
      " [-4290.0, -1994.0, -4391.0, -2267.0, -4173.0, -4399.0, -4439.0, -4484.0, -4597.0, -3671.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.34, 0.38, 0.28], [0.0, 0.43, 0.25, 0.32], [0.0, 0.34, 0.38, 0.29], [0.0, 0.37, 0.36, 0.27], [0.0, 0.34, 0.36, 0.3], [0.0, 0.37, 0.35, 0.28], [0.0, 0.35, 0.35, 0.3], [0.0, 0.34, 0.36, 0.3], [0.0, 0.34, 0.37, 0.29], [0.0, 0.34, 0.37, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -540.05\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2782.0, -3462.0, -3291.0, -4357.0, -3449.0, -4273.0, -4325.0, -4250.0, -4353.0, -4305.0] \n",
      " [-3033.0, -3588.0, -3406.0, -4137.0, -3641.0, -4288.0, -4346.0, -4707.0, -4294.0, -4278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.4, 0.33, 0.27], [0.0, 0.41, 0.33, 0.25], [0.0, 0.41, 0.34, 0.25], [0.0, 0.43, 0.32, 0.25], [0.0, 0.42, 0.34, 0.24], [0.0, 0.43, 0.33, 0.25], [0.0, 0.43, 0.33, 0.24], [0.0, 0.43, 0.34, 0.23], [0.0, 0.43, 0.33, 0.24], [0.0, 0.44, 0.32, 0.24]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -569.58\n",
      "Episode 34 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4569.0, -4271.0, -4729.0, -4424.0, -4450.0, -4491.0, -4544.0, -4439.0, -4569.0, -2178.0] \n",
      " [-4804.0, -4557.0, -4584.0, -4508.0, -4594.0, -4643.0, -4785.0, -4285.0, -4804.0, -2447.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.33, 0.34, 0.32], [0.0, 0.25, 0.39, 0.36], [0.0, 0.2, 0.46, 0.34], [0.0, 0.23, 0.43, 0.34], [0.0, 0.21, 0.42, 0.38], [0.0, 0.24, 0.4, 0.36], [0.0, 0.26, 0.3, 0.44], [0.0, 0.22, 0.41, 0.38], [0.0, 0.33, 0.34, 0.32], [0.0, 0.26, 0.41, 0.32]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -580.69\n",
      "Episode 35 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4381.0, -4374.0, -4338.0, -2768.0, -4538.0, -4622.0, -4557.0, -4350.0, -4829.0, -4489.0] \n",
      " [-4289.0, -4195.0, -4426.0, -3299.0, -4761.0, -4597.0, -4788.0, -4373.0, -4730.0, -4741.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.41, 0.27, 0.32], [0.0, 0.39, 0.27, 0.34], [0.0, 0.38, 0.27, 0.35], [0.0, 0.37, 0.32, 0.31], [0.0, 0.39, 0.28, 0.34], [0.0, 0.39, 0.29, 0.32], [0.0, 0.39, 0.27, 0.35], [0.0, 0.4, 0.26, 0.34], [0.0, 0.55, 0.16, 0.29], [0.0, 0.43, 0.24, 0.33]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -567.56\n",
      "Episode 36 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4244.0, -2435.0, -4284.0, -1643.0, -2895.0, -4250.0, -4217.0, -4344.0, -2166.0, -3001.0] \n",
      " [-4313.0, -2698.0, -4487.0, -1924.0, -2877.0, -4272.0, -4348.0, -4387.0, -2592.0, -2908.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.38, 0.32, 0.29], [0.0, 0.37, 0.33, 0.3], [0.0, 0.38, 0.32, 0.29], [0.0, 0.38, 0.35, 0.28], [0.0, 0.38, 0.34, 0.28], [0.0, 0.39, 0.32, 0.3], [0.0, 0.4, 0.31, 0.3], [0.0, 0.41, 0.31, 0.28], [0.0, 0.38, 0.31, 0.31], [0.0, 0.35, 0.35, 0.3]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -561.27\n",
      "Episode 37 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2754.0, -4405.0, -4347.0, -4280.0, -4519.0, -4356.0, -4393.0, -4341.0, -4431.0, -4241.0] \n",
      " [-3227.0, -4339.0, -4398.0, -4143.0, -4902.0, -4527.0, -4594.0, -4300.0, -4454.0, -4390.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.35, 0.35, 0.29], [0.0, 0.39, 0.35, 0.26], [0.0, 0.38, 0.36, 0.27], [0.0, 0.38, 0.35, 0.28], [0.0, 0.4, 0.33, 0.26], [0.0, 0.37, 0.35, 0.27], [0.0, 0.38, 0.34, 0.28], [0.0, 0.36, 0.36, 0.28], [0.0, 0.38, 0.35, 0.27], [0.0, 0.37, 0.34, 0.28]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -598.18\n",
      "Episode 38 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4440.0, -4369.0, -4651.0, -4285.0, -4357.0, -2552.0, -4339.0, -11.0, -4616.0, -4372.0] \n",
      " [-4594.0, -4110.0, -4174.0, -4124.0, -4276.0, -2530.0, -4394.0, -701.0, -4742.0, -4413.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.41, 0.35, 0.24], [0.0, 0.41, 0.37, 0.22], [0.0, 0.38, 0.33, 0.28], [0.0, 0.4, 0.36, 0.24], [0.0, 0.39, 0.37, 0.24], [0.0, 0.37, 0.36, 0.27], [0.0, 0.4, 0.36, 0.25], [0.25, 0.25, 0.25, 0.25], [0.0, 0.45, 0.33, 0.22], [0.0, 0.41, 0.35, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -576.81\n",
      "Episode 39 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4198.0, -2683.0, -4110.0, -4383.0, -1665.0, -4139.0, -4502.0, -4101.0, -97.0, -4202.0] \n",
      " [-4489.0, -2941.0, -4153.0, -4258.0, -2657.0, -4175.0, -4525.0, -4113.0, -653.0, -4045.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.24, 0.38, 0.38], [0.0, 0.27, 0.37, 0.36], [0.0, 0.26, 0.37, 0.37], [0.0, 0.24, 0.4, 0.36], [0.0, 0.3, 0.36, 0.33], [0.0, 0.24, 0.39, 0.38], [0.0, 0.23, 0.43, 0.34], [0.0, 0.25, 0.37, 0.38], [0.05, 0.32, 0.31, 0.32], [0.0, 0.23, 0.39, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -571.38\n",
      "Episode 40 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1344.0, -1167.0, -4193.0, -4235.0, -4295.0, -2481.0, -674.0, -4256.0, -4253.0, -4206.0] \n",
      " [-1884.0, -1544.0, -4384.0, -4314.0, -4292.0, -2637.0, -968.0, -4217.0, -4169.0, -4124.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.32, 0.35, 0.33], [0.0, 0.32, 0.36, 0.32], [0.0, 0.27, 0.39, 0.34], [0.0, 0.27, 0.4, 0.34], [0.0, 0.25, 0.41, 0.34], [0.0, 0.29, 0.37, 0.33], [0.0, 0.36, 0.42, 0.23], [0.0, 0.25, 0.41, 0.34], [0.0, 0.26, 0.41, 0.33], [0.0, 0.27, 0.4, 0.33]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -540.89\n",
      "Episode 41 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3664.0, -4182.0, -950.0, -1946.0, -2958.0, -4344.0, -550.0, -4284.0, -4313.0, -4219.0] \n",
      " [-3833.0, -4273.0, -1363.0, -2476.0, -3577.0, -4086.0, -1140.0, -4481.0, -4197.0, -4211.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.25, 0.37, 0.38], [0.0, 0.25, 0.35, 0.39], [0.0, 0.4, 0.42, 0.18], [0.0, 0.3, 0.35, 0.36], [0.0, 0.28, 0.35, 0.37], [0.0, 0.25, 0.37, 0.38], [0.0, 0.33, 0.42, 0.25], [0.0, 0.25, 0.36, 0.39], [0.0, 0.23, 0.39, 0.38], [0.0, 0.24, 0.38, 0.37]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -553.17\n",
      "Episode 42 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4342.0, -4485.0, -4310.0, -3497.0, -4400.0, -4327.0, -4383.0, -4359.0, -1659.0, -1917.0] \n",
      " [-4232.0, -4469.0, -4614.0, -3514.0, -4501.0, -4346.0, -4512.0, -4325.0, -2236.0, -2476.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.25, 0.34, 0.41], [0.0, 0.25, 0.34, 0.41], [0.0, 0.26, 0.35, 0.39], [0.0, 0.3, 0.33, 0.37], [0.0, 0.23, 0.38, 0.4], [0.0, 0.25, 0.35, 0.4], [0.0, 0.26, 0.36, 0.38], [0.0, 0.25, 0.34, 0.41], [0.0, 0.32, 0.34, 0.34], [0.0, 0.3, 0.34, 0.36]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -568.29\n",
      "Episode 43 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3862.0, -4303.0, -4261.0, -3012.0, -4277.0, -4263.0, -2556.0, -4436.0, -4314.0, -4380.0] \n",
      " [-4138.0, -4371.0, -4292.0, -3070.0, -4171.0, -4312.0, -3168.0, -4343.0, -4239.0, -4385.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.39, 0.31, 0.31], [0.0, 0.39, 0.3, 0.3], [0.0, 0.4, 0.3, 0.3], [0.0, 0.34, 0.34, 0.32], [0.0, 0.39, 0.31, 0.3], [0.0, 0.4, 0.3, 0.3], [0.0, 0.33, 0.35, 0.32], [0.0, 0.41, 0.28, 0.31], [0.0, 0.4, 0.31, 0.29], [0.0, 0.38, 0.31, 0.31]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -553.18\n",
      "Episode 44 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1051.0, -4243.0, -3263.0, -4235.0, -312.0, -4213.0, -4305.0, -4151.0, -4176.0, -4346.0] \n",
      " [-1410.0, -4136.0, -3459.0, -4254.0, -1130.0, -4112.0, -4275.0, -4332.0, -4143.0, -4369.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.34, 0.38, 0.27], [0.0, 0.39, 0.37, 0.25], [0.0, 0.37, 0.36, 0.27], [0.0, 0.39, 0.36, 0.25], [0.01, 0.37, 0.21, 0.41], [0.0, 0.38, 0.37, 0.26], [0.0, 0.38, 0.36, 0.26], [0.0, 0.38, 0.36, 0.26], [0.0, 0.38, 0.37, 0.25], [0.0, 0.39, 0.36, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -574.85\n",
      "Episode 45 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4302.0, -4416.0, -4538.0, -4328.0, -4510.0, -4244.0, -4423.0, -3263.0, -4330.0, -1986.0] \n",
      " [-4385.0, -4313.0, -4674.0, -4368.0, -4283.0, -4318.0, -4240.0, -3614.0, -4710.0, -2272.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.41, 0.34, 0.25], [0.0, 0.4, 0.36, 0.23], [0.0, 0.35, 0.39, 0.26], [0.0, 0.41, 0.34, 0.25], [0.0, 0.43, 0.33, 0.24], [0.0, 0.41, 0.34, 0.25], [0.0, 0.4, 0.33, 0.26], [0.0, 0.39, 0.34, 0.27], [0.0, 0.42, 0.34, 0.24], [0.0, 0.37, 0.34, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -580.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4305.0, -4223.0, -3215.0, -4197.0, -4348.0, -2482.0, -4352.0, -4255.0, -4331.0, -4381.0] \n",
      " [-4317.0, -4211.0, -3830.0, -4256.0, -4110.0, -2756.0, -4411.0, -4343.0, -4392.0, -4166.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.31, 0.3, 0.4], [0.0, 0.31, 0.3, 0.39], [0.0, 0.3, 0.31, 0.39], [0.0, 0.29, 0.3, 0.41], [0.0, 0.32, 0.29, 0.39], [0.0, 0.31, 0.32, 0.37], [0.0, 0.31, 0.3, 0.39], [0.0, 0.3, 0.3, 0.4], [0.0, 0.29, 0.3, 0.4], [0.0, 0.31, 0.29, 0.4]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -584.92\n",
      "Episode 47 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4345.0, -1525.0, -4271.0, -4307.0, -4375.0, -4232.0, -4377.0, -4526.0, -2677.0, -4322.0] \n",
      " [-4286.0, -2051.0, -4239.0, -4234.0, -4398.0, -4180.0, -4426.0, -4516.0, -2936.0, -4325.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.36, 0.31, 0.33], [0.0, 0.35, 0.35, 0.3], [0.0, 0.38, 0.3, 0.33], [0.0, 0.36, 0.31, 0.33], [0.0, 0.36, 0.32, 0.32], [0.0, 0.37, 0.31, 0.33], [0.0, 0.36, 0.3, 0.33], [0.0, 0.39, 0.29, 0.32], [0.0, 0.36, 0.32, 0.33], [0.0, 0.36, 0.32, 0.32]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -569.2\n",
      "Episode 48 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4271.0, -2572.0, -4389.0, -4343.0, -4279.0, -3981.0, -4279.0, -4380.0, -4317.0, -4305.0] \n",
      " [-4259.0, -3039.0, -4356.0, -4401.0, -3999.0, -4328.0, -3999.0, -4436.0, -4396.0, -4149.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.38, 0.36, 0.26], [0.0, 0.37, 0.35, 0.29], [0.0, 0.38, 0.36, 0.26], [0.0, 0.37, 0.36, 0.27], [0.0, 0.38, 0.36, 0.26], [0.0, 0.36, 0.39, 0.26], [0.0, 0.38, 0.36, 0.26], [0.0, 0.4, 0.36, 0.25], [0.0, 0.39, 0.36, 0.25], [0.0, 0.39, 0.36, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -568.69\n",
      "Episode 49 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2527.0, -4218.0, -347.0, -2647.0, -1124.0, -4295.0, -4332.0, -4401.0, -4327.0, -4264.0] \n",
      " [-2832.0, -4265.0, -827.0, -2987.0, -1367.0, -4257.0, -4604.0, -4303.0, -4251.0, -4310.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.39, 0.28, 0.33], [0.0, 0.41, 0.25, 0.34], [0.05, 0.27, 0.31, 0.36], [0.0, 0.39, 0.28, 0.33], [0.0, 0.34, 0.38, 0.28], [0.0, 0.41, 0.25, 0.34], [0.0, 0.4, 0.26, 0.34], [0.0, 0.42, 0.24, 0.34], [0.0, 0.4, 0.26, 0.34], [0.0, 0.41, 0.26, 0.33]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -581.22\n",
      "Episode 50 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4406.0, -4309.0, -3911.0, -4429.0, -4440.0, -2188.0, -4391.0, -4326.0, -2845.0, -4456.0] \n",
      " [-4233.0, -4246.0, -4117.0, -4185.0, -4389.0, -2630.0, -4357.0, -4347.0, -3068.0, -4377.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.37, 0.31, 0.32], [0.0, 0.38, 0.3, 0.32], [0.0, 0.38, 0.31, 0.31], [0.0, 0.39, 0.31, 0.3], [0.0, 0.37, 0.31, 0.32], [0.0, 0.36, 0.33, 0.32], [0.0, 0.38, 0.31, 0.31], [0.0, 0.39, 0.3, 0.31], [0.0, 0.36, 0.32, 0.32], [0.0, 0.39, 0.3, 0.31]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -568.46\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 51 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3508.0, -2908.0, -4383.0, -581.0, -4322.0, -4400.0, -4336.0, -425.0, -4379.0, -1721.0] \n",
      " [-3775.0, -3113.0, -4226.0, -1265.0, -4408.0, -4417.0, -4204.0, -877.0, -4654.0, -2133.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.24, 0.34, 0.41], [0.0, 0.26, 0.35, 0.39], [0.0, 0.23, 0.35, 0.42], [0.02, 0.39, 0.3, 0.29], [0.0, 0.23, 0.36, 0.42], [0.0, 0.22, 0.35, 0.43], [0.0, 0.23, 0.36, 0.41], [0.04, 0.36, 0.3, 0.29], [0.0, 0.22, 0.36, 0.42], [0.0, 0.3, 0.34, 0.36]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -559.2\n",
      "Episode 52 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4515.0, -549.0, -4578.0, -4367.0, -4423.0, -3274.0, -413.0, -1655.0, -4275.0, -4323.0] \n",
      " [-4298.0, -1112.0, -4622.0, -4347.0, -4495.0, -3301.0, -915.0, -2004.0, -4329.0, -4303.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.29, 0.33, 0.38], [0.09, 0.32, 0.29, 0.3], [0.0, 0.35, 0.29, 0.36], [0.0, 0.33, 0.28, 0.39], [0.0, 0.36, 0.26, 0.38], [0.0, 0.34, 0.29, 0.37], [0.03, 0.22, 0.23, 0.53], [0.0, 0.34, 0.31, 0.34], [0.0, 0.35, 0.28, 0.37], [0.0, 0.34, 0.28, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -527.18\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 53 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3894.0, -4674.0, -4422.0, -4453.0, -4577.0, -4415.0, -3841.0, -862.0, -1305.0, -4415.0] \n",
      " [-3719.0, -4674.0, -4216.0, -4394.0, -4282.0, -4283.0, -3828.0, -1556.0, -2086.0, -4283.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.35, 0.43, 0.22], [0.0, 0.39, 0.37, 0.24], [0.01, 0.33, 0.44, 0.22], [0.01, 0.33, 0.45, 0.21], [0.0, 0.38, 0.41, 0.21], [0.0, 0.33, 0.46, 0.22], [0.01, 0.37, 0.39, 0.23], [0.14, 0.3, 0.3, 0.26], [0.02, 0.26, 0.35, 0.37], [0.0, 0.33, 0.46, 0.22]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -577.24\n",
      "Episode 54 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-644.0, -436.0, -859.0, -640.0, -786.0, -13.0, -1014.0, -672.0, -222.0, -1199.0] \n",
      " [-746.0, -877.0, -697.0, -797.0, -726.0, -588.0, -1289.0, -882.0, -661.0, -870.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.12, 0.3, 0.3, 0.28], [0.15, 0.28, 0.24, 0.33], [0.13, 0.18, 0.16, 0.53], [0.11, 0.33, 0.24, 0.32], [0.49, 0.2, 0.09, 0.23], [0.26, 0.25, 0.25, 0.25], [0.27, 0.2, 0.23, 0.31], [0.35, 0.17, 0.25, 0.23], [0.19, 0.27, 0.2, 0.34], [0.09, 0.38, 0.17, 0.37]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -145.08\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 55 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1221.0, -784.0, -707.0, -1223.0, -1229.0, -775.0, -1024.0, -610.0, -1348.0, -1519.0] \n",
      " [-1318.0, -1471.0, -1329.0, -1367.0, -1382.0, -1411.0, -1050.0, -988.0, -1066.0, -1455.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.28, 0.32, 0.25, 0.15], [0.15, 0.42, 0.24, 0.19], [0.25, 0.27, 0.21, 0.28], [0.11, 0.15, 0.27, 0.48], [0.26, 0.19, 0.29, 0.25], [0.22, 0.28, 0.28, 0.22], [0.28, 0.11, 0.25, 0.36], [0.31, 0.3, 0.23, 0.16], [0.36, 0.21, 0.26, 0.17], [0.17, 0.31, 0.29, 0.24]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.24 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -165.44\n",
      "Episode 56 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-879.0, -1327.0, -290.0, -501.0, -726.0, -651.0, -1530.0, -943.0, -1541.0, -501.0] \n",
      " [-614.0, -1461.0, -1094.0, -618.0, -612.0, -649.0, -1453.0, -1003.0, -1573.0, -582.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.21, 0.23, 0.35, 0.21], [0.23, 0.25, 0.24, 0.29], [0.25, 0.24, 0.27, 0.24], [0.26, 0.22, 0.24, 0.27], [0.24, 0.3, 0.16, 0.29], [0.2, 0.24, 0.31, 0.25], [0.21, 0.15, 0.24, 0.4], [0.33, 0.21, 0.24, 0.21], [0.23, 0.26, 0.29, 0.22], [0.26, 0.23, 0.27, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.24 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -171.03\n",
      "Episode 57 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1484.0, -1150.0, -139.0, -2256.0, -351.0, -569.0, -984.0, -401.0, -1326.0, -1211.0] \n",
      " [-1812.0, -1485.0, -641.0, -1060.0, -939.0, -800.0, -890.0, -1243.0, -1621.0, -1502.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.4, 0.28, 0.16, 0.16], [0.16, 0.44, 0.19, 0.2], [0.3, 0.24, 0.24, 0.22], [0.37, 0.21, 0.28, 0.14], [0.2, 0.26, 0.28, 0.27], [0.32, 0.26, 0.23, 0.19], [0.23, 0.34, 0.18, 0.25], [0.27, 0.3, 0.21, 0.22], [0.31, 0.3, 0.24, 0.14], [0.27, 0.34, 0.15, 0.24]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.24 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -162.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 58 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-479.0, -1390.0, -1018.0, -2013.0, -1195.0, -1142.0, -1164.0, -1380.0, -1134.0, -738.0] \n",
      " [-803.0, -988.0, -867.0, -1476.0, -1269.0, -944.0, -1006.0, -1532.0, -1049.0, -1381.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.3, 0.22, 0.23, 0.25], [0.22, 0.29, 0.22, 0.27], [0.23, 0.3, 0.23, 0.25], [0.36, 0.27, 0.15, 0.22], [0.35, 0.19, 0.23, 0.23], [0.24, 0.37, 0.24, 0.15], [0.34, 0.19, 0.27, 0.2], [0.32, 0.31, 0.13, 0.24], [0.31, 0.19, 0.25, 0.24], [0.35, 0.24, 0.22, 0.19]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.24 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -171.19\n",
      "Episode 59 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1724.0, -870.0, -963.0, -867.0, -643.0, -975.0, -623.0, -537.0, -709.0, -1730.0] \n",
      " [-1138.0, -910.0, -994.0, -865.0, -804.0, -1527.0, -686.0, -956.0, -695.0, -1545.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.48, 0.24, 0.14, 0.14], [0.25, 0.26, 0.26, 0.23], [0.23, 0.29, 0.21, 0.28], [0.25, 0.28, 0.2, 0.27], [0.23, 0.3, 0.25, 0.22], [0.33, 0.28, 0.12, 0.28], [0.26, 0.23, 0.22, 0.29], [0.27, 0.26, 0.23, 0.23], [0.2, 0.32, 0.27, 0.22], [0.12, 0.28, 0.23, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -155.18\n",
      "Episode 60 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-498.0, -369.0, -618.0, -697.0, -1053.0, -456.0, -618.0, -1125.0, -1259.0, -795.0] \n",
      " [-614.0, -811.0, -1231.0, -962.0, -753.0, -534.0, -1231.0, -1001.0, -863.0, -669.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.24, 0.22, 0.28, 0.26], [0.27, 0.25, 0.25, 0.23], [0.31, 0.22, 0.24, 0.23], [0.28, 0.27, 0.17, 0.28], [0.28, 0.17, 0.22, 0.34], [0.3, 0.25, 0.24, 0.22], [0.31, 0.22, 0.24, 0.23], [0.32, 0.14, 0.27, 0.27], [0.41, 0.16, 0.16, 0.27], [0.2, 0.37, 0.23, 0.2]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -146.64\n",
      "Episode 61 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-781.0, -529.0, -1476.0, -602.0, -441.0, -508.0, -758.0, -1084.0, -816.0, -1089.0] \n",
      " [-862.0, -969.0, -1432.0, -957.0, -1224.0, -1104.0, -1123.0, -1246.0, -900.0, -1130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.25, 0.27, 0.23], [0.25, 0.26, 0.22, 0.27], [0.26, 0.25, 0.28, 0.21], [0.21, 0.31, 0.23, 0.25], [0.25, 0.28, 0.24, 0.23], [0.27, 0.23, 0.26, 0.23], [0.21, 0.26, 0.22, 0.31], [0.23, 0.3, 0.19, 0.28], [0.32, 0.22, 0.25, 0.2], [0.26, 0.21, 0.31, 0.22]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -147.95\n",
      "Episode 62 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-959.0, -1034.0, -445.0, -1007.0, -779.0, -570.0, -1007.0, -765.0, -961.0, -1375.0] \n",
      " [-815.0, -751.0, -684.0, -1415.0, -964.0, -858.0, -1415.0, -1094.0, -1013.0, -1304.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.26, 0.18, 0.34, 0.23], [0.2, 0.19, 0.33, 0.29], [0.26, 0.26, 0.25, 0.22], [0.24, 0.21, 0.22, 0.34], [0.22, 0.21, 0.29, 0.29], [0.18, 0.23, 0.32, 0.28], [0.24, 0.21, 0.22, 0.34], [0.28, 0.2, 0.3, 0.22], [0.25, 0.2, 0.22, 0.33], [0.23, 0.26, 0.28, 0.22]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.26 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -150.98\n",
      "Episode 63 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-768.0, -1155.0, -936.0, -1281.0, -974.0, -359.0, -865.0, -516.0, -608.0, -1154.0] \n",
      " [-1133.0, -673.0, -769.0, -1387.0, -1046.0, -978.0, -962.0, -868.0, -827.0, -1040.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.26, 0.27, 0.27, 0.2], [0.22, 0.2, 0.31, 0.27], [0.23, 0.21, 0.34, 0.22], [0.2, 0.18, 0.31, 0.3], [0.24, 0.24, 0.23, 0.28], [0.26, 0.25, 0.27, 0.22], [0.21, 0.31, 0.29, 0.19], [0.24, 0.25, 0.26, 0.25], [0.22, 0.26, 0.27, 0.25], [0.22, 0.27, 0.23, 0.28]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -141.22\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC4test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 64 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-496.0, -998.0, -896.0, -935.0, -933.0, -1014.0, -1041.0, -754.0, -777.0, -636.0] \n",
      " [-837.0, -1400.0, -977.0, -731.0, -824.0, -1101.0, -1113.0, -1039.0, -1118.0, -914.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.32, 0.21, 0.21, 0.26], [0.3, 0.19, 0.24, 0.28], [0.28, 0.23, 0.28, 0.21], [0.31, 0.24, 0.19, 0.26], [0.28, 0.26, 0.18, 0.27], [0.26, 0.26, 0.17, 0.3], [0.27, 0.22, 0.23, 0.28], [0.29, 0.23, 0.27, 0.22], [0.31, 0.27, 0.18, 0.24], [0.32, 0.25, 0.2, 0.22]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -155.33\n",
      "Episode 65 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-701.0, -1149.0, -818.0, -1154.0, -290.0, -968.0, -2222.0, -589.0, -383.0, -1486.0] \n",
      " [-709.0, -1118.0, -1052.0, -1027.0, -990.0, -915.0, -1753.0, -1065.0, -831.0, -1070.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.23, 0.22, 0.23, 0.32], [0.24, 0.19, 0.3, 0.27], [0.23, 0.26, 0.23, 0.27], [0.23, 0.22, 0.2, 0.35], [0.26, 0.23, 0.26, 0.25], [0.3, 0.15, 0.27, 0.29], [0.18, 0.3, 0.23, 0.29], [0.28, 0.26, 0.22, 0.24], [0.24, 0.26, 0.24, 0.27], [0.24, 0.17, 0.28, 0.31]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -164.62\n",
      "Episode 66 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1547.0, -444.0, -1556.0, -1196.0, -1376.0, -1035.0, -1196.0, -585.0, -708.0, -2806.0] \n",
      " [-1204.0, -894.0, -1241.0, -1358.0, -1170.0, -1283.0, -1358.0, -694.0, -952.0, -2284.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.27, 0.2, 0.26, 0.28], [0.26, 0.24, 0.23, 0.27], [0.24, 0.23, 0.18, 0.34], [0.27, 0.18, 0.31, 0.24], [0.37, 0.22, 0.19, 0.23], [0.25, 0.24, 0.27, 0.23], [0.27, 0.18, 0.31, 0.24], [0.25, 0.29, 0.21, 0.25], [0.28, 0.22, 0.24, 0.26], [0.32, 0.33, 0.19, 0.16]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -175.31\n",
      "Episode 67 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-697.0, -1434.0, -765.0, -1155.0, -1623.0, -1327.0, -1511.0, -1294.0, -896.0, -490.0] \n",
      " [-751.0, -958.0, -1371.0, -1200.0, -1292.0, -871.0, -1166.0, -1094.0, -1265.0, -736.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.26, 0.25, 0.25], [0.23, 0.28, 0.31, 0.18], [0.27, 0.2, 0.28, 0.25], [0.29, 0.34, 0.2, 0.17], [0.32, 0.29, 0.23, 0.17], [0.23, 0.24, 0.31, 0.21], [0.25, 0.17, 0.25, 0.34], [0.19, 0.33, 0.31, 0.17], [0.22, 0.25, 0.25, 0.28], [0.29, 0.25, 0.25, 0.21]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -163.49\n",
      "Episode 68 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-902.0, -803.0, -1210.0, -737.0, -358.0, -461.0, -665.0, -555.0, -746.0, -317.0] \n",
      " [-911.0, -889.0, -1146.0, -754.0, -682.0, -716.0, -691.0, -726.0, -770.0, -515.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.22, 0.29, 0.3, 0.19], [0.25, 0.26, 0.25, 0.24], [0.23, 0.19, 0.31, 0.27], [0.23, 0.32, 0.25, 0.21], [0.24, 0.24, 0.25, 0.27], [0.23, 0.24, 0.28, 0.24], [0.23, 0.26, 0.25, 0.25], [0.25, 0.23, 0.23, 0.29], [0.24, 0.21, 0.27, 0.28], [0.24, 0.23, 0.26, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -153.25\n",
      "Episode 69 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-879.0, -1292.0, -611.0, -969.0, -1211.0, -1252.0, -1209.0, -1535.0, -618.0, -1038.0] \n",
      " [-1161.0, -1047.0, -1036.0, -1388.0, -1224.0, -1023.0, -1116.0, -1180.0, -658.0, -698.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.24, 0.2, 0.3, 0.26], [0.21, 0.22, 0.31, 0.26], [0.24, 0.21, 0.29, 0.26], [0.28, 0.26, 0.23, 0.23], [0.24, 0.23, 0.28, 0.24], [0.32, 0.18, 0.24, 0.26], [0.23, 0.18, 0.22, 0.36], [0.28, 0.17, 0.25, 0.31], [0.24, 0.26, 0.25, 0.24], [0.2, 0.25, 0.33, 0.22]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -172.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-898.0, -345.0, -1139.0, -1073.0, -1029.0, -690.0, -374.0, -1171.0, -610.0, -932.0] \n",
      " [-1045.0, -660.0, -992.0, -1086.0, -931.0, -722.0, -672.0, -1037.0, -1004.0, -792.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.27, 0.2, 0.28, 0.26], [0.27, 0.25, 0.24, 0.24], [0.25, 0.21, 0.28, 0.26], [0.23, 0.2, 0.28, 0.29], [0.34, 0.15, 0.27, 0.25], [0.3, 0.22, 0.24, 0.24], [0.25, 0.24, 0.24, 0.27], [0.31, 0.25, 0.23, 0.2], [0.28, 0.25, 0.24, 0.24], [0.29, 0.26, 0.25, 0.2]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -142.6\n",
      "Episode 71 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-640.0, -282.0, -1038.0, -887.0, -1074.0, -631.0, -1263.0, -573.0, -371.0, -454.0] \n",
      " [-1089.0, -730.0, -1475.0, -950.0, -1184.0, -1104.0, -1920.0, -715.0, -1041.0, -752.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.26, 0.23, 0.22, 0.29], [0.27, 0.24, 0.25, 0.23], [0.23, 0.19, 0.26, 0.32], [0.23, 0.27, 0.22, 0.28], [0.26, 0.17, 0.26, 0.31], [0.27, 0.25, 0.24, 0.24], [0.35, 0.22, 0.21, 0.22], [0.26, 0.21, 0.26, 0.27], [0.26, 0.24, 0.26, 0.25], [0.27, 0.24, 0.24, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -165.57\n",
      "Episode 72 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1458.0, -872.0, -626.0, -770.0, -625.0, -843.0, -1201.0, -971.0, -1028.0, -1146.0] \n",
      " [-1142.0, -1122.0, -754.0, -1110.0, -814.0, -1016.0, -1182.0, -1043.0, -851.0, -1029.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.23, 0.19, 0.28, 0.29], [0.26, 0.24, 0.24, 0.26], [0.23, 0.25, 0.24, 0.28], [0.25, 0.25, 0.24, 0.25], [0.24, 0.22, 0.26, 0.28], [0.21, 0.27, 0.28, 0.23], [0.19, 0.28, 0.27, 0.26], [0.22, 0.25, 0.27, 0.26], [0.19, 0.28, 0.26, 0.27], [0.21, 0.2, 0.33, 0.26]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -145.73\n",
      "Episode 73 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-612.0, -985.0, -372.0, -1196.0, -1097.0, -922.0, -1215.0, -1225.0, -577.0, -469.0] \n",
      " [-951.0, -790.0, -640.0, -1347.0, -704.0, -1046.0, -1201.0, -1446.0, -1020.0, -891.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.23, 0.28, 0.23, 0.26], [0.25, 0.23, 0.29, 0.23], [0.23, 0.25, 0.26, 0.25], [0.23, 0.21, 0.31, 0.24], [0.24, 0.19, 0.32, 0.25], [0.24, 0.28, 0.29, 0.2], [0.21, 0.26, 0.27, 0.27], [0.21, 0.26, 0.27, 0.26], [0.26, 0.25, 0.22, 0.27], [0.25, 0.23, 0.24, 0.28]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Average Reward for Agent 0 this episode : -154.07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-60e49fdaf1fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSingle_Cross_Triple4_MultiAC_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\MasterAC_Agent.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, number_of_episode)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                                 \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_to_next_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                                 \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep_to_next_action\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                         \u001b[0mSarsd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m                 \u001b[1;31m# increase the update counter by one each step (until reach simulation length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 action DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "Session_ID = \"Single_Cross_Triple4_actions\"\n",
    "#Session_ID = \"DQN\"\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{ 'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' :    {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    "         \n",
    "         'all_actions' :       {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "        },\n",
    "     'demand' : { 'default' : [400, 400, 400, 400]}\n",
    "                  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 5\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 48)           672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48)           2352        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            49          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            196         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 4)            0           dense_5[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,973\n",
      "Trainable params: 7,973\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 0\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Experience Found: Loading into agents\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Single_Cross_Triple4_MultiDQN_Agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d8e12738e26c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSingle_Cross_Triple4_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Single_Cross_Triple4_MultiDQN_Agents' is not defined"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.train(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.load(best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of tensorflow.python.keras.layers.core failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 244, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 378, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\importlib\\__init__.py\", line 148, in reload\n",
      "    raise ImportError(msg.format(name), name=name)\n",
      "ImportError: module DQNAgents not in sys.modules\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: test\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.19 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBecause the cars never leave the nodes the delay is not computed corretly\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To be arranged for multy agents\n",
    "\n",
    "queues = np.array(Episode_Queues[0])\n",
    "queues = queues.T\n",
    "\n",
    "delay = Cumulative_Episode_Delays[0]\n",
    "\n",
    "# Plot the queues\n",
    "plt.figure(1)\n",
    "for queue in queues:\n",
    "    plt.plot(queue)\n",
    "\n",
    "# plot the junctions delays\n",
    "plt.figure(2)\n",
    "plt.plot(delay)\n",
    "\n",
    "#plot the total delays \n",
    "plt.figure(3)\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "\n",
    "# Dont freak out the 2 delays are not the same because the node is not covering all the junction\n",
    "\n",
    "\"\"\"\n",
    "Because the cars never leave the nodes the delay is not computed correctly (when the agent doesn't work) \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-172.39170302940516, -175.96571098841991, -168.68159561472137, -169.2000516807163, -166.66541390482826, -164.291001274276, -162.84620307544648, -162.23272010641864, -163.04435289914827, -164.79077226283263, -162.08064404204416, -161.69911379482826, -160.68224358582978, -159.8945838555579, -158.6736191194321, -158.00055069652402, -156.71092540267185, -155.61320009079571, -154.55386440405377, -153.29989674380948, -152.63115215851738, -151.8470001978877, -150.47262424901032, -149.5057842092851, -148.59345742499318, -147.91543696615065, -146.6716865012875, -146.00518580875973, -145.31754229115722, -144.385124530421, -143.48135958798323, -142.72781630871248, -141.9287002117344, -140.97758429638955, -140.33268841882162, -139.50984600691416, -139.29404873392855, -138.63797408940715, -138.2109921923747, -137.12216290315857, -136.7122657207284, -136.52348776037476, -135.96712215141082, -135.35771869104997, -135.145921254349, -134.77309168331135, -134.19189006653727, -133.5142349644523, -133.0643708169842, -132.77756427224082, -119.41160344990597, -118.93544756497973, -115.94018466666222, -118.42234331124212, -118.26972216317081, -116.85617488723472, -115.45371035070904, -113.7784753255137, -113.57422195646289, -113.27499861738315, -112.69964884857654, -112.11892221989345, -111.48305940026343, -110.7702645933302, -111.07211634841447, -111.43973813710727, -111.02684512624758, -110.90169749654004, -110.74361835771836, -110.56325629450546, -110.0142392382483, -109.49449848028445, -109.18030425668648, -109.18470803452716, -108.95724533153296, -108.62681436599833, -108.59601205614325, -108.2396857687945, -108.45999775157522, -108.35410542851456, -108.20205832111212, -107.94733669628623, -107.9143303912895, -107.7120590420443, -107.54829418336936, -107.33941313495787, -106.92548951275154, -106.82693098119103, -106.73238273258706, -106.61645086216585, -106.29656775418702, -106.34360760723547, -106.19334126944534, -106.14942315939834, -105.99790746367933, -105.84613777011626, -105.79824921027412, -105.73875898397367, -105.73969388318139, -105.66538790748501, -105.29326740673716, -99.57324282465918, -103.62347744000989, -103.23917253946651, -100.82332364220662, -99.97070439259643, -100.64749625072331, -100.1623013739057, -100.18828093871747, -99.34648837516943, -99.76502661377648, -99.63023127104802, -99.13030534940243, -99.42679119957789, -99.96469320189746, -99.85946179827954, -100.30739981228339, -100.69267729498586, -100.88416516483156, -100.63513926392208, -100.44418495065231, -100.81236246349705, -100.7164327651392, -100.71872388100924, -100.70535772431228, -100.66783932032808, -100.47794445352385, -100.26697109349452, -100.14166868575546, -100.29641331096481, -100.16906029246186, -100.18134871763525, -100.07446171795772, -99.93419964582915, -99.86044080853513, -99.79461797877276, -99.694027266517, -99.55153508226917, -99.48116072272238, -99.32010148887484, -99.43114821240664, -99.51345130866693, -99.48018495459125, -99.3434594129541, -99.30039028288977, -99.14553524815051, -99.21193969747678, -99.12190261401908, -98.98327673377565, -98.83123701817695, -91.02077120049783, -96.288022985813, -96.19736996407458, -95.03444635241611, -95.36341601252985, -95.35179300727911, -94.71015319287386, -95.21938106852643, -95.17139607950278, -96.08047517099656, -95.57056679398428, -95.91856219877728, -94.90746893935739, -94.53541225154392, -95.33469454448036, -94.92044694937196, -95.32808678070685, -95.66606546063213, -96.03045817802904, -96.29744453051715, -96.7211125104045, -96.59004693076427, -96.52656128769851, -96.72504059789853, -96.71238533500556, -96.64728378865524, -96.72456030326063, -96.66081502709481, -96.59511206424689, -96.63869002910006, -96.54209306683275, -96.58940567188671, -96.43528558153874, -96.45734247393807, -96.54828688275703, -96.41038385099966, -96.31155731878027, -96.34054256935075, -96.36596580870187, -96.30006493763513, -96.22897760283243, -96.21641080213475, -96.21364719229761, -96.33454163349545, -96.36648966674564, -96.28390671333445, -96.24395894886712, -96.23200935256304, -96.27796825236284, -96.4144138546997, -89.10082872473123, -89.56688556202747, -89.40858248026477, -89.52599435679583, -89.78799788443982, -91.94553028903147, -92.08065406519259, -92.6607059159116, -93.88412839132721, -93.73565317673994, -93.09450372325607, -92.8613375738443, -92.86012729731944, -92.89783205914851, -93.0250535295186, -93.2158354777321, -93.40433338196947, -93.36896978552168, -93.46207569033017, -94.0165248271189, -94.17637997140388, -94.90892276922406, -94.9768974543035, -94.97723835557773, -94.70834041203057, -94.80538048477247, -94.77338137776216, -94.56585826097603, -94.57389229199649, -94.58175840496757, -94.53574255167844, -94.64637247810823, -94.36223682094632, -94.22694548103053, -94.24696154469922, -94.23183716910923, -94.25599301194957, -94.31963840388723, -94.24454159010611, -94.28836242001937, -94.5120241256803, -94.4850259011947, -94.46783239934585, -94.52042390154793, -94.81219326319126, -94.97145564131701, -94.95611282921055, -95.04549800513675, -94.89800442546651, -94.90287946568183, -87.03904982758179, -91.73327558486154, -92.56561955002486, -92.00998792156558, -92.12334728344064, -90.91308066434785, -91.1868880886411, -91.17340289860132, -91.3182331313153, -90.93878705772794, -90.99125002847309, -91.47401105316918, -91.62305435500136, -92.27856392872393, -92.63173186174309, -92.88604603715471, -92.9356165912927, -93.18212698577796, -92.82158400340936, -93.0161083841174, -93.06968232667302, -93.0789603705806, -93.20961290561885, -93.23740066233448, -93.1982224898708, -93.1839795820268, -93.30111371460904, -93.41609178158892, -93.37294679543604, -93.44812331001822]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(4)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].loss)\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "print(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 380\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.52 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1801\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_TripleAC8test1\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "    },\n",
    "   'demand' : {\"default\" : [400,400,400,400] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.000001\n",
    "\n",
    "\n",
    "value = 5\n",
    "entropy = 50\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  672       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  2352      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2352      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  49        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  672       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits4 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  392       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 13,545\n",
      "Trainable params: 13,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 1801 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.09 seconds.\n",
      "\n",
      "Episode 51 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-173.0, -4327.0, -767.0, -5236.0, -4803.0, -5335.0, -739.0, -3925.0, -2130.0, -831.0] \n",
      " [-2862.0, -12537.0, -4320.0, -14834.0, -13537.0, -14874.0, -4151.0, -11794.0, -8196.0, -5035.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.2, 0.12, 0.21, 0.06, 0.08, 0.02, 0.05, 0.26], [0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07], [0.28, 0.17, 0.35, 0.02, 0.04, 0.0, 0.04, 0.11], [0.19, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.58], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.04, 0.0, 0.55, 0.0, 0.0, 0.0, 0.0, 0.41], [0.38, 0.04, 0.22, 0.0, 0.0, 0.0, 0.01, 0.35], [0.79, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.21], [0.62, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.36], [0.84, 0.0, 0.01, 0.01, 0.01, 0.0, 0.01, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -562.63\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC8test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 52 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4000.0, -5919.0, -5652.0, -6187.0, -5919.0, -6017.0, -1574.0, -6446.0, -3714.0, -2631.0] \n",
      " [-14340.0, -16903.0, -16719.0, -17685.0, -16903.0, -17108.0, -6804.0, -18144.0, -11910.0, -9101.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.07, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.6], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.01, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.2], [0.54, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.44], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.05, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.83], [0.0, 0.02, 0.89, 0.0, 0.0, 0.0, 0.0, 0.09], [0.28, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.38], [0.01, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -725.79\n",
      "Episode 53 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2011.0, -5087.0, -1155.0, -1084.0, -1662.0, -755.0, -575.0, -4796.0, -4210.0, -5874.0] \n",
      " [-8275.0, -13631.0, -4394.0, -4112.0, -6027.0, -3650.0, -2964.0, -13968.0, -13320.0, -16698.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.07, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.45], [0.0, 0.03, 0.64, 0.0, 0.0, 0.0, 0.0, 0.32], [0.56, 0.0, 0.03, 0.0, 0.02, 0.0, 0.0, 0.39], [0.05, 0.06, 0.57, 0.0, 0.0, 0.0, 0.0, 0.32], [0.03, 0.1, 0.54, 0.0, 0.01, 0.0, 0.0, 0.33], [0.09, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.62], [0.45, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.53], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -638.04\n",
      "Episode 54 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3121.0, -1227.0, -1482.0, -4501.0, -5144.0, -5564.0, -5707.0, -1506.0, -1482.0, -4518.0] \n",
      " [-10108.0, -4438.0, -6507.0, -13681.0, -15258.0, -15564.0, -15914.0, -6281.0, -6507.0, -13972.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.01, 0.05, 0.74, 0.0, 0.0, 0.0, 0.0, 0.2], [0.01, 0.02, 0.74, 0.0, 0.0, 0.0, 0.0, 0.22], [0.0, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.22], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.01, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.37, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.39], [0.46, 0.2, 0.24, 0.0, 0.0, 0.0, 0.0, 0.08], [0.01, 0.02, 0.74, 0.0, 0.0, 0.0, 0.0, 0.22], [0.01, 0.0, 0.58, 0.0, 0.0, 0.0, 0.0, 0.4]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -615.55\n",
      "Episode 55 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2136.0, -3480.0, -3529.0, -3941.0, -5434.0, -1110.0, -3941.0, -1047.0, -2246.0, -5190.0] \n",
      " [-6944.0, -11693.0, -11238.0, -12801.0, -15062.0, -4404.0, -12801.0, -4600.0, -8706.0, -14223.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.09, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.78], [0.03, 0.0, 0.58, 0.0, 0.0, 0.0, 0.0, 0.39], [0.11, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.12, 0.26, 0.44, 0.0, 0.0, 0.0, 0.01, 0.16], [0.11, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.81], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.33], [0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -557.61\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC8test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 56 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3023.0, -3023.0, -2459.0, -1442.0, -3827.0, -6239.0, -4945.0, -5241.0, -2459.0, -5189.0] \n",
      " [-10375.0, -10375.0, -8414.0, -6801.0, -12451.0, -16903.0, -14821.0, -15306.0, -8414.0, -14462.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.05, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.51], [0.05, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.51], [0.01, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.32], [0.18, 0.06, 0.45, 0.0, 0.0, 0.0, 0.0, 0.31], [0.4, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.46], [0.28, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.67], [0.0, 0.0, 0.43, 0.0, 0.0, 0.0, 0.0, 0.56], [0.29, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.45], [0.01, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.32], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -696.16\n",
      "Episode 57 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3154.0, -4999.0, -2329.0, -5021.0, -5705.0, -5819.0, -3646.0, -4516.0, -8.0, -5079.0] \n",
      " [-9983.0, -14286.0, -8434.0, -14649.0, -16064.0, -15803.0, -11409.0, -13083.0, -3794.0, -14231.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.42, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.57], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.04, 0.0, 0.68, 0.0, 0.0, 0.0, 0.0, 0.28], [0.04, 0.0, 0.73, 0.0, 0.0, 0.0, 0.0, 0.23], [0.26, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.62], [0.4, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.46], [0.01, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.4], [0.37, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.62], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -644.18\n",
      "Episode 58 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4190.0, -867.0, -3894.0, -1800.0, -3873.0, -4422.0, -706.0, -4193.0, -2509.0, -4527.0] \n",
      " [-11921.0, -5812.0, -11460.0, -8122.0, -11376.0, -12030.0, -6301.0, -11863.0, -9481.0, -13352.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.88], [0.33, 0.03, 0.31, 0.0, 0.0, 0.0, 0.01, 0.32], [0.01, 0.0, 0.36, 0.0, 0.0, 0.0, 0.0, 0.63], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.04, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.88], [0.01, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.94], [0.23, 0.02, 0.33, 0.01, 0.01, 0.0, 0.01, 0.39], [0.0, 0.0, 0.61, 0.0, 0.0, 0.0, 0.0, 0.39], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.01, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -684.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 59 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6053.0, -525.0, -6400.0, -6400.0, -951.0, -5962.0, -2133.0, -6352.0, -6214.0, -1665.0] \n",
      " [-16359.0, -5171.0, -17212.0, -17239.0, -5924.0, -16592.0, -9712.0, -17258.0, -16755.0, -8404.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11], [0.03, 0.13, 0.48, 0.01, 0.05, 0.0, 0.01, 0.3], [0.03, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.46], [0.03, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.46], [0.95, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03], [0.03, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.45], [0.9, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.1], [0.01, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.69], [0.0, 0.0, 0.42, 0.0, 0.0, 0.0, 0.0, 0.57], [0.11, 0.01, 0.63, 0.0, 0.0, 0.0, 0.0, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -720.23\n",
      "Episode 60 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5900.0, -6594.0, -6270.0, -91.0, -6401.0, -6346.0, -1153.0, -6104.0, -5355.0, -6271.0] \n",
      " [-16236.0, -17163.0, -16877.0, -4783.0, -16669.0, -17044.0, -8187.0, -16479.0, -14895.0, -17141.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.65, 0.0, 0.0, 0.0, 0.0, 0.35], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.79], [0.24, 0.09, 0.1, 0.16, 0.11, 0.05, 0.12, 0.13], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.01, 0.0, 0.62, 0.0, 0.0, 0.0, 0.0, 0.36], [0.04, 0.01, 0.47, 0.0, 0.0, 0.0, 0.0, 0.48], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1], [0.12, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.78]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -752.23\n",
      "Episode 61 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6295.0, -2692.0, -1083.0, -3520.0, -227.0, -227.0, -806.0, -6693.0, -1257.0, -6135.0] \n",
      " [-17453.0, -9029.0, -6228.0, -11351.0, -4718.0, -4718.0, -5511.0, -17924.0, -6322.0, -16730.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.86], [0.08, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.89], [0.02, 0.01, 0.52, 0.0, 0.0, 0.0, 0.0, 0.44], [0.03, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.95], [0.12, 0.06, 0.44, 0.04, 0.09, 0.01, 0.02, 0.24], [0.12, 0.06, 0.44, 0.04, 0.09, 0.01, 0.02, 0.24], [0.02, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.6], [0.83, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.14], [0.01, 0.03, 0.46, 0.0, 0.0, 0.0, 0.0, 0.49], [0.57, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.42]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -706.03\n",
      "Episode 62 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3277.0, -5758.0, -3183.0, -3395.0, -23.0, -5941.0, -5530.0, -5436.0, -5781.0, -3277.0] \n",
      " [-10393.0, -15667.0, -9943.0, -10033.0, -2889.0, -16197.0, -15373.0, -15128.0, -15613.0, -10687.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.54, 0.0, 0.0, 0.0, 0.0, 0.45], [0.09, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.79], [0.28, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.39], [0.75, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.24], [0.15, 0.11, 0.14, 0.14, 0.13, 0.09, 0.1, 0.13], [0.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27], [0.02, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.64], [0.0, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.62], [0.03, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.68], [0.01, 0.0, 0.54, 0.0, 0.0, 0.0, 0.0, 0.45]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -642.7\n",
      "Episode 63 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4930.0, -4042.0, -8.0, -2895.0, -3906.0, -1876.0, -5172.0, -1377.0, -2549.0, -4711.0] \n",
      " [-14273.0, -11274.0, -3792.0, -9091.0, -11714.0, -6481.0, -14428.0, -6383.0, -7644.0, -13093.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.08, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.02, 0.01, 0.76, 0.0, 0.0, 0.0, 0.0, 0.22], [0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.18], [0.27, 0.01, 0.15, 0.0, 0.0, 0.0, 0.0, 0.57], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -638.62\n",
      "Episode 64 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4578.0, -1564.0, -1195.0, -1867.0, -1486.0, -1953.0, -3875.0, -3813.0, -5082.0, -5110.0] \n",
      " [-12651.0, -5605.0, -4813.0, -7711.0, -4196.0, -8623.0, -11160.0, -11071.0, -13440.0, -13666.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.92], [0.01, 0.0, 0.47, 0.0, 0.0, 0.0, 0.0, 0.52], [0.15, 0.0, 0.45, 0.0, 0.0, 0.0, 0.0, 0.39], [0.02, 0.0, 0.53, 0.0, 0.0, 0.0, 0.0, 0.46], [0.65, 0.04, 0.05, 0.0, 0.0, 0.0, 0.0, 0.26], [0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1], [0.02, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.83], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.24, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.75]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -533.22\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC8test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 65 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3111.0, -5512.0, -8.0, -5581.0, -3076.0, -1485.0, -5551.0, -1652.0, -3061.0, -3093.0] \n",
      " [-9205.0, -14971.0, -3165.0, -14756.0, -9314.0, -4579.0, -15040.0, -4880.0, -9675.0, -9548.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.15], [0.04, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.88], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.55, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45], [0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.01, 0.67, 0.0, 0.0, 0.0, 0.0, 0.32], [0.07, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.72], [0.17, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.71], [0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.59]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -587.7\n",
      "Episode 66 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6540.0, -3412.0, -5436.0, -3755.0, -5699.0, -5548.0, -4261.0, -3715.0, -5787.0, -4910.0] \n",
      " [-17161.0, -10017.0, -14933.0, -10095.0, -15528.0, -14980.0, -12370.0, -11732.0, -14856.0, -13601.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.04, 0.0, 0.53, 0.0, 0.0, 0.0, 0.0, 0.43], [0.57, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.03, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.16], [0.07, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.87], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.33], [0.04, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.93]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -679.7\n",
      "Episode 67 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3785.0, -5565.0, -5987.0, -5622.0, -4319.0, -4217.0, -2975.0, -913.0, -4335.0, -5385.0] \n",
      " [-11221.0, -14261.0, -16039.0, -15153.0, -13361.0, -11562.0, -9646.0, -4467.0, -12031.0, -14329.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.72], [0.39, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.52], [0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.29], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [0.05, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.62, 0.0, 0.0, 0.0, 0.0, 0.38], [0.58, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42], [0.0, 0.03, 0.84, 0.0, 0.0, 0.0, 0.0, 0.12], [0.03, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -634.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 68 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5372.0, -6254.0, -6260.0, -1150.0, -5985.0, -6113.0, -5878.0, -392.0, -5274.0, -5382.0] \n",
      " [-14003.0, -17254.0, -16993.0, -5674.0, -16782.0, -16621.0, -15701.0, -4074.0, -13721.0, -14590.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.23], [0.01, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.81], [0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07], [0.28, 0.01, 0.23, 0.0, 0.0, 0.0, 0.0, 0.47], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07], [0.41, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.43], [0.61, 0.06, 0.05, 0.02, 0.02, 0.0, 0.03, 0.2], [0.13, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.36], [0.01, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.73]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -667.88\n",
      "Episode 69 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5457.0, -3423.0, -2378.0, -3630.0, -5073.0, -3423.0, -3527.0, -4786.0, -5318.0, -6224.0] \n",
      " [-15200.0, -10686.0, -8405.0, -12159.0, -13937.0, -10686.0, -11697.0, -13699.0, -15103.0, -16669.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.2], [0.53, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.36], [0.48, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.47], [0.04, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.69], [0.08, 0.0, 0.63, 0.0, 0.0, 0.0, 0.0, 0.29], [0.53, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.36], [0.51, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.47], [0.0, 0.0, 0.73, 0.0, 0.0, 0.0, 0.0, 0.27], [0.01, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -687.47\n",
      "Episode 70 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3637.0, -6525.0, -16.0, -1467.0, -517.0, -3644.0, -2964.0, -5074.0, -6446.0, -3909.0] \n",
      " [-12170.0, -17381.0, -2421.0, -6628.0, -2919.0, -10786.0, -9944.0, -14255.0, -16824.0, -11964.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.23, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.6], [0.14, 0.12, 0.13, 0.13, 0.13, 0.11, 0.11, 0.13], [0.12, 0.05, 0.34, 0.0, 0.02, 0.0, 0.0, 0.47], [0.6, 0.03, 0.05, 0.06, 0.05, 0.0, 0.04, 0.17], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.01, 0.0, 0.41, 0.0, 0.0, 0.0, 0.0, 0.58], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.08, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.91], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -637.75\n",
      "Episode 71 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2019.0, -5439.0, -4264.0, -5290.0, -5348.0, -3344.0, -8.0, -5387.0, -4711.0, -1747.0] \n",
      " [-8318.0, -15465.0, -11719.0, -14540.0, -13522.0, -10191.0, -3486.0, -13557.0, -12536.0, -7707.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.19, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.56], [0.08, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.91], [0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5], [0.07, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.92], [0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.08, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.9], [0.07, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.92], [0.04, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.67]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -646.96\n",
      "Episode 72 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2625.0, -1430.0, -5633.0, -4543.0, -538.0, -658.0, -5812.0, -1058.0, -5342.0, -404.0] \n",
      " [-7799.0, -6006.0, -14710.0, -12775.0, -2332.0, -2589.0, -14810.0, -5140.0, -14291.0, -3262.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.31, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.68], [0.02, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.47], [0.03, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.2], [0.94, 0.0, 0.0, 0.03, 0.0, 0.0, 0.01, 0.01], [0.05, 0.11, 0.48, 0.0, 0.07, 0.0, 0.01, 0.27], [0.01, 0.0, 0.36, 0.0, 0.0, 0.0, 0.0, 0.63], [0.81, 0.01, 0.02, 0.01, 0.01, 0.0, 0.01, 0.14], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.05, 0.03, 0.41, 0.01, 0.05, 0.0, 0.0, 0.45]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -539.66\n",
      "Episode 73 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4421.0, -1830.0, -6439.0, -6758.0, -6605.0, -3795.0, -921.0, -6520.0, -6606.0, -6692.0] \n",
      " [-11936.0, -8792.0, -16408.0, -16549.0, -16731.0, -11337.0, -7484.0, -16535.0, -17171.0, -16687.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.48], [0.07, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.15], [0.04, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.96], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.2], [0.27, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.72], [0.03, 0.07, 0.61, 0.0, 0.0, 0.0, 0.0, 0.29], [0.01, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.62], [0.04, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -746.98\n",
      "Episode 74 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6687.0, -1788.0, -4874.0, -5542.0, -6111.0, -1781.0, -1742.0, -5455.0, -3027.0, -4558.0] \n",
      " [-17054.0, -6744.0, -14047.0, -15326.0, -16231.0, -7416.0, -6464.0, -15120.0, -9491.0, -13370.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.78, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.21], [0.07, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.6], [0.01, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.48], [0.0, 0.0, 0.54, 0.0, 0.0, 0.0, 0.0, 0.46], [0.7, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.28], [0.02, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.39], [0.11, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.88], [0.02, 0.0, 0.76, 0.0, 0.0, 0.0, 0.0, 0.22], [0.01, 0.0, 0.38, 0.0, 0.0, 0.0, 0.0, 0.6]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -665.45\n",
      "Episode 75 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2330.0, -6367.0, -2891.0, -6441.0, -4213.0, -2232.0, -4329.0, -3234.0, -5252.0, -3974.0] \n",
      " [-8910.0, -16673.0, -9260.0, -16747.0, -11905.0, -8564.0, -11603.0, -9124.0, -12922.0, -11080.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.4, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.58], [0.22, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.74], [0.02, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.86], [0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12], [0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.3], [0.29, 0.0, 0.07, 0.0, 0.03, 0.0, 0.0, 0.61], [0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.75], [0.39, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.49], [0.01, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.79], [0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.69]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -665.86\n",
      "Episode 76 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2288.0, -4849.0, -5708.0, -2442.0, -6067.0, -4965.0, -7458.0, -6884.0, -5670.0, -6884.0] \n",
      " [-9070.0, -13552.0, -14970.0, -9441.0, -16520.0, -13811.0, -18504.0, -17946.0, -15042.0, -17946.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.23], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [0.53, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.45], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.01, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.73], [0.01, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.62], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09], [0.56, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -692.23\n",
      "Episode 77 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6797.0, -1296.0, -6638.0, -3735.0, -6638.0, -6666.0, -6666.0, -1170.0, -592.0, -1016.0] \n",
      " [-18264.0, -5363.0, -17992.0, -11742.0, -17992.0, -17314.0, -17314.0, -4727.0, -3738.0, -5040.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.81], [0.91, 0.02, 0.02, 0.0, 0.0, 0.0, 0.02, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.65, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35], [0.65, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35], [0.06, 0.01, 0.35, 0.0, 0.0, 0.0, 0.0, 0.57], [0.95, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.01], [0.01, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -717.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 78 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5891.0, -5236.0, -5424.0, -1407.0, -3889.0, -6063.0, -8.0, -5796.0, -2730.0, -5780.0] \n",
      " [-14470.0, -13674.0, -13781.0, -5092.0, -11414.0, -14468.0, -3151.0, -14574.0, -8814.0, -14438.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.38, 0.0, 0.0, 0.0, 0.0, 0.61], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.02, 0.0, 0.63, 0.0, 0.0, 0.0, 0.0, 0.35], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.57, 0.0, 0.0, 0.0, 0.0, 0.43], [0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.86], [0.0, 0.0, 0.43, 0.0, 0.0, 0.0, 0.0, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -612.06\n",
      "Episode 79 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5330.0, -6886.0, -5565.0, -4855.0, -6350.0, -6769.0, -5502.0, -5502.0, -6645.0, -6836.0] \n",
      " [-14061.0, -17187.0, -14594.0, -13298.0, -15535.0, -16409.0, -13927.0, -13927.0, -16685.0, -17604.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.36, 0.0, 0.0, 0.0, 0.0, 0.64], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [0.01, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.49], [0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15], [0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15], [0.01, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.96], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -739.11\n",
      "Episode 80 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2743.0, -392.0, -5301.0, -7059.0, -6892.0, -295.0, -4275.0, -5951.0, -6417.0, -3730.0] \n",
      " [-9393.0, -3981.0, -13501.0, -17330.0, -17387.0, -3661.0, -11725.0, -15184.0, -16318.0, -10540.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.86], [0.28, 0.11, 0.25, 0.03, 0.02, 0.01, 0.06, 0.24], [0.03, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.87], [0.52, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.08, 0.15, 0.27, 0.04, 0.11, 0.01, 0.03, 0.31], [0.01, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.95], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.03, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.85]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -680.37\n",
      "Episode 81 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6367.0, -6440.0, -2890.0, -7030.0, -4112.0, -3169.0, -5678.0, -7018.0, -6515.0, -6413.0] \n",
      " [-16225.0, -16566.0, -9862.0, -16901.0, -11172.0, -9683.0, -14613.0, -17004.0, -16766.0, -16371.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.0, 0.26], [0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.12], [0.0, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.74], [0.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23], [0.12, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.86], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.85], [0.04, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.95], [0.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -725.73\n",
      "Episode 82 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6074.0, -6416.0, -5552.0, -5987.0, -5856.0, -5588.0, -2171.0, -6025.0, -476.0, -6850.0] \n",
      " [-14235.0, -16759.0, -14315.0, -14286.0, -14242.0, -13520.0, -8802.0, -15294.0, -4343.0, -17154.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.82], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.05, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.83], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.69, 0.0, 0.0, 0.0, 0.0, 0.3], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.67, 0.01, 0.03, 0.11, 0.03, 0.0, 0.04, 0.1], [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -688.82\n",
      "Episode 83 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4824.0, -2205.0, -4139.0, -5825.0, -3185.0, -5965.0, -200.0, -1520.0, -4815.0, -6586.0] \n",
      " [-12770.0, -8123.0, -11860.0, -14288.0, -9885.0, -15335.0, -4290.0, -5086.0, -12156.0, -17009.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.18, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.81], [0.01, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.77], [0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17, 0.15, 0.21, 0.08, 0.1, 0.03, 0.09, 0.17], [0.23, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.55], [0.13, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.45, 0.0, 0.0, 0.0, 0.0, 0.55]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -694.71\n",
      "Episode 84 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5356.0, -5938.0, -3966.0, -946.0, -642.0, -6539.0, -1735.0, -6112.0, -839.0, -5698.0] \n",
      " [-13864.0, -15504.0, -11207.0, -4544.0, -3814.0, -16747.0, -6675.0, -15254.0, -4405.0, -14228.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.56], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55], [0.0, 0.02, 0.87, 0.0, 0.0, 0.0, 0.0, 0.11], [0.11, 0.04, 0.4, 0.01, 0.04, 0.0, 0.02, 0.38], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.07, 0.02, 0.26, 0.0, 0.02, 0.0, 0.0, 0.63], [0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.69], [0.01, 0.04, 0.65, 0.0, 0.01, 0.0, 0.0, 0.29], [0.05, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -654.48\n",
      "Episode 85 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3139.0, -5714.0, -5643.0, -6324.0, -5590.0, -2360.0, -5472.0, -5817.0, -6627.0, -6398.0] \n",
      " [-9915.0, -13579.0, -13507.0, -16645.0, -13367.0, -8907.0, -14241.0, -13422.0, -15115.0, -16768.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.63, 0.0, 0.0, 0.0, 0.0, 0.37], [0.7, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.29], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -670.16\n",
      "Episode 86 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6471.0, -5809.0, -6503.0, -7259.0, -4360.0, -2870.0, -1645.0, -487.0, -3557.0, -6504.0] \n",
      " [-15309.0, -14978.0, -16405.0, -17982.0, -12245.0, -10866.0, -6901.0, -4706.0, -12130.0, -16831.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.82], [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.05, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.9], [0.02, 0.03, 0.88, 0.0, 0.0, 0.0, 0.0, 0.08], [0.42, 0.11, 0.14, 0.02, 0.02, 0.0, 0.03, 0.26], [0.0, 0.0, 0.49, 0.0, 0.0, 0.0, 0.0, 0.5], [0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -724.56\n",
      "Episode 87 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2191.0, -1475.0, -6376.0, -1418.0, -1773.0, -7080.0, -7073.0, -6600.0, -6789.0, -6998.0] \n",
      " [-8358.0, -4355.0, -15397.0, -4977.0, -4679.0, -16304.0, -15930.0, -15709.0, -15982.0, -16297.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.19], [0.84, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.12], [0.0, 0.0, 0.63, 0.0, 0.0, 0.0, 0.0, 0.37], [0.01, 0.01, 0.72, 0.0, 0.0, 0.0, 0.0, 0.26], [0.24, 0.22, 0.38, 0.0, 0.0, 0.0, 0.0, 0.16], [0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.25], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75], [0.06, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.93], [0.01, 0.0, 0.28, 0.0, 0.0, 0.0, 0.0, 0.71]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -681.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 88 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6095.0, -5914.0, -4222.0, -4850.0, -6121.0, -5595.0, -4244.0, -3909.0, -356.0, -1357.0] \n",
      " [-14752.0, -13984.0, -11241.0, -12269.0, -14979.0, -13878.0, -10978.0, -10650.0, -3392.0, -5710.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.43, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57], [0.64, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36], [0.0, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.87], [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.12, 0.07, 0.37, 0.03, 0.08, 0.0, 0.02, 0.3], [0.06, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.74]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -585.02\n",
      "Episode 89 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5624.0, -1766.0, -5257.0, -5136.0, -2666.0, -5186.0, -5823.0, -5187.0, -4982.0, -3223.0] \n",
      " [-12840.0, -6974.0, -12916.0, -12651.0, -9445.0, -12493.0, -13063.0, -12501.0, -13015.0, -9969.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64], [0.55, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.42], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.71], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.82], [0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.7], [0.02, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.71]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -618.66\n",
      "Episode 90 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7111.0, -7025.0, -3332.0, -6593.0, -4019.0, -7025.0, -6981.0, -5495.0, -6811.0, -7106.0] \n",
      " [-17278.0, -16719.0, -9939.0, -16289.0, -11571.0, -16719.0, -17521.0, -13694.0, -16078.0, -16789.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.75], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.96], [0.02, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.75], [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.8], [0.01, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.79], [0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.21], [0.57, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -711.87\n",
      "Episode 91 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5724.0, -1660.0, -364.0, -364.0, -7727.0, -364.0, -1280.0, -7727.0, -7878.0, -2066.0] \n",
      " [-13734.0, -5944.0, -3334.0, -3334.0, -18147.0, -3334.0, -4909.0, -18147.0, -18135.0, -6183.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.36, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.63], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.14, 0.07, 0.44, 0.02, 0.03, 0.0, 0.02, 0.28], [0.14, 0.07, 0.44, 0.02, 0.03, 0.0, 0.02, 0.28], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14, 0.07, 0.44, 0.02, 0.03, 0.0, 0.02, 0.28], [0.49, 0.01, 0.03, 0.01, 0.07, 0.0, 0.01, 0.39], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -668.74\n",
      "Episode 92 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6501.0, -4319.0, -6914.0, -2255.0, -5881.0, -6757.0, -5982.0, -6465.0, -5853.0, -5982.0] \n",
      " [-15834.0, -12262.0, -16401.0, -8519.0, -15303.0, -15443.0, -14797.0, -15753.0, -15164.0, -14664.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.68, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32], [0.59, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41], [0.01, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.73], [0.38, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.59], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.3], [0.04, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.69], [0.04, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.94]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -693.04\n",
      "Episode 93 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7413.0, -5942.0, -2453.0, -5492.0, -7046.0, -3485.0, -5332.0, -6940.0, -6143.0, -6005.0] \n",
      " [-16471.0, -14688.0, -9212.0, -13579.0, -16267.0, -10546.0, -13518.0, -16565.0, -14476.0, -13933.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.55, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.39], [0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.21], [0.48, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52], [0.0, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.74], [0.03, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.85], [0.42, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.57], [0.0, 0.0, 0.62, 0.0, 0.0, 0.0, 0.0, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -695.44\n",
      "Episode 94 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5241.0, -4019.0, -7333.0, -6332.0, -7502.0, -5710.0, -4306.0, -6332.0, -5567.0, -6530.0] \n",
      " [-13197.0, -10292.0, -17038.0, -15279.0, -16933.0, -13970.0, -10817.0, -15213.0, -13478.0, -16465.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.0, 0.69, 0.0, 0.0, 0.0, 0.0, 0.31], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.02, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1], [0.03, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.92], [0.02, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.96], [0.02, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.76], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -676.58\n",
      "Episode 95 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6933.0, -2748.0, -7040.0, -6066.0, -7511.0, -6411.0, -6371.0, -6889.0, -6020.0, -6722.0] \n",
      " [-16772.0, -10601.0, -16639.0, -14905.0, -17108.0, -15554.0, -15436.0, -16021.0, -15374.0, -16210.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.7], [0.09, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.9], [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.97], [0.55, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.44], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.16], [0.08, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.92], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -701.57\n",
      "Episode 96 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1965.0, -7060.0, -7248.0, -2113.0, -5507.0, -7241.0, -2906.0, -4096.0, -3468.0, -7510.0] \n",
      " [-5471.0, -16674.0, -16608.0, -8087.0, -14145.0, -16288.0, -10140.0, -11910.0, -11104.0, -16561.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.72, 0.0, 0.0, 0.0, 0.0, 0.27], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.41], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3], [0.05, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.85], [0.07, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.92], [0.08, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.86], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.16]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -745.83\n",
      "Episode 97 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2189.0, -5229.0, -873.0, -7537.0, -6741.0, -4774.0, -1553.0, -7178.0, -6809.0, -7044.0] \n",
      " [-7813.0, -13768.0, -4813.0, -16771.0, -15539.0, -12989.0, -6121.0, -16699.0, -15798.0, -16327.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87], [0.1, 0.16, 0.15, 0.0, 0.0, 0.0, 0.0, 0.59], [0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49], [0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86], [0.04, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.95], [0.69, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.26], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -685.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 98 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2250.0, -4474.0, -5160.0, -1568.0, -6713.0, -5287.0, -2269.0, -3404.0, -5691.0, -3084.0] \n",
      " [-6994.0, -12100.0, -12959.0, -5976.0, -15988.0, -13069.0, -6705.0, -9924.0, -14238.0, -9357.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.19, 0.0, 0.01, 0.0, 0.0, 0.79], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.05, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.74], [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.16], [0.52, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.1, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.89]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -645.62\n",
      "Episode 99 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5476.0, -6421.0, -5995.0, -5883.0, -6445.0, -5478.0, -7192.0, -1116.0, -5558.0, -5693.0] \n",
      " [-12976.0, -14967.0, -14925.0, -13002.0, -14809.0, -13323.0, -14831.0, -5667.0, -13012.0, -14314.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.4, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.58], [0.02, 0.02, 0.7, 0.0, 0.0, 0.0, 0.0, 0.25], [0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -629.76\n",
      "Episode 100 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3593.0, -2564.0, -7529.0, -6371.0, -6145.0, -7611.0, -7551.0, -3967.0, -7703.0, -5597.0] \n",
      " [-9301.0, -8529.0, -16936.0, -14550.0, -14654.0, -17337.0, -16883.0, -9699.0, -17270.0, -13896.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31], [0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -700.86\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 101 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-826.0, -6096.0, -7010.0, -1476.0, -7995.0, -1507.0, -5075.0, -7010.0, -2249.0, -5289.0] \n",
      " [-4336.0, -14370.0, -16648.0, -5857.0, -17583.0, -5189.0, -12135.0, -16648.0, -7746.0, -12174.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.02, 0.05, 0.69, 0.0, 0.01, 0.0, 0.01, 0.22], [0.01, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.95], [0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63], [0.0, 0.01, 0.93, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.25], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07], [0.57, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.4], [0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63], [0.05, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.18], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -686.9\n",
      "Episode 102 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7467.0, -3523.0, -2124.0, -272.0, -8169.0, -6252.0, -7499.0, -5812.0, -7459.0, -761.0] \n",
      " [-17239.0, -10144.0, -6796.0, -3282.0, -18105.0, -15685.0, -16868.0, -14867.0, -16826.0, -4446.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.69], [0.0, 0.0, 0.62, 0.0, 0.0, 0.0, 0.0, 0.38], [0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.1, 0.05, 0.42, 0.03, 0.08, 0.0, 0.02, 0.29], [0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.28], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.05, 0.66, 0.0, 0.01, 0.0, 0.01, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -730.69\n",
      "Episode 103 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6982.0, -1652.0, -6894.0, -7891.0, -7300.0, -7396.0, -5849.0, -1944.0, -6383.0, -3805.0] \n",
      " [-15980.0, -7781.0, -16170.0, -17659.0, -17303.0, -16733.0, -14052.0, -8640.0, -15141.0, -10262.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.02, 0.02, 0.68, 0.0, 0.0, 0.0, 0.0, 0.28], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.97], [0.84, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16], [0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09], [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -741.47\n",
      "Episode 104 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6324.0, -6080.0, -5993.0, -6451.0, -6573.0, -6649.0, -6345.0, -7645.0, -1948.0, -6446.0] \n",
      " [-13986.0, -13453.0, -13135.0, -13925.0, -14750.0, -15233.0, -13151.0, -17023.0, -8787.0, -14438.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.04, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.92], [0.65, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.01, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.48], [0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -716.76\n",
      "Episode 105 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7827.0, -6530.0, -2887.0, -392.0, -6147.0, -6721.0, -5257.0, -6411.0, -6373.0, -5125.0] \n",
      " [-17276.0, -16134.0, -8473.0, -3781.0, -14474.0, -16467.0, -12870.0, -14404.0, -14835.0, -13011.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.89], [0.05, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.94], [0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15], [0.24, 0.1, 0.33, 0.04, 0.03, 0.01, 0.07, 0.18], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.87], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -675.22\n",
      "Episode 106 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6272.0, -974.0, -1458.0, -1698.0, -630.0, -6002.0, -3372.0, -560.0, -1942.0, -4519.0] \n",
      " [-14815.0, -3446.0, -3474.0, -5631.0, -2844.0, -14391.0, -9512.0, -4040.0, -7380.0, -13270.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78], [0.02, 0.05, 0.68, 0.0, 0.0, 0.0, 0.0, 0.25], [0.37, 0.05, 0.3, 0.0, 0.01, 0.0, 0.02, 0.25], [0.07, 0.01, 0.11, 0.0, 0.07, 0.0, 0.0, 0.75], [0.11, 0.25, 0.38, 0.0, 0.0, 0.0, 0.01, 0.25], [0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.05, 0.06, 0.36, 0.01, 0.09, 0.0, 0.02, 0.42], [0.07, 0.0, 0.24, 0.0, 0.08, 0.0, 0.0, 0.62], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -630.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 107 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6276.0, -7349.0, -2583.0, -7156.0, -5718.0, -1724.0, -6243.0, -2996.0, -5569.0, -6434.0] \n",
      " [-13084.0, -16180.0, -8089.0, -15967.0, -13320.0, -7018.0, -13201.0, -8662.0, -13254.0, -14360.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.18], [0.0, 0.0, 0.74, 0.0, 0.01, 0.0, 0.0, 0.24], [0.0, 0.0, 0.49, 0.0, 0.0, 0.0, 0.0, 0.51], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.94], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -635.02\n",
      "Episode 108 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6683.0, -6476.0, -7424.0, -8157.0, -10.0, -5440.0, -7803.0, -4085.0, -2597.0, -6411.0] \n",
      " [-15252.0, -14876.0, -16783.0, -18586.0, -3004.0, -13528.0, -17537.0, -12091.0, -8990.0, -14515.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09], [0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.97], [0.46, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12], [0.0, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.56], [0.0, 0.0, 0.57, 0.0, 0.0, 0.0, 0.0, 0.42], [0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.18], [0.02, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -725.04\n",
      "Episode 109 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-442.0, -2019.0, -1348.0, -7234.0, -6253.0, -8068.0, -6009.0, -8068.0, -3949.0, -5163.0] \n",
      " [-2917.0, -6903.0, -4359.0, -16142.0, -14923.0, -17072.0, -14459.0, -17072.0, -10859.0, -12293.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.18, 0.22, 0.23, 0.07, 0.05, 0.01, 0.09, 0.16], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05], [0.75, 0.06, 0.09, 0.01, 0.0, 0.0, 0.03, 0.06], [0.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.18, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.32], [0.18, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -663.33\n",
      "Episode 110 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6136.0, -8597.0, -8489.0, -942.0, -8591.0, -3991.0, -1671.0, -1337.0, -7566.0, -6468.0] \n",
      " [-14236.0, -18237.0, -18271.0, -4878.0, -18864.0, -11393.0, -4966.0, -6929.0, -16149.0, -14790.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.82], [0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.84], [0.08, 0.15, 0.49, 0.0, 0.0, 0.0, 0.0, 0.27], [0.0, 0.0, 0.35, 0.0, 0.0, 0.0, 0.0, 0.65], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03, 0.19, 0.72, 0.0, 0.0, 0.0, 0.0, 0.05], [0.08, 0.09, 0.6, 0.0, 0.0, 0.0, 0.01, 0.22], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -770.02\n",
      "Episode 111 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8254.0, -1196.0, -7344.0, -8142.0, -6760.0, -2741.0, -8140.0, -7228.0, -1277.0, -4262.0] \n",
      " [-18375.0, -6817.0, -16320.0, -16984.0, -15570.0, -10079.0, -16927.0, -15968.0, -5035.0, -12000.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.21, 0.01, 0.05, 0.0, 0.06, 0.0, 0.01, 0.64], [0.48, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.49, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.51], [0.08, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.87], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.02, 0.12, 0.71, 0.0, 0.02, 0.0, 0.0, 0.13], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -756.59\n",
      "Episode 112 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8262.0, -7076.0, -6889.0, -6221.0, -5299.0, -7417.0, -1240.0, -447.0, -1132.0, -3812.0] \n",
      " [-16940.0, -15636.0, -15072.0, -13873.0, -12730.0, -16332.0, -5182.0, -3714.0, -4701.0, -11138.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.15], [0.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23], [0.21, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.21, 0.01, 0.24, 0.0, 0.0, 0.0, 0.0, 0.53], [0.07, 0.08, 0.32, 0.02, 0.11, 0.01, 0.03, 0.37], [0.01, 0.02, 0.78, 0.0, 0.0, 0.0, 0.0, 0.19], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -678.32\n",
      "Episode 113 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5602.0, -1217.0, -499.0, -1382.0, -7447.0, -1379.0, -10.0, -6127.0, -6127.0, -8081.0] \n",
      " [-13495.0, -4548.0, -3677.0, -5692.0, -16195.0, -4421.0, -2537.0, -14565.0, -14565.0, -17552.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.72, 0.0, 0.01, 0.01, 0.06, 0.0, 0.01, 0.18], [0.31, 0.08, 0.2, 0.07, 0.15, 0.01, 0.07, 0.12], [0.12, 0.01, 0.09, 0.0, 0.05, 0.0, 0.0, 0.72], [0.01, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.02, 0.9, 0.0, 0.0, 0.0, 0.0, 0.08], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -659.05\n",
      "Episode 114 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7831.0, -8355.0, -975.0, -7692.0, -7905.0, -8323.0, -8085.0, -7157.0, -865.0, -8285.0] \n",
      " [-16175.0, -17320.0, -4617.0, -16009.0, -17093.0, -17269.0, -17356.0, -15726.0, -4080.0, -17308.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.02, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.96], [0.19, 0.04, 0.34, 0.01, 0.14, 0.0, 0.02, 0.27], [0.76, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24], [0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.97], [0.88, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.08], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -704.8\n",
      "Episode 115 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8842.0, -8842.0, -8723.0, -6131.0, -7530.0, -4888.0, -8902.0, -8757.0, -1493.0, -5644.0] \n",
      " [-18447.0, -18447.0, -18396.0, -14790.0, -17076.0, -12203.0, -18388.0, -18080.0, -6722.0, -13677.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67], [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.92], [0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.01, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.59], [0.19, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.71], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -783.64\n",
      "Episode 116 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7223.0, -9088.0, -8919.0, -7847.0, -1563.0, -7315.0, -7238.0, -3344.0, -3976.0, -1651.0] \n",
      " [-15482.0, -18907.0, -18903.0, -17143.0, -5702.0, -16172.0, -15712.0, -9225.0, -10980.0, -5558.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.13, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.82], [0.04, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.92], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.12, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.87], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.62, 0.0, 0.0, 0.0, 0.0, 0.38], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.01, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -741.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 117 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7583.0, -3167.0, -2671.0, -2372.0, -879.0, -7003.0, -2272.0, -7436.0, -678.0, -8760.0] \n",
      " [-16447.0, -10857.0, -10012.0, -9436.0, -7025.0, -15520.0, -8980.0, -16735.0, -4218.0, -18407.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31], [0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1], [0.01, 0.01, 0.49, 0.0, 0.01, 0.0, 0.0, 0.49], [0.24, 0.06, 0.2, 0.02, 0.05, 0.0, 0.03, 0.41], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.3, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.68], [0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.84], [0.65, 0.02, 0.03, 0.06, 0.05, 0.0, 0.03, 0.17], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.82]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -775.71\n",
      "Episode 118 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4514.0, -6965.0, -293.0, -7770.0, -2113.0, -10.0, -4314.0, -2468.0, -6367.0, -10.0] \n",
      " [-10692.0, -15471.0, -4579.0, -17122.0, -5665.0, -3867.0, -10977.0, -9273.0, -13170.0, -4106.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.35, 0.0, 0.0, 0.0, 0.0, 0.65], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84], [0.11, 0.17, 0.26, 0.07, 0.11, 0.03, 0.07, 0.18], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.12], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.01, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.92], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -646.79\n",
      "Episode 119 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6403.0, -6926.0, -5185.0, -5831.0, -1572.0, -5640.0, -5538.0, -8519.0, -5712.0, -6278.0] \n",
      " [-14725.0, -15525.0, -11644.0, -12022.0, -7793.0, -11947.0, -11910.0, -18239.0, -12136.0, -14974.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.57, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.88, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.11], [0.01, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.92], [0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68], [0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -746.65\n",
      "Episode 120 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5605.0, -7079.0, -7570.0, -7161.0, -2661.0, -7037.0, -6975.0, -722.0, -7248.0, -7068.0] \n",
      " [-12858.0, -15250.0, -16309.0, -14561.0, -9330.0, -14897.0, -14597.0, -3681.0, -14440.0, -14819.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.43, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57], [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.55, 0.02, 0.03, 0.06, 0.12, 0.0, 0.04, 0.18], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -661.23\n",
      "Episode 121 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8348.0, -7806.0, -8686.0, -4613.0, -8436.0, -8769.0, -1063.0, -3313.0, -997.0, -8568.0] \n",
      " [-17192.0, -16233.0, -18493.0, -13307.0, -18079.0, -17829.0, -5979.0, -12510.0, -5080.0, -18159.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.95], [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22], [0.22, 0.02, 0.14, 0.0, 0.0, 0.0, 0.01, 0.62], [0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.15], [0.91, 0.0, 0.0, 0.02, 0.01, 0.0, 0.0, 0.06], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -806.99\n",
      "Episode 122 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-176.0, -7922.0, -7931.0, -2818.0, -7964.0, -4252.0, -7404.0, -8466.0, -8012.0, -5028.0] \n",
      " [-3384.0, -16671.0, -16218.0, -8088.0, -16386.0, -12012.0, -15673.0, -16987.0, -16432.0, -12166.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.07, 0.06, 0.49, 0.03, 0.07, 0.0, 0.02, 0.25], [0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7], [0.39, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84], [0.49, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.51], [0.0, 0.0, 0.39, 0.0, 0.0, 0.0, 0.0, 0.61]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -689.47\n",
      "Episode 123 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5873.0, -7726.0, -7533.0, -6874.0, -8634.0, -6498.0, -7199.0, -6136.0, -7250.0, -1240.0] \n",
      " [-12932.0, -15826.0, -16165.0, -14207.0, -18323.0, -13844.0, -16440.0, -13482.0, -16106.0, -4815.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.76, 0.0, 0.0, 0.0, 0.0, 0.23], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.62, 0.0, 0.0, 0.0, 0.0, 0.38], [0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.89], [0.01, 0.05, 0.73, 0.0, 0.0, 0.0, 0.0, 0.21]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -664.38\n",
      "Episode 124 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9453.0, -8514.0, -9131.0, -2845.0, -1215.0, -4757.0, -3638.0, -8909.0, -6702.0, -8061.0] \n",
      " [-18865.0, -17133.0, -18617.0, -9683.0, -6261.0, -12323.0, -11344.0, -18504.0, -15765.0, -16990.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.3], [0.16, 0.01, 0.23, 0.0, 0.01, 0.0, 0.0, 0.59], [0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59], [0.02, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -776.88\n",
      "Episode 125 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3325.0, -7695.0, -7118.0, -8324.0, -7461.0, -7950.0, -8621.0, -4423.0, -8162.0, -7778.0] \n",
      " [-8937.0, -15255.0, -14833.0, -17661.0, -15161.0, -17253.0, -18389.0, -11176.0, -17057.0, -15629.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.21, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -757.42\n",
      "Episode 126 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9084.0, -5615.0, -2737.0, -6668.0, -7760.0, -7140.0, -5524.0, -1186.0, -4561.0, -5293.0] \n",
      " [-18413.0, -12701.0, -7826.0, -13968.0, -15726.0, -14644.0, -12482.0, -4755.0, -11430.0, -12365.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.74], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.01, 0.91, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.39, 0.0, 0.0, 0.0, 0.0, 0.61], [0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.19]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -682.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 127 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9541.0, -2406.0, -9117.0, -2491.0, -5321.0, -3082.0, -9187.0, -4115.0, -9391.0, -9408.0] \n",
      " [-18062.0, -8486.0, -18649.0, -7773.0, -13725.0, -9554.0, -18153.0, -12253.0, -18303.0, -18554.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.72, 0.0, 0.0, 0.0, 0.0, 0.26], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.63, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.36], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.1, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.74], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -786.16\n",
      "Episode 128 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7797.0, -5791.0, -5079.0, -7542.0, -9597.0, -4651.0, -391.0, -7542.0, -8817.0, -4013.0] \n",
      " [-15780.0, -13095.0, -11900.0, -15738.0, -18692.0, -10360.0, -3153.0, -15738.0, -17925.0, -9793.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83], [0.86, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.97], [0.27, 0.1, 0.17, 0.04, 0.04, 0.01, 0.07, 0.3], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.66, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -706.76\n",
      "Episode 129 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7400.0, -3690.0, -8633.0, -4824.0, -1065.0, -5079.0, -293.0, -7514.0, -5079.0, -8231.0] \n",
      " [-15140.0, -10248.0, -17336.0, -11602.0, -5314.0, -12134.0, -4694.0, -14139.0, -12134.0, -16458.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03, 0.13, 0.64, 0.0, 0.0, 0.0, 0.0, 0.2], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.11, 0.12, 0.28, 0.05, 0.14, 0.03, 0.06, 0.22], [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.2], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -712.16\n",
      "Episode 130 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7257.0, -3388.0, -2628.0, -6537.0, -6857.0, -8703.0, -8612.0, -5696.0, -7170.0, -4338.0] \n",
      " [-15985.0, -9084.0, -7174.0, -13727.0, -14218.0, -18171.0, -17272.0, -13075.0, -15186.0, -11230.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.6], [0.03, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.15], [0.0, 0.0, 0.76, 0.0, 0.0, 0.0, 0.0, 0.24], [0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91], [0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -717.58\n",
      "Episode 131 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9309.0, -6504.0, -1468.0, -4352.0, -6920.0, -8543.0, -8124.0, -6056.0, -6392.0, -7853.0] \n",
      " [-18638.0, -14086.0, -5658.0, -11355.0, -14681.0, -18248.0, -17309.0, -13667.0, -13994.0, -15631.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56], [0.03, 0.03, 0.76, 0.0, 0.01, 0.0, 0.0, 0.17], [0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19], [0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.19], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -726.38\n",
      "Episode 132 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7792.0, -8846.0, -7728.0, -2168.0, -3259.0, -8010.0, -8321.0, -9220.0, -5698.0, -9048.0] \n",
      " [-15396.0, -16963.0, -15363.0, -7810.0, -9689.0, -15708.0, -16144.0, -16778.0, -12801.0, -16930.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.01, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -716.17\n",
      "Episode 133 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7004.0, -7875.0, -9380.0, -9567.0, -9422.0, -7514.0, -7620.0, -7007.0, -9319.0, -8155.0] \n",
      " [-15639.0, -16826.0, -17861.0, -18110.0, -17860.0, -16219.0, -16441.0, -14713.0, -17811.0, -16617.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.81], [0.01, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.97], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -826.4\n",
      "Episode 134 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6540.0, -8412.0, -6996.0, -7285.0, -10055.0, -43.0, -5824.0, -11.0, -5040.0, -9767.0] \n",
      " [-14131.0, -17597.0, -14549.0, -15198.0, -19116.0, -5268.0, -12247.0, -4964.0, -11353.0, -19385.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.65, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.35], [0.16, 0.11, 0.16, 0.14, 0.14, 0.08, 0.09, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -775.51\n",
      "Episode 135 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2430.0, -7909.0, -7473.0, -1166.0, -7773.0, -4638.0, -3581.0, -7631.0, -6980.0, -7208.0] \n",
      " [-8780.0, -14843.0, -14716.0, -6140.0, -15490.0, -10956.0, -9760.0, -14988.0, -13642.0, -13735.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64], [0.08, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.84, 0.01, 0.01, 0.02, 0.02, 0.0, 0.02, 0.09], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.09, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.89], [0.42, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58], [0.74, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -674.97\n",
      "Episode 136 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3529.0, -915.0, -1607.0, -8454.0, -7550.0, -1578.0, -818.0, -259.0, -8236.0, -955.0] \n",
      " [-9491.0, -3487.0, -2588.0, -15289.0, -15017.0, -5348.0, -3009.0, -2331.0, -15449.0, -2897.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.9], [0.03, 0.03, 0.69, 0.0, 0.01, 0.0, 0.01, 0.23], [0.8, 0.02, 0.05, 0.0, 0.0, 0.0, 0.01, 0.11], [0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.01, 0.81, 0.0, 0.01, 0.0, 0.0, 0.16], [0.14, 0.03, 0.26, 0.02, 0.05, 0.0, 0.02, 0.48], [0.17, 0.11, 0.25, 0.05, 0.08, 0.02, 0.06, 0.26], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.03, 0.81, 0.0, 0.0, 0.0, 0.0, 0.15]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -581.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 137 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -10446.0, -9344.0, -6756.0, -8355.0, -4397.0, -5659.0, -9310.0, -9964.0, -9941.0] \n",
      " [-4185.0, -19524.0, -18513.0, -14731.0, -17007.0, -10076.0, -13474.0, -17637.0, -19365.0, -18910.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.54, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46], [0.21, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09], [0.0, 0.0, 0.76, 0.0, 0.0, 0.0, 0.0, 0.24], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.94]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -775.2\n",
      "Episode 138 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2121.0, -6998.0, -7594.0, -2755.0, -6976.0, -7594.0, -5665.0, -5457.0, -3254.0, -2272.0] \n",
      " [-7453.0, -14286.0, -14738.0, -8659.0, -13551.0, -14738.0, -12656.0, -11438.0, -9248.0, -8430.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.16, 0.0, 0.03, 0.0, 0.0, 0.81], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.83], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.2, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.68], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.35, 0.0, 0.0, 0.0, 0.0, 0.65]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -648.65\n",
      "Episode 139 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8235.0, -8431.0, -3540.0, -4625.0, -6456.0, -8080.0, -5675.0, -8182.0, -8907.0, -8832.0] \n",
      " [-15730.0, -15767.0, -10173.0, -11243.0, -13080.0, -15299.0, -12520.0, -15180.0, -17193.0, -17331.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.65, 0.0, 0.0, 0.0, 0.0, 0.35], [0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.16], [0.27, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.71], [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -726.57\n",
      "Episode 140 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6716.0, -7637.0, -2632.0, -2569.0, -3038.0, -7730.0, -3615.0, -1655.0, -9309.0, -6936.0] \n",
      " [-13968.0, -15521.0, -8289.0, -8784.0, -9171.0, -13899.0, -10405.0, -6585.0, -18456.0, -14666.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.05, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.88], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.92], [0.66, 0.0, 0.03, 0.01, 0.01, 0.0, 0.0, 0.29], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -766.13\n",
      "Episode 141 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9738.0, -6383.0, -9799.0, -8919.0, -6537.0, -2353.0, -9838.0, -8919.0, -2091.0, -7096.0] \n",
      " [-18216.0, -12486.0, -18252.0, -17227.0, -13398.0, -8625.0, -18593.0, -17227.0, -5156.0, -14498.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.97], [0.11, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -760.41\n",
      "Episode 142 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9641.0, -5525.0, -5354.0, -69.0, -7647.0, -971.0, -7647.0, -9664.0, -7940.0, -5614.0] \n",
      " [-18369.0, -11494.0, -12871.0, -3179.0, -15766.0, -3377.0, -15766.0, -18142.0, -15634.0, -13514.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33], [0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.85], [0.18, 0.09, 0.18, 0.15, 0.14, 0.06, 0.07, 0.13], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.5, 0.1, 0.14, 0.0, 0.0, 0.0, 0.02, 0.24], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.01, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.16], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -727.78\n",
      "Episode 143 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9691.0, -1017.0, -8970.0, -6381.0, -7700.0, -9611.0, -2703.0, -8550.0, -4398.0, -9341.0] \n",
      " [-17907.0, -4087.0, -17731.0, -13557.0, -15750.0, -18021.0, -9053.0, -16649.0, -10751.0, -17098.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.93], [0.89, 0.0, 0.0, 0.03, 0.01, 0.0, 0.01, 0.05], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.66, 0.0, 0.0, 0.0, 0.0, 0.34], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.29], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -695.07\n",
      "Episode 144 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1342.0, -4988.0, -8935.0, -7480.0, -1342.0, -3169.0, -9182.0, -8060.0, -2116.0, -5827.0] \n",
      " [-4104.0, -12243.0, -16274.0, -14540.0, -4104.0, -8277.0, -16837.0, -15898.0, -6188.0, -12770.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.88, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.11], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.35, 0.0, 0.0, 0.0, 0.0, 0.65], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.88, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.11], [0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.21], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.14, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.54], [0.28, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -608.57\n",
      "Episode 145 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8263.0, -7181.0, -8198.0, -9118.0, -9840.0, -9291.0, -9579.0, -7739.0, -4373.0, -599.0] \n",
      " [-15788.0, -13656.0, -14674.0, -17577.0, -17949.0, -17856.0, -18442.0, -14159.0, -10256.0, -6119.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2, 0.05, 0.18, 0.04, 0.06, 0.0, 0.04, 0.43]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -760.74\n",
      "Episode 146 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4089.0, -7597.0, -7468.0, -5114.0, -8131.0, -4168.0, -8679.0, -8700.0, -4713.0, -5992.0] \n",
      " [-9988.0, -14279.0, -14489.0, -11090.0, -14913.0, -9635.0, -16068.0, -16873.0, -10677.0, -12322.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.6], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -709.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 147 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7554.0, -8432.0, -653.0, -565.0, -8829.0, -8875.0, -1998.0, -7939.0, -7695.0, -565.0] \n",
      " [-14569.0, -16016.0, -3065.0, -2937.0, -16164.0, -17041.0, -6897.0, -14724.0, -14370.0, -2937.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.17, 0.2, 0.3, 0.01, 0.01, 0.0, 0.02, 0.29], [0.63, 0.02, 0.03, 0.08, 0.03, 0.0, 0.06, 0.16], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.01, 0.0, 0.11, 0.0, 0.01, 0.0, 0.0, 0.87], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.8], [0.63, 0.02, 0.03, 0.08, 0.03, 0.0, 0.06, 0.16]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -664.98\n",
      "Episode 148 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3235.0, -7030.0, -7459.0, -7490.0, -9410.0, -7839.0, -6986.0, -6581.0, -4834.0, -7562.0] \n",
      " [-8163.0, -13518.0, -14695.0, -13710.0, -16710.0, -15239.0, -12941.0, -12686.0, -11058.0, -13772.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81], [0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.89], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -670.88\n",
      "Episode 149 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7501.0, -8889.0, -5860.0, -4965.0, -6546.0, -7438.0, -7819.0, -8262.0, -9377.0, -4832.0] \n",
      " [-14734.0, -16747.0, -11974.0, -11103.0, -13550.0, -14272.0, -14414.0, -15988.0, -17493.0, -10565.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87], [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.8]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -732.2\n",
      "Episode 150 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1274.0, -857.0, -7988.0, -8372.0, -8372.0, -8755.0, -8308.0, -7880.0, -5311.0, -8629.0] \n",
      " [-5521.0, -3533.0, -13972.0, -14853.0, -14853.0, -15881.0, -14932.0, -14536.0, -11928.0, -15483.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.02, 0.85, 0.0, 0.0, 0.0, 0.0, 0.13], [0.63, 0.02, 0.06, 0.06, 0.02, 0.0, 0.04, 0.17], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.16], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -646.58\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 151 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9295.0, -8951.0, -1862.0, -2347.0, -7441.0, -12.0, -9002.0, -8658.0, -8355.0, -4043.0] \n",
      " [-16662.0, -16275.0, -5725.0, -7064.0, -13874.0, -4177.0, -15532.0, -16185.0, -15341.0, -9386.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.01, 0.01, 0.78, 0.0, 0.0, 0.0, 0.0, 0.2], [0.18, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.81], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.43, 0.0, 0.0, 0.0, 0.0, 0.57], [0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.84], [0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -750.96\n",
      "Episode 152 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9783.0, -5733.0, -9604.0, -9510.0, -486.0, -2172.0, -9368.0, -5766.0, -9961.0, -6119.0] \n",
      " [-17794.0, -11127.0, -16812.0, -16817.0, -3439.0, -7266.0, -16743.0, -11550.0, -17820.0, -12496.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.86], [0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.94], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.08, 0.11, 0.32, 0.01, 0.02, 0.0, 0.01, 0.44], [0.09, 0.0, 0.09, 0.0, 0.16, 0.0, 0.0, 0.65], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.84, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -727.51\n",
      "Episode 153 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7876.0, -8400.0, -7912.0, -2615.0, -7692.0, -3549.0, -4207.0, -5940.0, -8221.0, -940.0] \n",
      " [-14558.0, -14668.0, -13745.0, -6375.0, -13740.0, -9517.0, -10089.0, -11395.0, -14359.0, -4448.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.01, 0.0, 0.05, 0.0, 0.0, 0.92], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.41], [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67], [0.64, 0.01, 0.02, 0.04, 0.07, 0.0, 0.02, 0.2]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -624.78\n",
      "Episode 154 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5986.0, -9826.0, -9412.0, -10318.0, -9382.0, -3446.0, -9019.0, -2053.0, -9622.0, -1038.0] \n",
      " [-13041.0, -17649.0, -16757.0, -18467.0, -17042.0, -9533.0, -16570.0, -7063.0, -17512.0, -4795.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.84], [0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.97], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.94], [0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78], [0.01, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.18], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.86, 0.02, 0.02, 0.04, 0.01, 0.0, 0.03, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -745.97\n",
      "Episode 155 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9832.0, -7834.0, -9988.0, -2718.0, -9246.0, -2063.0, -8788.0, -8535.0, -5153.0, -9000.0] \n",
      " [-17782.0, -15100.0, -17944.0, -7106.0, -17258.0, -6939.0, -16038.0, -15962.0, -10571.0, -17172.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.45, 0.0, 0.0, 0.0, 0.0, 0.55], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.01, 0.0, 0.05, 0.0, 0.01, 0.0, 0.0, 0.93], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -687.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 156 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3294.0, -981.0, -9704.0, -6886.0, -2620.0, -9041.0, -4167.0, -8849.0, -3087.0, -4948.0] \n",
      " [-7800.0, -4799.0, -17229.0, -13426.0, -6499.0, -16225.0, -9441.0, -15977.0, -6959.0, -10804.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.96], [0.01, 0.04, 0.75, 0.0, 0.01, 0.0, 0.0, 0.19], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.66, 0.0, 0.0, 0.0, 0.0, 0.34], [0.08, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.91], [0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.76], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.26, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.73]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -628.69\n",
      "Episode 157 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8441.0, -3762.0, -10326.0, -8298.0, -2051.0, -6530.0, -7978.0, -1308.0, -2876.0, -1988.0] \n",
      " [-14992.0, -10420.0, -17886.0, -15026.0, -6715.0, -13732.0, -14857.0, -6119.0, -9197.0, -4454.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.84], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.17], [0.14, 0.01, 0.11, 0.0, 0.01, 0.0, 0.0, 0.73], [0.22, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.66], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -748.71\n",
      "Episode 158 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10523.0, -1338.0, -10201.0, -6612.0, -7731.0, -3995.0, -8838.0, -1779.0, -8715.0, -10293.0] \n",
      " [-19586.0, -4954.0, -18855.0, -13199.0, -15105.0, -11413.0, -15451.0, -5317.0, -15504.0, -18334.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.63, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37], [0.0, 0.04, 0.83, 0.0, 0.0, 0.0, 0.0, 0.13], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.73, 0.0, 0.0, 0.0, 0.0, 0.27], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.45, 0.05, 0.23, 0.0, 0.0, 0.0, 0.01, 0.27], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.75]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -775.17\n",
      "Episode 159 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8604.0, -10077.0, -2867.0, -7841.0, -1953.0, -10003.0, -6540.0, -5423.0, -6908.0, -637.0] \n",
      " [-15160.0, -17912.0, -8131.0, -14948.0, -5960.0, -17661.0, -12664.0, -11133.0, -12845.0, -3719.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.04, 0.0, 0.02, 0.0, 0.05, 0.0, 0.0, 0.89], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.48, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.49], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34, 0.07, 0.16, 0.08, 0.17, 0.0, 0.06, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -671.53\n",
      "Episode 160 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3370.0, -9258.0, -6761.0, -3696.0, -9190.0, -6855.0, -8622.0, -9260.0, -5394.0, -3288.0] \n",
      " [-10232.0, -15625.0, -11984.0, -10492.0, -16362.0, -13183.0, -15588.0, -15767.0, -11293.0, -9705.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.82], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.82]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -707.79\n",
      "Episode 161 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7419.0, -10064.0, -3153.0, -1445.0, -7250.0, -5305.0, -10142.0, -7341.0, -3905.0, -640.0] \n",
      " [-13194.0, -17156.0, -7113.0, -5520.0, -13566.0, -11383.0, -17302.0, -13186.0, -10111.0, -3473.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.88], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.13, 0.01, 0.06, 0.0, 0.09, 0.0, 0.01, 0.71], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.78, 0.01, 0.01, 0.07, 0.02, 0.0, 0.02, 0.09]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -630.63\n",
      "Episode 162 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-923.0, -957.0, -7946.0, -8505.0, -4552.0, -6444.0, -9196.0, -3054.0, -9098.0, -4552.0] \n",
      " [-5145.0, -3309.0, -13869.0, -14355.0, -9744.0, -12184.0, -16258.0, -6282.0, -15985.0, -9744.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.02, 0.9, 0.0, 0.02, 0.0, 0.0, 0.06], [0.04, 0.09, 0.7, 0.0, 0.0, 0.0, 0.0, 0.16], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.04, 0.0, 0.01, 0.0, 0.0, 0.94], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.04, 0.0, 0.01, 0.0, 0.0, 0.94]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -621.77\n",
      "Episode 163 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4805.0, -690.0, -8151.0, -5265.0, -5766.0, -5555.0, -4051.0, -7959.0, -5857.0, -8141.0] \n",
      " [-9393.0, -2571.0, -13716.0, -9694.0, -11686.0, -10467.0, -8739.0, -12836.0, -10614.0, -14178.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.62, 0.03, 0.04, 0.09, 0.02, 0.0, 0.06, 0.13], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -499.7\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Triple\\Agents_Results\\Single_Cross_TripleAC8test1\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 164 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3783.0, -7360.0, -4158.0, -10567.0, -11081.0, -11081.0, -7485.0, -11204.0, -335.0, -10275.0] \n",
      " [-9251.0, -14686.0, -11369.0, -17437.0, -18819.0, -18844.0, -15054.0, -18553.0, -4488.0, -17297.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.0, 0.26], [0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.26, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.19, 0.1, 0.24, 0.08, 0.07, 0.03, 0.09, 0.21], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -807.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 165 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6274.0, -11753.0, -9103.0, -10239.0, -10403.0, -11692.0, -12.0, -10891.0, -2043.0, -7287.0] \n",
      " [-12584.0, -20094.0, -17219.0, -18495.0, -18545.0, -19728.0, -2077.0, -18579.0, -7633.0, -13764.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.01, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.66], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.62], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -739.75\n",
      "Episode 166 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10444.0, -2512.0, -9590.0, -7051.0, -1118.0, -8113.0, -2213.0, -2242.0, -10130.0, -2189.0] \n",
      " [-17901.0, -6941.0, -16256.0, -12511.0, -3975.0, -14706.0, -5762.0, -5475.0, -17702.0, -5906.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.06, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.85], [0.95, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.04], [0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49], [0.86, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14], [0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -712.15\n",
      "Episode 167 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2595.0, -10376.0, -8080.0, -8129.0, -1245.0, -9083.0, -8357.0, -10112.0, -9147.0, -7204.0] \n",
      " [-7461.0, -18804.0, -14129.0, -13410.0, -5772.0, -15644.0, -14176.0, -17958.0, -15804.0, -12752.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.01, 0.0, 0.03, 0.0, 0.0, 0.95], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91], [0.12, 0.01, 0.07, 0.0, 0.06, 0.0, 0.01, 0.73], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.55, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -740.67\n",
      "Episode 168 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5124.0, -9540.0, -7033.0, -9284.0, -8522.0, -3759.0, -993.0, -7198.0, -7101.0, -8630.0] \n",
      " [-10794.0, -16257.0, -13116.0, -15239.0, -14250.0, -8240.0, -3688.0, -12784.0, -12652.0, -14216.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27], [0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49], [0.12, 0.27, 0.29, 0.0, 0.0, 0.0, 0.0, 0.32], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.03, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.88]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -632.78\n",
      "Episode 169 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9178.0, -11339.0, -10002.0, -11507.0, -10537.0, -10809.0, -11608.0, -9915.0, -8392.0, -8967.0] \n",
      " [-16810.0, -19143.0, -16847.0, -19071.0, -18396.0, -18297.0, -19050.0, -16941.0, -15849.0, -16592.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.53, 0.0, 0.0, 0.0, 0.0, 0.47], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -735.27\n",
      "Episode 170 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9831.0, -3537.0, -7424.0, -952.0, -10485.0, -6989.0, -8370.0, -8370.0, -5070.0, -10804.0] \n",
      " [-16504.0, -9642.0, -14608.0, -4455.0, -16497.0, -13554.0, -15147.0, -15147.0, -11061.0, -18111.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.62, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38], [0.74, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26], [0.03, 0.06, 0.79, 0.0, 0.0, 0.0, 0.01, 0.12], [0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -714.11\n",
      "Episode 171 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10792.0, -10291.0, -10414.0, -7135.0, -8913.0, -10863.0, -10582.0, -6809.0, -6245.0, -2394.0] \n",
      " [-17664.0, -17771.0, -17653.0, -12956.0, -14937.0, -18183.0, -17976.0, -11807.0, -11398.0, -7375.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.61, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.66], [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.2], [0.09, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.86]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -733.83\n",
      "Episode 172 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9941.0, -9355.0, -10084.0, -10179.0, -3632.0, -9340.0, -9065.0, -2207.0, -7481.0, -994.0] \n",
      " [-17240.0, -15574.0, -16657.0, -17128.0, -9431.0, -15133.0, -15457.0, -7297.0, -14766.0, -4862.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.17], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.07, 0.0, 0.01, 0.0, 0.0, 0.91], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.07, 0.61, 0.0, 0.0, 0.0, 0.0, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -734.99\n",
      "Episode 173 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10608.0, -10693.0, -10291.0, -3308.0, -9441.0, -4801.0, -6229.0, -8445.0, -8475.0, -6230.0] \n",
      " [-16891.0, -16921.0, -16699.0, -8162.0, -15894.0, -11217.0, -12554.0, -14174.0, -14697.0, -11952.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.66, 0.0, 0.0, 0.0, 0.0, 0.34], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.41], [0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.77], [0.55, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -742.21\n",
      "Episode 174 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9230.0, -2294.0, -10753.0, -426.0, -7581.0, -7314.0, -64.0, -4702.0, -9230.0, -10484.0] \n",
      " [-15404.0, -6653.0, -17754.0, -3158.0, -13227.0, -13355.0, -2996.0, -10057.0, -15404.0, -17209.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.01, 0.97, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.56, 0.06, 0.05, 0.08, 0.03, 0.01, 0.08, 0.14], [0.61, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.17, 0.1, 0.17, 0.15, 0.14, 0.07, 0.08, 0.13], [0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.47], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -713.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 175 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6970.0, -3754.0, -9974.0, -8280.0, -7952.0, -9634.0, -6771.0, -6832.0, -2921.0, -9604.0] \n",
      " [-11263.0, -7263.0, -16214.0, -14177.0, -13575.0, -16099.0, -12228.0, -11240.0, -7081.0, -15690.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.93], [0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.86, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14], [0.02, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.87], [0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.75]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -659.75\n",
      "Episode 176 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8887.0, -8453.0, -2710.0, -9368.0, -813.0, -10412.0, -8587.0, -8430.0, -9450.0, -7475.0] \n",
      " [-14706.0, -14077.0, -8175.0, -15972.0, -5349.0, -16945.0, -14496.0, -14420.0, -14724.0, -13108.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18], [0.43, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.06, 0.66, 0.0, 0.01, 0.0, 0.01, 0.23], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -740.01\n",
      "Episode 177 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10948.0, -10799.0, -12403.0, -12653.0, -6783.0, -6821.0, -12822.0, -11257.0, -11358.0, -5857.0] \n",
      " [-18132.0, -17638.0, -20355.0, -20279.0, -14108.0, -13689.0, -20410.0, -18052.0, -18019.0, -12370.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.45, 0.0, 0.0, 0.0, 0.0, 0.55], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.95], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.77], [0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.38, 0.0, 0.0, 0.0, 0.0, 0.62]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -905.08\n",
      "Episode 178 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8815.0, -9450.0, -8130.0, -6491.0, -5531.0, -8815.0, -1970.0, -8461.0, -7966.0, -8003.0] \n",
      " [-15534.0, -16069.0, -14486.0, -11830.0, -9622.0, -15534.0, -5730.0, -15426.0, -14638.0, -14808.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.33], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.18, 0.01, 0.33, 0.0, 0.0, 0.0, 0.0, 0.48], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -639.91\n",
      "Episode 179 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10971.0, -9984.0, -52.0, -3390.0, -9040.0, -4892.0, -2177.0, -9341.0, -8082.0, -11419.0] \n",
      " [-17624.0, -16434.0, -4503.0, -9262.0, -15821.0, -10551.0, -6077.0, -14896.0, -13808.0, -18815.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.16, 0.11, 0.16, 0.14, 0.14, 0.08, 0.09, 0.13], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.13], [0.74, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26], [0.13, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.86], [0.27, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.66], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -788.24\n",
      "Episode 180 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7882.0, -11186.0, -2751.0, -11615.0, -52.0, -1703.0, -11307.0, -7882.0, -11267.0, -4203.0] \n",
      " [-15600.0, -18391.0, -6666.0, -18195.0, -3199.0, -4609.0, -18260.0, -15600.0, -18468.0, -9007.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.04, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16, 0.11, 0.16, 0.14, 0.14, 0.08, 0.09, 0.13], [0.0, 0.03, 0.84, 0.0, 0.0, 0.0, 0.0, 0.13], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.54, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.45]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -742.18\n",
      "Episode 181 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9201.0, -9144.0, -4417.0, -8669.0, -4833.0, -8567.0, -5488.0, -8530.0, -10442.0, -8935.0] \n",
      " [-15485.0, -14783.0, -8308.0, -14405.0, -9890.0, -13448.0, -10235.0, -14040.0, -15791.0, -14100.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.67], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -629.51\n",
      "Episode 182 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9921.0, -9473.0, -5088.0, -6597.0, -9473.0, -8569.0, -4976.0, -8681.0, -8455.0, -558.0] \n",
      " [-16082.0, -15479.0, -10121.0, -12103.0, -15479.0, -13936.0, -9961.0, -13638.0, -14040.0, -4777.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.02, 0.72, 0.0, 0.01, 0.0, 0.0, 0.24]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -725.21\n",
      "Episode 183 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12226.0, -3697.0, -11559.0, -6364.0, -10768.0, -4290.0, -9235.0, -4290.0, -6903.0, -9248.0] \n",
      " [-19180.0, -7852.0, -18640.0, -11748.0, -18270.0, -8450.0, -15787.0, -8450.0, -12574.0, -15873.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.1, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.85], [0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18], [0.1, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.85], [0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13], [0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -751.24\n",
      "Episode 184 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3497.0, -9421.0, -3735.0, -10487.0, -2219.0, -2320.0, -10396.0, -5415.0, -4380.0, -9441.0] \n",
      " [-8136.0, -15296.0, -8481.0, -16377.0, -5457.0, -5899.0, -16647.0, -10401.0, -9358.0, -15799.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.08, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.29], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.01, 0.0, 0.06, 0.0, 0.01, 0.0, 0.0, 0.93], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.82], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -673.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 185 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10985.0, -10192.0, -3399.0, -9819.0, -10283.0, -11085.0, -10761.0, -11691.0, -11075.0, -2622.0] \n",
      " [-18398.0, -16537.0, -8914.0, -15138.0, -14750.0, -17296.0, -17195.0, -18838.0, -17288.0, -8215.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.52], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -779.52\n",
      "Episode 186 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11320.0, -10273.0, -5731.0, -3441.0, -3514.0, -4737.0, -8425.0, -10097.0, -10834.0, -9418.0] \n",
      " [-18000.0, -15690.0, -10605.0, -7932.0, -6796.0, -10141.0, -13439.0, -15988.0, -17156.0, -15630.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.22], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.49, 0.0, 0.01, 0.0, 0.0, 0.5], [0.0, 0.0, 0.72, 0.0, 0.0, 0.0, 0.0, 0.28], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -754.04\n",
      "Episode 187 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11444.0, -4708.0, -737.0, -4557.0, -11774.0, -11784.0, -11774.0, -11444.0, -11179.0, -737.0] \n",
      " [-17725.0, -8474.0, -4756.0, -8602.0, -17933.0, -18369.0, -17933.0, -17659.0, -17569.0, -4756.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22], [0.2, 0.05, 0.24, 0.04, 0.05, 0.0, 0.04, 0.38], [0.01, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.2, 0.05, 0.24, 0.04, 0.05, 0.0, 0.04, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -721.69\n",
      "Episode 188 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6827.0, -5770.0, -8264.0, -9250.0, -8751.0, -9250.0, -9824.0, -10483.0, -8594.0, -5720.0] \n",
      " [-11880.0, -10165.0, -12676.0, -15394.0, -13250.0, -15394.0, -16075.0, -16407.0, -13445.0, -9941.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6], [0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -698.56\n",
      "Episode 189 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10652.0, -6387.0, -10421.0, -4737.0, -10487.0, -7987.0, -11822.0, -10644.0, -8704.0, -11197.0] \n",
      " [-17515.0, -12344.0, -16138.0, -11695.0, -16611.0, -13973.0, -18682.0, -17640.0, -14312.0, -18262.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.58, 0.0, 0.0, 0.0, 0.0, 0.42], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -821.59\n",
      "Episode 190 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14.0, -3599.0, -5238.0, -11508.0, -9047.0, -6825.0, -7983.0, -10826.0, -9797.0, -4313.0] \n",
      " [-2893.0, -7215.0, -9801.0, -16280.0, -15123.0, -11353.0, -12885.0, -16498.0, -16479.0, -8768.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.06, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.92], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.82], [0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.62], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -650.85\n",
      "Episode 191 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7971.0, -10632.0, -12047.0, -14.0, -5113.0, -5724.0, -1401.0, -10734.0, -9680.0, -1067.0] \n",
      " [-13679.0, -16469.0, -18696.0, -2498.0, -11039.0, -11703.0, -4396.0, -16266.0, -14960.0, -4063.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.56], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.15], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.81, 0.0, 0.0, 0.06, 0.04, 0.0, 0.01, 0.08]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -706.07\n",
      "Episode 192 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5578.0, -1257.0, -7936.0, -387.0, -10365.0, -6986.0, -9459.0, -1210.0, -6014.0, -9459.0] \n",
      " [-10687.0, -5477.0, -13545.0, -3986.0, -14556.0, -11531.0, -14677.0, -5280.0, -10986.0, -14649.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.01, 0.88, 0.0, 0.01, 0.0, 0.0, 0.1], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.09, 0.52, 0.02, 0.07, 0.01, 0.02, 0.24], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -671.21\n",
      "Episode 193 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9536.0, -4487.0, -12631.0, -12664.0, -12923.0, -13310.0, -493.0, -3820.0, -10616.0, -12230.0] \n",
      " [-15210.0, -10505.0, -19288.0, -19033.0, -19308.0, -19264.0, -3571.0, -8544.0, -16562.0, -18777.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.58, 0.0, 0.0, 0.0, 0.0, 0.42], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.41, 0.0, 0.0, 0.0, 0.0, 0.59], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11], [0.64, 0.03, 0.02, 0.12, 0.03, 0.0, 0.06, 0.1], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.21, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -777.05\n",
      "Episode 194 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11326.0, -3472.0, -11901.0, -11567.0, -11421.0, -2543.0, -11577.0, -11444.0, -12131.0, -10602.0] \n",
      " [-17673.0, -9489.0, -17231.0, -17379.0, -17137.0, -8944.0, -17264.0, -17177.0, -17807.0, -16535.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.33], [0.0, 0.0, 0.41, 0.0, 0.0, 0.0, 0.0, 0.59], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.49], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.54, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -807.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 195 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3343.0, -7952.0, -6805.0, -1684.0, -10054.0, -9603.0, -2192.0, -7946.0, -4108.0, -9717.0] \n",
      " [-7797.0, -12811.0, -11866.0, -5115.0, -14094.0, -14874.0, -5347.0, -12895.0, -10291.0, -13973.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.96], [0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.74], [0.76, 0.01, 0.01, 0.03, 0.08, 0.0, 0.02, 0.09], [0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7], [0.02, 0.0, 0.02, 0.0, 0.03, 0.0, 0.0, 0.93], [0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -672.17\n",
      "Episode 196 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1612.0, -1115.0, -3183.0, -8513.0, -8797.0, -10050.0, -3513.0, -10587.0, -11146.0, -6015.0] \n",
      " [-5668.0, -5459.0, -7793.0, -12729.0, -12731.0, -16443.0, -7957.0, -16068.0, -17617.0, -10410.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.03], [0.01, 0.02, 0.27, 0.0, 0.09, 0.0, 0.01, 0.6], [0.04, 0.01, 0.43, 0.0, 0.04, 0.0, 0.0, 0.48], [0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.86], [0.0, 0.0, 0.69, 0.0, 0.0, 0.0, 0.0, 0.31], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.33, 0.0, 0.03, 0.0, 0.0, 0.61], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -694.2\n",
      "Episode 197 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9217.0, -7619.0, -10659.0, -8965.0, -8815.0, -7354.0, -11615.0, -8965.0, -11072.0, -11315.0] \n",
      " [-15694.0, -12327.0, -16080.0, -13706.0, -14328.0, -12021.0, -17162.0, -13706.0, -17044.0, -17673.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.01, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59], [0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -691.44\n",
      "Episode 198 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8167.0, -12797.0, -10665.0, -12213.0, -12346.0, -3650.0, -8493.0, -8441.0, -14.0, -8188.0] \n",
      " [-13899.0, -18594.0, -16750.0, -17592.0, -17215.0, -9223.0, -14841.0, -12909.0, -3795.0, -14039.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.36, 0.0, 0.0, 0.0, 0.0, 0.64], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.17], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.64, 0.0, 0.0, 0.0, 0.0, 0.36], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -781.29\n",
      "Episode 199 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8011.0, -12982.0, -5494.0, -8927.0, -5785.0, -8843.0, -10055.0, -11673.0, -4355.0, -7584.0] \n",
      " [-12331.0, -19020.0, -9252.0, -15367.0, -10485.0, -13512.0, -16385.0, -18239.0, -8394.0, -11938.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.46, 0.0, 0.0, 0.0, 0.0, 0.54]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -767.23\n",
      "Episode 200 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13603.0, -5015.0, -11899.0, -11048.0, -12239.0, -1513.0, -13094.0, -3076.0, -10549.0, -11713.0] \n",
      " [-18958.0, -9891.0, -17526.0, -16231.0, -18637.0, -5227.0, -19065.0, -7681.0, -16190.0, -17437.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71], [0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.84], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -762.33\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 201 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9693.0, -10516.0, -8668.0, -9209.0, -4273.0, -2863.0, -9371.0, -3699.0, -9927.0, -10516.0] \n",
      " [-14626.0, -14053.0, -12397.0, -14554.0, -8712.0, -5794.0, -15391.0, -8543.0, -14541.0, -14053.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.77], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.76, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -680.61\n",
      "Episode 202 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3565.0, -9514.0, -7156.0, -8944.0, -10024.0, -10529.0, -10131.0, -9835.0, -10847.0, -9619.0] \n",
      " [-8177.0, -14415.0, -12218.0, -13464.0, -14647.0, -16058.0, -15979.0, -15017.0, -16318.0, -15136.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.98], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -701.56\n",
      "Episode 203 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9723.0, -5187.0, -8294.0, -2509.0, -7638.0, -6828.0, -1679.0, -7526.0, -1679.0, -5843.0] \n",
      " [-15547.0, -9013.0, -13665.0, -4053.0, -11973.0, -10725.0, -5310.0, -11686.0, -5310.0, -9632.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.01, 0.47, 0.0, 0.0, 0.0, 0.0, 0.49], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.17, 0.04, 0.42, 0.01, 0.04, 0.0, 0.02, 0.3], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.17, 0.04, 0.42, 0.01, 0.04, 0.0, 0.02, 0.3], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -645.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 204 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12166.0, -9167.0, -10322.0, -2214.0, -11653.0, -10339.0, -11418.0, -7153.0, -11288.0, -11866.0] \n",
      " [-17190.0, -13792.0, -15416.0, -6828.0, -16578.0, -15356.0, -16564.0, -12357.0, -16574.0, -17311.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18], [0.25, 0.01, 0.43, 0.0, 0.0, 0.0, 0.0, 0.3], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -744.16\n",
      "Episode 205 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13487.0, -13189.0, -6869.0, -13327.0, -12807.0, -9139.0, -10549.0, -12850.0, -12869.0, -3344.0] \n",
      " [-18938.0, -18681.0, -12450.0, -18968.0, -18286.0, -14102.0, -17085.0, -19133.0, -18950.0, -8221.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.62, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.35]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -817.59\n",
      "Episode 206 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12840.0, -3422.0, -6699.0, -4217.0, -8277.0, -7647.0, -13205.0, -7157.0, -12405.0, -11491.0] \n",
      " [-18735.0, -8492.0, -11544.0, -9426.0, -13504.0, -12520.0, -18854.0, -11978.0, -18416.0, -17671.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.1, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.73], [0.01, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.96], [0.08, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.65], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.68], [0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.69], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -795.21\n",
      "Episode 207 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8658.0, -11434.0, -12074.0, -12131.0, -11668.0, -8856.0, -8856.0, -7268.0, -3253.0, -11427.0] \n",
      " [-14316.0, -16064.0, -17534.0, -16589.0, -16624.0, -14578.0, -14578.0, -13149.0, -5678.0, -16422.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.85], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84], [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.89], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.82]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -638.21\n",
      "Episode 208 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9028.0, -6151.0, -9535.0, -8217.0, -11802.0, -11063.0, -9459.0, -8225.0, -11694.0, -5306.0] \n",
      " [-14316.0, -10625.0, -14668.0, -13177.0, -16242.0, -16200.0, -14092.0, -12530.0, -17912.0, -9520.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.53, 0.0, 0.0, 0.0, 0.0, 0.47]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -670.99\n",
      "Episode 209 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2325.0, -9289.0, -10792.0, -7736.0, -12596.0, -105.0, -6020.0, -1615.0, -2149.0, -10030.0] \n",
      " [-8039.0, -13684.0, -15887.0, -12234.0, -17550.0, -3455.0, -10625.0, -5272.0, -7753.0, -15386.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.89, 0.0, 0.01, 0.0, 0.0, 0.1], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.18, 0.08, 0.19, 0.16, 0.15, 0.04, 0.06, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.07, 0.0, 0.06, 0.0, 0.02, 0.0, 0.0, 0.85], [0.11, 0.01, 0.05, 0.0, 0.16, 0.0, 0.01, 0.67], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -719.15\n",
      "Episode 210 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10458.0, -4339.0, -10688.0, -4817.0, -12705.0, -3748.0, -13460.0, -4758.0, -11674.0, -11874.0] \n",
      " [-15276.0, -10300.0, -15467.0, -10535.0, -18586.0, -10033.0, -18910.0, -10783.0, -15561.0, -18005.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.01, 0.0, 0.05, 0.0, 0.0, 0.92], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.21, 0.0, 0.01, 0.0, 0.0, 0.78], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03, 0.0, 0.04, 0.0, 0.16, 0.0, 0.0, 0.77], [0.46, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54], [0.0, 0.0, 0.19, 0.0, 0.02, 0.0, 0.0, 0.79], [0.0, 0.0, 0.58, 0.0, 0.0, 0.0, 0.0, 0.42], [0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.93]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -779.2\n",
      "Episode 211 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12083.0, -8697.0, -7428.0, -4322.0, -12721.0, -2479.0, -9146.0, -5979.0, -3377.0, -13365.0] \n",
      " [-17546.0, -14125.0, -13627.0, -8606.0, -18400.0, -5440.0, -14221.0, -12158.0, -7629.0, -19039.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.47, 0.0, 0.0, 0.0, 0.0, 0.53], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.04, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.95], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.76], [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -723.65\n",
      "Episode 212 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7797.0, -12336.0, -8345.0, -10183.0, -3258.0, -8345.0, -10116.0, -6494.0, -12028.0, -10474.0] \n",
      " [-12338.0, -17839.0, -12229.0, -13400.0, -8722.0, -12229.0, -14973.0, -10914.0, -17278.0, -14385.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.67], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.83], [0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -707.25\n",
      "Episode 213 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9074.0, -11031.0, -12932.0, -10936.0, -12932.0, -10490.0, -3160.0, -5176.0, -11503.0, -11828.0] \n",
      " [-13890.0, -16030.0, -18383.0, -15975.0, -18383.0, -15706.0, -6561.0, -9698.0, -16811.0, -16048.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -805.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 214 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1412.0, -11364.0, -12529.0, -9415.0, -12388.0, -12147.0, -3592.0, -13376.0, -9051.0, -10837.0] \n",
      " [-5149.0, -16619.0, -17833.0, -15062.0, -17408.0, -18059.0, -7325.0, -17805.0, -14431.0, -15507.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.04, 0.87, 0.0, 0.0, 0.0, 0.01, 0.07], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -794.85\n",
      "Episode 215 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8063.0, -12006.0, -1492.0, -1603.0, -7842.0, -8925.0, -7163.0, -11470.0, -9284.0, -11563.0] \n",
      " [-11672.0, -15722.0, -5012.0, -5803.0, -11923.0, -13074.0, -11348.0, -16060.0, -13475.0, -15752.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.17, 0.01, 0.44, 0.01, 0.01, 0.0, 0.01, 0.36], [0.0, 0.01, 0.93, 0.0, 0.0, 0.0, 0.0, 0.05], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.93], [0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.63, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -653.91\n",
      "Episode 216 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2102.0, -12029.0, -15.0, -298.0, -3875.0, -9926.0, -12833.0, -14298.0, -15.0, -12833.0] \n",
      " [-7515.0, -16200.0, -4435.0, -5344.0, -10169.0, -14789.0, -18903.0, -20827.0, -5036.0, -18903.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.01, 0.61, 0.0, 0.02, 0.0, 0.0, 0.32], [0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.92], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.11, 0.11, 0.21, 0.07, 0.16, 0.04, 0.08, 0.22], [0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -838.51\n",
      "Episode 217 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12916.0, -12903.0, -12010.0, -11822.0, -12771.0, -7640.0, -15.0, -11157.0, -12271.0, -12560.0] \n",
      " [-18049.0, -18294.0, -16229.0, -16644.0, -16638.0, -10959.0, -4360.0, -16270.0, -16372.0, -17821.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.18], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -777.07\n",
      "Episode 218 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10279.0, -8705.0, -13741.0, -9607.0, -10398.0, -1157.0, -1121.0, -12975.0, -9737.0, -9204.0] \n",
      " [-15135.0, -12474.0, -19007.0, -13613.0, -14714.0, -5900.0, -6757.0, -18187.0, -12712.0, -12705.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.54, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.88, 0.0, 0.0, 0.06, 0.01, 0.0, 0.02, 0.03], [0.67, 0.02, 0.02, 0.06, 0.04, 0.0, 0.04, 0.16], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -737.57\n",
      "Episode 219 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10255.0, -12211.0, -10226.0, -13354.0, -6724.0, -9020.0, -11097.0, -11551.0, -7573.0, -12404.0] \n",
      " [-14897.0, -17166.0, -14234.0, -18399.0, -11634.0, -13846.0, -15115.0, -15021.0, -11850.0, -17304.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.47, 0.0, 0.0, 0.0, 0.0, 0.53], [0.0, 0.0, 0.39, 0.0, 0.0, 0.0, 0.0, 0.61], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -795.45\n",
      "Episode 220 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10972.0, -10837.0, -4388.0, -12272.0, -4930.0, -9211.0, -11656.0, -1311.0, -9877.0, -12432.0] \n",
      " [-14657.0, -14570.0, -8743.0, -17191.0, -9523.0, -13257.0, -16553.0, -5723.0, -13372.0, -17699.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.99], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.02, 0.83, 0.0, 0.01, 0.0, 0.0, 0.14], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -709.87\n",
      "Episode 221 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12922.0, -11819.0, -2762.0, -7374.0, -4831.0, -10869.0, -5676.0, -12233.0, -11711.0, -11324.0] \n",
      " [-17431.0, -16167.0, -6901.0, -11280.0, -9321.0, -15841.0, -10547.0, -15744.0, -16111.0, -16051.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.64, 0.0, 0.0, 0.0, 0.0, 0.36], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -792.78\n",
      "Episode 222 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7274.0, -1136.0, -12431.0, -1531.0, -8832.0, -12904.0, -1613.0, -8305.0, -10973.0, -3504.0] \n",
      " [-11538.0, -3144.0, -17209.0, -4581.0, -12620.0, -17940.0, -5225.0, -11853.0, -16157.0, -6849.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.25, 0.08, 0.22, 0.01, 0.01, 0.0, 0.02, 0.42], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.02, 0.76, 0.0, 0.0, 0.0, 0.0, 0.21], [0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.86, 0.0, 0.0, 0.02, 0.01, 0.0, 0.01, 0.11], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -665.48\n",
      "Episode 223 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10184.0, -2079.0, -8878.0, -2521.0, -11746.0, -2445.0, -9128.0, -2105.0, -9554.0, -2521.0] \n",
      " [-14243.0, -5057.0, -13478.0, -6547.0, -16090.0, -3776.0, -13096.0, -5209.0, -13991.0, -6547.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.53, 0.01, 0.2, 0.0, 0.0, 0.0, 0.01, 0.25], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -629.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 224 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13689.0, -11919.0, -3266.0, -13845.0, -6823.0, -6499.0, -7212.0, -2974.0, -5219.0, -13754.0] \n",
      " [-18186.0, -17436.0, -7391.0, -19079.0, -11420.0, -10268.0, -11965.0, -6535.0, -8548.0, -18754.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.76], [0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.7], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -755.19\n",
      "Episode 225 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12873.0, -13538.0, -11376.0, -12178.0, -12934.0, -12922.0, -9079.0, -12550.0, -5872.0, -12686.0] \n",
      " [-16851.0, -17774.0, -16058.0, -16706.0, -18074.0, -17781.0, -14040.0, -17353.0, -10364.0, -16903.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.79], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -750.03\n",
      "Episode 226 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1964.0, -9363.0, -9363.0, -11701.0, -9089.0, -5676.0, -10140.0, -10628.0, -16.0, -11601.0] \n",
      " [-5081.0, -12289.0, -12289.0, -16001.0, -12050.0, -9656.0, -15228.0, -15706.0, -3697.0, -15963.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -709.14\n",
      "Episode 227 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13044.0, -13418.0, -514.0, -11102.0, -11469.0, -12958.0, -13056.0, -13129.0, -13169.0, -864.0] \n",
      " [-17547.0, -18317.0, -4866.0, -15937.0, -15675.0, -17851.0, -16787.0, -16974.0, -17048.0, -5370.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.05, 0.71, 0.02, 0.06, 0.0, 0.01, 0.13], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.02, 0.1, 0.65, 0.0, 0.01, 0.0, 0.01, 0.22]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -803.45\n",
      "Episode 228 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12566.0, -10555.0, -11785.0, -11368.0, -4785.0, -1682.0, -12928.0, -13845.0, -13913.0, -5013.0] \n",
      " [-17014.0, -14906.0, -16078.0, -15097.0, -9490.0, -6473.0, -17070.0, -17725.0, -17541.0, -9689.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.0, 0.26], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.21, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.07, 0.84, 0.0, 0.0, 0.0, 0.0, 0.08], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -798.76\n",
      "Episode 229 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9232.0, -13588.0, -11641.0, -11886.0, -11199.0, -6609.0, -11628.0, -7799.0, -14596.0, -13730.0] \n",
      " [-13839.0, -18840.0, -15619.0, -15706.0, -15470.0, -10953.0, -15360.0, -12130.0, -18219.0, -19050.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.92], [0.0, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.41], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -754.06\n",
      "Episode 230 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11624.0, -11274.0, -9758.0, -6256.0, -9255.0, -11255.0, -6877.0, -11003.0, -11749.0, -528.0] \n",
      " [-15298.0, -14883.0, -14067.0, -10352.0, -12327.0, -14810.0, -11371.0, -14617.0, -14659.0, -3197.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.57, 0.02, 0.01, 0.13, 0.1, 0.0, 0.05, 0.11]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -653.55\n",
      "Episode 231 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11217.0, -12736.0, -5080.0, -11932.0, -12553.0, -9485.0, -9910.0, -11792.0, -11863.0, -9487.0] \n",
      " [-15174.0, -16398.0, -9275.0, -15418.0, -16380.0, -13507.0, -13790.0, -15488.0, -15444.0, -13661.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -708.76\n",
      "Episode 232 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9861.0, -1960.0, -1793.0, -12173.0, -8599.0, -1347.0, -661.0, -11960.0, -3483.0, -12899.0] \n",
      " [-13777.0, -5777.0, -5343.0, -16883.0, -12194.0, -4555.0, -3642.0, -15775.0, -7425.0, -17203.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.78, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.2], [0.35, 0.03, 0.18, 0.0, 0.0, 0.0, 0.01, 0.44], [0.39, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61], [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8], [0.01, 0.05, 0.74, 0.0, 0.0, 0.0, 0.0, 0.19], [0.33, 0.09, 0.12, 0.03, 0.04, 0.01, 0.06, 0.32], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.12, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.71], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -692.93\n",
      "Episode 233 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4429.0, -10039.0, -12089.0, -10098.0, -9230.0, -8746.0, -12200.0, -8684.0, -11982.0, -12067.0] \n",
      " [-8563.0, -14348.0, -15974.0, -13900.0, -12964.0, -12688.0, -15619.0, -11647.0, -15570.0, -15649.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.92], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.56], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.49, 0.0, 0.0, 0.0, 0.0, 0.51], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -673.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 234 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8507.0, -2416.0, -13632.0, -14301.0, -14301.0, -13697.0, -13793.0, -10808.0, -6898.0, -14344.0] \n",
      " [-12587.0, -5764.0, -18211.0, -18579.0, -18579.0, -15996.0, -18003.0, -15866.0, -10559.0, -18373.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.08, 0.01, 0.12, 0.0, 0.24, 0.0, 0.0, 0.55], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.15], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.77]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -721.03\n",
      "Episode 235 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7987.0, -1574.0, -11462.0, -11628.0, -2255.0, -15197.0, -5734.0, -8621.0, -7263.0, -6111.0] \n",
      " [-12426.0, -5499.0, -15127.0, -15837.0, -6227.0, -20165.0, -9408.0, -13092.0, -11717.0, -10414.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.01, 0.5, 0.0, 0.02, 0.0, 0.0, 0.46], [0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.79], [0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.77], [0.02, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.96], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.63]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -809.22\n",
      "Episode 236 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10647.0, -2799.0, -3253.0, -4329.0, -12629.0, -8550.0, -5222.0, -5005.0, -10265.0, -13073.0] \n",
      " [-15831.0, -6395.0, -6980.0, -8321.0, -17278.0, -11938.0, -9582.0, -8964.0, -14314.0, -16780.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.01, 0.21, 0.0, 0.12, 0.0, 0.0, 0.66], [0.0, 0.0, 0.07, 0.0, 0.1, 0.0, 0.0, 0.83], [0.32, 0.0, 0.0, 0.0, 0.26, 0.0, 0.0, 0.42], [0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.62, 0.0, 0.0, 0.0, 0.0, 0.38], [0.0, 0.0, 0.84, 0.0, 0.0, 0.0, 0.0, 0.16], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -720.45\n",
      "Episode 237 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2519.0, -9096.0, -2288.0, -8651.0, -9249.0, -2177.0, -1417.0, -11443.0, -4618.0, -8318.0] \n",
      " [-3344.0, -12236.0, -5214.0, -11351.0, -12873.0, -4471.0, -3141.0, -14462.0, -8733.0, -12470.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.77, 0.0, 0.03, 0.0, 0.0, 0.19], [0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3], [0.3, 0.02, 0.36, 0.01, 0.01, 0.0, 0.01, 0.31], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.82, 0.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.16], [0.75, 0.04, 0.02, 0.05, 0.04, 0.0, 0.04, 0.07], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -598.08\n",
      "Episode 238 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12995.0, -4632.0, -10953.0, -13996.0, -11564.0, -11511.0, -13695.0, -9981.0, -11263.0, -1553.0] \n",
      " [-16829.0, -9415.0, -14880.0, -17032.0, -15754.0, -15360.0, -17133.0, -13276.0, -15577.0, -4750.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.22], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.29, 0.02, 0.1, 0.0, 0.0, 0.0, 0.01, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -666.22\n",
      "Episode 239 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11902.0, -11738.0, -7402.0, -7457.0, -82.0, -12597.0, -5599.0, -13009.0, -12625.0, -13009.0] \n",
      " [-15452.0, -15364.0, -11784.0, -12519.0, -3706.0, -16823.0, -9161.0, -16584.0, -16156.0, -16705.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.16, 0.1, 0.17, 0.15, 0.15, 0.06, 0.08, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.47], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -758.73\n",
      "Episode 240 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15094.0, -13916.0, -13027.0, -8431.0, -1274.0, -7190.0, -11647.0, -11878.0, -13313.0, -12027.0] \n",
      " [-18502.0, -17378.0, -17272.0, -13002.0, -6538.0, -12564.0, -15050.0, -14924.0, -17322.0, -14964.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.01, 0.93, 0.0, 0.01, 0.0, 0.0, 0.04], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.01, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.95], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.01, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.69]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -812.75\n",
      "Episode 241 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-887.0, -13970.0, -13734.0, -11949.0, -4256.0, -2788.0, -12048.0, -617.0, -10883.0, -12479.0] \n",
      " [-3641.0, -18359.0, -18203.0, -15381.0, -8398.0, -6321.0, -14788.0, -3003.0, -14705.0, -16233.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.07, 0.1, 0.44, 0.02, 0.13, 0.01, 0.03, 0.2], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.3], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.02, 0.74, 0.0, 0.01, 0.0, 0.0, 0.2], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -685.1\n",
      "Episode 242 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11925.0, -14686.0, -14343.0, -14270.0, -1056.0, -11953.0, -14326.0, -32.0, -32.0, -14752.0] \n",
      " [-16236.0, -17655.0, -17113.0, -17395.0, -4097.0, -15122.0, -17448.0, -2402.0, -2402.0, -17578.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.81, 0.0, 0.0, 0.03, 0.01, 0.0, 0.02, 0.12], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.14, 0.12, 0.13, 0.13, 0.13, 0.11, 0.11, 0.13], [0.14, 0.12, 0.13, 0.13, 0.13, 0.11, 0.11, 0.13], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -681.87\n",
      "Episode 243 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12849.0, -12030.0, -12994.0, -6052.0, -12717.0, -14769.0, -12464.0, -1853.0, -11400.0, -5393.0] \n",
      " [-15711.0, -15734.0, -15972.0, -9436.0, -16473.0, -17961.0, -16619.0, -3480.0, -15329.0, -8792.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.46, 0.0, 0.0, 0.0, 0.0, 0.54], [0.0, 0.0, 0.42, 0.0, 0.0, 0.0, 0.0, 0.58], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.72, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.86, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.11], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -671.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 244 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9693.0, -15695.0, -14582.0, -4571.0, -11740.0, -9102.0, -10279.0, -9728.0, -11829.0, -15556.0] \n",
      " [-13521.0, -19891.0, -19226.0, -8526.0, -14947.0, -12598.0, -14309.0, -13024.0, -14853.0, -19256.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -802.36\n",
      "Episode 245 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3462.0, -9407.0, -10654.0, -11897.0, -1962.0, -11689.0, -6125.0, -6003.0, -8033.0, -14104.0] \n",
      " [-7031.0, -12498.0, -14082.0, -15257.0, -5275.0, -15525.0, -9136.0, -10404.0, -12146.0, -17213.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.68, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32], [0.28, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72], [0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.25], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04, 0.14, 0.47, 0.0, 0.0, 0.0, 0.0, 0.35], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.66]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -689.97\n",
      "Episode 246 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1223.0, -9081.0, -15242.0, -7672.0, -10713.0, -5439.0, -14233.0, -3476.0, -10955.0, -10057.0] \n",
      " [-4538.0, -12557.0, -20373.0, -11505.0, -13766.0, -8621.0, -19167.0, -7856.0, -14009.0, -13647.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.04, 0.05, 0.79, 0.01, 0.01, 0.0, 0.02, 0.09], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.77], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.84, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -787.52\n",
      "Episode 247 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14378.0, -5092.0, -6919.0, -891.0, -6849.0, -11335.0, -7725.0, -6286.0, -13694.0, -10850.0] \n",
      " [-17367.0, -9155.0, -9791.0, -3864.0, -10628.0, -15251.0, -12826.0, -9697.0, -17614.0, -14909.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67], [0.34, 0.06, 0.12, 0.09, 0.22, 0.0, 0.05, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.29], [0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.19], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -740.03\n",
      "Episode 248 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6198.0, -10188.0, -12440.0, -13285.0, -11950.0, -13444.0, -14550.0, -1295.0, -17.0, -9176.0] \n",
      " [-10407.0, -13231.0, -16208.0, -17843.0, -17039.0, -17610.0, -19539.0, -5114.0, -4357.0, -12798.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.76, 0.0, 0.0, 0.0, 0.0, 0.24], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.02, 0.87, 0.0, 0.0, 0.0, 0.01, 0.08], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.85]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -795.98\n",
      "Episode 249 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6077.0, -13390.0, -7603.0, -13980.0, -9889.0, -7688.0, -13369.0, -4918.0, -9889.0, -14615.0] \n",
      " [-10060.0, -16189.0, -11742.0, -17099.0, -13070.0, -10524.0, -17022.0, -9663.0, -13070.0, -17840.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.63, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.35], [0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83], [0.0, 0.0, 0.72, 0.0, 0.0, 0.0, 0.0, 0.28], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -715.8\n",
      "Episode 250 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15066.0, -12348.0, -13074.0, -15431.0, -670.0, -17295.0, -1694.0, -17.0, -872.0, -14138.0] \n",
      " [-18048.0, -15856.0, -16110.0, -19550.0, -5001.0, -20517.0, -5500.0, -4225.0, -5271.0, -17125.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.26, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74], [0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84], [0.23, 0.13, 0.16, 0.09, 0.13, 0.02, 0.1, 0.15], [0.0, 0.0, 0.63, 0.0, 0.0, 0.0, 0.0, 0.37], [0.0, 0.01, 0.95, 0.0, 0.0, 0.0, 0.0, 0.04], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.16, 0.03, 0.1, 0.03, 0.08, 0.0, 0.02, 0.57], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -846.19\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 251 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11335.0, -14399.0, -6774.0, -14996.0, -6774.0, -11329.0, -9478.0, -13255.0, -14559.0, -3040.0] \n",
      " [-15050.0, -17266.0, -11455.0, -18668.0, -11455.0, -14566.0, -13701.0, -16615.0, -17273.0, -8225.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.22]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -761.42\n",
      "Episode 252 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1246.0, -13827.0, -14801.0, -13455.0, -14715.0, -12204.0, -9977.0, -12395.0, -15090.0, -13827.0] \n",
      " [-4076.0, -17124.0, -17108.0, -16943.0, -17624.0, -15950.0, -13721.0, -15307.0, -17383.0, -17124.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.02, 0.9, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.82], [0.0, 0.0, 0.49, 0.0, 0.0, 0.0, 0.0, 0.51], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -732.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 253 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12058.0, -9841.0, -4351.0, -7366.0, -3063.0, -4424.0, -12714.0, -3923.0, -10608.0, -11453.0] \n",
      " [-14766.0, -12827.0, -9160.0, -11144.0, -5738.0, -6955.0, -15123.0, -7210.0, -12957.0, -14632.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22], [0.51, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.49], [0.1, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.85], [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.83], [0.2, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.73], [0.43, 0.0, 0.02, 0.0, 0.05, 0.0, 0.0, 0.51], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -651.48\n",
      "Episode 254 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15266.0, -14357.0, -2681.0, -13265.0, -15286.0, -9746.0, -16412.0, -14370.0, -6743.0, -13242.0] \n",
      " [-19133.0, -18160.0, -5897.0, -16111.0, -18842.0, -13951.0, -19200.0, -18209.0, -10629.0, -15138.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.23], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -764.93\n",
      "Episode 255 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11761.0, -1346.0, -4295.0, -16146.0, -2450.0, -16243.0, -16431.0, -16459.0, -14358.0, -8545.0] \n",
      " [-16046.0, -4172.0, -8256.0, -19222.0, -6395.0, -19763.0, -19118.0, -19478.0, -19034.0, -12161.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.48, 0.09, 0.11, 0.07, 0.1, 0.0, 0.06, 0.08], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.18], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -766.43\n",
      "Episode 256 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12907.0, -12770.0, -13277.0, -13883.0, -5561.0, -14347.0, -994.0, -13277.0, -13737.0, -9166.0] \n",
      " [-15188.0, -15947.0, -16245.0, -16777.0, -9593.0, -16791.0, -4615.0, -16245.0, -16148.0, -13106.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.55, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.35, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65], [0.89, 0.0, 0.0, 0.06, 0.01, 0.0, 0.01, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -745.32\n",
      "Episode 257 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13396.0, -5701.0, -13109.0, -7500.0, -11430.0, -8492.0, -12082.0, -13452.0, -13099.0, -15764.0] \n",
      " [-17158.0, -7976.0, -15735.0, -10373.0, -14370.0, -12205.0, -14515.0, -17457.0, -15760.0, -18673.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.92], [0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -683.87\n",
      "Episode 258 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8050.0, -4932.0, -16292.0, -16372.0, -14074.0, -3849.0, -15734.0, -15734.0, -16145.0, -2252.0] \n",
      " [-14420.0, -9195.0, -18595.0, -18548.0, -17780.0, -8473.0, -18700.0, -18672.0, -19447.0, -6046.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.77, 0.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.21]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -816.01\n",
      "Episode 259 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11926.0, -8218.0, -15560.0, -7941.0, -15242.0, -16308.0, -14954.0, -8904.0, -7391.0, -9148.0] \n",
      " [-15910.0, -10647.0, -18434.0, -10992.0, -18123.0, -19432.0, -17888.0, -12074.0, -10508.0, -12712.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.57, 0.0, 0.0, 0.0, 0.0, 0.43], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -816.4\n",
      "Episode 260 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13992.0, -6245.0, -7507.0, -12835.0, -13395.0, -7127.0, -8256.0, -5927.0, -10502.0, -5337.0] \n",
      " [-16903.0, -9589.0, -10294.0, -15400.0, -15250.0, -9766.0, -9842.0, -8925.0, -13485.0, -7531.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83], [0.28, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.43, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57], [0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.62], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -680.34\n",
      "Episode 261 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4603.0, -16431.0, -9850.0, -290.0, -15705.0, -10795.0, -15263.0, -6947.0, -10651.0, -9850.0] \n",
      " [-8712.0, -17964.0, -14211.0, -4789.0, -19227.0, -15853.0, -18058.0, -12358.0, -14838.0, -14211.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.29, 0.07, 0.13, 0.19, 0.09, 0.02, 0.09, 0.12], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.76], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -823.97\n",
      "Episode 262 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15138.0, -15152.0, -8603.0, -15915.0, -14164.0, -11219.0, -14164.0, -14594.0, -15553.0, -15553.0] \n",
      " [-18606.0, -17933.0, -11943.0, -19022.0, -16832.0, -13788.0, -16832.0, -17787.0, -17727.0, -17777.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -782.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 263 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13881.0, -2882.0, -15980.0, -6674.0, -14836.0, -15251.0, -535.0, -6846.0, -7710.0, -15181.0] \n",
      " [-17218.0, -6858.0, -18037.0, -9011.0, -17631.0, -19109.0, -4947.0, -9826.0, -10553.0, -17035.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.07, 0.08, 0.19, 0.04, 0.19, 0.01, 0.05, 0.37], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -746.02\n",
      "Episode 264 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1837.0, -16847.0, -8423.0, -3157.0, -18.0, -9317.0, -1851.0, -11694.0, -3104.0, -4276.0] \n",
      " [-6501.0, -18724.0, -11615.0, -6936.0, -4157.0, -12738.0, -6000.0, -15854.0, -7146.0, -8480.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.01, 0.82, 0.0, 0.0, 0.0, 0.0, 0.17], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.02, 0.89, 0.0, 0.02, 0.0, 0.0, 0.07], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.25, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.73], [0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -835.22\n",
      "Episode 265 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16332.0, -15710.0, -16442.0, -6001.0, -16645.0, -8519.0, -15203.0, -16332.0, -14348.0, -16442.0] \n",
      " [-19663.0, -18634.0, -18562.0, -11092.0, -18878.0, -12822.0, -18742.0, -19711.0, -17902.0, -18562.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.71, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -828.03\n",
      "Episode 266 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15878.0, -11405.0, -8872.0, -10525.0, -15878.0, -15547.0, -7133.0, -15075.0, -9462.0, -2707.0] \n",
      " [-18187.0, -14817.0, -12629.0, -13520.0, -18187.0, -18200.0, -11246.0, -18323.0, -12069.0, -5331.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.39, 0.0, 0.0, 0.0, 0.0, 0.61], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.71], [0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -778.17\n",
      "Episode 267 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16808.0, -12593.0, -10399.0, -10283.0, -15906.0, -14688.0, -14139.0, -5252.0, -12946.0, -1316.0] \n",
      " [-18520.0, -15575.0, -14393.0, -14259.0, -18887.0, -18420.0, -17274.0, -8823.0, -16020.0, -3472.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.02, 0.91, 0.0, 0.0, 0.0, 0.0, 0.06]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -742.39\n",
      "Episode 268 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18135.0, -15767.0, -6645.0, -2298.0, -19244.0, -14781.0, -16934.0, -15016.0, -16671.0, -15442.0] \n",
      " [-21991.0, -16962.0, -10005.0, -5677.0, -21837.0, -16949.0, -20435.0, -18717.0, -20656.0, -19840.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.76, 0.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.21], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -833.7\n",
      "Episode 269 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12661.0, -11021.0, -7151.0, -9994.0, -14198.0, -9163.0, -5008.0, -12264.0, -4873.0, -12737.0] \n",
      " [-14886.0, -13741.0, -10283.0, -12939.0, -16924.0, -12760.0, -8866.0, -14137.0, -7618.0, -15497.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -711.89\n",
      "Episode 270 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11956.0, -1683.0, -2668.0, -1905.0, -10968.0, -8526.0, -5487.0, -15628.0, -4268.0, -1492.0] \n",
      " [-15286.0, -4911.0, -5348.0, -4189.0, -14715.0, -13007.0, -8619.0, -17851.0, -6809.0, -4078.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.01, 0.92, 0.0, 0.0, 0.0, 0.0, 0.07], [0.39, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.58], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.55, 0.0, 0.0, 0.0, 0.0, 0.45], [0.02, 0.01, 0.31, 0.0, 0.05, 0.0, 0.01, 0.6]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -747.47\n",
      "Episode 271 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12609.0, -15859.0, -1960.0, -12669.0, -12642.0, -1816.0, -15009.0, -13542.0, -12669.0, -17084.0] \n",
      " [-16480.0, -19102.0, -6689.0, -14871.0, -14787.0, -4998.0, -18141.0, -16961.0, -14871.0, -19116.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.8, 0.01, 0.0, 0.03, 0.03, 0.0, 0.02, 0.11], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -763.9\n",
      "Episode 272 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3303.0, -14136.0, -17748.0, -16736.0, -4610.0, -4752.0, -14691.0, -16966.0, -14190.0, -245.0] \n",
      " [-7581.0, -17049.0, -19704.0, -19901.0, -9283.0, -8791.0, -18542.0, -19987.0, -17469.0, -3749.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.34, 0.01, 0.23, 0.0, 0.0, 0.0, 0.0, 0.41], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.04, 0.06, 0.57, 0.02, 0.06, 0.01, 0.02, 0.23]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -820.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 273 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15829.0, -18666.0, -18528.0, -19050.0, -16618.0, -1907.0, -16496.0, -13111.0, -3980.0, -17473.0] \n",
      " [-19914.0, -21282.0, -21310.0, -21010.0, -18367.0, -4636.0, -18016.0, -16795.0, -7818.0, -20607.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.66], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.31, 0.01, 0.07, 0.01, 0.01, 0.0, 0.01, 0.58], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.75, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.21], [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -856.19\n",
      "Episode 274 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6124.0, -15428.0, -6865.0, -1538.0, -15962.0, -16526.0, -12920.0, -15044.0, -213.0, -1764.0] \n",
      " [-10361.0, -18196.0, -9172.0, -4308.0, -17692.0, -18596.0, -15909.0, -17751.0, -2827.0, -3399.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.04, 0.78, 0.0, 0.0, 0.0, 0.0, 0.17], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.62], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.05, 0.07, 0.51, 0.02, 0.07, 0.01, 0.03, 0.25], [0.93, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.05]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -740.09\n",
      "Episode 275 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17820.0, -92.0, -16022.0, -10124.0, -13717.0, -19.0, -13633.0, -13851.0, -8550.0, -17397.0] \n",
      " [-18813.0, -4147.0, -19039.0, -12817.0, -16374.0, -3892.0, -17156.0, -17304.0, -11245.0, -18440.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.16, 0.1, 0.17, 0.15, 0.15, 0.06, 0.08, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.13, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -767.53\n",
      "Episode 276 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18317.0, -10040.0, -3725.0, -15453.0, -17822.0, -16662.0, -7941.0, -12805.0, -10980.0, -11712.0] \n",
      " [-19916.0, -13479.0, -6752.0, -18303.0, -19945.0, -20301.0, -12088.0, -15227.0, -14161.0, -16339.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -785.97\n",
      "Episode 277 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6155.0, -12004.0, -16921.0, -3650.0, -16247.0, -15763.0, -17125.0, -10177.0, -1810.0, -17396.0] \n",
      " [-9137.0, -13935.0, -20138.0, -6997.0, -18045.0, -18182.0, -19458.0, -12470.0, -4189.0, -19405.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.97], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.68, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.28], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.77], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.23, 0.03, 0.31, 0.0, 0.0, 0.0, 0.01, 0.41], [0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.15]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -780.38\n",
      "Episode 278 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3034.0, -14724.0, -7361.0, -11661.0, -14497.0, -5616.0, -14095.0, -816.0, -4401.0, -12478.0] \n",
      " [-5854.0, -18587.0, -10935.0, -13459.0, -17202.0, -9375.0, -16900.0, -3873.0, -8264.0, -14801.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.93, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.0, 0.26], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.73, 0.01, 0.01, 0.12, 0.05, 0.0, 0.03, 0.07], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -699.68\n",
      "Episode 279 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11304.0, -14886.0, -930.0, -7719.0, -15544.0, -16917.0, -6118.0, -930.0, -18372.0, -6021.0] \n",
      " [-14526.0, -17142.0, -2708.0, -11635.0, -18624.0, -20853.0, -9218.0, -2708.0, -21436.0, -9416.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.01, 0.88, 0.0, 0.02, 0.0, 0.0, 0.08], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.01, 0.88, 0.0, 0.02, 0.0, 0.0, 0.08], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -744.87\n",
      "Episode 280 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5399.0, -15752.0, -8088.0, -15865.0, -5495.0, -9128.0, -15827.0, -2892.0, -947.0, -5733.0] \n",
      " [-8427.0, -17561.0, -10912.0, -17737.0, -7383.0, -11232.0, -17892.0, -5451.0, -3678.0, -8773.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33], [0.1, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.86], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24, 0.0, 0.0, 0.0, 0.19, 0.0, 0.0, 0.57], [0.07, 0.12, 0.54, 0.01, 0.02, 0.0, 0.01, 0.23], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -721.98\n",
      "Episode 281 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17019.0, -17984.0, -1642.0, -9935.0, -17220.0, -16681.0, -5956.0, -16694.0, -2230.0, -16069.0] \n",
      " [-18715.0, -19493.0, -3778.0, -13468.0, -19674.0, -19121.0, -9796.0, -19210.0, -5091.0, -18385.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.21, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79], [0.49, 0.01, 0.01, 0.03, 0.11, 0.0, 0.01, 0.34], [0.84, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.13, 0.01, 0.07, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -756.07\n",
      "Episode 282 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12817.0, -667.0, -6850.0, -3381.0, -16566.0, -4082.0, -11116.0, -15680.0, -5899.0, -13053.0] \n",
      " [-14856.0, -3181.0, -10518.0, -8020.0, -19183.0, -8523.0, -13340.0, -17349.0, -9520.0, -16500.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.59, 0.04, 0.03, 0.17, 0.04, 0.0, 0.07, 0.05], [0.0, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.96], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.74, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.18], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -759.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 283 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2711.0, -14736.0, -17632.0, -15620.0, -9381.0, -4035.0, -1492.0, -14595.0, -15561.0, -7884.0] \n",
      " [-6425.0, -16886.0, -20224.0, -18374.0, -11769.0, -8327.0, -4762.0, -17339.0, -18253.0, -10460.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.98], [0.24, 0.09, 0.34, 0.03, 0.02, 0.0, 0.04, 0.23], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.55, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -806.35\n",
      "Episode 284 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10529.0, -8885.0, -16714.0, -6780.0, -15525.0, -17918.0, -14925.0, -12860.0, -17913.0, -17547.0] \n",
      " [-15151.0, -12575.0, -19273.0, -10777.0, -17355.0, -20161.0, -17699.0, -16210.0, -20030.0, -19402.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.61, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -815.37\n",
      "Episode 285 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8761.0, -17884.0, -15410.0, -6549.0, -16212.0, -3581.0, -2285.0, -1470.0, -14667.0, -10235.0] \n",
      " [-11816.0, -20105.0, -16612.0, -8471.0, -18066.0, -6222.0, -5130.0, -4499.0, -17478.0, -13181.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.3], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.1, 0.0, 0.0, 0.0, 0.26, 0.0, 0.0, 0.63], [0.7, 0.0, 0.0, 0.05, 0.12, 0.0, 0.01, 0.11], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -726.14\n",
      "Episode 286 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10229.0, -8233.0, -16875.0, -10315.0, -1786.0, -11119.0, -10921.0, -10570.0, -6490.0, -15419.0] \n",
      " [-12947.0, -10623.0, -19408.0, -12287.0, -5365.0, -13696.0, -13482.0, -12791.0, -9464.0, -18282.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4], [0.08, 0.0, 0.01, 0.0, 0.11, 0.0, 0.0, 0.8], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55], [0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.76]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -760.61\n",
      "Episode 287 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4748.0, -10714.0, -10262.0, -10730.0, -4677.0, -10212.0, -12211.0, -14663.0, -15866.0, -18177.0] \n",
      " [-8872.0, -13513.0, -13248.0, -13796.0, -8245.0, -13085.0, -14809.0, -16599.0, -19225.0, -19892.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.99], [0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7], [0.73, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.26], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -847.22\n",
      "Episode 288 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12017.0, -10493.0, -12204.0, -15452.0, -10510.0, -11230.0, -9419.0, -963.0, -15443.0, -14110.0] \n",
      " [-14447.0, -12864.0, -14846.0, -18096.0, -12300.0, -13192.0, -12464.0, -3518.0, -17565.0, -15921.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.06, 0.55, 0.0, 0.01, 0.0, 0.0, 0.36], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.71, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -747.52\n",
      "Episode 289 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10018.0, -6400.0, -16229.0, -17605.0, -19079.0, -13238.0, -2259.0, -18154.0, -18023.0, -11116.0] \n",
      " [-11462.0, -10018.0, -18942.0, -19926.0, -19438.0, -16270.0, -5291.0, -19664.0, -19424.0, -14112.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.02, 0.91, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -777.83\n",
      "Episode 290 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16129.0, -14302.0, -1078.0, -3946.0, -15320.0, -10094.0, -6397.0, -2878.0, -14722.0, -1775.0] \n",
      " [-18106.0, -15994.0, -3057.0, -6414.0, -15805.0, -13560.0, -9961.0, -5746.0, -16486.0, -4918.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.29, 0.05, 0.1, 0.01, 0.03, 0.0, 0.03, 0.5], [0.15, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.82], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02, 0.0, 0.01, 0.0, 0.13, 0.0, 0.0, 0.84], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.1, 0.09, 0.57, 0.01, 0.0, 0.0, 0.02, 0.21]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -742.56\n",
      "Episode 291 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-855.0, -14947.0, -14682.0, -14534.0, -11029.0, -1890.0, -6968.0, -15783.0, -11673.0, -14691.0] \n",
      " [-3854.0, -16709.0, -16778.0, -16499.0, -12766.0, -6533.0, -10332.0, -18025.0, -13990.0, -17681.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.69, 0.02, 0.02, 0.03, 0.02, 0.0, 0.04, 0.18], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4], [0.17, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.79], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -786.41\n",
      "Episode 292 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11834.0, -2217.0, -2317.0, -3926.0, -11911.0, -15379.0, -15196.0, -3192.0, -15067.0, -7414.0] \n",
      " [-13561.0, -5550.0, -5764.0, -4645.0, -13345.0, -16994.0, -17460.0, -6332.0, -16911.0, -9367.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.76, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24], [0.04, 0.0, 0.03, 0.0, 0.01, 0.0, 0.0, 0.92], [0.08, 0.0, 0.16, 0.0, 0.01, 0.0, 0.0, 0.75], [0.23, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.2], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -669.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 293 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4090.0, -11002.0, -9858.0, -16406.0, -15645.0, -15969.0, -9220.0, -11064.0, -17407.0, -14078.0] \n",
      " [-6755.0, -12972.0, -11994.0, -18967.0, -17826.0, -17730.0, -11654.0, -12717.0, -18983.0, -16860.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.95], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -723.17\n",
      "Episode 294 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12571.0, -509.0, -17236.0, -19414.0, -17035.0, -15469.0, -17643.0, -4682.0, -7268.0, -19414.0] \n",
      " [-15811.0, -4206.0, -18391.0, -19880.0, -18886.0, -18374.0, -18410.0, -6381.0, -9598.0, -19880.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.1, 0.08, 0.51, 0.06, 0.11, 0.03, 0.03, 0.08], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.68, 0.0, 0.0, 0.0, 0.0, 0.32], [0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.47, 0.0, 0.0, 0.0, 0.0, 0.53], [0.0, 0.0, 0.72, 0.0, 0.0, 0.0, 0.0, 0.28], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -786.56\n",
      "Episode 295 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12986.0, -14884.0, -14691.0, -16468.0, -10954.0, -5299.0, -14504.0, -8578.0, -678.0, -14027.0] \n",
      " [-15109.0, -16306.0, -17429.0, -19106.0, -13388.0, -7912.0, -17305.0, -10013.0, -5245.0, -16648.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.86], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9], [0.05, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.93], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.61, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39], [0.56, 0.02, 0.02, 0.17, 0.08, 0.0, 0.05, 0.1], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -756.35\n",
      "Episode 296 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2359.0, -17595.0, -13626.0, -1381.0, -12745.0, -4100.0, -5966.0, -14142.0, -11885.0, -3686.0] \n",
      " [-3441.0, -18344.0, -16224.0, -2816.0, -15870.0, -6825.0, -10690.0, -16322.0, -14405.0, -6475.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.08, 0.8, 0.0, 0.0, 0.0, 0.01, 0.08], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.11, 0.2, 0.24, 0.0, 0.01, 0.0, 0.01, 0.43], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.86, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -711.43\n",
      "Episode 297 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14460.0, -3314.0, -8326.0, -10566.0, -16708.0, -17302.0, -13967.0, -15244.0, -15984.0, -14480.0] \n",
      " [-17127.0, -6237.0, -12177.0, -14600.0, -18480.0, -19286.0, -15451.0, -17204.0, -18371.0, -15735.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.06, 0.0, 0.55, 0.0, 0.0, 0.0, 0.0, 0.38], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -763.34\n",
      "Episode 298 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17564.0, -7988.0, -14987.0, -16331.0, -9183.0, -4162.0, -7823.0, -17316.0, -17127.0, -9690.0] \n",
      " [-17956.0, -9751.0, -16842.0, -17768.0, -12756.0, -5693.0, -9368.0, -17459.0, -17820.0, -13903.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.41, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.55], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -740.76\n",
      "Episode 299 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13834.0, -18140.0, -10629.0, -15191.0, -10521.0, -14316.0, -11402.0, -11621.0, -18093.0, -10347.0] \n",
      " [-16060.0, -20326.0, -12558.0, -17109.0, -11196.0, -16599.0, -12348.0, -12358.0, -20173.0, -11966.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -754.46\n",
      "Episode 300 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8679.0, -11237.0, -15439.0, -15017.0, -3070.0, -17967.0, -14755.0, -12012.0, -15810.0, -11679.0] \n",
      " [-11223.0, -12482.0, -17257.0, -17030.0, -6390.0, -16821.0, -15606.0, -13884.0, -17505.0, -13773.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -735.23\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Episode 301 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18019.0, -1505.0, -10873.0, -13740.0, -18019.0, -1728.0, -5217.0, -17174.0, -4855.0, -16150.0] \n",
      " [-20089.0, -4079.0, -14469.0, -16145.0, -20089.0, -3551.0, -7379.0, -18768.0, -7030.0, -18612.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.2, 0.06, 0.2, 0.0, 0.01, 0.0, 0.01, 0.52], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.83], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -752.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 302 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16696.0, -14934.0, -7025.0, -3799.0, -15507.0, -14547.0, -9014.0, -14631.0, -5228.0, -6523.0] \n",
      " [-16174.0, -16080.0, -10239.0, -7414.0, -16476.0, -15238.0, -10947.0, -15781.0, -8406.0, -9735.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13], [0.31, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.6], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.02, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -745.99\n",
      "Episode 303 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12352.0, -2747.0, -16878.0, -3831.0, -16012.0, -2835.0, -4698.0, -16373.0, -1002.0, -2151.0] \n",
      " [-14823.0, -5648.0, -18352.0, -4042.0, -16748.0, -5860.0, -8866.0, -16650.0, -3267.0, -3721.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.97], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.02, 0.06, 0.08, 0.0, 0.0, 0.0, 0.0, 0.84], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.03, 0.0, 0.02, 0.0, 0.0, 0.93], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.61, 0.05, 0.03, 0.14, 0.06, 0.0, 0.07, 0.05], [0.15, 0.11, 0.6, 0.01, 0.01, 0.0, 0.03, 0.09]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -697.11\n",
      "Episode 304 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9847.0, -13592.0, -14195.0, -3957.0, -18091.0, -12683.0, -6604.0, -18505.0, -5821.0, -13706.0] \n",
      " [-12678.0, -15485.0, -16151.0, -5704.0, -19959.0, -15113.0, -9392.0, -20125.0, -7997.0, -16074.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -790.71\n",
      "Episode 305 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18426.0, -17298.0, -17564.0, -17356.0, -2197.0, -4458.0, -18108.0, -17743.0, -9028.0, -13535.0] \n",
      " [-18843.0, -17895.0, -18644.0, -17883.0, -7478.0, -8431.0, -19586.0, -19437.0, -11662.0, -16386.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49], [0.01, 0.06, 0.9, 0.0, 0.0, 0.0, 0.0, 0.03], [0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.23], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -837.34\n",
      "Episode 306 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18356.0, -2010.0, -17584.0, -10005.0, -10077.0, -15193.0, -1175.0, -18004.0, -2830.0, -15083.0] \n",
      " [-18575.0, -4721.0, -18974.0, -13175.0, -12333.0, -16204.0, -4534.0, -18548.0, -7348.0, -17230.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.93], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.52], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.73, 0.01, 0.01, 0.04, 0.01, 0.0, 0.03, 0.17], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.02, 0.08, 0.44, 0.0, 0.0, 0.0, 0.0, 0.46], [0.0, 0.0, 0.43, 0.0, 0.0, 0.0, 0.0, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -782.37\n",
      "Episode 307 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14660.0, -16083.0, -3119.0, -1690.0, -4353.0, -15343.0, -1624.0, -17529.0, -14743.0, -141.0] \n",
      " [-16018.0, -16800.0, -5686.0, -3761.0, -7208.0, -16680.0, -4081.0, -19384.0, -15942.0, -2693.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.47, 0.0, 0.0, 0.0, 0.0, 0.53], [0.05, 0.01, 0.69, 0.0, 0.0, 0.0, 0.0, 0.24], [0.78, 0.0, 0.0, 0.02, 0.01, 0.0, 0.01, 0.18], [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.15, 0.02, 0.43, 0.01, 0.01, 0.0, 0.02, 0.37], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.87], [0.18, 0.08, 0.19, 0.16, 0.16, 0.04, 0.06, 0.13]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -737.19\n",
      "Episode 308 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11512.0, -15509.0, -12965.0, -14362.0, -13426.0, -12494.0, -7331.0, -18188.0, -10602.0, -17995.0] \n",
      " [-12399.0, -16761.0, -15287.0, -14534.0, -13578.0, -14091.0, -10307.0, -20179.0, -11670.0, -19090.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.73, 0.0, 0.0, 0.0, 0.0, 0.27], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -784.04\n",
      "Episode 309 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14466.0, -8190.0, -7907.0, -16440.0, -7765.0, -11951.0, -16767.0, -5623.0, -16440.0, -15297.0] \n",
      " [-15259.0, -10681.0, -10059.0, -17833.0, -8843.0, -13660.0, -17661.0, -7507.0, -17833.0, -17357.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.46, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.31], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -689.96\n",
      "Episode 310 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11484.0, -9747.0, -18675.0, -12624.0, -18206.0, -3863.0, -6657.0, -17715.0, -10308.0, -8666.0] \n",
      " [-13702.0, -11549.0, -19482.0, -15237.0, -18377.0, -7494.0, -8179.0, -19400.0, -12911.0, -10494.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.4], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -813.36\n",
      "Episode 311 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1999.0, -2897.0, -16136.0, -2943.0, -15820.0, -59.0, -2282.0, -3353.0, -5379.0, -14734.0] \n",
      " [-2427.0, -2488.0, -16130.0, -2473.0, -16198.0, -2391.0, -2495.0, -5415.0, -7534.0, -15515.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.89, 0.0, 0.0, 0.02, 0.03, 0.0, 0.0, 0.06], [0.01, 0.01, 0.87, 0.0, 0.0, 0.0, 0.0, 0.11], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.14, 0.11, 0.14, 0.14, 0.14, 0.09, 0.1, 0.13], [0.38, 0.0, 0.0, 0.01, 0.17, 0.0, 0.0, 0.43], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.94], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -583.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 312 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-19594.0, -17695.0, -13995.0, -13371.0, -7856.0, -13008.0, -12141.0, -8223.0, -1593.0, -18770.0] \n",
      " [-19754.0, -18782.0, -15664.0, -15031.0, -10891.0, -14859.0, -13810.0, -9566.0, -4830.0, -19666.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85], [0.0, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.7], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.33, 0.01, 0.01, 0.01, 0.02, 0.0, 0.01, 0.62], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -787.63\n",
      "Episode 313 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9608.0, -9389.0, -16041.0, -16629.0, -10101.0, -15604.0, -12852.0, -11000.0, -16858.0, -4334.0] \n",
      " [-11429.0, -10781.0, -16416.0, -17058.0, -11979.0, -16261.0, -14714.0, -12610.0, -16967.0, -6204.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.47, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -705.12\n",
      "Episode 314 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11019.0, -11273.0, -13526.0, -9320.0, -15573.0, -16960.0, -10544.0, -17857.0, -17280.0, -16822.0] \n",
      " [-14054.0, -13384.0, -16021.0, -11747.0, -17893.0, -18298.0, -13262.0, -18410.0, -19641.0, -17870.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.41], [0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -759.87\n",
      "Episode 315 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18294.0, -13093.0, -16201.0, -18942.0, -9936.0, -574.0, -11010.0, -15938.0, -19100.0, -18793.0] \n",
      " [-18863.0, -15179.0, -19123.0, -19434.0, -12058.0, -3990.0, -13651.0, -16964.0, -19300.0, -19400.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.28, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.46, 0.03, 0.02, 0.16, 0.14, 0.0, 0.06, 0.12], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.18], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -800.15\n",
      "Episode 316 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1265.0, -12851.0, -12205.0, -11969.0, -3542.0, -9520.0, -15412.0, -11039.0, -5695.0, -11068.0] \n",
      " [-3840.0, -14240.0, -14190.0, -14088.0, -6184.0, -12138.0, -16663.0, -12999.0, -8355.0, -13167.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.82, 0.0, 0.0, 0.06, 0.06, 0.0, 0.01, 0.05], [0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.01, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -709.6\n",
      "Episode 317 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17352.0, -12978.0, -4153.0, -14463.0, -14427.0, -3550.0, -19284.0, -14796.0, -9593.0, -7030.0] \n",
      " [-18935.0, -15063.0, -6573.0, -15660.0, -16234.0, -5650.0, -19187.0, -15648.0, -11515.0, -9387.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.46, 0.0, 0.0, 0.0, 0.0, 0.54], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -724.45\n",
      "Episode 318 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18426.0, -11906.0, -18954.0, -13867.0, -13657.0, -9578.0, -20533.0, -18774.0, -13912.0, -5763.0] \n",
      " [-19494.0, -13649.0, -20507.0, -14678.0, -14122.0, -11248.0, -21351.0, -20347.0, -14885.0, -8320.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81], [0.28, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -767.27\n",
      "Episode 319 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13309.0, -10502.0, -4620.0, -819.0, -17345.0, -14188.0, -17415.0, -13309.0, -18998.0, -2769.0] \n",
      " [-14060.0, -13312.0, -5752.0, -3362.0, -17611.0, -14569.0, -17895.0, -14060.0, -17709.0, -3999.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9], [0.02, 0.03, 0.65, 0.0, 0.01, 0.0, 0.0, 0.29], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.01, 0.94, 0.0, 0.0, 0.0, 0.0, 0.05]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -693.47\n",
      "Episode 320 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2602.0, -4375.0, -14732.0, -17777.0, -18027.0, -11225.0, -18210.0, -18258.0, -12794.0, -14187.0] \n",
      " [-5839.0, -7575.0, -14608.0, -16914.0, -17567.0, -12122.0, -18080.0, -16989.0, -14111.0, -14724.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.44, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.53], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -712.4\n",
      "Episode 321 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21.0, -20038.0, -15815.0, -20464.0, -1233.0, -4415.0, -15982.0, -9714.0, -16089.0, -16128.0] \n",
      " [-2617.0, -20182.0, -16186.0, -19547.0, -4707.0, -8197.0, -17942.0, -12726.0, -17823.0, -18296.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.55, 0.01, 0.01, 0.1, 0.17, 0.0, 0.03, 0.13], [0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.6], [0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.75], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -811.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 322 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17676.0, -15025.0, -4973.0, -15710.0, -14259.0, -16164.0, -19419.0, -12496.0, -548.0, -14037.0] \n",
      " [-18982.0, -15480.0, -7123.0, -16717.0, -14420.0, -17975.0, -18545.0, -14544.0, -4018.0, -14677.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.57, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.77, 0.01, 0.0, 0.13, 0.02, 0.0, 0.03, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -766.65\n",
      "Episode 323 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11888.0, -17867.0, -3964.0, -12980.0, -7333.0, -21.0, -18943.0, -17243.0, -11768.0, -19160.0] \n",
      " [-14871.0, -17307.0, -5904.0, -15797.0, -10149.0, -2575.0, -18882.0, -17655.0, -14355.0, -19088.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.8]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -769.84\n",
      "Episode 324 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14723.0, -5170.0, -18374.0, -16206.0, -16913.0, -13445.0, -16089.0, -14302.0, -1066.0, -17993.0] \n",
      " [-16523.0, -6461.0, -18673.0, -16532.0, -16792.0, -14065.0, -16771.0, -15779.0, -3144.0, -17355.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.72, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.38, 0.06, 0.07, 0.02, 0.03, 0.0, 0.04, 0.4], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -736.56\n",
      "Episode 325 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1804.0, -7269.0, -15009.0, -16219.0, -10421.0, -7247.0, -10249.0, -9848.0, -10418.0, -16678.0] \n",
      " [-5523.0, -8545.0, -16496.0, -16271.0, -10930.0, -8740.0, -11431.0, -10792.0, -11506.0, -16195.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.63, 0.0, 0.0, 0.04, 0.1, 0.0, 0.01, 0.22], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.47, 0.0, 0.0, 0.0, 0.0, 0.53], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -686.67\n",
      "Episode 326 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14501.0, -14713.0, -17173.0, -18942.0, -424.0, -5319.0, -16860.0, -424.0, -5431.0, -1771.0] \n",
      " [-16834.0, -16245.0, -18450.0, -20281.0, -4435.0, -7972.0, -17894.0, -4435.0, -8166.0, -5080.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.1, 0.1, 0.18, 0.07, 0.2, 0.03, 0.07, 0.26], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.28, 0.0, 0.0, 0.0, 0.0, 0.72], [0.1, 0.1, 0.18, 0.07, 0.2, 0.03, 0.07, 0.26], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.04, 0.84, 0.0, 0.0, 0.0, 0.0, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -808.44\n",
      "Episode 327 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3668.0, -17889.0, -16494.0, -13083.0, -13083.0, -17889.0, -3780.0, -13459.0, -14834.0, -14335.0] \n",
      " [-4647.0, -18403.0, -18002.0, -14373.0, -14373.0, -18403.0, -4847.0, -14836.0, -14594.0, -15536.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.76, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -718.23\n",
      "Episode 328 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2286.0, -2698.0, -17037.0, -17808.0, -2629.0, -13619.0, -20145.0, -17039.0, -18957.0, -22220.0] \n",
      " [-3854.0, -6997.0, -18463.0, -18706.0, -5854.0, -16436.0, -19126.0, -18380.0, -19576.0, -20117.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.48], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -772.73\n",
      "Episode 329 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17835.0, -19922.0, -17684.0, -13690.0, -19543.0, -15276.0, -16889.0, -15593.0, -3294.0, -3294.0] \n",
      " [-16386.0, -19881.0, -18514.0, -15085.0, -18503.0, -16117.0, -17642.0, -16658.0, -6668.0, -6668.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -826.65\n",
      "Episode 330 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14822.0, -18086.0, -12287.0, -621.0, -15542.0, -20154.0, -7192.0, -9536.0, -2270.0, -15542.0] \n",
      " [-16894.0, -18751.0, -14862.0, -3084.0, -17714.0, -19737.0, -9266.0, -11730.0, -5881.0, -17714.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.07, 0.1, 0.44, 0.02, 0.03, 0.0, 0.01, 0.34], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.06, 0.0, 0.11, 0.0, 0.0, 0.82], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -770.21\n",
      "Episode 331 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8371.0, -4888.0, -19005.0, -17784.0, -8550.0, -4888.0, -2821.0, -4487.0, -12753.0, -8328.0] \n",
      " [-10509.0, -8682.0, -19387.0, -18826.0, -12188.0, -8682.0, -4997.0, -8440.0, -14781.0, -11135.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -775.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 332 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16017.0, -6832.0, -1182.0, -10412.0, -7211.0, -17610.0, -12440.0, -17350.0, -16700.0, -11084.0] \n",
      " [-17141.0, -8533.0, -3880.0, -11256.0, -9125.0, -17948.0, -13198.0, -18256.0, -17802.0, -11424.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.26, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74], [0.55, 0.06, 0.08, 0.12, 0.02, 0.0, 0.09, 0.07], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.7]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -714.72\n",
      "Episode 333 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6584.0, -16140.0, -15379.0, -15765.0, -16666.0, -7583.0, -483.0, -12695.0, -16404.0, -18974.0] \n",
      " [-9944.0, -16223.0, -15019.0, -16055.0, -18127.0, -11673.0, -2766.0, -14701.0, -18250.0, -19653.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.22, 0.06, 0.05, 0.12, 0.23, 0.01, 0.08, 0.23], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -758.85\n",
      "Episode 334 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21.0, -4089.0, -18836.0, -16656.0, -18877.0, -7272.0, -16618.0, -11567.0, -16609.0, -18186.0] \n",
      " [-2382.0, -7354.0, -17970.0, -17647.0, -17160.0, -8973.0, -17576.0, -13208.0, -17021.0, -18159.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.93], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -698.93\n",
      "Episode 335 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17519.0, -11618.0, -15434.0, -17501.0, -944.0, -1356.0, -1522.0, -18055.0, -18950.0, -16492.0] \n",
      " [-16163.0, -13416.0, -16154.0, -16267.0, -4615.0, -3800.0, -2848.0, -16492.0, -18018.0, -16639.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.63, 0.0, 0.0, 0.0, 0.0, 0.37], [0.02, 0.1, 0.63, 0.0, 0.01, 0.0, 0.01, 0.23], [0.91, 0.0, 0.0, 0.05, 0.01, 0.0, 0.01, 0.03], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -750.22\n",
      "Episode 336 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4675.0, -12155.0, -12318.0, -17461.0, -13183.0, -12963.0, -16021.0, -17523.0, -8902.0, -14644.0] \n",
      " [-6929.0, -13943.0, -14267.0, -18168.0, -15176.0, -15053.0, -16950.0, -18249.0, -10570.0, -15928.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -645.11\n",
      "Episode 337 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17597.0, -19120.0, -15717.0, -15489.0, -7240.0, -12465.0, -808.0, -15442.0, -14633.0, -16618.0] \n",
      " [-18763.0, -19643.0, -16146.0, -15323.0, -9992.0, -14122.0, -5467.0, -16601.0, -15245.0, -17321.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.76], [0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.92], [0.03, 0.09, 0.59, 0.01, 0.03, 0.01, 0.02, 0.23], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -785.03\n",
      "Episode 338 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20799.0, -9618.0, -16709.0, -19724.0, -9171.0, -7808.0, -18183.0, -4957.0, -9890.0, -17812.0] \n",
      " [-20145.0, -12965.0, -17984.0, -19329.0, -12107.0, -11608.0, -18966.0, -6588.0, -12520.0, -18590.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -861.01\n",
      "Episode 339 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17761.0, -2125.0, -3189.0, -1981.0, -10571.0, -13148.0, -11419.0, -12626.0, -6637.0, -10814.0] \n",
      " [-18574.0, -6084.0, -4763.0, -3980.0, -11875.0, -14362.0, -13149.0, -12531.0, -9937.0, -11514.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.95, 0.0, 0.01, 0.0, 0.0, 0.04], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.04, 0.06, 0.8, 0.01, 0.02, 0.0, 0.01, 0.06], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.47, 0.0, 0.0, 0.0, 0.0, 0.53], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.83], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -665.22\n",
      "Episode 340 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16558.0, -16058.0, -17858.0, -14955.0, -20300.0, -16187.0, -17425.0, -19302.0, -5516.0, -17643.0] \n",
      " [-16863.0, -16276.0, -15918.0, -15340.0, -19795.0, -17931.0, -18091.0, -18489.0, -7653.0, -18903.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -802.51\n",
      "Episode 341 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16746.0, -16192.0, -16090.0, -12795.0, -17481.0, -15790.0, -17298.0, -15790.0, -7684.0, -17096.0] \n",
      " [-15023.0, -16321.0, -14895.0, -13002.0, -17886.0, -15561.0, -17783.0, -15561.0, -10153.0, -17650.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -746.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 342 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-19707.0, -3734.0, -7076.0, -21031.0, -16827.0, -20976.0, -6606.0, -5732.0, -6142.0, -20100.0] \n",
      " [-20565.0, -7819.0, -10978.0, -20073.0, -18093.0, -19727.0, -10227.0, -9465.0, -9722.0, -19863.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.9], [0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.96], [0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17], [0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.75], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -898.99\n",
      "Episode 343 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18475.0, -19245.0, -16803.0, -16449.0, -11394.0, -11551.0, -21060.0, -18794.0, -14346.0, -19905.0] \n",
      " [-18232.0, -19685.0, -17710.0, -16900.0, -13224.0, -11512.0, -19950.0, -18228.0, -16212.0, -19435.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -814.98\n",
      "Episode 344 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17524.0, -18024.0, -1796.0, -446.0, -20056.0, -19373.0, -18579.0, -18210.0, -3652.0, -16354.0] \n",
      " [-19304.0, -18163.0, -6273.0, -4663.0, -19374.0, -18113.0, -18205.0, -18820.0, -7558.0, -17313.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.76, 0.01, 0.0, 0.0, 0.0, 0.0, 0.01, 0.21], [0.08, 0.06, 0.51, 0.06, 0.1, 0.01, 0.05, 0.13], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.2, 0.01, 0.04, 0.0, 0.0, 0.0, 0.0, 0.75], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -815.08\n",
      "Episode 345 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18192.0, -12991.0, -21351.0, -2334.0, -12582.0, -18243.0, -7560.0, -2334.0, -571.0, -1694.0] \n",
      " [-17405.0, -14786.0, -20397.0, -4964.0, -14661.0, -19783.0, -11360.0, -4964.0, -2979.0, -3395.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.21], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.47, 0.0, 0.0, 0.0, 0.0, 0.53], [0.56, 0.0, 0.0, 0.02, 0.15, 0.0, 0.0, 0.27], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.85], [0.56, 0.0, 0.0, 0.02, 0.15, 0.0, 0.0, 0.27], [0.0, 0.01, 0.88, 0.0, 0.01, 0.0, 0.0, 0.09], [0.0, 0.04, 0.75, 0.0, 0.0, 0.0, 0.0, 0.2]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -802.21\n",
      "Episode 346 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10562.0, -152.0, -14262.0, -13839.0, -15844.0, -19166.0, -16504.0, -19265.0, -13436.0, -17482.0] \n",
      " [-12101.0, -3582.0, -15047.0, -14992.0, -15255.0, -18916.0, -16715.0, -18949.0, -14236.0, -17923.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.18, 0.08, 0.19, 0.16, 0.16, 0.04, 0.06, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.88], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -782.16\n",
      "Episode 347 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18780.0, -20339.0, -22.0, -11262.0, -6466.0, -19078.0, -2937.0, -19147.0, -17013.0, -12559.0] \n",
      " [-18528.0, -20339.0, -4127.0, -12172.0, -8564.0, -19365.0, -4817.0, -19264.0, -16827.0, -12448.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.9], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -787.33\n",
      "Episode 348 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10177.0, -12577.0, -3057.0, -12828.0, -12575.0, -16217.0, -16434.0, -16156.0, -13234.0, -15341.0] \n",
      " [-11890.0, -13865.0, -3885.0, -12784.0, -13239.0, -16757.0, -16479.0, -16995.0, -14605.0, -15828.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.83]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -709.62\n",
      "Episode 349 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18791.0, -19348.0, -5701.0, -4224.0, -14842.0, -9238.0, -11539.0, -16023.0, -22267.0, -15496.0] \n",
      " [-19683.0, -19058.0, -9298.0, -8342.0, -14600.0, -11540.0, -12315.0, -14625.0, -19789.0, -16149.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.99], [0.0, 0.0, 0.48, 0.0, 0.01, 0.0, 0.0, 0.51], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -819.19\n",
      "Episode 350 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1592.0, -13593.0, -19324.0, -19917.0, -19109.0, -17629.0, -15959.0, -5247.0, -16697.0, -17870.0] \n",
      " [-5330.0, -14454.0, -18696.0, -19363.0, -18164.0, -18253.0, -15328.0, -7866.0, -17043.0, -18546.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.13, 0.18, 0.48, 0.01, 0.01, 0.0, 0.03, 0.16], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.86, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -816.91\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 351 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16727.0, -13819.0, -20976.0, -21313.0, -13147.0, -10974.0, -17295.0, -15746.0, -19898.0, -20310.0] \n",
      " [-16198.0, -14858.0, -20685.0, -20517.0, -14598.0, -12868.0, -16088.0, -16572.0, -21160.0, -20748.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.56], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -830.15\n",
      "Episode 352 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-19989.0, -7136.0, -18402.0, -18402.0, -10413.0, -19878.0, -19740.0, -14587.0, -20353.0, -14587.0] \n",
      " [-20249.0, -9088.0, -18481.0, -18481.0, -11306.0, -19644.0, -20052.0, -16436.0, -19642.0, -16436.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.27, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.7], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -874.58\n",
      "Episode 353 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6237.0, -18547.0, -16165.0, -15698.0, -14829.0, -16113.0, -15684.0, -16089.0, -3899.0, -14786.0] \n",
      " [-8882.0, -19671.0, -15481.0, -14596.0, -15809.0, -15631.0, -14795.0, -14672.0, -6334.0, -14772.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.54, 0.0, 0.0, 0.0, 0.0, 0.46], [0.01, 0.01, 0.34, 0.0, 0.29, 0.0, 0.0, 0.34], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -787.68\n",
      "Episode 354 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4153.0, -9126.0, -10675.0, -12164.0, -15136.0, -18633.0, -1183.0, -4597.0, -19940.0, -6625.0] \n",
      " [-6552.0, -11633.0, -12642.0, -14552.0, -15827.0, -17848.0, -2963.0, -7996.0, -20783.0, -9141.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.91], [0.0, 0.0, 0.94, 0.0, 0.01, 0.0, 0.0, 0.05], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -805.68\n",
      "Episode 355 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21308.0, -15592.0, -12545.0, -16102.0, -14329.0, -22.0, -7551.0, -4002.0, -15172.0, -2525.0] \n",
      " [-22193.0, -15219.0, -12804.0, -13980.0, -15397.0, -4733.0, -10172.0, -8385.0, -14968.0, -7287.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.22, 0.0, 0.0, 0.77], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.71, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.28]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -849.22\n",
      "Episode 356 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13862.0, -8937.0, -16857.0, -14982.0, -13155.0, -5805.0, -16506.0, -13760.0, -14233.0, -13469.0] \n",
      " [-13500.0, -10541.0, -18247.0, -14789.0, -12703.0, -8831.0, -18094.0, -14035.0, -14668.0, -13137.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.75], [0.0, 0.0, 0.56, 0.0, 0.0, 0.0, 0.0, 0.44], [0.02, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.92], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -717.32\n",
      "Episode 357 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17583.0, -18322.0, -16802.0, -43.0, -8363.0, -3127.0, -15874.0, -5075.0, -14684.0, -4697.0] \n",
      " [-16378.0, -16220.0, -16015.0, -2580.0, -10803.0, -6264.0, -15026.0, -7176.0, -15088.0, -7554.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.13, 0.12, 0.13, 0.13, 0.13, 0.11, 0.11, 0.13], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.9, 0.0, 0.0, 0.01, 0.04, 0.0, 0.0, 0.05], [0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.13], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -703.83\n",
      "Episode 358 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2046.0, -14336.0, -9567.0, -12232.0, -9786.0, -16207.0, -2637.0, -15616.0, -13966.0, -14281.0] \n",
      " [-4616.0, -15662.0, -10865.0, -13294.0, -11274.0, -16551.0, -5107.0, -16365.0, -15031.0, -15112.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.7, 0.0, 0.0, 0.04, 0.07, 0.0, 0.01, 0.19], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.89], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -759.3\n",
      "Episode 359 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16104.0, -15541.0, -17941.0, -14754.0, -14833.0, -16298.0, -17657.0, -15007.0, -1869.0, -17380.0] \n",
      " [-14567.0, -15658.0, -17622.0, -14858.0, -14432.0, -14427.0, -17229.0, -15517.0, -5620.0, -15809.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.83], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.68, 0.01, 0.01, 0.03, 0.01, 0.0, 0.02, 0.25], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -680.72\n",
      "Episode 360 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21628.0, -17606.0, -21354.0, -20265.0, -9270.0, -21118.0, -19663.0, -20592.0, -22.0, -15634.0] \n",
      " [-21017.0, -16736.0, -20724.0, -20638.0, -12642.0, -19565.0, -20311.0, -20268.0, -4093.0, -16118.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.84], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -885.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 361 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18170.0, -18979.0, -19541.0, -20695.0, -20574.0, -17960.0, -19756.0, -13681.0, -11189.0, -18025.0] \n",
      " [-17543.0, -19117.0, -19103.0, -19373.0, -19055.0, -17798.0, -19694.0, -13849.0, -13083.0, -17584.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -808.15\n",
      "Episode 362 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18606.0, -12449.0, -1956.0, -14155.0, -15799.0, -1944.0, -7327.0, -214.0, -19510.0, -18269.0] \n",
      " [-18208.0, -12814.0, -5079.0, -13505.0, -17031.0, -4870.0, -9805.0, -3298.0, -17887.0, -18193.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96], [0.58, 0.0, 0.0, 0.03, 0.11, 0.0, 0.0, 0.27], [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.01, 0.78, 0.0, 0.01, 0.0, 0.0, 0.2], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.07, 0.1, 0.4, 0.05, 0.11, 0.03, 0.05, 0.2], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -737.87\n",
      "Episode 363 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-22.0, -9002.0, -17635.0, -16545.0, -13125.0, -13430.0, -15781.0, -19417.0, -19128.0, -16836.0] \n",
      " [-4615.0, -12181.0, -18027.0, -16274.0, -15061.0, -14346.0, -15521.0, -19008.0, -18796.0, -15765.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.94], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -811.3\n",
      "Episode 364 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18889.0, -22.0, -6176.0, -16240.0, -4756.0, -5822.0, -19955.0, -3996.0, -5727.0, -19570.0] \n",
      " [-17425.0, -3884.0, -8692.0, -15844.0, -6481.0, -9590.0, -17059.0, -5497.0, -7419.0, -19712.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91], [0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.62], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -755.06\n",
      "Episode 365 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18195.0, -11868.0, -670.0, -15618.0, -14789.0, -6347.0, -17627.0, -4829.0, -14767.0, -16815.0] \n",
      " [-15591.0, -12717.0, -3674.0, -15744.0, -14016.0, -8094.0, -16861.0, -7464.0, -14588.0, -16396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.07, 0.07, 0.17, 0.04, 0.23, 0.01, 0.04, 0.36], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.05], [0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -694.96\n",
      "Episode 366 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14574.0, -16292.0, -16021.0, -18498.0, -5021.0, -17783.0, -16231.0, -16292.0, -17444.0, -17413.0] \n",
      " [-14864.0, -15820.0, -17378.0, -18709.0, -7498.0, -16896.0, -15856.0, -15579.0, -16768.0, -17886.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.64, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36], [0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -760.85\n",
      "Episode 367 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17380.0, -18628.0, -18935.0, -10736.0, -18977.0, -3143.0, -17028.0, -20507.0, -18891.0, -14604.0] \n",
      " [-17174.0, -18854.0, -17120.0, -13669.0, -16194.0, -7177.0, -17265.0, -18845.0, -19214.0, -15536.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.03, 0.52, 0.0, 0.0, 0.0, 0.0, 0.44], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -812.58\n",
      "Episode 368 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4079.0, -13190.0, -15535.0, -19332.0, -12757.0, -16705.0, -4580.0, -11274.0, -20326.0, -6069.0] \n",
      " [-4995.0, -13395.0, -15586.0, -18521.0, -13920.0, -16425.0, -5747.0, -13308.0, -20581.0, -7514.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.04], [0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.97], [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0, 0.88]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -772.82\n",
      "Episode 369 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17137.0, -18440.0, -13927.0, -19618.0, -16161.0, -19821.0, -4341.0, -20351.0, -15036.0, -20209.0] \n",
      " [-17859.0, -17127.0, -14466.0, -19715.0, -15733.0, -19078.0, -9197.0, -20043.0, -14785.0, -19568.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.68], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -808.03\n",
      "Episode 370 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4312.0, -16454.0, -6704.0, -13242.0, -15198.0, -16985.0, -16830.0, -13640.0, -10724.0, -12496.0] \n",
      " [-7423.0, -17270.0, -9251.0, -12715.0, -14517.0, -16962.0, -16562.0, -12688.0, -11938.0, -13421.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.98], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.98], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.88], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.04, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.95]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -772.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 371 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-19856.0, -18520.0, -14784.0, -18283.0, -20108.0, -20127.0, -20074.0, -6902.0, -4750.0, -20542.0] \n",
      " [-19421.0, -18396.0, -15095.0, -17388.0, -19048.0, -19852.0, -20068.0, -10332.0, -8539.0, -20252.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.91, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.08], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -869.06\n",
      "Episode 372 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16652.0, -20133.0, -16515.0, -11483.0, -15680.0, -21687.0, -6660.0, -21510.0, -16744.0, -18065.0] \n",
      " [-16665.0, -19177.0, -16372.0, -13261.0, -15999.0, -20733.0, -8461.0, -20707.0, -17323.0, -18662.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.41, 0.0, 0.0, 0.0, 0.0, 0.59]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -795.33\n",
      "Episode 373 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17384.0, -18172.0, -7501.0, -1760.0, -14644.0, -20046.0, -17401.0, -13821.0, -14725.0, -18338.0] \n",
      " [-16109.0, -17629.0, -11049.0, -4719.0, -14210.0, -18359.0, -16416.0, -15324.0, -14845.0, -17702.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.65, 0.0, 0.0, 0.0, 0.0, 0.35], [0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98], [0.01, 0.01, 0.52, 0.0, 0.03, 0.0, 0.0, 0.43], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.48], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -749.84\n",
      "Episode 374 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15427.0, -16780.0, -15912.0, -17307.0, -17481.0, -8839.0, -8839.0, -11794.0, -17481.0, -9098.0] \n",
      " [-15033.0, -15632.0, -15899.0, -16555.0, -15647.0, -10237.0, -10237.0, -13579.0, -15647.0, -10387.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.17], [0.0, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.63], [0.0, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.63], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.17], [0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -624.18\n",
      "Episode 375 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-19664.0, -20703.0, -20526.0, -4835.0, -7493.0, -8732.0, -16635.0, -19749.0, -21518.0, -23.0] \n",
      " [-18682.0, -19205.0, -18718.0, -7616.0, -9288.0, -10177.0, -17090.0, -18549.0, -18578.0, -4307.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55], [0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -827.89\n",
      "Episode 376 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17186.0, -20676.0, -16204.0, -21432.0, -14384.0, -15472.0, -18165.0, -8425.0, -17672.0, -14384.0] \n",
      " [-17189.0, -19555.0, -14967.0, -19601.0, -15211.0, -16098.0, -18219.0, -9393.0, -16467.0, -15211.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.52], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.92], [0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -782.37\n",
      "Episode 377 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-23157.0, -23.0, -20241.0, -20502.0, -6577.0, -14511.0, -20182.0, -4607.0, -23062.0, -17347.0] \n",
      " [-19875.0, -4232.0, -20413.0, -20713.0, -10920.0, -14127.0, -20263.0, -6432.0, -19333.0, -17172.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23], [0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -839.01\n",
      "Episode 378 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1718.0, -7680.0, -23.0, -15134.0, -9497.0, -19623.0, -6979.0, -2211.0, -17426.0, -8038.0] \n",
      " [-5075.0, -8246.0, -4097.0, -13992.0, -11628.0, -17822.0, -8626.0, -5782.0, -16131.0, -9639.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.11, 0.09, 0.45, 0.02, 0.02, 0.0, 0.03, 0.28], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12, 0.13, 0.12, 0.13, 0.13, 0.12, 0.13, 0.13], [0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.72, 0.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.26], [0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.41, 0.0, 0.0, 0.0, 0.0, 0.59]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -771.99\n",
      "Episode 379 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18866.0, -11367.0, -20483.0, -2302.0, -23424.0, -15096.0, -16183.0, -16152.0, -6473.0, -11367.0] \n",
      " [-18980.0, -13335.0, -19051.0, -5119.0, -21016.0, -15186.0, -17201.0, -16049.0, -11098.0, -13335.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.7, 0.01, 0.01, 0.0, 0.0, 0.0, 0.01, 0.27], [0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.09], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.74], [0.09, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.8], [0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.11]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -853.28\n",
      "Episode 380 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18896.0, -921.0, -13639.0, -21120.0, -22146.0, -6221.0, -3048.0, -16428.0, -18541.0, -23329.0] \n",
      " [-19302.0, -3168.0, -13845.0, -19773.0, -20873.0, -10041.0, -4109.0, -16871.0, -18029.0, -20726.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.43, 0.02, 0.01, 0.13, 0.21, 0.0, 0.04, 0.15], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.64, 0.0, 0.0, 0.0, 0.0, 0.36], [0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09], [0.04, 0.01, 0.27, 0.0, 0.0, 0.0, 0.0, 0.67], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08], [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -797.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 381 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-22083.0, -15563.0, -14699.0, -15469.0, -13808.0, -13808.0, -11121.0, -19547.0, -17754.0, -2869.0] \n",
      " [-20697.0, -15139.0, -14376.0, -14256.0, -14471.0, -14471.0, -11423.0, -18959.0, -17027.0, -6457.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.58, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9], [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81], [0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -821.28\n",
      "Episode 382 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5697.0, -17472.0, -17094.0, -11867.0, -18547.0, -8842.0, -17937.0, -9711.0, -17094.0, -16938.0] \n",
      " [-8373.0, -16312.0, -16242.0, -10795.0, -16934.0, -10146.0, -16451.0, -10744.0, -16242.0, -15496.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11], [0.0, 0.0, 0.61, 0.0, 0.0, 0.0, 0.0, 0.39], [0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.95], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -709.79\n",
      "Episode 383 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3021.0, -9459.0, -9641.0, -17022.0, -17951.0, -18212.0, -19245.0, -17801.0, -15423.0, -18212.0] \n",
      " [-6336.0, -11934.0, -10142.0, -16397.0, -16921.0, -17908.0, -18386.0, -15964.0, -16299.0, -17908.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.02, 0.62, 0.0, 0.0, 0.0, 0.0, 0.36], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.67], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -754.14\n",
      "Episode 384 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15631.0, -6510.0, -504.0, -16970.0, -5444.0, -15902.0, -4784.0, -16444.0, -15842.0, -19058.0] \n",
      " [-15092.0, -10172.0, -4132.0, -14988.0, -8546.0, -15470.0, -5012.0, -15262.0, -16401.0, -17289.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.19, 0.13, 0.2, 0.12, 0.1, 0.03, 0.11, 0.12], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.73], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -692.14\n",
      "Episode 385 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17922.0, -8491.0, -2586.0, -2058.0, -21893.0, -22488.0, -16218.0, -13752.0, -18418.0, -15912.0] \n",
      " [-17564.0, -10266.0, -5963.0, -5498.0, -19655.0, -19277.0, -16499.0, -13731.0, -18052.0, -16646.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.75, 0.02, 0.04, 0.04, 0.03, 0.0, 0.02, 0.1], [0.19, 0.01, 0.03, 0.02, 0.11, 0.0, 0.01, 0.63], [0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.48], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -801.54\n",
      "Episode 386 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16116.0, -7635.0, -13183.0, -9344.0, -16309.0, -19714.0, -14308.0, -17332.0, -16776.0, -13905.0] \n",
      " [-15472.0, -8843.0, -13379.0, -10732.0, -16131.0, -18449.0, -15114.0, -15909.0, -15599.0, -13532.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.84], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -749.39\n",
      "Episode 387 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-19465.0, -20653.0, -10850.0, -3967.0, -20407.0, -18592.0, -18782.0, -20242.0, -3481.0, -19051.0] \n",
      " [-18814.0, -18727.0, -12279.0, -4891.0, -17745.0, -17615.0, -18415.0, -17906.0, -4654.0, -16630.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.67], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.19], [0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.48], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -755.36\n",
      "Episode 388 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21162.0, -19818.0, -18294.0, -15325.0, -16544.0, -17764.0, -13993.0, -18028.0, -15376.0, -2973.0] \n",
      " [-19214.0, -18981.0, -17904.0, -15431.0, -15832.0, -17292.0, -14470.0, -17484.0, -14465.0, -6379.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.03], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.86], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -745.03\n",
      "Episode 389 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2688.0, -5054.0, -18236.0, -11445.0, -12455.0, -17979.0, -13279.0, -11417.0, -13961.0, -9579.0] \n",
      " [-3953.0, -7744.0, -17625.0, -11999.0, -13443.0, -17166.0, -13792.0, -12945.0, -14501.0, -11203.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.61, 0.02, 0.05, 0.03, 0.06, 0.0, 0.02, 0.21], [0.93, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.05], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.78], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -732.33\n",
      "Episode 390 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1718.0, -4389.0, -6203.0, -15074.0, -17379.0, -14451.0, -13761.0, -6203.0, -10842.0, -14451.0] \n",
      " [-4922.0, -7399.0, -8180.0, -15345.0, -16236.0, -14085.0, -14208.0, -8180.0, -11932.0, -14085.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.08, 0.07, 0.67, 0.02, 0.02, 0.0, 0.03, 0.11], [0.01, 0.0, 0.0, 0.0, 0.49, 0.0, 0.0, 0.5], [0.0, 0.0, 0.03, 0.0, 0.09, 0.0, 0.0, 0.88], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92], [0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.48], [0.62, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38], [0.0, 0.0, 0.03, 0.0, 0.09, 0.0, 0.0, 0.88], [0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.96], [0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.48]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -721.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 391 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18558.0, -7562.0, -14182.0, -10185.0, -18837.0, -17684.0, -18558.0, -16516.0, -17466.0, -14335.0] \n",
      " [-17733.0, -9591.0, -14148.0, -12037.0, -18664.0, -17390.0, -17733.0, -17318.0, -16526.0, -14575.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -716.64\n",
      "Episode 392 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18336.0, -20082.0, -13312.0, -18128.0, -7907.0, -15904.0, -21338.0, -1682.0, -17598.0, -11654.0] \n",
      " [-16376.0, -19499.0, -13978.0, -16556.0, -8716.0, -16500.0, -19901.0, -3367.0, -17823.0, -12610.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.22], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.92, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.07], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21], [0.21, 0.15, 0.43, 0.01, 0.01, 0.0, 0.03, 0.16], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -713.23\n",
      "Episode 393 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17073.0, -18360.0, -19430.0, -18635.0, -11376.0, -3523.0, -18364.0, -6394.0, -15088.0, -9204.0] \n",
      " [-15837.0, -17669.0, -17400.0, -15976.0, -13416.0, -6475.0, -17601.0, -9086.0, -15568.0, -11046.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.98], [0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.15], [0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.02], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.86, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -750.79\n",
      "Episode 394 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17302.0, -17718.0, -20222.0, -15583.0, -19588.0, -13191.0, -15723.0, -12584.0, -1551.0, -20253.0] \n",
      " [-16922.0, -18024.0, -18961.0, -15327.0, -18966.0, -12254.0, -16412.0, -12654.0, -5034.0, -18938.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.77, 0.0, 0.0, 0.06, 0.06, 0.0, 0.01, 0.1], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -723.23\n",
      "Episode 395 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2141.0, -2159.0, -4632.0, -10007.0, -20828.0, -3296.0, -17378.0, -20871.0, -2219.0, -10858.0] \n",
      " [-2458.0, -4378.0, -6998.0, -11119.0, -17715.0, -6352.0, -15063.0, -18097.0, -3501.0, -12055.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.01, 0.01, 0.03, 0.47, 0.0, 0.01, 0.22], [0.24, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.74], [0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.01], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.41], [0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.14], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.06, 0.11, 0.69, 0.0, 0.0, 0.0, 0.01, 0.11], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -669.18\n",
      "Episode 396 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6336.0, -14199.0, -20972.0, -21328.0, -17879.0, -21472.0, -17420.0, -17538.0, -17376.0, -22706.0] \n",
      " [-9802.0, -14378.0, -19717.0, -19705.0, -16418.0, -20222.0, -16201.0, -17467.0, -15850.0, -19379.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.08, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.87], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.39, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27], [0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.97], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -850.2\n",
      "Episode 397 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8689.0, -16824.0, -14165.0, -11230.0, -14771.0, -2310.0, -17987.0, -10184.0, -17670.0, -18508.0] \n",
      " [-10597.0, -16067.0, -13913.0, -11868.0, -14199.0, -5837.0, -17483.0, -11350.0, -16922.0, -16115.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.65, 0.0, 0.0, 0.0, 0.0, 0.35], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.08], [0.0, 0.06, 0.77, 0.0, 0.0, 0.0, 0.0, 0.17], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.03, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.96], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -751.73\n",
      "Episode 398 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18886.0, -14943.0, -18412.0, -17928.0, -15212.0, -6323.0, -18293.0, -9411.0, -15594.0, -19240.0] \n",
      " [-17574.0, -14630.0, -18106.0, -17225.0, -15473.0, -8580.0, -16856.0, -9669.0, -16006.0, -17782.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.06], [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -727.06\n",
      "Episode 399 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20077.0, -19616.0, -10621.0, -12841.0, -1240.0, -12375.0, -21353.0, -20356.0, -11317.0, -19752.0] \n",
      " [-19109.0, -19457.0, -10570.0, -12332.0, -5631.0, -12110.0, -19711.0, -19623.0, -11363.0, -18959.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.86, 0.0, 0.0, 0.07, 0.01, 0.0, 0.01, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -816.61\n",
      "Episode 400 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13270.0, -17359.0, -17792.0, -12644.0, -16586.0, -19677.0, -17038.0, -15712.0, -2696.0, -17731.0] \n",
      " [-13753.0, -15911.0, -16159.0, -13387.0, -15330.0, -17770.0, -15760.0, -14995.0, -5451.0, -16123.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.99], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.41, 0.0, 0.0, 0.0, 0.0, 0.59], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.07], [0.0, 0.0, 0.01, 0.0, 0.07, 0.0, 0.0, 0.92], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.13 0.12 0.13 0.13]\n",
      "Average Reward for Agent 0 this episode : -722.75\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  672       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  2352      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2352      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  49        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  672       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits4 (Dense)       multiple                  2352      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  392       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 13,545\n",
      "Trainable params: 13,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple8_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.33 seconds.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3bb6c45ba805>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSingle_Cross_Triple8_MultiAC_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\MasterAC_Agent.py\u001b[0m in \u001b[0;36mdemo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                         \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m                 \u001b[1;31m# increase the update counter by one each step (until reach simulation length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_20c\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "    },\n",
    "   'demand' : {\"default\" : [400,400,400,400] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 48)           672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48)           2352        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            49          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            392         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_5[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 0\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Experience Found: Loading into agents\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 466\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.11 seconds.\n",
      "\n",
      "Episode 367 is finished\n",
      "Average Reward for Agent 0 this episode : -88.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4133.1792\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 368 is finished\n",
      "Average Reward for Agent 0 this episode : -94.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3670.2183\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 369 is finished\n",
      "Average Reward for Agent 0 this episode : -94.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3824.4919\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 370 is finished\n",
      "Average Reward for Agent 0 this episode : -89.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3704.9753\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 371 is finished\n",
      "Average Reward for Agent 0 this episode : -92.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3853.7910\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 372 is finished\n",
      "Average Reward for Agent 0 this episode : -87.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3281.9280\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 373 is finished\n",
      "Average Reward for Agent 0 this episode : -94.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3457.7920\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 374 is finished\n",
      "Average Reward for Agent 0 this episode : -89.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5442.4595\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 375 is finished\n",
      "Average Reward for Agent 0 this episode : -100.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3856.9253\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 376 is finished\n",
      "Average Reward for Agent 0 this episode : -96.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4162.2339\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 377 is finished\n",
      "Average Reward for Agent 0 this episode : -92.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4461.5630\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 378 is finished\n",
      "Average Reward for Agent 0 this episode : -97.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3928.1562\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 379 is finished\n",
      "Average Reward for Agent 0 this episode : -89.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4048.7905\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 380 is finished\n",
      "Average Reward for Agent 0 this episode : -89.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3905.2812\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 381 is finished\n",
      "Average Reward for Agent 0 this episode : -90.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3829.8113\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 382 is finished\n",
      "Average Reward for Agent 0 this episode : -87.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3268.9590\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 383 is finished\n",
      "Average Reward for Agent 0 this episode : -92.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3892.5029\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 384 is finished\n",
      "Average Reward for Agent 0 this episode : -94.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4340.0044\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 385 is finished\n",
      "Average Reward for Agent 0 this episode : -95.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3523.4573\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 386 is finished\n",
      "Average Reward for Agent 0 this episode : -93.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3728.8413\n",
      "Reducing exploration for all agents to 0.0013\n",
      "Episode 387 is finished\n",
      "Average Reward for Agent 0 this episode : -102.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4206.3833\n",
      "Reducing exploration for all agents to 0.0012\n",
      "Episode 388 is finished\n",
      "Average Reward for Agent 0 this episode : -93.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2387.5144\n",
      "Reducing exploration for all agents to 0.0012\n",
      "Episode 389 is finished\n",
      "Average Reward for Agent 0 this episode : -100.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4533.0107\n",
      "Reducing exploration for all agents to 0.0012\n",
      "Episode 390 is finished\n",
      "Average Reward for Agent 0 this episode : -92.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3856.8267\n",
      "Reducing exploration for all agents to 0.0012\n",
      "Episode 391 is finished\n",
      "Average Reward for Agent 0 this episode : -95.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3766.0525\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 392 is finished\n",
      "Average Reward for Agent 0 this episode : -88.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3107.6187\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 393 is finished\n",
      "Average Reward for Agent 0 this episode : -95.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3638.4866\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 394 is finished\n",
      "Average Reward for Agent 0 this episode : -94.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4022.6802\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 395 is finished\n",
      "Average Reward for Agent 0 this episode : -96.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3886.2585\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 396 is finished\n",
      "Average Reward for Agent 0 this episode : -88.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3676.6709\n",
      "Reducing exploration for all agents to 0.0011\n",
      "Episode 397 is finished\n",
      "Average Reward for Agent 0 this episode : -91.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4026.7410\n",
      "Reducing exploration for all agents to 0.001\n",
      "Episode 398 is finished\n",
      "Average Reward for Agent 0 this episode : -93.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3861.5090\n",
      "Reducing exploration for all agents to 0.001\n",
      "Episode 399 is finished\n",
      "Average Reward for Agent 0 this episode : -98.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2697.6853\n",
      "Reducing exploration for all agents to 0.001\n",
      "Episode 400 is finished\n",
      "Average Reward for Agent 0 this episode : -97.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3779.6677\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.001\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 500\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: test\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.09 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple8_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2529b166048>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To be arranged for multy agents\n",
    "\n",
    "queues = np.array(Episode_Queues[0])\n",
    "queues = queues.T\n",
    "\n",
    "delay = Cumulative_Episode_Delays[0]\n",
    "\n",
    "# Plot the queues\n",
    "plt.figure(1)\n",
    "for queue in queues:\n",
    "    plt.plot(queue[::50])\n",
    "\n",
    "# plot the junctions delays\n",
    "plt.figure(2)\n",
    "plt.plot(delay)\n",
    "\n",
    "#plot the total delays \n",
    "plt.figure(3)\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "\n",
    "# Dont freak out the 2 delays are not the same because the node is not covering all the junction\n",
    "\n",
    "\"\"\"\n",
    "Because the cars never leave the nodes the delay is not computed correctly (when the agent doesn't work) \n",
    "\"\"\"\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].loss)\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "\n",
    "#print(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "#print(np.min(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].reward_storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 500\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.37 seconds.\n",
      "\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Une exception s’est produite.', (0, None, None, None, 0, -2147467259), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d9255ff0a2ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSingle_Cross_Triple8_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\MasterDQN_Agent.py\u001b[0m in \u001b[0;36mdemo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                         \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;31m# increase the update counter by one each step (until reach simulation length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mRunSingleStep\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Une exception s’est produite.', (0, None, None, None, 0, -2147467259), None)"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Five intersection DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Five_intersection'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions\"\n",
    "\n",
    "# all controller actions\n",
    "Five_intersection_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [11, 12, 13, 14],\n",
    "         'lane' : ['11-1', '11-2', '11-3', '12-1', '12-2', '12-3', '13-1', '13-2', '13-3', '14-1', '14-2', '14-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "                  1 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [21, 22, 23, 24],\n",
    "         'lane' : ['21-1', '21-2', '21-3', '22-1', '22-2', '22-3', '23-1', '23-2', '23-3', '24-1', '24-2', '24-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "                  2 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [31, 32, 33, 34],\n",
    "         'lane' : ['31-1', '31-2', '31-3', '32-1', '32-2', '32-3', '33-1', '33-2', '33-3', '34-1', '34-2', '34-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "                  3 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [41, 42, 43, 44],\n",
    "         'lane' : ['41-1', '41-2', '41-3', '42-1', '42-2', '42-3', '43-1', '43-2', '43-3', '44-1', '44-2', '44-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "                  4 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [51, 52, 53, 54],\n",
    "         'lane' : ['51-1', '51-2', '51-3', '52-1', '52-2', '52-3', '53-1', '53-2', '53-3', '54-1', '54-2', '54-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "    },\n",
    "   'demand' : {\"default\" : [400,400,400,400] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 48)           672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48)           2352        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            49          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            392         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_5[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 0\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 48)           672         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 48)           2352        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 48)           2352        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 48)           2352        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            49          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 8)            392         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_17[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 1\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 48)           672         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 48)           2352        dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 48)           2352        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 48)           2352        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            49          dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 8)            392         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_29[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 2\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 48)           672         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 48)           2352        dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 48)           2352        dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 48)           2352        dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 1)            49          dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 8)            392         dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_41[0][0]                   \n",
      "                                                                 dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 3\n",
      "Model: \"model_8\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 48)           672         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 48)           2352        dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 48)           2352        dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 48)           2352        dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 1)            49          dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 8)            392         dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_53[0][0]                   \n",
      "                                                                 dense_51[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,169\n",
      "Trainable params: 8,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 4\n"
     ]
    }
   ],
   "source": [
    "Five_intersection_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Five_intersection_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 1, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 2, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 3, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 4, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Five_intersection_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\Single_Cross_Triple\\\\Agents_Results\\\\Single_Cross_Triple8_actions\\\\Episode400Agent0_Train.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-183ae37fcff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mFive_intersection_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Single_Cross_Triple'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Single_Cross_Triple8_actions'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\General_agent.py\u001b[0m in \u001b[0;36mload_agent\u001b[1;34m(self, vissim_working_directory, model_name, Session_ID, episode, best)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMemory_Filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mTraining_Progress_Filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Agents_Results\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSession_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Episode'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'Agent'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_Train'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_storage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTraining_Progress_Filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                 \u001b[0mLoss_Filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Agents_Results\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSession_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Episode'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'Agent'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_Loss'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLoss_Filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\Single_Cross_Triple\\\\Agents_Results\\\\Single_Cross_Triple8_actions\\\\Episode400Agent0_Train.p'"
     ]
    }
   ],
   "source": [
    "Five_intersection_MultiDQN_Agents.Agents[0].load_agent(vissim_working_directory, 'Single_Cross_Triple', 'Single_Cross_Triple8_actions',400 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple8_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To be arranged for multy agents\n",
    "\n",
    "queues = np.array(Episode_Queues[0])\n",
    "queues = queues.T\n",
    "\n",
    "delay = Cumulative_Episode_Delays[0]\n",
    "\n",
    "# Plot the queues\n",
    "plt.figure(1)\n",
    "for queue in queues:\n",
    "    plt.plot(queue[::50])\n",
    "\n",
    "# plot the junctions delays\n",
    "plt.figure(2)\n",
    "plt.plot(delay)\n",
    "\n",
    "#plot the total delays \n",
    "plt.figure(3)\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "\n",
    "# Dont freak out the 2 delays are not the same because the node is not covering all the junction\n",
    "\n",
    "\"\"\"\n",
    "Because the cars never leave the nodes the delay is not computed correctly (when the agent doesn't work) \n",
    "\"\"\"\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].loss)\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "\n",
    "#print(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "#print(np.min(Single_Cross_Triple8_MultiDQN_Agents.Agents[0].reward_storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of tensorflow.python.keras.layers.core failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 244, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 378, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\importlib\\__init__.py\", line 148, in reload\n",
      "    raise ImportError(msg.format(name), name=name)\n",
      "ImportError: module DQNAgents not in sys.modules\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Five_intersection.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 3.18 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Five_intersection_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
