{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_critic_class import ACAgent\n",
    "from MasterAC_Agent import MasterAC_Agent\n",
    "from MasterDQN_Agent import MasterDQN_Agent\n",
    "\n",
    "# Network Specific Libraries\n",
    "from Balance_Functions import balance_dictionary\n",
    "\n",
    "# General Libraries\n",
    "import numpy as np \n",
    "import pylab as plt\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = \"E:\\Backup - Onedrive\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\"\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "agent_type = 'AC'\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Agent hyperparameters\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "timesteps_per_second = 1\n",
    "\n",
    "\n",
    "# for the monitoring only for AC\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiAc_Agents.train(1000)\n",
    "\n",
    "Balance_MultiAc_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n",
    "\n",
    "Balance_MultiAc_Agents.load(best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Agents = []\n",
    "for idx, info in Balance_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        \n",
    "        \n",
    "        # Only for AC\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance DQN Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "## Initialization Parameters ##\n",
    "###############################\n",
    "\n",
    "intersection = 3\n",
    "map_name  = 'Balance_int'+str(intersection)\n",
    "model_name = map_name\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = \"E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "\n",
    "## Simulation Parameters\n",
    "Random_Seed = 44\n",
    "sim_length = 10801\n",
    "timesteps_per_second = 1\n",
    "agent_type = \"DDQN\"\n",
    "#actions_set = 'default_actions'     # 'default_actions' or 'all_actions'\n",
    "actions_set = 'all_actions'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 500\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 128\n",
    "batch_size = 128\n",
    "batches_per_episode = 10\n",
    "\n",
    "alpha = 0.00005\n",
    "gamma = 0.95\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "if intersection == \"1_2_4\":\n",
    "    intersection = 1\n",
    "elif intersection == \"11_12\":\n",
    "    intersection = 11\n",
    "partial_dictionary = {\"junctions\": { (intersection-1) : Balance_dictionary[\"junctions\"][intersection-1]},\\\n",
    "                      \"demand\": Balance_dictionary[\"demand\"]}\n",
    "\n",
    "Session_ID = map_name + \"_\" + actions_set + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type + \"debug\"\n",
    "print(\"Current simulation: {}\".format(Session_ID))\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Deploy Agents\n",
    "Balance_int_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, partial_dictionary, actions_set,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, batches_per_episode, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)\n",
    "Balance_int_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Deploy Environment\n",
    "env = None\n",
    "env = environment(model_name, vissim_working_directory, sim_length, partial_dictionary, actions_set,\\\n",
    "                  Random_Seed = Random_Seed, timesteps_per_second = timesteps_per_second, mode = 'debug', delete_results = True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test to ensure correct deployment of agents\n",
    "\n",
    "# Test 1: Check number of agents\n",
    "print(env.SCUs.items())\n",
    "\n",
    "# Test 2: Check Dictionary for each agent\n",
    "agent = 0\n",
    "print(\"state_type: \" + env.SCUs[agent].state_type)\n",
    "print(\"state_size: \")\n",
    "print(env.SCUs[agent].state_size)\n",
    "print(\"reward_type: \")\n",
    "print(env.SCUs[agent].reward_type)\n",
    "print(\"compatible_actions: \")\n",
    "print(env.SCUs[agent].compatible_actions)\n",
    "print(\"all_actions: \")\n",
    "print(env.SCUs[agent].all_actions)\n",
    "print(\"Lanes_names: \" )\n",
    "print(env.SCUs[agent].Lanes_names)\n",
    "print(\"Links_names: \")\n",
    "print(env.SCUs[agent].Links_names)\n",
    "print(\"time_steps_per_second: \" + str(env.SCUs[agent].time_steps_per_second))\n",
    "print(\"queues_counter_ID: \" )\n",
    "print(env.SCUs[agent].queues_counter_ID)\n",
    "print(\"queues_counters: \")\n",
    "print(env.SCUs[agent].queues_counters)\n",
    "print(\"signal_controller: \")\n",
    "print(env.SCUs[agent].signal_controller)\n",
    "print(\"Signal_Groups: \" )\n",
    "print(env.SCUs[agent].signal_groups)\n",
    "print(\"Node: \" + str(env.SCUs[agent].Node))\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Tests to ensure correct STATE READING\n",
    "timesteps = 3\n",
    "for i in range(timesteps):\n",
    "    env.Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "## Test 3: Correct Reading of queues from QUEUE COUNTERS\n",
    "print(\"queues_counter_ID: \" )\n",
    "print(env.SCUs[0].queues_counter_ID)\n",
    "print([env.Vissim.Net.QueueCounters.ItemByKey(i).AttValue('QLen(Current, Last)') for i in env.SCUs[0].queues_counter_ID])\n",
    "    \n",
    "# Test 4: Correct Reading of Aggregated Queues by SCU\n",
    "print(env.SCUs[0].calculate_queues())\n",
    "\n",
    "## Test 5: Correct Reading of Global Queues by ENVIRONMENT\n",
    "print(env.get_queues())\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 6: Correct Reading of Initial State, and Generation of according actions\n",
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "print(\"Dict([(Agent_ID, array(state))])\")\n",
    "print(start_state.items())\n",
    "print(\"\")\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = Balance_int_MultiDQN_Agents.Agents[idx].choose_action(s)\n",
    "print(\"{Agent_ID : Chosen_Action}\")\n",
    "print(actions)\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 7: Correct Reading of General State from SCU and Generation of according actions\n",
    "SARSDs = env.step_to_next_action(actions)\n",
    "actions = dict()\n",
    "for idx , sarsd in SARSDs.items():\n",
    "    s,a,r,ns,d = sarsd\n",
    "    \n",
    "print(\"Agent_ID: \" + str(SARSDs.keys()))\n",
    "print(\"Agent_State:\")\n",
    "print(SARSDs[0][0][0])\n",
    "print(\"Agent_Action: \" + str(SARSDs[0][1]))\n",
    "print(\"Agent_Reward: \" + str(SARSDs[0][2]))\n",
    "print(\"Agent_Next_State:\")\n",
    "print(SARSDs[0][3][0])\n",
    "print(\"Done: \" + str(SARSDs[0][4]))\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 8: Correct operation of signal groups\n",
    "signal_group = 2\n",
    "env.SCUs[0].signal_groups[signal_group].SetAttValue(\"SigState\", \"RED\")\n",
    "env.Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###### Test 9-1: Correct implementation of actions (SETUP)\n",
    "for idx, agent in Balance_int_MultiDQN_Agents.Agents.items():\n",
    "    agent.reset()\n",
    "\n",
    "start_state = env.get_state()\n",
    "print(\"Initial State: {Agent_ID: initual queues}\")\n",
    "print(start_state)\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = Balance_int_MultiDQN_Agents.Agents[idx].choose_action(s)\n",
    "print(\"Initial Choice of Actions: {Agent_ID: action}\")    \n",
    "print(actions)\n",
    "\n",
    "# That is not a clean way to do this\n",
    "def to_dictionary(dictionary,idx,value):\n",
    "    \"\"\"\n",
    "    Assign a value to an index in a dictionary\n",
    "    \"\"\"\n",
    "    dictionary[idx] = value\n",
    "    \n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Test 9-2: Correct implementation of actions (EXECUTION)\n",
    "##\n",
    "## ATTENTION: If an \"index out of range\" is requested, the system will break an will\\\n",
    "##            require a reset. This does not affect normal simulation.\n",
    "\n",
    "actions[0] = 0\n",
    "# This is step_to_next_action() function\n",
    "while not env.action_required:\n",
    "    \n",
    "    # This is the step() function\n",
    "    Sarsd = dict()\n",
    "    \n",
    "    # The default position is that no action is required, only a step of simulator\n",
    "    env.action_required = False\n",
    "    #print(\"false 1\")\n",
    "    \n",
    "    [scu.action_update(actions[0] , green_time = 5 ) for idx, scu in env.SCUs.items() if scu.action_required]\n",
    "    \n",
    "    [scu.update() for idx,scu in env.SCUs.items()]\n",
    "    \n",
    "    env.Vissim.Simulation.RunSingleStep()\n",
    "    \n",
    "    [to_dictionary(Sarsd,idx,scu.sars()+[env.done]) for idx,scu in env.SCUs.items() if scu.action_required ]\n",
    "    \n",
    "    if len(Sarsd) > 0 or env.done :\n",
    "        env.action_required = True\n",
    "        #print(\"TRUE\")\n",
    "print(\"State [W, S1, S2, S3, W, N1, N2]:\")   \n",
    "print(Sarsd[0][0][0])\n",
    "print(\"Action:\")\n",
    "print(Sarsd[0][1])\n",
    "print(\"Reward:\")\n",
    "print(rew())\n",
    "print(\"NextState [W, S1, S2, S3, W, N1, N2]:\")\n",
    "print(Sarsd[0][3][0])\n",
    "env.action_required = False\n",
    "#print(\"false 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rew():\n",
    "    return(-env.Vissim.Net.VehicleNetworkPerformanceMeasurement.AttValue('DelayTot(Current, Last, All)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "movements = list(Node.movements)\n",
    "movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 10: Correct changing of phases based on actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 11: Correct calculation of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance RL DQN Partial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current simulation: Balance_int8_default_actions_100_10800_DuelingDDQN_Queue_squared\n"
     ]
    }
   ],
   "source": [
    "intersection = 8\n",
    "map_name  = 'Balance_int'+str(intersection)\n",
    "model_name = map_name\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = \"E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "\n",
    "## Simulation Parameters\n",
    "Random_Seed = 42\n",
    "sim_length = 10801\n",
    "timesteps_per_second = 1\n",
    "agent_type = \"DuelingDDQN\"\n",
    "actions = 'default_actions'     # 'default_actions' or 'all_actions'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 100\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "batches_per_episode = 10\n",
    "\n",
    "alpha = 0.0001\n",
    "gamma = 0.95\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "if intersection == \"1_2_4\":\n",
    "    intersection = 1\n",
    "elif intersection == \"11_12\":\n",
    "    intersection = 11\n",
    "partial_dictionary = {\"junctions\": { (intersection-1) : Balance_dictionary[\"junctions\"][intersection-1]},\\\n",
    "                      \"demand\": Balance_dictionary[\"demand\"]}\n",
    "\n",
    "Session_ID = map_name + \"_\" + actions + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type + \"_Queue_squared\"\n",
    "print(\"Current simulation: {}\".format(Session_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 7: SETTING UP AGENT\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 24)           192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 24)           600         dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 24)           600         dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 24)           600         dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            25          dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 3)            75          dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_29[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,092\n",
      "Trainable params: 2,092\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Double Deep Q Learning Agent(s) at Intersection 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, partial_dictionary, actions,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, batches_per_episode, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience file not found. Generating now...\n",
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int8.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 10801 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.13 seconds.\n",
      "\n",
      "After 0 actions taken by the Agents,  Agent 0 memory is 0.0 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 0 memory is 10.0 percent full\n",
      "Random Seed Set to 43\n",
      "After 2000 actions taken by the Agents,  Agent 0 memory is 20.0 percent full\n",
      "Random Seed Set to 44\n",
      "After 3000 actions taken by the Agents,  Agent 0 memory is 30.0 percent full\n",
      "Random Seed Set to 45\n",
      "After 4000 actions taken by the Agents,  Agent 0 memory is 40.0 percent full\n",
      "Random Seed Set to 46\n",
      "After 5000 actions taken by the Agents,  Agent 0 memory is 50.0 percent full\n",
      "Random Seed Set to 47\n",
      "After 6000 actions taken by the Agents,  Agent 0 memory is 60.0 percent full\n",
      "After 7000 actions taken by the Agents,  Agent 0 memory is 70.0 percent full\n",
      "Random Seed Set to 48\n",
      "After 8000 actions taken by the Agents,  Agent 0 memory is 80.0 percent full\n",
      "Random Seed Set to 49\n",
      "After 9000 actions taken by the Agents,  Agent 0 memory is 90.0 percent full\n",
      "Random Seed Set to 50\n",
      "Memory filled. Saving as:C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Balance_int8\\Agents_Results\\DuelingDDQN\\Balance_int8_default_actions_100_10800_DuelingDDQN_Queue_squared\\Agent0_PERPre_10000.p\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int8.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 10801 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.15 seconds.\n",
      "\n",
      "start\n",
      "Random Seed Set to 43\n",
      "Episode 1: Finished running.\n",
      "Agent 0, Average Reward: -6410.03\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27998896.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25165264.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20285952.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33428908.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18623112.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30408944.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27173804.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23895610.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29585268.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20590892.0000\n",
      "Reducing exploration for all agents to 0.9545\n",
      "\n",
      "Episode 2: Starting computation.\n",
      "Random Seed Set to 44\n",
      "Episode 2: Finished running.\n",
      "Agent 0, Average Reward: -5765.29\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18973184.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32806040.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26975100.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35796088.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30144714.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38392344.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24837280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37515192.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30275858.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28744280.0000\n",
      "Reducing exploration for all agents to 0.9112\n",
      "\n",
      "Episode 3: Starting computation.\n",
      "Random Seed Set to 45\n",
      "Episode 3: Finished running.\n",
      "Agent 0, Average Reward: -2299.4\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36063968.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39550440.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26963312.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31751578.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31033044.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33626252.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32114102.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37764976.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33682912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28779434.0000\n",
      "Reducing exploration for all agents to 0.8697\n",
      "\n",
      "Episode 4: Starting computation.\n",
      "Random Seed Set to 46\n",
      "Episode 4: Finished running.\n",
      "Agent 0, Average Reward: -2804.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24622784.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34328464.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27008032.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28472390.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26682894.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26079442.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41183600.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26765890.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21789052.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21287174.0000\n",
      "Reducing exploration for all agents to 0.8302\n",
      "\n",
      "Episode 5: Starting computation.\n",
      "Random Seed Set to 47\n",
      "Episode 5: Finished running.\n",
      "Agent 0, Average Reward: -2281.17\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28392608.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33301172.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18021052.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29622742.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19173932.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31230126.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41865080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24074288.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30213776.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27529542.0000\n",
      "Reducing exploration for all agents to 0.7925\n",
      "\n",
      "Episode 6: Starting computation.\n",
      "Random Seed Set to 48\n",
      "Episode 6: Finished running.\n",
      "Agent 0, Average Reward: -3540.96\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27157174.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34568576.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32399124.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24702522.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23412606.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36943992.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40029384.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32348902.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24845764.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30542732.0000\n",
      "Reducing exploration for all agents to 0.7565\n",
      "\n",
      "Episode 7: Starting computation.\n",
      "Random Seed Set to 49\n",
      "Episode 7: Finished running.\n",
      "Agent 0, Average Reward: -8202.31\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30819208.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42212760.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40154976.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34206944.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45463600.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40829536.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36966268.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37885592.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29944526.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41014272.0000\n",
      "Reducing exploration for all agents to 0.7221\n",
      "\n",
      "Episode 8: Starting computation.\n",
      "Random Seed Set to 50\n",
      "Episode 8: Finished running.\n",
      "Agent 0, Average Reward: -6194.18\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39450096.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36481340.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58108856.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47752856.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42973252.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40173552.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43614144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43667688.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50189936.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53983220.0000\n",
      "Reducing exploration for all agents to 0.6893\n",
      "\n",
      "Episode 9: Starting computation.\n",
      "Random Seed Set to 51\n",
      "Episode 9: Finished running.\n",
      "Agent 0, Average Reward: -6123.23\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45983652.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35472664.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37928060.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43339748.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43677424.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48544368.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42074144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40619560.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39028496.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51124360.0000\n",
      "Reducing exploration for all agents to 0.6579\n",
      "\n",
      "Episode 10: Starting computation.\n",
      "Random Seed Set to 52\n",
      "Episode 10: Finished running.\n",
      "Agent 0, Average Reward: -8354.54\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53628632.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55234816.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43170760.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54705672.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47265332.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37362656.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37965728.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45072272.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41026192.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39365936.0000\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Reducing exploration for all agents to 0.628\n",
      "\n",
      "Episode 11: Starting computation.\n",
      "Random Seed Set to 53\n",
      "Episode 11: Finished running.\n",
      "Agent 0, Average Reward: -9527.96\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 61821392.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 61361056.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55784012.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68104320.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51973124.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58675264.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49382920.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59507660.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59749084.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54201600.0000\n",
      "Reducing exploration for all agents to 0.5995\n",
      "\n",
      "Episode 12: Starting computation.\n",
      "Random Seed Set to 54\n",
      "Episode 12: Finished running.\n",
      "Agent 0, Average Reward: -8171.58\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66619244.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68913304.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 63194068.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64987428.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54389760.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66818724.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66819432.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 69202728.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81819632.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68471248.0000\n",
      "Reducing exploration for all agents to 0.5722\n",
      "\n",
      "Episode 13: Starting computation.\n",
      "Random Seed Set to 55\n",
      "Episode 13: Finished running.\n",
      "Agent 0, Average Reward: -15248.56\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 89758976.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 96619872.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 92731584.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 93437752.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 104679968.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 97714664.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 96127728.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 87858224.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 98872120.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 94349080.0000\n",
      "Reducing exploration for all agents to 0.5462\n",
      "\n",
      "Episode 14: Starting computation.\n",
      "Random Seed Set to 56\n",
      "Episode 14: Finished running.\n",
      "Agent 0, Average Reward: -13830.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 111856960.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 99481360.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 130664448.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 101514464.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 101089480.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 93872832.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 112244424.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 103627616.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 113785976.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 113631472.0000\n",
      "Reducing exploration for all agents to 0.5214\n",
      "\n",
      "Episode 15: Starting computation.\n",
      "Random Seed Set to 57\n",
      "Episode 15: Finished running.\n",
      "Agent 0, Average Reward: -12009.37\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 114653144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 124162320.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 132778192.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 138379040.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 131668480.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 123601792.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 117991808.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 134428000.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 101518144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 117485824.0000\n",
      "Reducing exploration for all agents to 0.4977\n",
      "\n",
      "Episode 16: Starting computation.\n",
      "Random Seed Set to 58\n",
      "Episode 16: Finished running.\n",
      "Agent 0, Average Reward: -14606.37\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 128073192.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 112643280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 131117080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 125975792.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 118334720.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 139805264.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 144123088.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 139098896.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 137997472.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 137342944.0000\n",
      "Reducing exploration for all agents to 0.4751\n",
      "\n",
      "Episode 17: Starting computation.\n",
      "Random Seed Set to 59\n",
      "Episode 17: Finished running.\n",
      "Agent 0, Average Reward: -14463.42\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 173998144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 135475520.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 155097056.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 137864192.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 153125616.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 148793712.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 163197120.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 150915872.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 152569952.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 155890608.0000\n",
      "Reducing exploration for all agents to 0.4535\n",
      "\n",
      "Episode 18: Starting computation.\n",
      "Random Seed Set to 60\n",
      "Episode 18: Finished running.\n",
      "Agent 0, Average Reward: -21754.41\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 157811296.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 181023520.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 182454272.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 163541888.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 181133136.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 176641024.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 172840432.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 185649968.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 164141264.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 188313936.0000\n",
      "Reducing exploration for all agents to 0.4329\n",
      "\n",
      "Episode 19: Starting computation.\n",
      "Random Seed Set to 61\n",
      "Episode 19: Finished running.\n",
      "Agent 0, Average Reward: -15344.46\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 191987600.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 193191808.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 191181680.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 200038512.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 185518112.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 211949808.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 163574768.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 181872704.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 176876336.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 188829152.0000\n",
      "Reducing exploration for all agents to 0.4132\n",
      "\n",
      "Episode 20: Starting computation.\n",
      "Random Seed Set to 62\n",
      "Episode 20: Finished running.\n",
      "Agent 0, Average Reward: -18887.91\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 214920384.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 225386080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 215871200.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 224138224.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 223231216.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 219580128.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 213569408.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 197076768.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 232165760.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 221319936.0000\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Saving architecture, weights and optimizer state for agent-7\n",
      "Dumping agent-7 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.3944\n",
      "\n",
      "Episode 21: Starting computation.\n",
      "Random Seed Set to 63\n",
      "Episode 21: Finished running.\n",
      "Agent 0, Average Reward: -15143.48\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 223099600.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 202879056.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 222454816.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 211655168.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 209294880.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 220117376.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 219848672.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 227672592.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 230931776.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 231642960.0000\n",
      "Reducing exploration for all agents to 0.3765\n",
      "\n",
      "Episode 22: Starting computation.\n",
      "Random Seed Set to 64\n",
      "Episode 22: Finished running.\n",
      "Agent 0, Average Reward: -2221.94\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 179397536.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 215077216.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 203618224.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 192559600.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 197088288.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 190318640.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 206955104.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 203366912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 199083968.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 186038320.0000\n",
      "Reducing exploration for all agents to 0.3594\n",
      "\n",
      "Episode 23: Starting computation.\n",
      "Random Seed Set to 65\n",
      "Episode 23: Finished running.\n",
      "Agent 0, Average Reward: -1767.92\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 176996160.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 193061280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 170881056.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 176926560.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 181650128.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 173470208.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 170141296.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 184340096.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 171965312.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 185546816.0000\n",
      "Reducing exploration for all agents to 0.343\n",
      "\n",
      "Episode 24: Starting computation.\n",
      "Random Seed Set to 66\n",
      "Episode 24: Finished running.\n",
      "Agent 0, Average Reward: -1291.72\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 153919040.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 152161472.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 161498624.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 154313376.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 165065504.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 151414080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 162933024.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 144098496.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 156613360.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 155612128.0000\n",
      "Reducing exploration for all agents to 0.3275\n",
      "\n",
      "Episode 25: Starting computation.\n",
      "Random Seed Set to 67\n",
      "Episode 25: Finished running.\n",
      "Agent 0, Average Reward: -3950.41\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 132758576.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 162762016.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 136273088.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 137245120.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 152035904.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 158702416.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 124486656.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 137173504.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 128100016.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 152319408.0000\n",
      "Reducing exploration for all agents to 0.3126\n",
      "\n",
      "Episode 26: Starting computation.\n",
      "Random Seed Set to 68\n",
      "Episode 26: Finished running.\n",
      "Agent 0, Average Reward: -9493.14\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 124563744.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 114318272.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 124155384.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 126160272.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 126473440.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 152779136.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 129832208.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 136284336.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 129781040.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 118825024.0000\n",
      "Reducing exploration for all agents to 0.2984\n",
      "\n",
      "Episode 27: Starting computation.\n",
      "Random Seed Set to 69\n",
      "Episode 27: Finished running.\n",
      "Agent 0, Average Reward: -22829.59\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 224663936.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 166687104.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 226254336.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 188505632.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 169666560.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 180012688.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 208856352.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 226114624.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 213612288.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 224320400.0000\n",
      "Reducing exploration for all agents to 0.2848\n",
      "\n",
      "Episode 28: Starting computation.\n",
      "Random Seed Set to 70\n",
      "Episode 28: Finished running.\n",
      "Agent 0, Average Reward: -5377.11\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 207531392.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 163113792.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 183159520.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 180114992.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 179133280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 202655552.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 174853728.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 160778544.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 176016576.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 199651808.0000\n",
      "Reducing exploration for all agents to 0.2719\n",
      "\n",
      "Episode 29: Starting computation.\n",
      "Random Seed Set to 71\n",
      "Episode 29: Finished running.\n",
      "Agent 0, Average Reward: -974.15\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 123084960.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 173100416.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 173860384.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 177187184.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 145853776.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 170016832.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 157944224.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 123965328.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 124226800.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 147050752.0000\n",
      "Reducing exploration for all agents to 0.2595\n",
      "\n",
      "Episode 30: Starting computation.\n",
      "Random Seed Set to 72\n",
      "Episode 30: Finished running.\n",
      "Agent 0, Average Reward: -18196.99\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 179380704.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 166400352.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 150102048.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 169445504.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 187587936.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 156295136.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 172680064.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 170382592.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 183067888.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 202998016.0000\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Reducing exploration for all agents to 0.2477\n",
      "\n",
      "Episode 31: Starting computation.\n",
      "Random Seed Set to 73\n",
      "Episode 31: Finished running.\n",
      "Agent 0, Average Reward: -44508.07\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 269322752.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 321677184.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 272578048.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 292461536.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 287040000.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 332936000.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 319393728.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 308458880.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 309435168.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 351690752.0000\n",
      "Reducing exploration for all agents to 0.2364\n",
      "\n",
      "Episode 32: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 74\n",
      "Episode 32: Finished running.\n",
      "Agent 0, Average Reward: -42021.1\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 416630624.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 420435008.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 414595872.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 394570880.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 420737568.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 384621440.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 420484352.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 421667072.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 408608704.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 451713760.0000\n",
      "Reducing exploration for all agents to 0.2257\n",
      "\n",
      "Episode 33: Starting computation.\n",
      "Random Seed Set to 75\n",
      "Episode 33: Finished running.\n",
      "Agent 0, Average Reward: -41203.57\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 520636928.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 519722080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 503918464.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 504492256.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 500440768.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 554944064.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 516313600.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 536819136.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 517424704.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 507140608.0000\n",
      "Reducing exploration for all agents to 0.2154\n",
      "\n",
      "Episode 34: Starting computation.\n",
      "Random Seed Set to 76\n",
      "Episode 34: Finished running.\n",
      "Agent 0, Average Reward: -30638.34\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 577198848.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 538115136.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 542486528.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 554247936.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 569069120.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 559512576.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 533166592.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 525568960.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 557353920.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 531854240.0000\n",
      "Reducing exploration for all agents to 0.2057\n",
      "\n",
      "Episode 35: Starting computation.\n",
      "Random Seed Set to 77\n",
      "Episode 35: Finished running.\n",
      "Agent 0, Average Reward: -14940.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 481663872.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 472694912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 463518976.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 477523840.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 494780160.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 493046240.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 476765184.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 468504384.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 473206144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 484145760.0000\n",
      "Reducing exploration for all agents to 0.1963\n",
      "\n",
      "Episode 36: Starting computation.\n",
      "Random Seed Set to 78\n",
      "Episode 36: Finished running.\n",
      "Agent 0, Average Reward: -13126.32\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 464715776.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 462728096.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 489488736.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 510732224.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 497911616.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 494053344.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 501782656.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 495510720.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 526671200.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 490982912.0000\n",
      "Reducing exploration for all agents to 0.1874\n",
      "\n",
      "Episode 37: Starting computation.\n",
      "Random Seed Set to 79\n",
      "Episode 37: Finished running.\n",
      "Agent 0, Average Reward: -8734.33\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 511764608.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 460982176.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 492585024.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 469031520.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 472173248.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 492905280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 459343488.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 496550976.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 463972800.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 479839872.0000\n",
      "Reducing exploration for all agents to 0.1789\n",
      "\n",
      "Episode 38: Starting computation.\n",
      "Random Seed Set to 80\n",
      "Episode 38: Finished running.\n",
      "Agent 0, Average Reward: -3113.18\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 394796448.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 388181792.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 388266880.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 407691136.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 408601152.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 406258560.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 408930816.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 428827264.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 394734144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 394332480.0000\n",
      "Reducing exploration for all agents to 0.1707\n",
      "\n",
      "Episode 39: Starting computation.\n",
      "Random Seed Set to 81\n",
      "Episode 39: Finished running.\n",
      "Agent 0, Average Reward: -3715.23\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 305469056.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 283488576.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 282859392.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 254418848.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 284784288.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 271289472.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 263564112.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 285273824.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 293236576.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 266315632.0000\n",
      "Reducing exploration for all agents to 0.163\n",
      "\n",
      "Episode 40: Starting computation.\n",
      "Random Seed Set to 82\n",
      "Episode 40: Finished running.\n",
      "Agent 0, Average Reward: -3431.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 183065344.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 185714304.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 187433344.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 170922384.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 187100848.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 179527872.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 173636416.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 192220992.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 176965184.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 179735968.0000\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Saving architecture, weights and optimizer state for agent-7\n",
      "Dumping agent-7 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.1556\n",
      "\n",
      "Episode 41: Starting computation.\n",
      "Random Seed Set to 83\n",
      "Episode 41: Finished running.\n",
      "Agent 0, Average Reward: -5188.24\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 96152080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 104543512.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 100264840.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 116934624.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 97783240.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 101500480.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 118814432.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 106078464.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 107819112.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 113388400.0000\n",
      "Reducing exploration for all agents to 0.1485\n",
      "\n",
      "Episode 42: Starting computation.\n",
      "Random Seed Set to 84\n",
      "Episode 42: Finished running.\n",
      "Agent 0, Average Reward: -14376.98\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 90406392.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 106662624.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 110078784.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 109213952.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 118611272.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 112462688.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 105843296.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 102585912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 112465600.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 103380496.0000\n",
      "Reducing exploration for all agents to 0.1417\n",
      "\n",
      "Episode 43: Starting computation.\n",
      "Random Seed Set to 85\n",
      "Episode 43: Finished running.\n",
      "Agent 0, Average Reward: -30033.78\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 227583056.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 203532592.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 163230304.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 227782000.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 211453280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 179730432.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 184011968.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 229654624.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 220734816.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 177841984.0000\n",
      "Reducing exploration for all agents to 0.1353\n",
      "\n",
      "Episode 44: Starting computation.\n",
      "Random Seed Set to 86\n",
      "Episode 44: Finished running.\n",
      "Agent 0, Average Reward: -10525.7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 237729248.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 221099136.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 201202496.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 185546608.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 182389664.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 205912144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 195096832.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 215459968.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 211379584.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 191925408.0000\n",
      "Reducing exploration for all agents to 0.1292\n",
      "\n",
      "Episode 45: Starting computation.\n",
      "Random Seed Set to 87\n",
      "Episode 45: Finished running.\n",
      "Agent 0, Average Reward: -2444.13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 213175024.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 215980496.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 191251280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 227909216.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 215765568.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 172168000.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 200714272.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 219984832.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 174262928.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 211781568.0000\n",
      "Reducing exploration for all agents to 0.1233\n",
      "\n",
      "Episode 46: Starting computation.\n",
      "Random Seed Set to 88\n",
      "Episode 46: Finished running.\n",
      "Agent 0, Average Reward: -1147.6\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 193505792.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 226401472.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 168858912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 235701600.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 187085760.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 183455056.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 180913184.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 222885664.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 223966816.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 196323168.0000\n",
      "Reducing exploration for all agents to 0.1177\n",
      "\n",
      "Episode 47: Starting computation.\n",
      "Random Seed Set to 89\n",
      "Episode 47: Finished running.\n",
      "Agent 0, Average Reward: -975.96\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 209108192.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 233936464.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 219470544.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 182521328.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 171119104.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 202534880.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 194889504.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 192482528.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 188574784.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 224008288.0000\n",
      "Reducing exploration for all agents to 0.1123\n",
      "\n",
      "Episode 48: Starting computation.\n",
      "Random Seed Set to 90\n",
      "Episode 48: Finished running.\n",
      "Agent 0, Average Reward: -46505.25\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 469432768.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 488654336.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 496676864.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 534392512.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 471162752.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 466580736.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 465515328.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 467628352.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 445304448.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 482443104.0000\n",
      "Reducing exploration for all agents to 0.1072\n",
      "\n",
      "Episode 49: Starting computation.\n",
      "Random Seed Set to 91\n",
      "Episode 49: Finished running.\n",
      "Agent 0, Average Reward: -2388.9\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 409002240.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 394111296.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 453239488.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 407284960.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 375802784.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 433738304.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 447387328.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 438117952.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 408623968.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 420887168.0000\n",
      "Reducing exploration for all agents to 0.1024\n",
      "\n",
      "Episode 50: Starting computation.\n",
      "Random Seed Set to 92\n",
      "Episode 50: Finished running.\n",
      "Agent 0, Average Reward: -2248.68\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 318459840.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 335044416.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 326323104.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 343241248.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 340186176.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 329076800.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 345596640.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 349393152.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 322190464.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 370253888.0000\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Reducing exploration for all agents to 0.0977\n",
      "\n",
      "Episode 51: Starting computation.\n",
      "Random Seed Set to 93\n",
      "Episode 51: Finished running.\n",
      "Agent 0, Average Reward: -1884.76\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 271223360.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 277219744.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 273898752.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 293274080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 310794912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 256853696.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 289778048.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 288671968.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 276503424.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 297195872.0000\n",
      "Reducing exploration for all agents to 0.0933\n",
      "\n",
      "Episode 52: Starting computation.\n",
      "Random Seed Set to 94\n",
      "Episode 52: Finished running.\n",
      "Agent 0, Average Reward: -71229.29\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 581822080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 604955392.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 593384704.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 556460736.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 560033152.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 573629120.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 546583936.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 619279744.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 565790080.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 605887040.0000\n",
      "Reducing exploration for all agents to 0.089\n",
      "\n",
      "Episode 53: Starting computation.\n",
      "Random Seed Set to 95\n",
      "Episode 53: Finished running.\n",
      "Agent 0, Average Reward: -217548.06\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3086735360.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3025795584.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3120835584.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3074785792.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3103172096.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3134251008.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3220345856.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3048557056.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3239645696.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3117905664.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.085\n",
      "\n",
      "Episode 54: Starting computation.\n",
      "Random Seed Set to 96\n",
      "Episode 54: Finished running.\n",
      "Agent 0, Average Reward: -1082.35\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3008815104.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3177282560.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3096847104.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3131449856.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3120006656.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3058275328.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3188475392.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3053982208.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3205372928.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3178612224.0000\n",
      "Reducing exploration for all agents to 0.0811\n",
      "\n",
      "Episode 55: Starting computation.\n",
      "Random Seed Set to 97\n",
      "Episode 55: Finished running.\n",
      "Agent 0, Average Reward: -147.32\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2978029824.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3156900864.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3022953984.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3165627392.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3076540928.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3023289856.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3140197120.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3119623680.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2972111616.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3140274432.0000\n",
      "Reducing exploration for all agents to 0.0774\n",
      "\n",
      "Episode 56: Starting computation.\n",
      "Random Seed Set to 98\n",
      "Episode 56: Finished running.\n",
      "Agent 0, Average Reward: -104196.43\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3410982912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3496070912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3540587008.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3483101696.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3489204736.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3408116992.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3440033280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3543919104.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3410911488.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3437215232.0000\n",
      "Reducing exploration for all agents to 0.0739\n",
      "\n",
      "Episode 57: Starting computation.\n",
      "Random Seed Set to 99\n",
      "Episode 57: Finished running.\n",
      "Agent 0, Average Reward: -89524.77\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4172787200.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3994533888.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3835854336.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4027479552.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4052225792.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3860000768.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3911934208.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3842808576.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3841970176.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3917442560.0000\n",
      "Reducing exploration for all agents to 0.0705\n",
      "\n",
      "Episode 58: Starting computation.\n",
      "Random Seed Set to 100\n",
      "Episode 58: Finished running.\n",
      "Agent 0, Average Reward: -80290.69\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4386269184.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4501705728.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4214891776.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4336328704.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4479696896.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4344897024.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4309647872.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4480039936.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4470262272.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4354769408.0000\n",
      "Reducing exploration for all agents to 0.0673\n",
      "\n",
      "Episode 59: Starting computation.\n",
      "Random Seed Set to 101\n",
      "Episode 59: Finished running.\n",
      "Agent 0, Average Reward: -26335.6\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4077954048.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4194958336.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4110175488.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4225540864.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4249075712.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4255828224.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4140072704.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4074948608.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4143465728.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4206992384.0000\n",
      "Reducing exploration for all agents to 0.0643\n",
      "\n",
      "Episode 60: Starting computation.\n",
      "Random Seed Set to 102\n",
      "Episode 60: Finished running.\n",
      "Agent 0, Average Reward: -96875.16\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3031584512.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2993371648.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2942246144.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2999206912.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3032160512.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2945776640.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3036246016.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2897653248.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3097969152.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3046354432.0000\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Saving architecture, weights and optimizer state for agent-7\n",
      "Dumping agent-7 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0614\n",
      "\n",
      "Episode 61: Starting computation.\n",
      "Random Seed Set to 103\n",
      "Episode 61: Finished running.\n",
      "Agent 0, Average Reward: -98189.79\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2914425344.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2898337280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2905505280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2923078656.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2844814336.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3011623424.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2946779392.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2923759616.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2961260800.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2961644544.0000\n",
      "Reducing exploration for all agents to 0.0586\n",
      "\n",
      "Episode 62: Starting computation.\n",
      "Random Seed Set to 104\n",
      "Episode 62: Finished running.\n",
      "Agent 0, Average Reward: -118875.01\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3873028352.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3928010752.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3915090176.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3981066752.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3874143488.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3862254336.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3993746176.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3900996864.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3992115200.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3898515712.0000\n",
      "Reducing exploration for all agents to 0.0559\n",
      "\n",
      "Episode 63: Starting computation.\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.train(episodes - Balance_int_MultiDQN_Agents.number_of_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.save(Balance_int_MultiDQN_Agents.number_of_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGENT TRAINING RESULTS\n",
    "# Path to results folder\n",
    "results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "\n",
    "# Loop over each agent\n",
    "for idx , agent in Balance_int_MultiDQN_Agents.Agents.items():\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[0].signal_id + 1\n",
    "    print(\"Intersection \"+str(intersection_number_in_vissim))\n",
    "    \n",
    "    ## SAVE TRAINING DATA TO JSON.\n",
    "    json_filename = \"Agent{}_Loss_average_reward.json\".format(intersection_number_in_vissim)\n",
    "    Loss_reward = dict()   \n",
    "    # Loss dictionary\n",
    "    for epoch, loss in enumerate(agent.loss):\n",
    "        loss_dict = { epoch : loss }\n",
    "    Loss_reward['Agent{} loss'.format(intersection_number_in_vissim)] = loss_dict\n",
    "    # Reward dictionary            \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    Loss_reward['Agent{} Average_Reward'.format(intersection_number_in_vissim)] = agent.reward_storage\n",
    "    # Store as JSON\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Loss_reward, f)\n",
    "    print(\"Agent {}: Training Loss and Average Reward during training successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "    \n",
    "    ## LOADING DATA FROM JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Agent{}_Loss_average_reward.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "    \n",
    "    ## TRAINING PLOTS\n",
    "    loss_plot_filename  = \"Agent{}_Loss.png\".format(intersection_number_in_vissim)\n",
    "    reward_plot_filename  = \"Agent{}_average_reward.png\".format(intersection_number_in_vissim) \n",
    "    \n",
    "    ## Loss Plot\n",
    "    plt.figure('LossAgent'+str(idx),figsize=(16,9))\n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Training Epoch',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    #plt.yscale('log')\n",
    "    plt.title('Agent {} Loss over training'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.savefig(results_path + \"/\" + loss_plot_filename)\n",
    "\n",
    "    ## Average Reward Plot\n",
    "    plt.figure('RewardAgent'+str(idx),figsize=(16,9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Training Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent {} average reward over training'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.savefig(results_path + \"/\" + reward_plot_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.load(100, best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int5.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 10801 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 142\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.22 seconds.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2a992ec70e68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBalance_int_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\MasterDQN_Agent.py\u001b[0m in \u001b[0;36mdemo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m                         \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions, green_time)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps_per_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m                 \u001b[1;31m# increase the update counter by one each step (until reach simulation length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissim\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    514\u001b[0m                         \u001b[0mdebug_attr_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Getting property Id 0x%x from OLE object\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mretEntry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m                                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretEntry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minvoke_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m                         \u001b[1;32mexcept\u001b[0m \u001b[0mpythoncom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom_error\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mERRORS_BAD_CONTEXT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "########################################\n",
    "## Queues over time for each junction ##\n",
    "########################################\n",
    "for idx, queues in Balance_int_MultiDQN_Agents.Episode_Queues.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[0].signal_id + 1\n",
    "    \n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    number_queues = np.size(queues,0)\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queues = dict()\n",
    "    Queues['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queues[str(i)] = queue.tolist()\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "    \n",
    "    ## Plot the queues\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    filename = \"Junction{}_Queues.png\".format(intersection_number_in_vissim)           \n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Queues.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Queues, f)\n",
    "        \n",
    "    ### LOADING DATA FROM JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #json_filename = \"Junction{}_Queues.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "        \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Queues during Test successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "       \n",
    "        \n",
    "###################################################        \n",
    "## Accumulated delay over time for each junction ##\n",
    "###################################################\n",
    "for idx, delay in Balance_int_MultiDQN_Agents.Cumulative_Episode_Delays.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[idx].signal_id + 1\n",
    "\n",
    "    # Extract and process delay data\n",
    "    Delay = dict()   \n",
    "    Delay['Time'] = time\n",
    "    Delay['Junction {} delay'.format(intersection_number_in_vissim)] = delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Cumulative_Delay.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Delay, f)\n",
    "        \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Delay successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "    \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Junction{}_Cumulative_Delay.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    # Plot the cumulative delay\n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    filename = \"Junction{}_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "    \n",
    "    \n",
    "    \n",
    "########################################################    \n",
    "## Accumulated stop delay over time for each junction ##\n",
    "########################################################\n",
    "for idx, stop_delay in Balance_int_MultiDQN_Agents.Cumulative_Episode_stop_Delays.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[idx].signal_id + 1    \n",
    "    \n",
    "    # Extract and process stop delay data\n",
    "    Stop_delay = dict()   \n",
    "    Stop_delay['Time'] = time\n",
    "    Stop_delay['Junction {} stop delay'.format(intersection_number_in_vissim)] = stop_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Cumulative_Stop_Delay.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Stop_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Stop Delay successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Junction{}_Cumulative_Stop_Delay.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "    \n",
    "    # Plot the cumulative stop delay\n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    filename = \"Junction{}_Cumulative_Stop_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n",
    "    \n",
    "    \n",
    "###############################################\n",
    "## ONLY IF THERE IS MORE THAN ONE CONTROLLER ##\n",
    "##    These are the global network plots     ##\n",
    "###############################################\n",
    "\n",
    "if len(Balance_int_MultiDQN_Agents.Agents) > 1:\n",
    "    ########################################    \n",
    "    ## Global Accumulated delay over time ##\n",
    "    ########################################\n",
    "    \n",
    "    # Process global delay data\n",
    "    Global_delay = dict()   \n",
    "    Global_delay['Time'] = time\n",
    "    Global_delay['Global accumulated Delay'] = Balance_int_MultiDQN_Agents.Cumulative_Totale_network_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Global_Cumulative_Delay.json\"\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Global_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Global Delay successfuly saved to file:\")\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Global_Cumulative_Delay.json\"\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    \n",
    "    # Plot the global delay\n",
    "    plt.figure('4',figsize=(16,9))\n",
    "    plt.plot(Cumulative_Totale_network_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "    plt.title('Global accumulated Delay',fontsize=18)\n",
    "    plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "    filename = \"Global_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    ## Global Accumulated stop delay over time ##\n",
    "    #############################################\n",
    "    \n",
    "    # Process global stop delay data\n",
    "    Global_stop_delay = dict()   \n",
    "    Global_stop_delay['Time'] = time\n",
    "    Global_stop_delay['Global accumulated stop Delay'] = Balance_int_MultiDQN_Agents.Cumulative_Totale_network_stop_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Global_Cumulative_Stop_Delay.json\"\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Global_stop_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Global Stop Delay successfuly saved to file:\")\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Global_Cumulative_Stop_Delay.json\"\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    # Plot the global stop delay\n",
    "    plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "    plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "    plt.gca().legend('Global accumulated stop Delay')\n",
    "    \n",
    "    filename = \"Global_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "map_name  = 'Balance'\n",
    "model_name = map_name\n",
    "\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = \"E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "\n",
    "## Simulation Parameters\n",
    "Random_Seed = 44\n",
    "sim_length = 3601\n",
    "timesteps_per_second = 1\n",
    "agent_type = \"DQN\"\n",
    "actions = 'default_actions'     # 'default_actions' or 'all_actions'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 500\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 5000\n",
    "batch_size = 128\n",
    "batches_per_episode = 10\n",
    "\n",
    "alpha = 0.00005\n",
    "gamma = 0.95\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "\n",
    "Session_ID = map_name + \"_\" + actions + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type\n",
    "print(\"Current simulation: {}\".format(Session_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, actions,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, batches_per_episode, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.save(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For the agent training\n",
    "ploty = 1\n",
    "for idx , agent in Balance_MultiDQN_Agents.Agents.items():\n",
    "    print(\"Agent \"+str(idx))\n",
    "    #print(ploty)\n",
    "    #plt.subplot(14, 2, ploty)\n",
    "\n",
    "    plt.figure('6'+str(idx),figsize=(4.5, 3))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    ploty+=1\n",
    "    #print(ploty)\n",
    "\n",
    "    \n",
    "    #plt.subplot(14, 2, ploty)\n",
    "    plt.figure('7'+str(idx),figsize=(4.5, 3))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    #Loss_rewarddf.to_csv(csv_Path,index=False)\n",
    "    ploty+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Balance_MultiDQN_Agents.Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Balance_MultiDQN_Agents.Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Balance_MultiDQN_Agents.Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Balance_MultiDQN_Agents.Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Balance_MultiDQN_Agents.Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.load(498, best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight AC\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Straigth_AC\"\n",
    "\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.00001\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 0.5\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.train(200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.save(401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.load(200, best = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay = Single_Cross_Straight_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in  Single_Cross_Straight_MultiAC_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.Agents[0].Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDQN\"\n",
    "Session_ID = \"Single_Cross_Straigth_DuelingDQN20c0\"\n",
    "\n",
    "# all controller actions\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4]  }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 300\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.load(300 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Single_Cross_Straight_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in  Single_Cross_Straight_MultiDQN_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3600\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_TripleAC4test1\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "    },\n",
    "   'demand' : {\"default\" : [400,400,400,400] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 5000\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 action DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "Session_ID = \"Single_Cross_Triple4_actions\"\n",
    "#Session_ID = \"DQN\"\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{ 'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' :    {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    "         \n",
    "         'all_actions' :       {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "        },\n",
    "     'demand' : { 'default' : [400, 400, 400, 400]}\n",
    "                  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 5\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.train(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.load(best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To be arranged for multy agents\n",
    "\n",
    "queues = np.array(Episode_Queues[0])\n",
    "queues = queues.T\n",
    "\n",
    "delay = Cumulative_Episode_Delays[0]\n",
    "\n",
    "# Plot the queues\n",
    "plt.figure(1)\n",
    "for queue in queues:\n",
    "    plt.plot(queue)\n",
    "\n",
    "# plot the junctions delays\n",
    "plt.figure(2)\n",
    "plt.plot(delay)\n",
    "\n",
    "#plot the total delays \n",
    "plt.figure(3)\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "\n",
    "# Dont freak out the 2 delays are not the same because the node is not covering all the junction\n",
    "\n",
    "\"\"\"\n",
    "Because the cars never leave the nodes the delay is not computed correctly (when the agent doesn't work) \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].loss)\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "print(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_AC10\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]             \n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [300,300,300,300],\n",
    "             1 : [600,600,600,600],\n",
    "             2 : [1350,750,1350,750],\n",
    "             3 : [1500,750,1500,750],\n",
    "             4 : [1050,750,1050,750],\n",
    "             5 : [750,1050,750,1050],\n",
    "             6 : [750,1500,750,1500],\n",
    "             7 : [750,1350,750,1350],\n",
    "             8 : [600,600,600,600],\n",
    "             9 : [300,300,300,300]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.000001\n",
    "\n",
    "\n",
    "value = 5\n",
    "entropy = 500\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple8_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_DuelingDDQN20c10\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]             \n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [300,300,300,300],\n",
    "             1 : [600,600,600,600],\n",
    "             2 : [1350,750,1350,750],\n",
    "             3 : [1500,750,1500,750],\n",
    "             4 : [1050,750,1050,750],\n",
    "             5 : [750,1050,750,1050],\n",
    "             6 : [750,1500,750,1500],\n",
    "             7 : [750,1350,750,1350],\n",
    "             8 : [600,600,600,600],\n",
    "             9 : [300,300,300,300]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400 \n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.train(episodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Single_Cross_Triple8_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in Single_Cross_Triple8_MultiDQN_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Five intersection DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Five_intersection'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Five5transfert\"\n",
    "\n",
    "# all controller actions\n",
    "Five_intersection_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['11-1', '11-2', '11-3', '12-1', '12-2', '12-3', '13-1', '13-2', '13-3', '14-1', '14-2', '14-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "         },\n",
    "                  1 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['21-1', '21-2', '21-3', '22-1', '22-2', '22-3', '23-1', '23-2', '23-3', '24-1', '24-2', '24-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "        'queues_counter_ID' : [13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "         },\n",
    "                  2 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['31-1', '31-2', '31-3', '32-1', '32-2', '32-3', '33-1', '33-2', '33-3', '34-1', '34-2', '34-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [25,26,27,28,29,30,31,32,33,34,35,36]\n",
    "         },\n",
    "                  3 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['41-1', '41-2', '41-3', '42-1', '42-2', '42-3', '43-1', '43-2', '43-3', '44-1', '44-2', '44-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "          'queues_counter_ID' : [37,38,39,40,41,42,43,44,45,46,47,48]\n",
    "         },\n",
    "                  4 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['51-1', '51-2', '51-3', '52-1', '52-2', '52-3', '53-1', '53-2', '53-3', '54-1', '54-2', '54-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [49,50,51,52,53,54,55,56,57,58,59,60]\n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             \n",
    "             0 : [200,200,200,200,200,200,200,200,200,200,200,200],\n",
    "             1 : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             2 : [500,900,500,500,900,500,500,900,500,500,900,500],\n",
    "             3 : [500,1000,500,500,1000,500,500,1000,500,500,1000,500],\n",
    "             4 : [500,700,500,500,700,500,500,700,500,500,700,500],\n",
    "             5 : [500,700,500,500,700,500,500,700,500,500,700,500],\n",
    "             6 : [500,1000,500,500,1000,500,500,1000,500,500,1000,500],\n",
    "             7 : [500,900,500,500,900,500,500,900,500,500,900,500],\n",
    "             8 : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             9 : [200,200,200,200,200,200,200,200,200,200,200,200]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Five_intersection_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.save(401)\n",
    "Five_intersection_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.Agents[0].load_agent(vissim_working_directory, 'Single_Cross_Triple', 'Single_Cross_Triple8_actions',400 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay = Five_intersection_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        \n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Queue Length')\n",
    "    plt.title('Junction {} Queue length'.format(idx))\n",
    "    plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Accumulated Delay')\n",
    "    plt.title('Junction {} Delay'.format(idx))\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Accumulated Stop Delay')\n",
    "    plt.title('Junction {} Stop Delay'.format(idx))\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Global accumulated Delay')\n",
    "plt.title('Global accumulated Delay')\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Global accumulated stop Delay')\n",
    "plt.title('Global accumulated stop Delay')\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.Agents[2] = Five_intersection_MultiDQN_Agents.Agents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
