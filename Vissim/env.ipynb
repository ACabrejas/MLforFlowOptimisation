{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_Critic_Class import ACAgent\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{ 'junctions' : {\n",
    "        # Controller Number 2 \n",
    "        0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                        1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                        2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "\n",
    "             'link' : [2, 40, 7, 38],\n",
    "             'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [8],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "\n",
    "\n",
    "            },\n",
    "        # Controller Number 3\n",
    "        1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                        1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                        2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "\n",
    "             'link' : [5, 48, 70, 46],\n",
    "             'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [8],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Controller Number 4\n",
    "        2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                        1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                        2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                        3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "\n",
    "             'link' : [73, 100, 84, 95],\n",
    "             'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                      '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [14],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Controller Number 5\n",
    "        3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                        1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                        2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "\n",
    "             'link' : [87, 36, 10, 34],\n",
    "             'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [8],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Controller Number 6 \n",
    "        4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                        1 : [1, 1, 0, 0, 0],\n",
    "                                        2 : [0, 0, 0, 1, 0]},\n",
    "             'link' : [8, 24, 13],\n",
    "             'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [6],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Controller Number 8\n",
    "        5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                        1 : [0, 1, 0, 1, 0, 1]},\n",
    "             'link' : [26, 23, 35],\n",
    "             'lane' : ['26-1', '23-1', '35-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [3],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "\n",
    "            },\n",
    "        # Controller Number 9\n",
    "        6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                        1 : [1, 0, 1, 0, 0, 0]},\n",
    "             'link' : [51, 92, 64, 19],\n",
    "             'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [6],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Contoller Number 10\n",
    "        7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                        1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                        2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "             'link' : [18, 66, 16],\n",
    "             'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [7],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Controller Number 12\n",
    "        8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                        1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "             'link' : [62, 45, 44],\n",
    "             'lane' : ['62-1', '45-1', '44-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [3],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Controller Number 13\n",
    "        9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                        1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "             'link' : [60, 43, 55, 58],\n",
    "             'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [4],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "\n",
    "            },\n",
    "        # Controller 15\n",
    "        10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                        1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "             'link' : [32, 42, 30, 39],\n",
    "             'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [4],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Controller 16\n",
    "        11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                        1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "             'link' : [29, 50, 28, 47],\n",
    "             'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [4],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            },\n",
    "        # Controller 17\n",
    "        12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                        1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                        2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                        3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "             'link' : [27, 22, 25, 77],\n",
    "             'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [7],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "\n",
    "            },\n",
    "        # Controller 33 \n",
    "        13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                        1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                        2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "             'link' : [68, 71, 75],\n",
    "             'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "\n",
    "             'controled_by_com' : True,\n",
    "             'green_time' : 8,\n",
    "             'redamber_time' : 0,\n",
    "             'amber_time' : 3, \n",
    "             'red_time' : 0,\n",
    "             'state_size' : [6],\n",
    "             'state_type' : 'Queues',\n",
    "             'reward_type' : 'Queues'\n",
    "            }\n",
    "                },\n",
    "     \"demand\" : { \"default\" : [125, 505, 54, 39, 34, 654, 604, 198, 165, 164, 972, 124, 48, 43, 203, 76] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 1800 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.9688537120819092\n"
     ]
    }
   ],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  960       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  630       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_2 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,307\n",
      "Trainable params: 12,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_3 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_4 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_5 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_6 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,373\n",
      "Trainable params: 11,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_7 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,522\n",
      "Trainable params: 11,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_9 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_10  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_11  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_12  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_13  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agent hyperparameters\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "\n",
    "# for the monitoring only for AC\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Balance_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patate\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -72.0, -64.0, -6.0, -71.0, -6.0, -6.0, -63.0, -19.0, -72.0] \n",
      " [-56.0, -72.0, -90.0, -15.0, -69.0, -25.0, -49.0, -78.0, -24.0, -72.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.33, 0.35, 0.32], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34], [0.34, 0.34, 0.33], [0.32, 0.35, 0.32], [0.34, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.34 0.33]\n",
      "Agent 1 : Predicted Values and True Return : \n",
      " [-32.0, -71.0, -44.0, -28.0, -54.0, -60.0, -5.0, -75.0, -93.0, -5.0] \n",
      " [-55.0, -76.0, -118.0, -105.0, -85.0, -82.0, -64.0, -52.0, -87.0, -37.0]\n",
      "Agent 1 : Proba distribution on those states : \n",
      " [[0.33, 0.34, 0.33], [0.32, 0.34, 0.34], [0.3, 0.36, 0.34], [0.33, 0.33, 0.33], [0.32, 0.35, 0.33], [0.33, 0.33, 0.34], [0.33, 0.34, 0.34], [0.32, 0.35, 0.33], [0.33, 0.34, 0.33], [0.33, 0.34, 0.34]]\n",
      "Agent 1 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.34]\n",
      "Agent 2 : Predicted Values and True Return : \n",
      " [-7.0, -108.0, -72.0, -127.0, -14.0, -146.0, -38.0, -193.0, -161.0, -161.0] \n",
      " [-31.0, -178.0, -160.0, -175.0, -42.0, -171.0, -58.0, -159.0, -153.0, -153.0]\n",
      "Agent 2 : Proba distribution on those states : \n",
      " [[0.25, 0.26, 0.24, 0.25], [0.24, 0.23, 0.26, 0.26], [0.26, 0.24, 0.25, 0.25], [0.24, 0.24, 0.22, 0.31], [0.24, 0.26, 0.25, 0.25], [0.23, 0.23, 0.26, 0.28], [0.24, 0.25, 0.24, 0.27], [0.26, 0.22, 0.27, 0.25], [0.26, 0.24, 0.22, 0.28], [0.26, 0.24, 0.22, 0.28]]\n",
      "Agent 2 : Proba distribution on the 0 state : \n",
      " [0.25 0.26 0.24 0.25]\n",
      "Agent 3 : Predicted Values and True Return : \n",
      " [-7.0, -11.0, -7.0, -13.0, -16.0, -4.0, -12.0, -13.0, -7.0, -16.0] \n",
      " [-24.0, -28.0, -20.0, -42.0, -38.0, -33.0, -29.0, -19.0, -38.0, -26.0]\n",
      "Agent 3 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.34, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.34, 0.33, 0.34], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.33, 0.33], [0.31, 0.34, 0.35]]\n",
      "Agent 3 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 4 : Predicted Values and True Return : \n",
      " [-7.0, -25.0, -36.0, -13.0, -25.0, -45.0, -30.0, -20.0, -19.0, -7.0] \n",
      " [-30.0, -16.0, -64.0, -20.0, -15.0, -34.0, -18.0, -19.0, -5.0, -8.0]\n",
      "Agent 4 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.34, 0.34, 0.31], [0.33, 0.34, 0.33], [0.34, 0.33, 0.33], [0.35, 0.33, 0.31], [0.34, 0.35, 0.31], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.33, 0.33]]\n",
      "Agent 4 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 5 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -3.0, -1.0] \n",
      " [-5.0, -3.0, -0.0, -1.0, -2.0, -1.0, -1.0, -2.0, -3.0, -2.0]\n",
      "Agent 5 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 5 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 6 : Predicted Values and True Return : \n",
      " [-36.0, -3.0, -6.0, -6.0, -19.0, -3.0, -7.0, -13.0, -3.0, -3.0] \n",
      " [-21.0, -8.0, -5.0, -11.0, -12.0, -7.0, -18.0, -13.0, -13.0, -10.0]\n",
      "Agent 6 : Proba distribution on those states : \n",
      " [[0.49, 0.51], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.48, 0.52], [0.5, 0.5], [0.5, 0.5], [0.49, 0.51], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 6 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 7 : Predicted Values and True Return : \n",
      " [-43.0, -8.0, -4.0, -7.0, -12.0, -7.0, -32.0, -43.0, -7.0, -24.0] \n",
      " [-86.0, -66.0, -30.0, -19.0, -59.0, -21.0, -27.0, -65.0, -34.0, -43.0]\n",
      "Agent 7 : Proba distribution on those states : \n",
      " [[0.3, 0.34, 0.36], [0.34, 0.34, 0.33], [0.33, 0.34, 0.34], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.33, 0.34], [0.32, 0.33, 0.35], [0.3, 0.34, 0.36], [0.33, 0.33, 0.34], [0.33, 0.34, 0.34]]\n",
      "Agent 7 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.34]\n",
      "Agent 8 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -3.0, -4.0, -7.0, -3.0, -4.0, -4.0, -3.0, -3.0] \n",
      " [-4.0, -4.0, -3.0, -6.0, -13.0, -6.0, -10.0, -3.0, -11.0, -2.0]\n",
      "Agent 8 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 8 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 9 : Predicted Values and True Return : \n",
      " [-4.0, -3.0, -3.0, -3.0, -4.0, -6.0, -7.0, -3.0, -3.0, -7.0] \n",
      " [-3.0, -5.0, -7.0, -5.0, -11.0, -7.0, -2.0, -3.0, -4.0, -8.0]\n",
      "Agent 9 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 9 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 10 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -3.0, -9.0, -5.0, -18.0, -6.0, -6.0, -3.0, -6.0] \n",
      " [-5.0, -2.0, -3.0, -4.0, -4.0, -10.0, -5.0, -8.0, -4.0, -3.0]\n",
      "Agent 10 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 10 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 11 : Predicted Values and True Return : \n",
      " [-2.0, -2.0, -5.0, -4.0, -3.0, -7.0, -5.0, -2.0, -3.0, -4.0] \n",
      " [-2.0, -5.0, -4.0, -7.0, -11.0, -11.0, -6.0, -3.0, -3.0, -5.0]\n",
      "Agent 11 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 11 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 12 : Predicted Values and True Return : \n",
      " [-15.0, -19.0, -24.0, -15.0, -15.0, -22.0, -71.0, -12.0, -25.0, -59.0] \n",
      " [-16.0, -50.0, -49.0, -16.0, -17.0, -31.0, -61.0, -26.0, -56.0, -39.0]\n",
      "Agent 12 : Proba distribution on those states : \n",
      " [[0.25, 0.25, 0.25, 0.25], [0.24, 0.25, 0.25, 0.25], [0.25, 0.25, 0.26, 0.24], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.27, 0.24, 0.24, 0.25], [0.25, 0.25, 0.24, 0.26], [0.25, 0.25, 0.25, 0.25], [0.24, 0.26, 0.25, 0.24], [0.25, 0.26, 0.25, 0.24]]\n",
      "Agent 12 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Agent 13 : Predicted Values and True Return : \n",
      " [-70.0, -121.0, -102.0, -52.0, -113.0, -90.0, -113.0, -4.0, -137.0, -113.0] \n",
      " [-105.0, -123.0, -99.0, -111.0, -104.0, -99.0, -135.0, -12.0, -126.0, -104.0]\n",
      "Agent 13 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.34], [0.32, 0.32, 0.36], [0.33, 0.32, 0.35], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.32, 0.35], [0.34, 0.32, 0.34], [0.33, 0.33, 0.33], [0.33, 0.31, 0.35], [0.35, 0.34, 0.31]]\n",
      "Agent 13 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 8 : Entropy reduced to 500.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_12  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 12 : Entropy reduced to 500.0 \n",
      "Model: \"model2_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_13  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 13 : Entropy reduced to 500.0 \n",
      "patate\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-56.0, -56.0, -67.0, -22.0, -70.0, -56.0, -22.0, -57.0, -30.0, -34.0] \n",
      " [-93.0, -81.0, -30.0, -5.0, -78.0, -81.0, -61.0, -63.0, -19.0, -21.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.33, 0.35, 0.32], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.33, 0.33], [0.33, 0.33, 0.34], [0.34, 0.34, 0.32], [0.33, 0.33, 0.33], [0.33, 0.35, 0.32], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 1 : Predicted Values and True Return : \n",
      " [-89.0, -134.0, -17.0, -151.0, -32.0, -25.0, -27.0, -79.0, -55.0, -17.0] \n",
      " [-52.0, -95.0, -7.0, -68.0, -100.0, -71.0, -87.0, -71.0, -99.0, -9.0]\n",
      "Agent 1 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.34], [0.34, 0.33, 0.33], [0.33, 0.33, 0.33], [0.34, 0.33, 0.34], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33]]\n",
      "Agent 1 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 2 : Predicted Values and True Return : \n",
      " [-151.0, -189.0, -186.0, -115.0, -189.0, -159.0, -179.0, -104.0, -262.0, -61.0] \n",
      " [-155.0, -149.0, -106.0, -91.0, -104.0, -97.0, -120.0, -96.0, -160.0, -175.0]\n",
      "Agent 2 : Proba distribution on those states : \n",
      " [[0.26, 0.25, 0.25, 0.25], [0.25, 0.26, 0.25, 0.24], [0.25, 0.25, 0.26, 0.24], [0.25, 0.25, 0.25, 0.25], [0.24, 0.25, 0.26, 0.25], [0.26, 0.25, 0.25, 0.24], [0.25, 0.26, 0.25, 0.24], [0.25, 0.24, 0.26, 0.25], [0.24, 0.27, 0.26, 0.23], [0.23, 0.26, 0.25, 0.26]]\n",
      "Agent 2 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Agent 3 : Predicted Values and True Return : \n",
      " [-23.0, -22.0, -29.0, -22.0, -22.0, -22.0, -23.0, -49.0, -29.0, -33.0] \n",
      " [-24.0, -32.0, -14.0, -38.0, -38.0, -18.0, -32.0, -11.0, -28.0, -13.0]\n",
      "Agent 3 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33]]\n",
      "Agent 3 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 4 : Predicted Values and True Return : \n",
      " [-100.0, -12.0, -62.0, -18.0, -43.0, -42.0, -18.0, -22.0, -100.0, -71.0] \n",
      " [-11.0, -31.0, -13.0, -19.0, -36.0, -16.0, -19.0, -30.0, -11.0, -34.0]\n",
      "Agent 4 : Proba distribution on those states : \n",
      " [[0.34, 0.35, 0.31], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.34, 0.35, 0.31], [0.34, 0.35, 0.32]]\n",
      "Agent 4 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 5 : Predicted Values and True Return : \n",
      " [-6.0, -9.0, -17.0, -6.0, -6.0, -6.0, -6.0, -7.0, -7.0, -6.0] \n",
      " [-1.0, -2.0, -2.0, -3.0, -2.0, -3.0, -3.0, -2.0, -2.0, -1.0]\n",
      "Agent 5 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.51, 0.49], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 5 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 6 : Predicted Values and True Return : \n",
      " [-16.0, -19.0, -14.0, -24.0, -14.0, -20.0, -40.0, -26.0, -14.0, -14.0] \n",
      " [-18.0, -33.0, -14.0, -9.0, -9.0, -18.0, -26.0, -36.0, -5.0, -6.0]\n",
      "Agent 6 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.49, 0.51], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 6 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 7 : Predicted Values and True Return : \n",
      " [-55.0, -27.0, -21.0, -16.0, -20.0, -16.0, -27.0, -16.0, -16.0, -49.0] \n",
      " [-37.0, -78.0, -33.0, -27.0, -65.0, -42.0, -20.0, -18.0, -50.0, -21.0]\n",
      "Agent 7 : Proba distribution on those states : \n",
      " [[0.34, 0.34, 0.32], [0.33, 0.34, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.34, 0.33, 0.33]]\n",
      "Agent 7 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 8 : Predicted Values and True Return : \n",
      " [-4.0, -4.0, -4.0, -4.0, -5.0, -4.0, -4.0, -4.0, -5.0, -7.0] \n",
      " [-9.0, -7.0, -6.0, -3.0, -3.0, -8.0, -3.0, -3.0, -9.0, -4.0]\n",
      "Agent 8 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49]]\n",
      "Agent 8 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 9 : Predicted Values and True Return : \n",
      " [-7.0, -4.0, -5.0, -4.0, -4.0, -4.0, -4.0, -4.0, -4.0, -4.0] \n",
      " [-8.0, -11.0, -6.0, -11.0, -3.0, -7.0, -5.0, -4.0, -4.0, -5.0]\n",
      "Agent 9 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 9 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 10 : Predicted Values and True Return : \n",
      " [-7.0, -7.0, -7.0, -8.0, -6.0, -9.0, -10.0, -7.0, -8.0, -7.0] \n",
      " [-8.0, -4.0, -1.0, -4.0, -8.0, -6.0, -2.0, -5.0, -10.0, -7.0]\n",
      "Agent 10 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 10 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 11 : Predicted Values and True Return : \n",
      " [-5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0] \n",
      " [-6.0, -6.0, -10.0, -8.0, -5.0, -10.0, -6.0, -3.0, -2.0, -3.0]\n",
      "Agent 11 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 11 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 12 : Predicted Values and True Return : \n",
      " [-47.0, -25.0, -61.0, -16.0, -37.0, -16.0, -54.0, -49.0, -31.0, -16.0] \n",
      " [-44.0, -19.0, -18.0, -8.0, -39.0, -7.0, -16.0, -19.0, -12.0, -6.0]\n",
      "Agent 12 : Proba distribution on those states : \n",
      " [[0.26, 0.24, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.26, 0.25, 0.24, 0.25], [0.26, 0.25, 0.24, 0.25], [0.26, 0.25, 0.25, 0.25], [0.26, 0.25, 0.24, 0.25], [0.25, 0.24, 0.26, 0.24], [0.26, 0.25, 0.24, 0.25], [0.24, 0.26, 0.25, 0.24], [0.26, 0.25, 0.24, 0.25]]\n",
      "Agent 12 : Proba distribution on the 0 state : \n",
      " [0.26 0.25 0.24 0.25]\n",
      "Agent 13 : Predicted Values and True Return : \n",
      " [-13.0, -86.0, -43.0, -83.0, -26.0, -105.0, -63.0, -13.0, -98.0, -31.0] \n",
      " [-9.0, -105.0, -83.0, -103.0, -66.0, -98.0, -99.0, -54.0, -103.0, -49.0]\n",
      "Agent 13 : Proba distribution on those states : \n",
      " [[0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.35, 0.33, 0.33], [0.35, 0.33, 0.33], [0.34, 0.33, 0.33], [0.35, 0.32, 0.34], [0.35, 0.31, 0.34], [0.33, 0.34, 0.33], [0.35, 0.32, 0.33], [0.34, 0.33, 0.33]]\n",
      "Agent 13 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.33]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ef3781782ae8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_step_size\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Actor_Critic_Class.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[0macts_and_advs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                 \u001b[1;31m# performs a full training step on the collected batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m                 \u001b[1;31m# note: no need to mess around with gradients, Keras API handles it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0macts_and_advs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3510\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3512\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 572\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 445\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    446\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For some unknown reason  nothing is printing in this cell...\n",
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        \n",
    "        \n",
    "        # Only for AC\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single_Cross_Straight\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot\n"
     ]
    }
   ],
   "source": [
    "print('tot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.08910298347473145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x190b93b9160>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  384       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  252       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,267\n",
      "Trainable params: 11,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 0.05\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Straight_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-60.0, -72.0, -72.0, -72.0, -72.0, -69.0, -72.0, -72.0, -30.0, -40.0] \n",
      " [-439.0, -527.0, -527.0, -527.0, -527.0, -507.0, -527.0, -526.0, -255.0, -324.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.02, 0.98], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -5.0, -5.0, -3.0, -5.0, -4.0, -3.0, -13.0, -10.0, -9.0] \n",
      " [-217.0, -527.0, -417.0, -527.0, -72.0, -527.0, -527.0, -243.0, -451.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.37, 0.63], [0.71, 0.29], [0.67, 0.33], [0.36, 0.64], [0.71, 0.29], [0.27, 0.73], [0.37, 0.63], [0.78, 0.22], [0.07, 0.93], [0.15, 0.85]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -6.0, -11.0, -6.0, -9.0, -14.0, -6.0, -6.0, -13.0, -10.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -525.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.76, 0.24], [0.19, 0.81], [0.05, 0.95], [0.76, 0.24], [0.81, 0.19], [0.84, 0.16], [0.76, 0.24], [0.19, 0.81], [0.02, 0.98], [0.04, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.4 0.6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-45ab2e412337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_to_next_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep_to_next_action\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                         \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSarsd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    104\u001b[0m \t\t\"\"\"\n\u001b[0;32m    105\u001b[0m                 \u001b[1;31m#global timesteps_per_second\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m                 \u001b[1;31m# increase the update counter by one each step (until reach simulation length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    514\u001b[0m                         \u001b[0mdebug_attr_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Getting property Id 0x%x from OLE object\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mretEntry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m                                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretEntry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minvoke_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m                         \u001b[1;32mexcept\u001b[0m \u001b[0mpythoncom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom_error\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mERRORS_BAD_CONTEXT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for _ in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn()\n",
    "                \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Agents[0].save_agent(vissim_working_directory, model_name, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  384       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  252       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,267\n",
      "Trainable params: 11,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Agents[0].load_agent(vissim_working_directory, model_name, 'TEST', best = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 2000\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{ 'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 10,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 2000 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.12496423721313477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1f26b747860>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "# not bad with the first model. need a fonction to decrease entropy now \n",
    "\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 100\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary4['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 500.0 \n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-87.0, -152.0, -163.0, -103.0, -62.0, -157.0, -120.0, -183.0, -309.0, -60.0] \n",
      " [-103.0, -152.0, -130.0, -100.0, -145.0, -264.0, -139.0, -286.0, -226.0, -159.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.27, 0.25, 0.26, 0.22], [0.19, 0.29, 0.29, 0.23], [0.22, 0.27, 0.23, 0.28], [0.29, 0.24, 0.25, 0.22], [0.24, 0.29, 0.22, 0.24], [0.24, 0.29, 0.28, 0.2], [0.16, 0.31, 0.25, 0.28], [0.29, 0.27, 0.22, 0.22], [0.27, 0.25, 0.25, 0.22], [0.24, 0.24, 0.24, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 50.0 \n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-447.0, -678.0, -264.0, -324.0, -299.0, -567.0, -126.0, -472.0, -615.0, -660.0] \n",
      " [-150.0, -129.0, -296.0, -151.0, -248.0, -139.0, -159.0, -104.0, -116.0, -120.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.59, 0.0, 0.0, 0.4], [0.45, 0.0, 0.01, 0.54], [0.6, 0.0, 0.03, 0.37], [0.67, 0.0, 0.02, 0.3], [0.48, 0.0, 0.04, 0.48], [0.41, 0.0, 0.01, 0.58], [0.36, 0.19, 0.13, 0.32], [0.39, 0.0, 0.06, 0.55], [0.1, 0.0, 0.0, 0.9], [0.26, 0.0, 0.0, 0.74]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.17 0.28 0.29 0.27]\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 5.0 \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 0.5 \n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2102.0, -2327.0, -1796.0, -1867.0, -1442.0, -2310.0, -1867.0, -187.0, -1211.0, -2339.0] \n",
      " [-138.0, -147.0, -117.0, -129.0, -104.0, -150.0, -129.0, -294.0, -109.0, -143.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.   0.28 0.03 0.68]\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 0.05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1732.0, -2332.0, -2332.0, -2277.0, -2305.0, -371.0, -2332.0, -2332.0, -1959.0, -2332.0] \n",
      " [-104.0, -221.0, -202.0, -128.0, -138.0, -207.0, -155.0, -166.0, -116.0, -150.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1370.0, -2332.0, -2029.0, -1640.0, -1370.0, -1888.0, -2277.0, -1821.0, -2332.0, -723.0] \n",
      " [-152.0, -160.0, -118.0, -95.0, -152.0, -97.0, -134.0, -88.0, -147.0, -286.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1729.0, -659.0, -1742.0, -2328.0, -1902.0, -2328.0, -2132.0, -1720.0, -1250.0, -1123.0] \n",
      " [-104.0, -264.0, -101.0, -158.0, -99.0, -143.0, -135.0, -104.0, -188.0, -248.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1820.0, -1747.0, -2298.0, -1831.0, -2380.0, -1557.0, -380.0, -1831.0, -2298.0, -2380.0] \n",
      " [-95.0, -103.0, -140.0, -86.0, -166.0, -148.0, -174.0, -86.0, -140.0, -133.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1629.0, -829.0, -2379.0, -986.0, -2113.0, -2126.0, -2286.0, -1057.0, -1355.0, -2379.0] \n",
      " [-101.0, -294.0, -180.0, -301.0, -126.0, -127.0, -147.0, -302.0, -170.0, -142.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2203.0, -2360.0, -1893.0, -1235.0, -2341.0, -141.0, -685.0, -2174.0, -979.0, -1097.0] \n",
      " [-139.0, -156.0, -87.0, -211.0, -133.0, -114.0, -278.0, -130.0, -301.0, -290.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1237.0, -2376.0, -1398.0, -2267.0, -557.0, -518.0, -1703.0, -1401.0, -1949.0, -1833.0] \n",
      " [-211.0, -161.0, -152.0, -152.0, -241.0, -226.0, -91.0, -155.0, -100.0, -95.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2376.0, -1398.0, -2376.0, -2376.0, -1398.0, -2376.0, -1145.0, -2376.0, -2376.0, -2169.0] \n",
      " [-189.0, -152.0, -142.0, -137.0, -152.0, -147.0, -248.0, -137.0, -154.0, -129.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2333.0, -2377.0, -2377.0, -2309.0, -2377.0, -2261.0, -1476.0, -1576.0, -2377.0, -680.0] \n",
      " [-129.0, -139.0, -154.0, -139.0, -161.0, -152.0, -147.0, -141.0, -189.0, -278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2358.0, -868.0, -517.0, -1547.0, -2376.0, -1960.0, -2376.0, -2376.0, -664.0, -2324.0] \n",
      " [-149.0, -290.0, -226.0, -148.0, -132.0, -99.0, -150.0, -161.0, -264.0, -128.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-999.0, -2000.0, -2232.0, -441.0, -225.0, -1023.0, -2191.0, -288.0, -2107.0, -1546.0] \n",
      " [-306.0, -97.0, -146.0, -189.0, -114.0, -308.0, -135.0, -132.0, -118.0, -148.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-225.0, -999.0, -1959.0, -493.0, -2374.0, -2108.0, -1528.0, -2040.0, -1475.0, -2374.0] \n",
      " [-97.0, -306.0, -100.0, -207.0, -142.0, -118.0, -150.0, -116.0, -147.0, -144.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1051.0, -2374.0, -2091.0, -1935.0, -1683.0, -720.0, -720.0, -1395.0, -2012.0, -336.0] \n",
      " [-302.0, -158.0, -120.0, -99.0, -90.0, -286.0, -286.0, -152.0, -101.0, -145.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2374.0, -2210.0, -1701.0, -2137.0, -2374.0, -1780.0, -2041.0, -2374.0, -2374.0, -817.0] \n",
      " [-133.0, -139.0, -91.0, -127.0, -233.0, -104.0, -116.0, -221.0, -154.0, -294.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-903.0, -1730.0, -2374.0, -2339.0, -1145.0, -1402.0, -2374.0, -1755.0, -1755.0, -1588.0] \n",
      " [-290.0, -101.0, -147.0, -133.0, -248.0, -155.0, -150.0, -103.0, -103.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1613.0, -815.0, -2357.0, -1655.0, -1713.0, -865.0, -225.0, -1613.0, -1164.0, -2288.0] \n",
      " [-123.0, -294.0, -156.0, -95.0, -94.0, -290.0, -114.0, -123.0, -226.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1052.0, -2262.0, -1475.0, -2211.0, -1633.0, -1545.0, -1431.0, -680.0, -1961.0, -1402.0] \n",
      " [-302.0, -152.0, -147.0, -139.0, -101.0, -148.0, -151.0, -278.0, -100.0, -155.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-287.0, -2374.0, -287.0, -953.0, -2374.0, -2263.0, -2374.0, -225.0, -2374.0, -1165.0] \n",
      " [-132.0, -156.0, -132.0, -296.0, -144.0, -152.0, -147.0, -114.0, -155.0, -226.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1908.0, -1908.0, -2374.0, -680.0, -735.0, -1587.0, -2374.0, -2374.0, -445.0, -2374.0] \n",
      " [-88.0, -88.0, -150.0, -278.0, -292.0, -130.0, -215.0, -154.0, -189.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-563.0, -1911.0, -1842.0, -1313.0, -997.0, -1164.0, -621.0, -2373.0, -2094.0, -2356.0] \n",
      " [-241.0, -87.0, -95.0, -188.0, -306.0, -226.0, -255.0, -154.0, -120.0, -149.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2262.0, -1587.0, -1090.0, -1090.0, -2373.0, -1527.0, -2373.0, -2373.0, -2035.0, -1360.0] \n",
      " [-152.0, -130.0, -290.0, -290.0, -143.0, -150.0, -150.0, -132.0, -109.0, -170.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1391.0, -681.0, -1621.0, -2374.0, -2374.0, -225.0, -2374.0, -620.0, -2374.0, -721.0] \n",
      " [-152.0, -278.0, -109.0, -161.0, -202.0, -114.0, -160.0, -255.0, -133.0, -286.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1544.0, -2138.0, -2373.0, -2373.0, -1313.0, -1166.0, -2373.0, -2115.0, -1853.0, -2233.0] \n",
      " [-148.0, -127.0, -133.0, -150.0, -188.0, -226.0, -138.0, -122.0, -86.0, -146.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2374.0, -1853.0, -1587.0, -2094.0, -2139.0, -2374.0, -1053.0, -1919.0, -620.0, -2374.0] \n",
      " [-208.0, -86.0, -130.0, -120.0, -127.0, -202.0, -302.0, -93.0, -255.0, -180.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1794.0, -2183.0, -2147.0, -1911.0, -387.0, -2373.0, -1909.0, -1963.0, -2373.0, -2178.0] \n",
      " [-104.0, -130.0, -131.0, -87.0, -174.0, -155.0, -88.0, -100.0, -150.0, -129.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1730.0, -2373.0, -1526.0, -2373.0, -1429.0, -1114.0, -1544.0, -1782.0, -2373.0, -2234.0] \n",
      " [-101.0, -154.0, -150.0, -154.0, -151.0, -268.0, -148.0, -104.0, -161.0, -146.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2372.0, -1279.0, -2372.0, -1313.0, -2372.0, -2372.0, -2372.0, -2372.0, -2372.0] \n",
      " [-156.0, -215.0, -202.0, -154.0, -188.0, -180.0, -142.0, -144.0, -142.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2350.0, -1847.0, -1025.0, -1314.0, -2373.0, -446.0, -1886.0, -2373.0, -681.0, -1799.0] \n",
      " [-138.0, -95.0, -308.0, -188.0, -161.0, -189.0, -88.0, -147.0, -278.0, -104.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1753.0, -1492.0, -1391.0, -2372.0, -2193.0, -1964.0, -2324.0, -1024.0, -1819.0, -225.0] \n",
      " [-103.0, -146.0, -152.0, -133.0, -135.0, -100.0, -128.0, -308.0, -99.0, -114.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -1149.0, -2372.0, -681.0, -2372.0, -2289.0, -1929.0, -2146.0, -2304.0, -2259.0] \n",
      " [-138.0, -248.0, -202.0, -278.0, -147.0, -147.0, -97.0, -131.0, -140.0, -152.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1167.0, -2096.0, -1755.0, -1706.0, -2372.0, -2372.0, -2140.0, -2372.0, -1715.0, -2096.0] \n",
      " [-226.0, -120.0, -103.0, -91.0, -143.0, -166.0, -127.0, -139.0, -94.0, -120.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1842.0, -2372.0, -2148.0, -2372.0, -1621.0, -2213.0, -1525.0, -2372.0, -1621.0, -2263.0] \n",
      " [-95.0, -150.0, -131.0, -147.0, -109.0, -139.0, -150.0, -161.0, -109.0, -152.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1846.0, -1914.0, -1943.0, -2373.0, -1472.0, -2373.0, -2011.0, -2119.0, -1149.0, -1823.0] \n",
      " [-95.0, -88.0, -99.0, -208.0, -147.0, -147.0, -97.0, -122.0, -248.0, -99.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2324.0, -1427.0, -1210.0, -899.0, -2372.0, -2306.0, -1685.0, -2372.0, -1706.0, -1114.0] \n",
      " [-128.0, -151.0, -212.0, -290.0, -137.0, -140.0, -90.0, -160.0, -91.0, -268.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-565.0, -1360.0, -1821.0, -735.0, -1885.0, -225.0, -2372.0, -814.0, -2372.0, -1968.0] \n",
      " [-241.0, -170.0, -99.0, -292.0, -88.0, -97.0, -221.0, -294.0, -142.0, -100.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2179.0, -2372.0, -520.0, -859.0, -2148.0, -2372.0, -2372.0, -2372.0, -1612.0, -2372.0] \n",
      " [-129.0, -147.0, -226.0, -290.0, -131.0, -189.0, -139.0, -147.0, -123.0, -138.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1815.0, -2184.0, -2371.0, -288.0, -1359.0, -1706.0, -2371.0, -2371.0, -1815.0, -2039.0] \n",
      " [-101.0, -130.0, -150.0, -132.0, -170.0, -91.0, -142.0, -143.0, -101.0, -109.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1586.0, -1519.0, -2037.0, -2372.0, -1969.0, -1882.0, -2214.0, -1925.0, -2310.0, -2372.0] \n",
      " [-130.0, -145.0, -109.0, -132.0, -99.0, -88.0, -139.0, -97.0, -139.0, -154.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2372.0, -681.0, -2292.0, -2372.0, -2236.0, -2308.0, -2372.0, -1798.0, -2117.0] \n",
      " [-154.0, -154.0, -278.0, -147.0, -180.0, -146.0, -140.0, -144.0, -104.0, -118.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -1312.0, -225.0, -1280.0, -2310.0, -1925.0, -2372.0, -2042.0, -1986.0, -2036.0] \n",
      " [-154.0, -188.0, -97.0, -202.0, -139.0, -97.0, -154.0, -116.0, -99.0, -109.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2350.0, -1754.0, -2372.0, -332.0, -2129.0, -2372.0, -388.0, -520.0, -2129.0, -2067.0] \n",
      " [-138.0, -103.0, -166.0, -145.0, -126.0, -215.0, -174.0, -226.0, -126.0, -117.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2162.0, -2141.0, -663.0, -490.0, -1810.0, -2009.0, -2356.0, -2372.0, -1620.0] \n",
      " [-147.0, -131.0, -127.0, -264.0, -207.0, -101.0, -97.0, -149.0, -180.0, -109.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-288.0, -2371.0, -2043.0, -2371.0, -2371.0, -1988.0, -1395.0, -2371.0, -2371.0, -2127.0] \n",
      " [-132.0, -138.0, -116.0, -143.0, -154.0, -99.0, -155.0, -147.0, -208.0, -126.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2335.0, -2018.0, -1940.0, -2184.0, -1966.0, -1716.0, -1840.0, -1925.0, -1053.0, -2372.0] \n",
      " [-129.0, -101.0, -99.0, -130.0, -100.0, -94.0, -95.0, -97.0, -302.0, -144.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1312.0, -2117.0, -2235.0, -225.0, -1916.0, -2372.0, -1246.0, -898.0, -1390.0, -2335.0] \n",
      " [-188.0, -118.0, -146.0, -114.0, -87.0, -215.0, -211.0, -290.0, -152.0, -129.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-721.0, -1396.0, -2372.0, -367.0, -2372.0, -1612.0, -1707.0, -617.0, -997.0, -1467.0] \n",
      " [-286.0, -155.0, -221.0, -159.0, -150.0, -123.0, -91.0, -255.0, -306.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2018.0, -1246.0, -1568.0, -2371.0, -1517.0, -2184.0, -859.0, -1689.0, -2371.0, -859.0] \n",
      " [-101.0, -211.0, -141.0, -158.0, -145.0, -130.0, -290.0, -90.0, -146.0, -290.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1359.0, -2037.0, -1489.0, -289.0, -2064.0, -2372.0, -2335.0, -2184.0, -2372.0, -947.0] \n",
      " [-170.0, -109.0, -146.0, -132.0, -117.0, -141.0, -129.0, -130.0, -142.0, -296.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1585.0, -1914.0, -2372.0, -2351.0, -446.0, -2043.0, -2372.0, -2372.0, -2356.0, -1612.0] \n",
      " [-130.0, -87.0, -142.0, -138.0, -189.0, -116.0, -208.0, -221.0, -149.0, -123.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2180.0, -1053.0, -2372.0, -2356.0, -2372.0, -1925.0, -1359.0, -1708.0, -2372.0, -2372.0] \n",
      " [-129.0, -302.0, -142.0, -149.0, -154.0, -97.0, -170.0, -91.0, -147.0, -189.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1986.0, -1390.0, -1150.0, -2274.0, -2371.0, -2371.0, -1358.0, -681.0, -2263.0, -390.0] \n",
      " [-99.0, -152.0, -248.0, -152.0, -141.0, -208.0, -170.0, -278.0, -152.0, -174.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -1910.0, -2064.0, -2372.0, -1851.0, -814.0, -2372.0, -2372.0, -1920.0, -2228.0] \n",
      " [-154.0, -88.0, -117.0, -132.0, -86.0, -294.0, -166.0, -146.0, -93.0, -143.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2371.0, -2371.0, -1281.0, -738.0, -488.0, -2341.0, -2371.0, -2371.0, -2292.0, -2275.0] \n",
      " [-143.0, -166.0, -202.0, -292.0, -207.0, -133.0, -154.0, -160.0, -147.0, -152.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2163.0, -662.0, -1540.0, -2372.0, -2035.0, -681.0, -2372.0, -2372.0, -2184.0] \n",
      " [-202.0, -131.0, -264.0, -148.0, -142.0, -109.0, -278.0, -202.0, -199.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2063.0, -1911.0, -2356.0, -2372.0, -1395.0, -2275.0, -2372.0, -2327.0, -2327.0, -1584.0] \n",
      " [-117.0, -87.0, -149.0, -132.0, -155.0, -152.0, -150.0, -128.0, -128.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2356.0, -2227.0, -1170.0, -330.0, -1986.0, -2126.0, -2356.0, -997.0, -2371.0, -2311.0] \n",
      " [-156.0, -143.0, -226.0, -145.0, -99.0, -126.0, -156.0, -306.0, -142.0, -139.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2228.0, -2372.0, -1151.0, -1151.0, -1358.0, -2372.0, -2180.0, -1170.0, -681.0] \n",
      " [-199.0, -143.0, -208.0, -248.0, -248.0, -170.0, -154.0, -129.0, -226.0, -278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2035.0, -2308.0, -2327.0, -662.0, -615.0, -1967.0, -2372.0, -2372.0, -2215.0, -1909.0] \n",
      " [-109.0, -140.0, -128.0, -264.0, -255.0, -99.0, -208.0, -143.0, -139.0, -88.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2356.0, -2372.0, -2372.0, -522.0, -1987.0, -2372.0, -2308.0, -289.0, -1691.0, -2116.0] \n",
      " [-149.0, -150.0, -132.0, -226.0, -99.0, -141.0, -140.0, -132.0, -90.0, -118.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2041.0, -1720.0, -330.0, -2372.0, -2372.0, -1488.0, -945.0, -2328.0, -2372.0] \n",
      " [-158.0, -116.0, -94.0, -145.0, -166.0, -150.0, -146.0, -296.0, -134.0, -143.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-572.0, -1214.0, -2372.0, -2356.0, -2372.0, -2356.0, -615.0, -2372.0, -2036.0, -2264.0] \n",
      " [-241.0, -212.0, -139.0, -156.0, -142.0, -156.0, -255.0, -215.0, -109.0, -152.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2357.0, -447.0, -2181.0, -486.0, -2264.0, -1835.0, -447.0, -2264.0, -2372.0, -1488.0] \n",
      " [-149.0, -189.0, -129.0, -207.0, -152.0, -95.0, -189.0, -152.0, -142.0, -146.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2371.0, -2371.0, -1965.0, -2119.0, -962.0, -573.0, -903.0, -2371.0, -1539.0, -2342.0] \n",
      " [-158.0, -199.0, -100.0, -122.0, -301.0, -241.0, -290.0, -142.0, -148.0, -133.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-663.0, -522.0, -2371.0, -1772.0, -741.0, -2371.0, -2035.0, -2371.0, -1610.0, -2327.0] \n",
      " [-264.0, -226.0, -143.0, -104.0, -292.0, -147.0, -109.0, -147.0, -123.0, -128.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2309.0, -1752.0, -2371.0, -2371.0, -1805.0, -2229.0, -2236.0, -2295.0, -2193.0, -2371.0] \n",
      " [-140.0, -103.0, -180.0, -154.0, -101.0, -143.0, -146.0, -147.0, -135.0, -156.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2193.0, -2352.0, -1664.0, -1910.0, -1988.0, -2371.0, -2371.0, -1634.0, -1583.0, -2185.0] \n",
      " [-135.0, -138.0, -95.0, -88.0, -99.0, -221.0, -199.0, -101.0, -130.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1312.0, -1312.0, -1732.0, -288.0, -1539.0, -1719.0, -683.0, -2193.0, -1053.0, -1610.0] \n",
      " [-188.0, -188.0, -101.0, -132.0, -148.0, -94.0, -278.0, -135.0, -302.0, -123.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2322.0, -2260.0, -2279.0, -2322.0, -1750.0, -2322.0, -709.0, -1585.0, -885.0, -1977.0] \n",
      " [-143.0, -140.0, -134.0, -138.0, -104.0, -208.0, -286.0, -109.0, -290.0, -101.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2307.0, -2136.0, -2322.0, -2322.0, -1257.0, -977.0, -2247.0, -1772.0, -284.0, -1873.0] \n",
      " [-156.0, -129.0, -202.0, -147.0, -202.0, -306.0, -147.0, -99.0, -132.0, -87.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2323.0, -2323.0, -2323.0, -360.0, -1885.0, -2230.0, -385.0, -2323.0, -1674.0, -2323.0] \n",
      " [-137.0, -143.0, -147.0, -159.0, -97.0, -152.0, -174.0, -208.0, -91.0, -142.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2322.0, -943.0, -2322.0, -1994.0, -2322.0, -2189.0, -1771.0, -1881.0, -2322.0, -1068.0] \n",
      " [-180.0, -301.0, -154.0, -109.0, -158.0, -146.0, -99.0, -93.0, -166.0, -290.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2075.0, -2323.0, -2323.0, -2323.0, -2122.0, -513.0, -977.0, -2323.0, -2107.0, -669.0] \n",
      " [-118.0, -142.0, -154.0, -189.0, -131.0, -226.0, -306.0, -166.0, -131.0, -278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2077.0, -2077.0, -1874.0, -2077.0, -2322.0, -1695.0, -2322.0, -2184.0, -1286.0, -1695.0] \n",
      " [-122.0, -122.0, -87.0, -122.0, -146.0, -101.0, -154.0, -143.0, -188.0, -101.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-924.0, -2023.0, -1979.0, -223.0, -1509.0, -359.0, -2322.0, -2322.0, -2149.0, -2265.0] \n",
      " [-296.0, -117.0, -101.0, -97.0, -148.0, -159.0, -199.0, -144.0, -135.0, -139.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2321.0, -2321.0, -1995.0, -2321.0, -2321.0, -2231.0, -1842.0, -2141.0, -2321.0, -1366.0] \n",
      " [-154.0, -156.0, -109.0, -143.0, -161.0, -152.0, -88.0, -130.0, -160.0, -155.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1901.0, -2281.0, -1147.0, -886.0, -2190.0, -1979.0, -2322.0, -359.0, -1491.0, -2322.0] \n",
      " [-99.0, -128.0, -226.0, -290.0, -146.0, -101.0, -143.0, -159.0, -150.0, -215.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1223.0, -2321.0, -1771.0, -1995.0, -2321.0, -2321.0, -1128.0, -2123.0, -1949.0, -2321.0] \n",
      " [-211.0, -189.0, -99.0, -109.0, -147.0, -154.0, -248.0, -131.0, -99.0, -143.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2321.0, -708.0, -836.0, -2321.0, -2290.0, -441.0, -1034.0, -2321.0, -2321.0, -2262.0] \n",
      " [-156.0, -286.0, -290.0, -215.0, -129.0, -189.0, -302.0, -143.0, -143.0, -140.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-836.0, -2321.0, -1995.0, -1949.0, -1949.0, -2191.0, -2321.0, -1330.0, -1970.0, -2321.0] \n",
      " [-290.0, -150.0, -109.0, -99.0, -99.0, -146.0, -150.0, -170.0, -97.0, -132.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2321.0, -1771.0, -2321.0, -1873.0, -1970.0, -2321.0, -2191.0, -1360.0, -1585.0, -2321.0] \n",
      " [-160.0, -99.0, -215.0, -88.0, -97.0, -133.0, -146.0, -152.0, -109.0, -166.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1771.0, -1771.0, -2321.0, -2308.0, -1435.0, -2321.0, -976.0, -1550.0, -1970.0, -2078.0] \n",
      " [-99.0, -99.0, -156.0, -156.0, -147.0, -155.0, -306.0, -130.0, -97.0, -122.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2138.0, -2308.0, -1147.0, -2321.0, -886.0, -1223.0, -2321.0, -1885.0, -284.0, -2321.0] \n",
      " [-129.0, -149.0, -226.0, -144.0, -290.0, -211.0, -233.0, -97.0, -132.0, -132.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2023.0, -283.0, -2321.0, -797.0, -2321.0, -1367.0, -2321.0, -283.0, -1577.0, -2250.0] \n",
      " [-117.0, -132.0, -143.0, -294.0, -142.0, -155.0, -161.0, -132.0, -123.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1674.0, -1881.0, -2321.0, -1550.0, -1751.0, -2232.0, -796.0, -1674.0, -2142.0, -669.0] \n",
      " [-91.0, -93.0, -208.0, -130.0, -104.0, -152.0, -294.0, -91.0, -130.0, -278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352562, 'Nombre de paramètres non valide.', None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cb712339892e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_to_next_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep_to_next_action\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                         \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSarsd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# Udapte all the SCUs nearly simutaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# not a nice way of doing this,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# Udapte all the SCUs nearly simutaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# not a nice way of doing this,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                                         \u001b[1;31m#Compute the state for the RL agent to find the next best agent action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mcalculate_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"QueuesSig\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_state\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlane\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim_Lanes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_state\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_action_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"QueuesSig\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_state\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlane\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim_Lanes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_state\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_action_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mget_queue\u001b[1;34m(lane)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mvehicles_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlane\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVehs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Collecte the attribute in queue of the vehicle of the lane and sum them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mqueue_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'InQueue'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicles_in_lane\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue_in_lane\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mvehicles_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlane\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVehs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Collecte the attribute in queue of the vehicle of the lane and sum them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mqueue_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'InQueue'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicles_in_lane\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue_in_lane\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mAttValue\u001b[1;34m(self, Attribut, arg1)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352562, 'Nombre de paramètres non valide.', None, None)"
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required :\n",
    "        \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Agents[0].model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                     4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                     5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                     6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                     7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [12],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 2000\n",
    "n_step_size = 16\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary8.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for i in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                Agents[idx].reduce_entropy()\n",
    "                print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
