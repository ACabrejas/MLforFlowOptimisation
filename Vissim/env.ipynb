{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_Critic_Class import ACAgent\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'env' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c20cfc66944e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m env = env(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n\u001b[1;32m----> 2\u001b[1;33m             timesteps_per_second = 1, mode = 'demo', delete_results = True, verbose = True)\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'env' object is not callable"
     ]
    }
   ],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1e38760bf28>,\n",
       " 1: <Vissim_SCU_class.Signal_Control_Unit at 0x1e38732c128>,\n",
       " 2: <Vissim_SCU_class.Signal_Control_Unit at 0x1e3876252b0>,\n",
       " 3: <Vissim_SCU_class.Signal_Control_Unit at 0x1e38762d2e8>,\n",
       " 4: <Vissim_SCU_class.Signal_Control_Unit at 0x1e387641ef0>,\n",
       " 5: <Vissim_SCU_class.Signal_Control_Unit at 0x1e38765a208>,\n",
       " 6: <Vissim_SCU_class.Signal_Control_Unit at 0x1e38765ad68>,\n",
       " 7: <Vissim_SCU_class.Signal_Control_Unit at 0x1e38766d668>,\n",
       " 8: <Vissim_SCU_class.Signal_Control_Unit at 0x1e387673320>,\n",
       " 9: <Vissim_SCU_class.Signal_Control_Unit at 0x1e3876863c8>,\n",
       " 10: <Vissim_SCU_class.Signal_Control_Unit at 0x1e387686d30>,\n",
       " 11: <Vissim_SCU_class.Signal_Control_Unit at 0x1e387691908>,\n",
       " 12: <Vissim_SCU_class.Signal_Control_Unit at 0x1e38769d4e0>,\n",
       " 13: <Vissim_SCU_class.Signal_Control_Unit at 0x1e38769df98>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0}\n"
     ]
    }
   ],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-18307391b7aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# Udapte all the SCUs nearly simutaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# not a nice way of doing this,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# Udapte all the SCUs nearly simutaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# not a nice way of doing this,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                                         \u001b[1;31m#Compute the state for the RL agent to find the next best agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mcalculate_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m \t\t\"\"\"\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Queues'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlane\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim_Lanes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    127\u001b[0m \t\t\"\"\"\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Queues'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlane\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim_Lanes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mget_queue\u001b[1;34m(lane)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0mvehicles_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlane\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVehs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;31m# Collecte the attribute in lane of the vehicle of the lane and sum them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mqueue_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'InQueue'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicles_in_lane\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue_in_lane\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0mvehicles_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlane\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVehs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;31m# Collecte the attribute in lane of the vehicle of the lane and sum them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mqueue_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'InQueue'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicles_in_lane\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue_in_lane\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mAttValue\u001b[1;34m(self, Attribut)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36m_ApplyTypes_\u001b[1;34m(self, dispid, wFlags, retType, argTypes, user, resultCLSID, *args)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_ApplyTypes_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwFlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margTypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultCLSID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvokeTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLCID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwFlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margTypes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_good_object_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultCLSID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  960       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  630       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_2 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,307\n",
      "Trainable params: 12,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_3 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_4 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_5 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_6 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,373\n",
      "Trainable params: 11,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_7 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,522\n",
      "Trainable params: 11,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_9 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_10  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_11  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_12  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_13  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "# not bad with the first model. need a fonction to decrease entropy now \n",
    "\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9ffeebd45546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_to_next_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep_to_next_action\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                         \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSarsd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# Udapte all the SCUs nearly simutaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# not a nice way of doing this,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# Udapte all the SCUs nearly simutaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# not a nice way of doing this,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                                         \u001b[1;31m#Compute the state for the RL agent to find the next best agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mcalculate_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m \t\t\"\"\"\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Queues'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlane\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim_Lanes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    127\u001b[0m \t\t\"\"\"\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Queues'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlane\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim_Lanes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mget_queue\u001b[1;34m(lane)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mvehicles_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlane\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVehs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;31m# Collecte the attribute in lane of the vehicle of the lane and sum them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mqueue_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'InQueue'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicles_in_lane\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue_in_lane\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mvehicles_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlane\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVehs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;31m# Collecte the attribute in lane of the vehicle of the lane and sum them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mqueue_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'InQueue'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicles_in_lane\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue_in_lane\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mAttValue\u001b[1;34m(self, Attribut)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36m_ApplyTypes_\u001b[1;34m(self, dispid, wFlags, retType, argTypes, user, resultCLSID, *args)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_ApplyTypes_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwFlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margTypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultCLSID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvokeTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLCID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwFlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margTypes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_good_object_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultCLSID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        print('patate')\n",
    "        \n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n"
     ]
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot\n"
     ]
    }
   ],
   "source": [
    "print('tot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.07547640800476074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1e6913145c0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  210       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  210       \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 549\n",
      "Trainable params: 549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 0.05\n",
    "n_step_size = 16\n",
    "state_size = [4]\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Straight_dictionary.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -12.0, -31.0, -31.0, -31.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -86.0, -527.0, -301.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.53 0.47]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -49.0, -23.0, -49.0, -43.0, -15.0, -49.0, -49.0, -49.0, -24.0] \n",
      " [-54.0, -527.0, -527.0, -439.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.14, 0.86], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.02, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -4.0, -8.0, -4.0, -11.0, -6.0, -2.0, -4.0, -1.0, -3.0] \n",
      " [-426.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -228.0, -27.0, -279.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.22, 0.78], [0.16, 0.84], [0.04, 0.96], [0.25, 0.75], [0.01, 0.99], [0.16, 0.84], [0.88, 0.12], [0.18, 0.82], [0.6, 0.4], [0.43, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -2.0, -1.0, -8.0, -5.0, -2.0, -2.0, -6.0, -1.0, -1.0] \n",
      " [-49.0, -527.0, -527.0, -527.0, -527.0, -527.0, -110.0, -32.0, -228.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.9], [0.33, 0.67], [0.76, 0.24], [0.03, 0.97], [0.13, 0.87], [0.33, 0.67], [0.94, 0.06], [0.03, 0.97], [0.76, 0.24], [0.8, 0.2]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.46 0.54]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -6.0, -1.0, -1.0, -4.0, -2.0, -6.0, -1.0, -2.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -466.0, -527.0, -527.0, -27.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.32, 0.68], [0.97, 0.03], [0.07, 0.93], [0.44, 0.56], [0.69, 0.31], [0.87, 0.13], [0.25, 0.75], [0.02, 0.98], [0.44, 0.56], [0.25, 0.75]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.44 0.56]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -2.0, -1.0, -1.0, -1.0, -8.0, -4.0, -1.0, -8.0, -5.0] \n",
      " [-527.0, -527.0, -27.0, -527.0, -527.0, -31.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.43, 0.57], [0.19, 0.81], [0.75, 0.25], [0.42, 0.58], [0.42, 0.58], [0.56, 0.44], [0.48, 0.52], [0.75, 0.25], [0.0, 1.0], [0.23, 0.77]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.42 0.58]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -3.0, -3.0, -12.0, -3.0, -7.0, -4.0, -6.0, -8.0, -3.0] \n",
      " [-527.0, -527.0, -23.0, -527.0, -527.0, -527.0, -527.0, -202.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.99, 0.01], [0.08, 0.92], [0.0, 1.0], [0.99, 0.01], [0.02, 0.98], [0.46, 0.54], [0.05, 0.95], [0.03, 0.97], [0.08, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.4 0.6]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -8.0, -6.0, -2.0, -6.0, -5.0, -4.0, -9.0, -3.0, -2.0] \n",
      " [-527.0, -527.0, -523.0, -527.0, -453.0, -104.0, -527.0, -527.0, -301.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.05, 0.95], [0.95, 0.05], [0.12, 0.88], [0.39, 0.61], [0.01, 0.99], [0.86, 0.14], [0.05, 0.95], [0.0, 1.0], [0.09, 0.91], [0.39, 0.61]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.39 0.61]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -15.0, -6.0, -4.0, -6.0, -5.0, -8.0, -5.0, -4.0, -9.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.06, 0.94], [0.0, 1.0], [0.0, 1.0], [0.03, 0.97], [0.01, 0.99], [1.0, 0.0], [0.09, 0.91], [0.96, 0.04], [0.06, 0.94], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.38 0.62]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10.0, -16.0, -4.0, -2.0, -3.0, -12.0, -7.0, -6.0, -17.0, -2.0] \n",
      " [-400.0, -527.0, -527.0, -14.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.61, 0.39], [0.0, 1.0], [0.88, 0.12], [0.37, 0.63], [0.94, 0.06], [0.09, 0.91], [0.97, 0.03], [0.98, 0.02], [0.0, 1.0], [0.37, 0.63]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.37 0.63]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -2.0, -13.0, -8.0, -13.0, -14.0, -2.0, -19.0, -10.0, -3.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -348.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.36, 0.64], [0.36, 0.64], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.36, 0.64], [0.07, 0.93], [0.03, 0.97], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.36 0.64]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-44.0, -14.0, -16.0, -16.0, -14.0, -11.0, -14.0, -19.0, -10.0, -9.0] \n",
      " [-527.0, -242.0, -527.0, -527.0, -374.0, -104.0, -527.0, -98.0, -267.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.03, 0.97], [0.0, 1.0], [0.0, 1.0], [0.47, 0.53], [0.0, 1.0], [0.99, 0.01], [0.0, 1.0], [0.03, 0.97], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.35 0.65]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -10.0, -3.0, -3.0, -10.0, -10.0, -7.0, -4.0, -26.0, -27.0] \n",
      " [-527.0, -527.0, -14.0, -527.0, -527.0, -527.0, -527.0, -426.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.97, 0.03], [0.0, 1.0], [0.34, 0.66], [0.34, 0.66], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.99, 0.01], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -5.0, -7.0, -10.0, -3.0, -49.0, -28.0, -30.0, -18.0, -7.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.33, 0.67], [0.0, 1.0], [0.74, 0.26], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.67]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -38.0, -9.0, -19.0, -12.0, -11.0, -11.0, -12.0, -4.0, -20.0] \n",
      " [-527.0, -453.0, -527.0, -527.0, -62.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.33, 0.67], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.67]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -8.0, -15.0, -6.0, -12.0, -16.0, -27.0, -4.0, -27.0, -9.0] \n",
      " [-527.0, -527.0, -527.0, -518.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.9, 0.1], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.35, 0.65], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.35 0.65]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -28.0, -19.0, -12.0, -37.0, -48.0, -8.0, -18.0, -9.0, -37.0] \n",
      " [-301.0, -527.0, -527.0, -527.0, -505.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.81, 0.19], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -16.0, -5.0, -25.0, -29.0, -10.0, -5.0, -8.0, -21.0, -10.0] \n",
      " [-16.0, -527.0, -527.0, -527.0, -527.0, -527.0, -156.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.34, 0.66], [0.0, 1.0], [0.34, 0.66], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.34, 0.66], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-25.0, -13.0, -20.0, -31.0, -42.0, -25.0, -16.0, -5.0, -14.0, -5.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -37.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.94, 0.06], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.37, 0.63], [0.0, 1.0], [0.37, 0.63]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.37 0.63]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-29.0, -20.0, -24.0, -6.0, -10.0, -18.0, -20.0, -6.0, -9.0, -6.0] \n",
      " [-527.0, -228.0, -527.0, -527.0, -527.0, -527.0, -35.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.94, 0.06], [0.0, 1.0], [0.43, 0.57], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.43, 0.57], [0.0, 1.0], [0.43, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.43 0.57]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -10.0, -24.0, -14.0, -17.0, -14.0, -19.0, -13.0, -6.0, -13.0] \n",
      " [-323.0, -453.0, -527.0, -527.0, -348.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.5, 0.5], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18.0, -17.0, -23.0, -16.0, -19.0, -19.0, -23.0, -19.0, -36.0, -10.0] \n",
      " [-255.0, -527.0, -527.0, -242.0, -527.0, -527.0, -32.0, -527.0, -527.0, -515.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -28.0, -7.0, -16.0, -16.0, -11.0, -17.0, -22.0, -7.0, -7.0] \n",
      " [-527.0, -527.0, -156.0, -527.0, -527.0, -527.0, -527.0, -32.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.51, 0.49], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.51, 0.49], [0.51, 0.49]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-394.0, -394.0, -394.0, -389.0, -163.0, -394.0, -394.0, -394.0, -394.0, -394.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -54.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.2 0.8]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -17.0, -11.0, -11.0, -7.0, -43.0, -16.0, -37.0, -22.0, -21.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -19.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.09, 0.91], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.09 0.91]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -24.0, -8.0, -13.0, -13.0, -25.0, -16.0, -13.0, -25.0, -8.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -289.0, -527.0, -527.0, -523.0, -527.0, -19.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.06, 0.94], [0.0, 1.0], [0.06, 0.94], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.06, 0.94]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.06 0.94]\n"
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for _ in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single_Cross_Triple 4 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 2000\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 10,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [12],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 2000 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.0936899185180664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1fcde8bb7f0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  832       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  546       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,095\n",
      "Trainable params: 12,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "# not bad with the first model. need a fonction to decrease entropy now \n",
    "\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary4.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method ACAgent._logits_loss of <Agent_class.ACAgent object at 0x000002BA33964AC8>>,\n",
       " <bound method ACAgent._value_loss of <Agent_class.ACAgent object at 0x000002BA33964AC8>>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agents[0].model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                     4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                     5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                     6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                     7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [12],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.13463735580444336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1bbe8d2ca58>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  780       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  1281      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  462       \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  22        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  780       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1281      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  462       \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  176       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 5,244\n",
      "Trainable params: 5,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 2000\n",
    "n_step_size = 16\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary8.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Entropy reduced to 1900.0 \n",
      "Agent 0 : Entropy reduced to 1805.0 \n",
      "Agent 0 : Entropy reduced to 1714.75 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -16.0, -17.0, -19.0, -19.0, -11.0, -9.0, -10.0, -19.0, -12.0] \n",
      " [-1282.0, -1466.0, -1467.0, -1512.0, -1508.0, -1245.0, -1169.0, -1294.0, -1508.0, -1324.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.16, 0.0, 0.31, 0.0, 0.42, 0.06, 0.04, 0.0], [0.25, 0.0, 0.1, 0.03, 0.43, 0.13, 0.06, 0.0], [0.23, 0.0, 0.14, 0.05, 0.28, 0.2, 0.09, 0.0], [0.14, 0.0, 0.28, 0.1, 0.1, 0.26, 0.11, 0.0], [0.14, 0.0, 0.35, 0.11, 0.08, 0.22, 0.1, 0.0], [0.26, 0.0, 0.15, 0.01, 0.45, 0.08, 0.05, 0.0], [0.19, 0.0, 0.39, 0.01, 0.16, 0.15, 0.1, 0.0], [0.14, 0.0, 0.21, 0.0, 0.57, 0.05, 0.04, 0.0], [0.14, 0.0, 0.35, 0.11, 0.08, 0.22, 0.1, 0.0], [0.16, 0.0, 0.07, 0.0, 0.68, 0.06, 0.03, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.12 0.12 0.13 0.13 0.13]\n",
      "Agent 0 : Entropy reduced to 1629.0124999999998 \n",
      "Agent 0 : Entropy reduced to 1547.5618749999996 \n",
      "Agent 0 : Entropy reduced to 1470.1837812499996 \n",
      "Agent 0 : Entropy reduced to 1396.6745921874995 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -25.0, -34.0, -33.0, -33.0, -43.0, -9.0, -45.0, -36.0, -12.0] \n",
      " [-1340.0, -1264.0, -1340.0, -1297.0, -1392.0, -1554.0, -525.0, -1498.0, -1353.0, -359.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.11, 0.36, 0.22, 0.0, 0.24, 0.0, 0.01, 0.06], [0.15, 0.26, 0.2, 0.0, 0.21, 0.01, 0.03, 0.14], [0.11, 0.36, 0.22, 0.0, 0.24, 0.0, 0.01, 0.06], [0.14, 0.27, 0.28, 0.0, 0.19, 0.0, 0.01, 0.11], [0.14, 0.35, 0.17, 0.0, 0.25, 0.0, 0.01, 0.07], [0.1, 0.52, 0.07, 0.0, 0.29, 0.0, 0.0, 0.02], [0.17, 0.15, 0.13, 0.05, 0.15, 0.08, 0.12, 0.15], [0.1, 0.63, 0.07, 0.0, 0.18, 0.0, 0.0, 0.01], [0.11, 0.35, 0.24, 0.0, 0.22, 0.0, 0.01, 0.07], [0.18, 0.15, 0.12, 0.04, 0.17, 0.07, 0.12, 0.15]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.12 0.12 0.12 0.13 0.13]\n",
      "Agent 0 : Entropy reduced to 1326.8408625781244 \n",
      "Agent 0 : Entropy reduced to 1260.4988194492182 \n",
      "Agent 0 : Entropy reduced to 1197.4738784767574 \n",
      "Agent 0 : Entropy reduced to 1137.6001845529195 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-42.0, -15.0, -44.0, -49.0, -29.0, -41.0, -7.0, -41.0, -45.0, -43.0] \n",
      " [-1310.0, -282.0, -1575.0, -1586.0, -1153.0, -1285.0, -317.0, -1513.0, -1577.0, -1577.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.13, 0.12, 0.11, 0.12, 0.11, 0.18, 0.14, 0.1], [0.11, 0.11, 0.1, 0.14, 0.11, 0.14, 0.16, 0.14], [0.1, 0.16, 0.13, 0.08, 0.13, 0.15, 0.15, 0.1], [0.12, 0.12, 0.11, 0.15, 0.15, 0.09, 0.12, 0.15], [0.13, 0.13, 0.11, 0.11, 0.1, 0.15, 0.14, 0.14], [0.11, 0.12, 0.12, 0.16, 0.14, 0.08, 0.11, 0.15], [0.1, 0.14, 0.16, 0.11, 0.1, 0.13, 0.14, 0.12], [0.14, 0.11, 0.11, 0.14, 0.11, 0.14, 0.14, 0.1], [0.12, 0.1, 0.11, 0.17, 0.14, 0.09, 0.12, 0.14], [0.1, 0.16, 0.14, 0.08, 0.14, 0.14, 0.14, 0.1]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.12 0.13 0.13 0.13]\n",
      "Agent 0 : Entropy reduced to 1080.7201753252734 \n",
      "Agent 0 : Entropy reduced to 1026.6841665590098 \n",
      "Agent 0 : Entropy reduced to 975.3499582310592 \n",
      "Agent 0 : Entropy reduced to 926.5824603195061 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-84.0, -77.0, -79.0, -68.0, -88.0, -82.0, -44.0, -78.0, -89.0, -85.0] \n",
      " [-1293.0, -1384.0, -1567.0, -431.0, -1054.0, -689.0, -242.0, -1577.0, -1559.0, -1259.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.15, 0.11, 0.11, 0.14, 0.12, 0.12, 0.12, 0.12], [0.14, 0.1, 0.11, 0.15, 0.12, 0.13, 0.12, 0.13], [0.13, 0.12, 0.12, 0.11, 0.12, 0.14, 0.13, 0.12], [0.13, 0.12, 0.11, 0.13, 0.13, 0.13, 0.13, 0.12], [0.15, 0.11, 0.11, 0.13, 0.12, 0.14, 0.12, 0.13], [0.09, 0.17, 0.17, 0.06, 0.13, 0.13, 0.12, 0.12], [0.14, 0.14, 0.1, 0.1, 0.09, 0.13, 0.13, 0.17], [0.14, 0.11, 0.1, 0.16, 0.12, 0.14, 0.12, 0.11], [0.11, 0.14, 0.13, 0.1, 0.14, 0.13, 0.13, 0.11], [0.14, 0.1, 0.11, 0.15, 0.12, 0.13, 0.12, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.11 0.14 0.13 0.13 0.12 0.13 0.13 0.13]\n",
      "Agent 0 : Entropy reduced to 880.2533373035308 \n",
      "Agent 0 : Entropy reduced to 836.2406704383542 \n",
      "Agent 0 : Entropy reduced to 794.4286369164364 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-129.0, -67.0, -151.0, -136.0, -126.0, -131.0, -132.0, -134.0, -9.0, -145.0] \n",
      " [-1156.0, -889.0, -1486.0, -1409.0, -1328.0, -1455.0, -1394.0, -1550.0, -171.0, -1299.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.15, 0.12, 0.13, 0.12, 0.1, 0.13, 0.14, 0.11], [0.15, 0.1, 0.11, 0.17, 0.1, 0.12, 0.14, 0.12], [0.11, 0.14, 0.13, 0.13, 0.13, 0.1, 0.13, 0.12], [0.14, 0.13, 0.14, 0.13, 0.1, 0.14, 0.14, 0.08], [0.13, 0.13, 0.12, 0.16, 0.11, 0.14, 0.13, 0.08], [0.13, 0.14, 0.12, 0.15, 0.11, 0.13, 0.14, 0.08], [0.14, 0.11, 0.13, 0.14, 0.1, 0.12, 0.13, 0.12], [0.09, 0.14, 0.14, 0.11, 0.15, 0.11, 0.14, 0.12], [0.12, 0.13, 0.12, 0.15, 0.13, 0.11, 0.11, 0.13], [0.15, 0.13, 0.14, 0.1, 0.1, 0.11, 0.14, 0.13]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.11 0.13 0.13 0.13 0.12 0.13 0.13 0.12]\n",
      "Agent 0 : Entropy reduced to 754.7072050706146 \n",
      "Agent 0 : Entropy reduced to 716.9718448170838 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7938735d5891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[1;31m#print(actions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Agent_class.py\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Agent_class.py\u001b[0m in \u001b[0;36maction_value\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;31m# executes call() under the hood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;31m# a simpler option, will become clear later why we don't use it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;31m# Setup work for each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    830\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_output_loss_metrics'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_loss_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m         \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m     \u001b[1;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \"\"\"\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3054\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3055\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3056\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3057\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3058\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m   1145\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m-> 1147\u001b[1;33m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[0;32m   1148\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"AssignVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         value)\n\u001b[0m\u001b[0;32m    183\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for i in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                Agents[idx].reduce_entropy()\n",
    "                print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
