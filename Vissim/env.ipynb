{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import env\n",
    "from Agent_class import ACAgent\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionnary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "1.0622756481170654\n"
     ]
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Balance_dictionnary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5737c88>,\n",
       " 1: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5737ef0>,\n",
       " 2: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5746a20>,\n",
       " 3: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5758518>,\n",
       " 4: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5761860>,\n",
       " 5: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5761780>,\n",
       " 6: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d576fbe0>,\n",
       " 7: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d576f320>,\n",
       " 8: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5778ac8>,\n",
       " 9: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5778160>,\n",
       " 10: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d578aac8>,\n",
       " 11: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d578a5c0>,\n",
       " 12: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5792908>,\n",
       " 13: <Vissim_SCU_class.Signal_Control_Unit at 0x2b2d5792f98>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0}\n"
     ]
    }
   ],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionnary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(len(env.SCUs[i].compatible_actions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Controllers_Actions =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {   0 : [1, 0, 1, 0],\n",
    "            1 : [0, 1, 0, 1],\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.05682992935180664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1449b150978>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Controllers_Actions,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  252       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  252       \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 633\n",
      "Trainable params: 633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.000005\n",
    "ID = 0\n",
    "value = 0.5\n",
    "entropy = 0.1\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for i , acts in Controllers_Actions.items():\n",
    "    Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "    Agents.append(Agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -4.0, -4.0, -4.0, -4.0, -4.0, -4.0, -4.0, -5.0, -3.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3382.0, -2971.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10.0, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -8.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3340.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -15.0, -15.0, -15.0, -15.0, -15.0, -15.0, -15.0, -15.0, -11.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3270.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -20.0, -20.0, -20.0, -20.0, -20.0, -17.0, -20.0, -20.0, -1.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3340.0, -3396.0, -3396.0, -898.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.65, 0.35]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-26.0, -26.0, -26.0, -26.0, -16.0, -26.0, -26.0, -26.0, -26.0, -26.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -2817.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -32.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -31.0] \n",
      " [-898.0, -3394.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.58, 0.42], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-36.0, -36.0, -36.0, -36.0, -36.0, -0.0, -36.0, -1.0, -36.0, -36.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -665.0, -3396.0, -1026.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.49, 0.51], [1.0, 0.0], [0.71, 0.29], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-41.0, -41.0, -41.0, -41.0, -41.0, -26.0, -41.0, -41.0, -41.0, -41.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -2971.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-47.0, -47.0, -47.0, -47.0, -47.0, -47.0, -22.0, -47.0, -47.0, -47.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -2518.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-52.0, -52.0, -38.0, -52.0, -52.0, -52.0, -52.0, -52.0, -52.0, -52.0] \n",
      " [-3396.0, -3396.0, -3270.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-41.0, -58.0, 0.0, -58.0, -58.0, -58.0, -58.0, -58.0, -58.0, -58.0] \n",
      " [-3270.0, -3396.0, -1288.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-63.0, -63.0, -63.0, -36.0, -63.0, -63.0, -63.0, -55.0, -63.0, -63.0] \n",
      " [-3396.0, -3396.0, -3396.0, -2720.0, -3396.0, -3396.0, -3396.0, -3364.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.47 0.53]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-69.0, -69.0, -6.0, -69.0, -69.0, -69.0, -3.0, -69.0, -6.0, -50.0] \n",
      " [-3396.0, -3396.0, -2385.0, -3396.0, -3396.0, -3396.0, -3364.0, -3396.0, -3046.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.92, 0.08], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.47 0.53]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-74.0, -1.0, -73.0, -74.0, -74.0, -74.0, -74.0, -74.0, -74.0, -74.0] \n",
      " [-3396.0, -1288.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.97, 0.03], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.47 0.53]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-79.0, -79.0, -79.0, -79.0, -79.0, -79.0, -79.0, -79.0, -79.0, -79.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.47 0.53]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -75.0, -84.0, -84.0, -84.0, -84.0, -84.0, -84.0, -84.0, -69.0] \n",
      " [-1026.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.47, 0.53], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.46 0.54]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-90.0, -90.0, -90.0, -90.0, -90.0, -90.0, -90.0, -87.0, -90.0, -90.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.89, 0.11], [0.89, 0.11], [0.89, 0.11], [0.89, 0.11], [0.89, 0.11], [0.89, 0.11], [0.89, 0.11], [1.0, 0.0], [0.89, 0.11], [0.89, 0.11]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.46 0.54]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -2.0, -16.0, -6.0, -4.0, -3.0, -10.0, -4.0, -17.0, -13.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -2074.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.78, 0.22], [0.36, 0.64], [0.81, 0.19], [0.24, 0.76], [0.26, 0.74], [0.03, 0.97], [0.83, 0.17], [0.51, 0.49], [0.97, 0.03], [0.61, 0.39]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.46 0.54]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-35.0, -8.0, -20.0, -15.0, -13.0, -7.0, -9.0, -6.0, -1.0, -7.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3340.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.93, 0.07], [0.21, 0.79], [0.49, 0.51], [0.0, 1.0], [0.0, 1.0], [0.39, 0.61], [0.17, 0.83], [0.95, 0.05], [0.38, 0.62], [0.48, 0.52]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.45 0.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14.0, -5.0, -17.0, -1.0, -3.0, -4.0, -4.0, -10.0, -5.0, -4.0] \n",
      " [-3388.0, -3396.0, -3396.0, -3396.0, -3396.0, -2971.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.12, 0.88], [0.2, 0.8], [0.0, 1.0], [0.39, 0.61], [0.34, 0.66], [0.03, 0.97], [0.01, 0.99], [0.04, 0.96], [0.13, 0.87], [0.62, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.45 0.55]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -6.0, -8.0, -3.0, -7.0, -12.0, -14.0, -4.0, -13.0, -17.0] \n",
      " [-2074.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3378.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.14, 0.86], [0.18, 0.82], [0.0, 1.0], [0.51, 0.49], [0.0, 1.0], [0.16, 0.84], [0.0, 1.0], [0.25, 0.75], [0.03, 0.97], [0.02, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.45 0.55]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7.0, -16.0, -5.0, -4.0, -16.0, -11.0, -2.0, -5.0, -16.0, -11.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -2621.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.21, 0.79], [0.63, 0.37], [0.89, 0.11], [0.26, 0.74], [0.44, 0.56], [0.68, 0.32], [0.75, 0.25], [0.03, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.44 0.56]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -5.0, -4.0, -3.0, -8.0, -7.0, -13.0, -1.0, -7.0, -9.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.18, 0.82], [0.34, 0.66], [0.35, 0.65], [0.36, 0.64], [0.77, 0.23], [0.07, 0.93], [0.0, 1.0], [0.42, 0.58], [0.73, 0.27], [0.03, 0.97]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.44 0.56]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21.0, -22.0, -7.0, -6.0, -11.0, -11.0, -13.0, -5.0, -12.0, -11.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -1677.0, -3382.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.33, 0.67], [0.0, 1.0], [0.51, 0.49], [0.58, 0.42], [0.42, 0.58], [0.82, 0.18], [0.01, 0.99], [0.0, 1.0], [0.15, 0.85]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.44 0.56]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -6.0, -13.0, -8.0, -1.0, -7.0, -29.0, -27.0, -6.0, -6.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3270.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.49, 0.51], [0.49, 0.51], [0.7, 0.3], [0.56, 0.44], [0.4, 0.6], [0.11, 0.89], [0.21, 0.79], [0.09, 0.91], [0.1, 0.9], [0.46, 0.54]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.44 0.56]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -2.0, -12.0, -6.0, -12.0, -20.0, -12.0, -33.0, -4.0, -12.0] \n",
      " [-3396.0, -3396.0, -3396.0, -3396.0, -2621.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.58, 0.42], [0.31, 0.69], [0.22, 0.78], [0.23, 0.77], [0.53, 0.47], [0.02, 0.98], [0.23, 0.77], [0.01, 0.99], [0.2, 0.8], [0.36, 0.64]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.43 0.57]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -33.0, -11.0, -9.0, -14.0, -7.0, -9.0, -3.0, -5.0, -30.0] \n",
      " [-898.0, -3396.0, -3388.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0, -3396.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.38, 0.62], [0.0, 1.0], [0.23, 0.77], [0.02, 0.98], [0.12, 0.88], [0.0, 1.0], [0.02, 0.98], [0.03, 0.97], [0.09, 0.91], [0.74, 0.26]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.43 0.57]\n"
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for _ in range(100000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
