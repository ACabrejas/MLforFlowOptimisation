{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_Critic_Class import ACAgent\n",
    "import numpy as np \n",
    "from balance_dictionary import balance_dictionary\n",
    "from DQNAgents import DQNAgent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Executive Control\n",
    "timesteps_per_second = 1\n",
    "agent_type = 'DQN'\n",
    "agent_h_layer =  [ 3, 3, 5, 1, 2, 4,5,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Starting Deployments of Signal Control Units\n",
      "SCUs successfully deployed. Elapsed time 0.67 seconds.\n"
     ]
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "          mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f2fc18>,\n",
       " 1: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f2fda0>,\n",
       " 2: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f34f28>,\n",
       " 3: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f47f60>,\n",
       " 4: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f5ec88>,\n",
       " 5: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f69f28>,\n",
       " 6: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f6fac8>,\n",
       " 7: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f7e3c8>,\n",
       " 8: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f7ef60>,\n",
       " 9: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f92128>,\n",
       " 10: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f92a90>,\n",
       " 11: <Vissim_SCU_class.Signal_Control_Unit at 0x22010f9f668>,\n",
       " 12: <Vissim_SCU_class.Signal_Control_Unit at 0x22010fa7240>,\n",
       " 13: <Vissim_SCU_class.Signal_Control_Unit at 0x22010fa7dd8>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NParser.NetworkParser at 0x22010e18eb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.npa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-17ba4cd6f487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAgents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompatible_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_weights_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVissim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPER_activated\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mDoubleDQN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0magent_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"DDQN\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"DuelingDDQN\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mDueling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0magent_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"DQN\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"DDQN\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mID\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal_controllers_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-17ba4cd6f487>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAgents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompatible_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_weights_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVissim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPER_activated\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mDoubleDQN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0magent_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"DDQN\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"DuelingDDQN\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mDueling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0magent_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"DQN\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"DDQN\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mID\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal_controllers_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'state_type' is not defined"
     ]
    }
   ],
   "source": [
    "Agents = [DQNAgent(state_size = env.SCUs[idx].state_size, compatible_actions = len(env.SCUs[idx].compatible_actions), ID, state_type = env.SCUs, memory_size,\\\n",
    "         gamma, epsilon_sequence[0], alpha, copy_weights_frequency, Vissim, PER_activated,\\\n",
    "         DoubleDQN = True if agent_type == \"DDQN\" or \"DuelingDDQN\" else False,\\\n",
    "         Dueling = False if agent_type == \"DQN\" or \"DDQN\" else True) for idx, ID in enumerate(env.npa.signal_controllers_ids)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.SCUs[0].state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    print(len(env.SCUs[i].compatible_actions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 0.05\n",
    "n_step_size = 16\n",
    "state_size = [4]\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Straight_dictionary.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -12.0, -31.0, -31.0, -31.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -86.0, -527.0, -301.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.53 0.47]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -49.0, -23.0, -49.0, -43.0, -15.0, -49.0, -49.0, -49.0, -24.0] \n",
      " [-54.0, -527.0, -527.0, -439.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.14, 0.86], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.02, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -4.0, -8.0, -4.0, -11.0, -6.0, -2.0, -4.0, -1.0, -3.0] \n",
      " [-426.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -228.0, -27.0, -279.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.22, 0.78], [0.16, 0.84], [0.04, 0.96], [0.25, 0.75], [0.01, 0.99], [0.16, 0.84], [0.88, 0.12], [0.18, 0.82], [0.6, 0.4], [0.43, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -2.0, -1.0, -8.0, -5.0, -2.0, -2.0, -6.0, -1.0, -1.0] \n",
      " [-49.0, -527.0, -527.0, -527.0, -527.0, -527.0, -110.0, -32.0, -228.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.9], [0.33, 0.67], [0.76, 0.24], [0.03, 0.97], [0.13, 0.87], [0.33, 0.67], [0.94, 0.06], [0.03, 0.97], [0.76, 0.24], [0.8, 0.2]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.46 0.54]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -6.0, -1.0, -1.0, -4.0, -2.0, -6.0, -1.0, -2.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -466.0, -527.0, -527.0, -27.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.32, 0.68], [0.97, 0.03], [0.07, 0.93], [0.44, 0.56], [0.69, 0.31], [0.87, 0.13], [0.25, 0.75], [0.02, 0.98], [0.44, 0.56], [0.25, 0.75]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.44 0.56]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -2.0, -1.0, -1.0, -1.0, -8.0, -4.0, -1.0, -8.0, -5.0] \n",
      " [-527.0, -527.0, -27.0, -527.0, -527.0, -31.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.43, 0.57], [0.19, 0.81], [0.75, 0.25], [0.42, 0.58], [0.42, 0.58], [0.56, 0.44], [0.48, 0.52], [0.75, 0.25], [0.0, 1.0], [0.23, 0.77]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.42 0.58]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -3.0, -3.0, -12.0, -3.0, -7.0, -4.0, -6.0, -8.0, -3.0] \n",
      " [-527.0, -527.0, -23.0, -527.0, -527.0, -527.0, -527.0, -202.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.99, 0.01], [0.08, 0.92], [0.0, 1.0], [0.99, 0.01], [0.02, 0.98], [0.46, 0.54], [0.05, 0.95], [0.03, 0.97], [0.08, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.4 0.6]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -8.0, -6.0, -2.0, -6.0, -5.0, -4.0, -9.0, -3.0, -2.0] \n",
      " [-527.0, -527.0, -523.0, -527.0, -453.0, -104.0, -527.0, -527.0, -301.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.05, 0.95], [0.95, 0.05], [0.12, 0.88], [0.39, 0.61], [0.01, 0.99], [0.86, 0.14], [0.05, 0.95], [0.0, 1.0], [0.09, 0.91], [0.39, 0.61]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.39 0.61]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -15.0, -6.0, -4.0, -6.0, -5.0, -8.0, -5.0, -4.0, -9.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.06, 0.94], [0.0, 1.0], [0.0, 1.0], [0.03, 0.97], [0.01, 0.99], [1.0, 0.0], [0.09, 0.91], [0.96, 0.04], [0.06, 0.94], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.38 0.62]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10.0, -16.0, -4.0, -2.0, -3.0, -12.0, -7.0, -6.0, -17.0, -2.0] \n",
      " [-400.0, -527.0, -527.0, -14.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.61, 0.39], [0.0, 1.0], [0.88, 0.12], [0.37, 0.63], [0.94, 0.06], [0.09, 0.91], [0.97, 0.03], [0.98, 0.02], [0.0, 1.0], [0.37, 0.63]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.37 0.63]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -2.0, -13.0, -8.0, -13.0, -14.0, -2.0, -19.0, -10.0, -3.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -348.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.36, 0.64], [0.36, 0.64], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.36, 0.64], [0.07, 0.93], [0.03, 0.97], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.36 0.64]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-44.0, -14.0, -16.0, -16.0, -14.0, -11.0, -14.0, -19.0, -10.0, -9.0] \n",
      " [-527.0, -242.0, -527.0, -527.0, -374.0, -104.0, -527.0, -98.0, -267.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.03, 0.97], [0.0, 1.0], [0.0, 1.0], [0.47, 0.53], [0.0, 1.0], [0.99, 0.01], [0.0, 1.0], [0.03, 0.97], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.35 0.65]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -10.0, -3.0, -3.0, -10.0, -10.0, -7.0, -4.0, -26.0, -27.0] \n",
      " [-527.0, -527.0, -14.0, -527.0, -527.0, -527.0, -527.0, -426.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.97, 0.03], [0.0, 1.0], [0.34, 0.66], [0.34, 0.66], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.99, 0.01], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -5.0, -7.0, -10.0, -3.0, -49.0, -28.0, -30.0, -18.0, -7.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.33, 0.67], [0.0, 1.0], [0.74, 0.26], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.67]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -38.0, -9.0, -19.0, -12.0, -11.0, -11.0, -12.0, -4.0, -20.0] \n",
      " [-527.0, -453.0, -527.0, -527.0, -62.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.33, 0.67], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.67]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -8.0, -15.0, -6.0, -12.0, -16.0, -27.0, -4.0, -27.0, -9.0] \n",
      " [-527.0, -527.0, -527.0, -518.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.9, 0.1], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.35, 0.65], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.35 0.65]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -28.0, -19.0, -12.0, -37.0, -48.0, -8.0, -18.0, -9.0, -37.0] \n",
      " [-301.0, -527.0, -527.0, -527.0, -505.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.81, 0.19], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -16.0, -5.0, -25.0, -29.0, -10.0, -5.0, -8.0, -21.0, -10.0] \n",
      " [-16.0, -527.0, -527.0, -527.0, -527.0, -527.0, -156.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.34, 0.66], [0.0, 1.0], [0.34, 0.66], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.34, 0.66], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-25.0, -13.0, -20.0, -31.0, -42.0, -25.0, -16.0, -5.0, -14.0, -5.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -37.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.94, 0.06], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.37, 0.63], [0.0, 1.0], [0.37, 0.63]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.37 0.63]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-29.0, -20.0, -24.0, -6.0, -10.0, -18.0, -20.0, -6.0, -9.0, -6.0] \n",
      " [-527.0, -228.0, -527.0, -527.0, -527.0, -527.0, -35.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.94, 0.06], [0.0, 1.0], [0.43, 0.57], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.43, 0.57], [0.0, 1.0], [0.43, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.43 0.57]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -10.0, -24.0, -14.0, -17.0, -14.0, -19.0, -13.0, -6.0, -13.0] \n",
      " [-323.0, -453.0, -527.0, -527.0, -348.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.5, 0.5], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18.0, -17.0, -23.0, -16.0, -19.0, -19.0, -23.0, -19.0, -36.0, -10.0] \n",
      " [-255.0, -527.0, -527.0, -242.0, -527.0, -527.0, -32.0, -527.0, -527.0, -515.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -28.0, -7.0, -16.0, -16.0, -11.0, -17.0, -22.0, -7.0, -7.0] \n",
      " [-527.0, -527.0, -156.0, -527.0, -527.0, -527.0, -527.0, -32.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.51, 0.49], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.51, 0.49], [0.51, 0.49]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-394.0, -394.0, -394.0, -389.0, -163.0, -394.0, -394.0, -394.0, -394.0, -394.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -54.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.2 0.8]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -17.0, -11.0, -11.0, -7.0, -43.0, -16.0, -37.0, -22.0, -21.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -19.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.09, 0.91], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.09 0.91]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -24.0, -8.0, -13.0, -13.0, -25.0, -16.0, -13.0, -25.0, -8.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -289.0, -527.0, -527.0, -523.0, -527.0, -19.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.06, 0.94], [0.0, 1.0], [0.06, 0.94], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.06, 0.94]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.06 0.94]\n"
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for _ in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 2000\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 10,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [12],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 2000 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.0936899185180664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1fcde8bb7f0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  832       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  546       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,095\n",
      "Trainable params: 12,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "# not bad with the first model. need a fonction to decrease entropy now \n",
    "\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary4.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method ACAgent._logits_loss of <Agent_class.ACAgent object at 0x000002BA33964AC8>>,\n",
       " <bound method ACAgent._value_loss of <Agent_class.ACAgent object at 0x000002BA33964AC8>>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agents[0].model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                     4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                     5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                     6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                     7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [12],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.13463735580444336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1bbe8d2ca58>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  780       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  1281      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  462       \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  22        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  780       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1281      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  462       \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  176       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 5,244\n",
      "Trainable params: 5,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 2000\n",
    "n_step_size = 16\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "## Agents Deployment\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary8.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for i in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                Agents[idx].reduce_entropy()\n",
    "                print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = range(5)\n",
    "b = range(3)\n",
    "\n",
    "b = [[[] for _ in a] for bla in b]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = 3\n",
    "def foo():\n",
    "    global a\n",
    "    b = a\n",
    "    for i in range(b):\n",
    "        print(i)\n",
    "foo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
