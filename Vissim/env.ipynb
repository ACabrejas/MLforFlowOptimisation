{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import env\n",
    "from Agent_class import ACAgent\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 5,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 4,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 7,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 1,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n"
     ]
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Balance_dictionnary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x17767ec6be0>,\n",
       " 1: <Vissim_SCU_class.Signal_Control_Unit at 0x17767ec6e48>,\n",
       " 2: <Vissim_SCU_class.Signal_Control_Unit at 0x17767ed6978>,\n",
       " 3: <Vissim_SCU_class.Signal_Control_Unit at 0x17767ee4470>,\n",
       " 4: <Vissim_SCU_class.Signal_Control_Unit at 0x17767eec7b8>,\n",
       " 5: <Vissim_SCU_class.Signal_Control_Unit at 0x17767eec6d8>,\n",
       " 6: <Vissim_SCU_class.Signal_Control_Unit at 0x17767efab38>,\n",
       " 7: <Vissim_SCU_class.Signal_Control_Unit at 0x17767efa278>,\n",
       " 8: <Vissim_SCU_class.Signal_Control_Unit at 0x17767f07a20>,\n",
       " 9: <Vissim_SCU_class.Signal_Control_Unit at 0x17767f070b8>,\n",
       " 10: <Vissim_SCU_class.Signal_Control_Unit at 0x17767f14a20>,\n",
       " 11: <Vissim_SCU_class.Signal_Control_Unit at 0x17767f14518>,\n",
       " 12: <Vissim_SCU_class.Signal_Control_Unit at 0x17767f1b860>,\n",
       " 13: <Vissim_SCU_class.Signal_Control_Unit at 0x17767f1bef0>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0}\n"
     ]
    }
   ],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionnary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Une exception s’est produite.', (0, 'VISSIM.Vissim.1100', 'AttValue failed: Object 1 - 3: SG2: Attribute Signal state is no subject to changes.', None, 0, -2147352567), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-18307391b7aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[1;31m# not a nice way of doing this creatung the dictionnary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[1;31m# not a nice way of doing this creatung the dictionnary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    374\u001b[0m                                         \u001b[1;31m#print('_color_changer')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                                         \u001b[1;31m#tic = t.time()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_color_changer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m                                         \u001b[1;31m#tac = t.time()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m                                         \u001b[1;31m#print(tac-tic)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m_color_changer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mredamber_time\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \t\t\t\t\t[self.signal_groups[idx].SetAttValue(\"SigState\", \"REDAMBER\") \\\n\u001b[1;32m--> 250\u001b[1;33m \t\t\t\t\t\t\t\t\tfor idx,value in enumerate(self.change_vector) if value == 1]\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mredamber_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    248\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mredamber_time\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \t\t\t\t\t[self.signal_groups[idx].SetAttValue(\"SigState\", \"REDAMBER\") \\\n\u001b[1;32m--> 250\u001b[1;33m \t\t\t\t\t\t\t\t\tfor idx,value in enumerate(self.change_vector) if value == 1]\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mredamber_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mSetAttValue\u001b[1;34m(self, Attribut, arg1)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Une exception s’est produite.', (0, 'VISSIM.Vissim.1100', 'AttValue failed: Object 1 - 3: SG2: Attribute Signal state is no subject to changes.', None, 0, -2147352567), None)"
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# not bad with the first model. need a fonction to decrease entropy now \n",
    "\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(len(env.SCUs[i].compatible_actions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.07547640800476074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1e6913145c0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  210       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  210       \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 549\n",
      "Trainable params: 549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 0.05\n",
    "n_step_size = 16\n",
    "state_size = [4]\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Straight_dictionary.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0, -13.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-31.0, -31.0, -31.0, -31.0, -31.0, -31.0, -12.0, -31.0, -31.0, -31.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -86.0, -527.0, -301.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.53 0.47]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -49.0, -23.0, -49.0, -43.0, -15.0, -49.0, -49.0, -49.0, -24.0] \n",
      " [-54.0, -527.0, -527.0, -439.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.14, 0.86], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.02, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -4.0, -8.0, -4.0, -11.0, -6.0, -2.0, -4.0, -1.0, -3.0] \n",
      " [-426.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -228.0, -27.0, -279.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.22, 0.78], [0.16, 0.84], [0.04, 0.96], [0.25, 0.75], [0.01, 0.99], [0.16, 0.84], [0.88, 0.12], [0.18, 0.82], [0.6, 0.4], [0.43, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.48 0.52]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -2.0, -1.0, -8.0, -5.0, -2.0, -2.0, -6.0, -1.0, -1.0] \n",
      " [-49.0, -527.0, -527.0, -527.0, -527.0, -527.0, -110.0, -32.0, -228.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.9], [0.33, 0.67], [0.76, 0.24], [0.03, 0.97], [0.13, 0.87], [0.33, 0.67], [0.94, 0.06], [0.03, 0.97], [0.76, 0.24], [0.8, 0.2]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.46 0.54]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -6.0, -1.0, -1.0, -4.0, -2.0, -6.0, -1.0, -2.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -466.0, -527.0, -527.0, -27.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.32, 0.68], [0.97, 0.03], [0.07, 0.93], [0.44, 0.56], [0.69, 0.31], [0.87, 0.13], [0.25, 0.75], [0.02, 0.98], [0.44, 0.56], [0.25, 0.75]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.44 0.56]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -2.0, -1.0, -1.0, -1.0, -8.0, -4.0, -1.0, -8.0, -5.0] \n",
      " [-527.0, -527.0, -27.0, -527.0, -527.0, -31.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.43, 0.57], [0.19, 0.81], [0.75, 0.25], [0.42, 0.58], [0.42, 0.58], [0.56, 0.44], [0.48, 0.52], [0.75, 0.25], [0.0, 1.0], [0.23, 0.77]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.42 0.58]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -3.0, -3.0, -12.0, -3.0, -7.0, -4.0, -6.0, -8.0, -3.0] \n",
      " [-527.0, -527.0, -23.0, -527.0, -527.0, -527.0, -527.0, -202.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.99, 0.01], [0.08, 0.92], [0.0, 1.0], [0.99, 0.01], [0.02, 0.98], [0.46, 0.54], [0.05, 0.95], [0.03, 0.97], [0.08, 0.92]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.4 0.6]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -8.0, -6.0, -2.0, -6.0, -5.0, -4.0, -9.0, -3.0, -2.0] \n",
      " [-527.0, -527.0, -523.0, -527.0, -453.0, -104.0, -527.0, -527.0, -301.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.05, 0.95], [0.95, 0.05], [0.12, 0.88], [0.39, 0.61], [0.01, 0.99], [0.86, 0.14], [0.05, 0.95], [0.0, 1.0], [0.09, 0.91], [0.39, 0.61]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.39 0.61]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -15.0, -6.0, -4.0, -6.0, -5.0, -8.0, -5.0, -4.0, -9.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.06, 0.94], [0.0, 1.0], [0.0, 1.0], [0.03, 0.97], [0.01, 0.99], [1.0, 0.0], [0.09, 0.91], [0.96, 0.04], [0.06, 0.94], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.38 0.62]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10.0, -16.0, -4.0, -2.0, -3.0, -12.0, -7.0, -6.0, -17.0, -2.0] \n",
      " [-400.0, -527.0, -527.0, -14.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.61, 0.39], [0.0, 1.0], [0.88, 0.12], [0.37, 0.63], [0.94, 0.06], [0.09, 0.91], [0.97, 0.03], [0.98, 0.02], [0.0, 1.0], [0.37, 0.63]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.37 0.63]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -2.0, -13.0, -8.0, -13.0, -14.0, -2.0, -19.0, -10.0, -3.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -348.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.36, 0.64], [0.36, 0.64], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.36, 0.64], [0.07, 0.93], [0.03, 0.97], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.36 0.64]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-44.0, -14.0, -16.0, -16.0, -14.0, -11.0, -14.0, -19.0, -10.0, -9.0] \n",
      " [-527.0, -242.0, -527.0, -527.0, -374.0, -104.0, -527.0, -98.0, -267.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.03, 0.97], [0.0, 1.0], [0.0, 1.0], [0.47, 0.53], [0.0, 1.0], [0.99, 0.01], [0.0, 1.0], [0.03, 0.97], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.35 0.65]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -10.0, -3.0, -3.0, -10.0, -10.0, -7.0, -4.0, -26.0, -27.0] \n",
      " [-527.0, -527.0, -14.0, -527.0, -527.0, -527.0, -527.0, -426.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.97, 0.03], [0.0, 1.0], [0.34, 0.66], [0.34, 0.66], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.99, 0.01], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -5.0, -7.0, -10.0, -3.0, -49.0, -28.0, -30.0, -18.0, -7.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.33, 0.67], [0.0, 1.0], [0.74, 0.26], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.67]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -38.0, -9.0, -19.0, -12.0, -11.0, -11.0, -12.0, -4.0, -20.0] \n",
      " [-527.0, -453.0, -527.0, -527.0, -62.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.33, 0.67], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.67]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -8.0, -15.0, -6.0, -12.0, -16.0, -27.0, -4.0, -27.0, -9.0] \n",
      " [-527.0, -527.0, -527.0, -518.0, -527.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.9, 0.1], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.35, 0.65], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.35 0.65]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -28.0, -19.0, -12.0, -37.0, -48.0, -8.0, -18.0, -9.0, -37.0] \n",
      " [-301.0, -527.0, -527.0, -527.0, -505.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.81, 0.19], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -16.0, -5.0, -25.0, -29.0, -10.0, -5.0, -8.0, -21.0, -10.0] \n",
      " [-16.0, -527.0, -527.0, -527.0, -527.0, -527.0, -156.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.34, 0.66], [0.0, 1.0], [0.34, 0.66], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.34, 0.66], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-25.0, -13.0, -20.0, -31.0, -42.0, -25.0, -16.0, -5.0, -14.0, -5.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -37.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.94, 0.06], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.37, 0.63], [0.0, 1.0], [0.37, 0.63]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.37 0.63]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-29.0, -20.0, -24.0, -6.0, -10.0, -18.0, -20.0, -6.0, -9.0, -6.0] \n",
      " [-527.0, -228.0, -527.0, -527.0, -527.0, -527.0, -35.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.94, 0.06], [0.0, 1.0], [0.43, 0.57], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.43, 0.57], [0.0, 1.0], [0.43, 0.57]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.43 0.57]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -10.0, -24.0, -14.0, -17.0, -14.0, -19.0, -13.0, -6.0, -13.0] \n",
      " [-323.0, -453.0, -527.0, -527.0, -348.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.5, 0.5], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18.0, -17.0, -23.0, -16.0, -19.0, -19.0, -23.0, -19.0, -36.0, -10.0] \n",
      " [-255.0, -527.0, -527.0, -242.0, -527.0, -527.0, -32.0, -527.0, -527.0, -515.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -28.0, -7.0, -16.0, -16.0, -11.0, -17.0, -22.0, -7.0, -7.0] \n",
      " [-527.0, -527.0, -156.0, -527.0, -527.0, -527.0, -527.0, -32.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.51, 0.49], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.51, 0.49], [0.51, 0.49]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-394.0, -394.0, -394.0, -389.0, -163.0, -394.0, -394.0, -394.0, -394.0, -394.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -54.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.2 0.8]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -17.0, -11.0, -11.0, -7.0, -43.0, -16.0, -37.0, -22.0, -21.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -19.0, -527.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.09, 0.91], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.09 0.91]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -24.0, -8.0, -13.0, -13.0, -25.0, -16.0, -13.0, -25.0, -8.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -289.0, -527.0, -527.0, -523.0, -527.0, -19.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.06, 0.94], [0.0, 1.0], [0.06, 0.94], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.06, 0.94]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.06 0.94]\n"
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for _ in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single_Cross_Triple 4 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 2000\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 10,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [12],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 2000 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.12905240058898926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1ba7bfc0e48>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  832       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  546       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,095\n",
      "Trainable params: 12,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "# not bad with the first model. need a fonction to decrease entropy now \n",
    "\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary4.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-111.0, -180.0, -156.0, -179.0, -78.0, -47.0, -78.0, -106.0, -111.0, -143.0] \n",
      " [-216.0, -274.0, -253.0, -351.0, -214.0, -223.0, -214.0, -240.0, -256.0, -220.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.28, 0.27, 0.22, 0.23], [0.32, 0.26, 0.23, 0.18], [0.28, 0.23, 0.27, 0.22], [0.28, 0.27, 0.24, 0.2], [0.34, 0.26, 0.2, 0.19], [0.26, 0.24, 0.25, 0.25], [0.34, 0.26, 0.2, 0.19], [0.23, 0.27, 0.25, 0.25], [0.29, 0.27, 0.24, 0.2], [0.3, 0.26, 0.23, 0.21]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-594.0, -339.0, -359.0, -315.0, -472.0, -67.0, -211.0, -11.0, -432.0, -450.0] \n",
      " [-240.0, -253.0, -274.0, -278.0, -198.0, -223.0, -380.0, -148.0, -273.0, -239.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.23, 0.25, 0.26, 0.26], [0.25, 0.23, 0.25, 0.27], [0.24, 0.24, 0.26, 0.26], [0.25, 0.24, 0.25, 0.27], [0.24, 0.22, 0.27, 0.27], [0.24, 0.25, 0.25, 0.25], [0.25, 0.24, 0.25, 0.25], [0.24, 0.26, 0.25, 0.25], [0.24, 0.23, 0.28, 0.25], [0.25, 0.22, 0.25, 0.28]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.24 0.26 0.25 0.25]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-297.0, -351.0, -374.0, -222.0, -315.0, -108.0, -312.0, -342.0, -256.0, -248.0] \n",
      " [-210.0, -240.0, -380.0, -210.0, -273.0, -198.0, -270.0, -239.0, -198.0, -205.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.24, 0.24, 0.26, 0.26], [0.23, 0.25, 0.25, 0.27], [0.24, 0.24, 0.25, 0.27], [0.25, 0.23, 0.25, 0.27], [0.25, 0.24, 0.25, 0.26], [0.25, 0.25, 0.24, 0.26], [0.25, 0.24, 0.25, 0.26], [0.23, 0.25, 0.26, 0.26], [0.25, 0.25, 0.24, 0.26], [0.25, 0.24, 0.25, 0.26]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.23 0.26 0.26 0.26]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-278.0, -112.0, -181.0, -187.0, -84.0, -172.0, -197.0, -189.0, -184.0, -153.0] \n",
      " [-205.0, -372.0, -221.0, -230.0, -362.0, -224.0, -198.0, -272.0, -227.0, -244.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.24, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.24, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.26, 0.25, 0.26, 0.24], [0.25, 0.24, 0.25, 0.25], [0.26, 0.24, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.24, 0.26, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.24 0.25 0.25 0.25]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-225.0, -327.0, -284.0, -254.0, -319.0, -278.0, -241.0, -235.0, -164.0, -311.0] \n",
      " [-231.0, -270.0, -214.0, -373.0, -286.0, -218.0, -221.0, -258.0, -219.0, -372.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.26, 0.24, 0.25, 0.25], [0.26, 0.25, 0.25, 0.24], [0.26, 0.25, 0.25, 0.25], [0.26, 0.24, 0.25, 0.25], [0.26, 0.24, 0.25, 0.25], [0.26, 0.24, 0.25, 0.25], [0.25, 0.24, 0.25, 0.25], [0.26, 0.25, 0.25, 0.25], [0.25, 0.24, 0.25, 0.25], [0.26, 0.24, 0.25, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.24 0.25 0.25 0.25]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-293.0, -131.0, -377.0, -129.0, -546.0, -437.0, -207.0, -43.0, -146.0, -211.0] \n",
      " [-248.0, -258.0, -258.0, -317.0, -221.0, -231.0, -272.0, -126.0, -257.0, -337.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.26, 0.24, 0.26, 0.24], [0.26, 0.25, 0.25, 0.24], [0.26, 0.25, 0.25, 0.24], [0.25, 0.25, 0.25, 0.25], [0.27, 0.26, 0.25, 0.23], [0.26, 0.24, 0.25, 0.24], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.24, 0.25, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  832       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  546       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,095\n",
      "Trainable params: 12,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 500.0 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-70.0, -101.0, -103.0, -133.0, -135.0, -117.0, -182.0, -103.0, -85.0, -85.0] \n",
      " [-256.0, -175.0, -205.0, -219.0, -297.0, -222.0, -239.0, -210.0, -221.0, -220.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.24, 0.25, 0.25, 0.26], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.24 0.25 0.25 0.25]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-228.0, -118.0, -231.0, -137.0, -149.0, -102.0, -186.0, -168.0, -94.0, -209.0] \n",
      " [-223.0, -252.0, -218.0, -227.0, -175.0, -253.0, -219.0, -263.0, -257.0, -351.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.26, 0.25, 0.24], [0.3, 0.24, 0.23, 0.24], [0.24, 0.26, 0.24, 0.25], [0.26, 0.26, 0.24, 0.25], [0.27, 0.23, 0.25, 0.25], [0.28, 0.23, 0.24, 0.24], [0.25, 0.25, 0.25, 0.25], [0.25, 0.24, 0.27, 0.24], [0.25, 0.25, 0.26, 0.25], [0.24, 0.26, 0.24, 0.26]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.18 0.28 0.27 0.27]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-269.0, -439.0, -421.0, -351.0, -171.0, -332.0, -227.0, -118.0, -282.0, -187.0] \n",
      " [-212.0, -235.0, -290.0, -205.0, -258.0, -184.0, -240.0, -274.0, -242.0, -253.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.26, 0.25, 0.27, 0.22], [0.27, 0.25, 0.21, 0.27], [0.26, 0.23, 0.25, 0.26], [0.27, 0.25, 0.22, 0.26], [0.24, 0.24, 0.24, 0.28], [0.27, 0.26, 0.25, 0.23], [0.26, 0.26, 0.26, 0.23], [0.25, 0.25, 0.25, 0.25], [0.24, 0.25, 0.26, 0.25], [0.23, 0.24, 0.24, 0.29]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.21 0.26 0.26 0.26]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-234.0, -184.0, -206.0, -752.0, -469.0, -393.0, -537.0, -528.0, -528.0, -234.0] \n",
      " [-261.0, -373.0, -377.0, -227.0, -258.0, -227.0, -198.0, -212.0, -212.0, -261.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.25, 0.26, 0.24], [0.26, 0.24, 0.26, 0.25], [0.26, 0.25, 0.25, 0.24], [0.27, 0.24, 0.24, 0.25], [0.26, 0.22, 0.26, 0.26], [0.24, 0.25, 0.26, 0.24], [0.27, 0.24, 0.25, 0.24], [0.28, 0.2, 0.29, 0.23], [0.28, 0.2, 0.29, 0.23], [0.25, 0.25, 0.26, 0.24]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.24 0.25 0.25 0.25]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-183.0, -115.0, -242.0, -248.0, -266.0, -251.0, -180.0, -142.0, -130.0, -179.0] \n",
      " [-217.0, -253.0, -214.0, -223.0, -242.0, -282.0, -297.0, -184.0, -258.0, -222.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.25, 0.24, 0.25, 0.26], [0.25, 0.26, 0.25, 0.25], [0.25, 0.26, 0.24, 0.25], [0.25, 0.26, 0.24, 0.26], [0.31, 0.21, 0.24, 0.23], [0.26, 0.28, 0.21, 0.26], [0.26, 0.25, 0.25, 0.24], [0.28, 0.21, 0.26, 0.25], [0.26, 0.26, 0.24, 0.24], [0.27, 0.22, 0.26, 0.26]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.24 0.26 0.26 0.25]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-130.0, -122.0, -162.0, -144.0, -139.0, -187.0, -112.0, -133.0, -218.0, -174.0] \n",
      " [-198.0, -243.0, -258.0, -309.0, -236.0, -282.0, -219.0, -218.0, -248.0, -351.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.26, 0.27, 0.22, 0.25], [0.27, 0.25, 0.24, 0.24], [0.27, 0.24, 0.25, 0.25], [0.25, 0.26, 0.24, 0.25], [0.25, 0.25, 0.24, 0.26], [0.27, 0.25, 0.23, 0.26], [0.24, 0.25, 0.26, 0.26], [0.24, 0.25, 0.24, 0.26], [0.26, 0.23, 0.27, 0.24], [0.27, 0.23, 0.25, 0.26]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.23 0.26 0.26 0.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-617.0, -748.0, -616.0, -689.0, -581.0, -599.0, -169.0, -586.0, -507.0, -218.0] \n",
      " [-212.0, -210.0, -230.0, -238.0, -248.0, -218.0, -373.0, -230.0, -288.0, -318.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.26, 0.24, 0.23, 0.26], [0.27, 0.23, 0.28, 0.22], [0.23, 0.25, 0.26, 0.25], [0.27, 0.24, 0.24, 0.24], [0.29, 0.21, 0.25, 0.25], [0.28, 0.21, 0.24, 0.27], [0.27, 0.25, 0.25, 0.24], [0.28, 0.26, 0.23, 0.23], [0.26, 0.23, 0.24, 0.28], [0.26, 0.26, 0.23, 0.25]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.27 0.24 0.25 0.24]\n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  832       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  546       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,095\n",
      "Trainable params: 12,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 50.0 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-330.0, -379.0, -261.0, -428.0, -305.0, -369.0, -291.0, -291.0, -379.0, -447.0] \n",
      " [-236.0, -265.0, -193.0, -287.0, -205.0, -221.0, -224.0, -224.0, -265.0, -274.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.28, 0.21, 0.23, 0.29], [0.33, 0.21, 0.21, 0.25], [0.29, 0.24, 0.23, 0.24], [0.26, 0.28, 0.22, 0.24], [0.26, 0.2, 0.19, 0.35], [0.24, 0.29, 0.22, 0.25], [0.27, 0.22, 0.23, 0.28], [0.27, 0.22, 0.23, 0.28], [0.33, 0.21, 0.21, 0.25], [0.27, 0.29, 0.21, 0.23]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.22 0.26 0.26 0.26]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-137.0, -263.0, -149.0, -125.0, -156.0, -135.0, -189.0, -111.0, -126.0, -100.0] \n",
      " [-218.0, -265.0, -220.0, -230.0, -220.0, -261.0, -224.0, -214.0, -219.0, -126.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.23, 0.3, 0.29, 0.18], [0.24, 0.28, 0.28, 0.21], [0.26, 0.22, 0.25, 0.27], [0.3, 0.29, 0.19, 0.22], [0.24, 0.2, 0.25, 0.31], [0.08, 0.25, 0.34, 0.33], [0.19, 0.19, 0.23, 0.4], [0.26, 0.26, 0.21, 0.28], [0.19, 0.27, 0.3, 0.24], [0.31, 0.24, 0.24, 0.21]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.31 0.24 0.24 0.21]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-176.0, -120.0, -161.0, -180.0, -166.0, -112.0, -230.0, -221.0, -176.0, -212.0] \n",
      " [-261.0, -220.0, -245.0, -240.0, -273.0, -256.0, -318.0, -316.0, -258.0, -223.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.23, 0.22, 0.28, 0.27], [0.24, 0.43, 0.13, 0.19], [0.32, 0.47, 0.0, 0.21], [0.29, 0.16, 0.12, 0.43], [0.4, 0.18, 0.0, 0.43], [0.11, 0.34, 0.29, 0.27], [0.42, 0.06, 0.0, 0.52], [0.53, 0.13, 0.0, 0.34], [0.27, 0.1, 0.0, 0.62], [0.43, 0.29, 0.0, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.23 0.32 0.15 0.29]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-105.0, -174.0, -120.0, -105.0, -143.0, -139.0, -103.0, -113.0, -262.0, -173.0] \n",
      " [-218.0, -230.0, -217.0, -218.0, -288.0, -261.0, -219.0, -244.0, -377.0, -337.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.47, 0.2, 0.07, 0.26], [0.34, 0.37, 0.0, 0.29], [0.28, 0.24, 0.16, 0.32], [0.47, 0.2, 0.07, 0.26], [0.31, 0.18, 0.17, 0.34], [0.12, 0.19, 0.32, 0.37], [0.16, 0.3, 0.21, 0.33], [0.17, 0.26, 0.35, 0.23], [0.23, 0.46, 0.0, 0.31], [0.25, 0.41, 0.0, 0.35]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.37 0.25 0.18 0.21]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-156.0, -108.0, -142.0, -156.0, -121.0, -122.0, -157.0, -105.0, -105.0, -149.0] \n",
      " [-218.0, -269.0, -377.0, -220.0, -286.0, -240.0, -221.0, -174.0, -174.0, -258.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.36, 0.21, 0.24, 0.2], [0.14, 0.23, 0.28, 0.34], [0.16, 0.24, 0.2, 0.4], [0.04, 0.32, 0.24, 0.39], [0.16, 0.51, 0.0, 0.33], [0.08, 0.31, 0.23, 0.39], [0.34, 0.16, 0.26, 0.24], [0.29, 0.27, 0.17, 0.27], [0.29, 0.27, 0.17, 0.27], [0.0, 0.25, 0.37, 0.37]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.31 0.2  0.29 0.2 ]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-142.0, -179.0, -94.0, -129.0, -113.0, -95.0, -106.0, -100.0, -153.0, -147.0] \n",
      " [-337.0, -230.0, -238.0, -248.0, -221.0, -300.0, -269.0, -174.0, -317.0, -362.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.58, 0.08, 0.01, 0.32], [0.34, 0.12, 0.18, 0.36], [0.52, 0.25, 0.2, 0.03], [0.41, 0.21, 0.17, 0.2], [0.19, 0.18, 0.3, 0.32], [0.3, 0.26, 0.21, 0.23], [0.43, 0.28, 0.0, 0.29], [0.41, 0.2, 0.19, 0.21], [0.41, 0.32, 0.0, 0.27], [0.32, 0.09, 0.0, 0.59]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.41 0.2  0.19 0.21]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-148.0, -120.0, -150.0, -92.0, -130.0, -120.0, -123.0, -109.0, -116.0, -118.0] \n",
      " [-210.0, -359.0, -235.0, -287.0, -337.0, -359.0, -309.0, -265.0, -239.0, -317.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.32, 0.2, 0.0, 0.48], [0.2, 0.27, 0.21, 0.32], [0.35, 0.25, 0.0, 0.4], [0.27, 0.24, 0.23, 0.25], [0.41, 0.16, 0.0, 0.43], [0.2, 0.27, 0.21, 0.32], [0.28, 0.26, 0.0, 0.47], [0.43, 0.22, 0.0, 0.35], [0.31, 0.16, 0.09, 0.44], [0.46, 0.22, 0.0, 0.32]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.56 0.22 0.02 0.2 ]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-170.0, -198.0, -152.0, -138.0, -142.0, -128.0, -217.0, -132.0, -160.0, -158.0] \n",
      " [-264.0, -227.0, -230.0, -245.0, -184.0, -240.0, -238.0, -219.0, -219.0, -261.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.37, 0.41, 0.0, 0.22], [0.43, 0.22, 0.0, 0.35], [0.38, 0.5, 0.0, 0.11], [0.32, 0.33, 0.0, 0.36], [0.46, 0.32, 0.0, 0.22], [0.37, 0.33, 0.0, 0.3], [0.2, 0.55, 0.0, 0.24], [0.2, 0.37, 0.19, 0.24], [0.48, 0.34, 0.0, 0.19], [0.36, 0.38, 0.21, 0.05]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.32 0.19 0.28 0.21]\n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  832       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  546       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,095\n",
      "Trainable params: 12,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 5.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-326.0, -83.0, -462.0, -265.0, -182.0, -145.0, -147.0, -161.0, -119.0, -113.0] \n",
      " [-240.0, -240.0, -261.0, -271.0, -316.0, -210.0, -377.0, -212.0, -248.0, -261.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.0, 0.0, 0.98], [0.0, 0.05, 0.81, 0.14], [0.0, 0.0, 0.0, 1.0], [0.0, 0.07, 0.0, 0.93], [0.1, 0.76, 0.04, 0.1], [0.33, 0.03, 0.34, 0.3], [0.26, 0.06, 0.08, 0.6], [0.13, 0.13, 0.0, 0.74], [0.0, 0.04, 0.94, 0.02], [0.11, 0.07, 0.65, 0.17]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.4  0.14 0.31 0.15]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1199.0, -631.0, -1137.0, -1368.0, -1242.0, -1724.0, -1555.0, -1664.0, -1844.0, -1475.0] \n",
      " [-218.0, -261.0, -238.0, -218.0, -240.0, -203.0, -241.0, -183.0, -181.0, -236.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.36, 0.64], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.98, 0.02], [0.0, 0.0, 0.99, 0.01], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.04 0.02 0.65 0.29]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-855.0, -300.0, -1762.0, -1991.0, -1751.0, -1837.0, -641.0, -1656.0, -2271.0, -855.0] \n",
      " [-269.0, -273.0, -217.0, -283.0, -222.0, -224.0, -372.0, -240.0, -149.0, -274.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1676.0, -1834.0, -2281.0, -2303.0, -1243.0, -2162.0, -591.0, -1361.0, -1919.0, -1762.0] \n",
      " [-230.0, -221.0, -164.0, -158.0, -278.0, -191.0, -377.0, -248.0, -236.0, -235.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1730.0, -1526.0, -279.0, -1187.0, -959.0, -1242.0, -1154.0, -1472.0, -1050.0, -335.0] \n",
      " [-244.0, -193.0, -248.0, -264.0, -265.0, -278.0, -261.0, -230.0, -253.0, -297.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  832       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  546       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,095\n",
      "Trainable params: 12,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 0.5 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1697.0, -665.0, -2323.0, -869.0, -2261.0, -2082.0, -1053.0, -167.0, -1094.0, -2287.0] \n",
      " [-239.0, -372.0, -165.0, -300.0, -202.0, -207.0, -253.0, -126.0, -252.0, -181.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1edf018df2e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_step_size\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Agent_class.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[1;31m# performs a full training step on the collected batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[1;31m# note: no need to mess around with gradients, Keras API handles it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                 \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0macts_and_advs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;31m#def save_agent(self)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3510\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3512\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 572\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 445\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    446\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method ACAgent._logits_loss of <Agent_class.ACAgent object at 0x000002BA33964AC8>>,\n",
       " <bound method ACAgent._value_loss of <Agent_class.ACAgent object at 0x000002BA33964AC8>>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agents[0].model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                     4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                     5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                     6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                     7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [12],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.13463735580444336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1bbe8d2ca58>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  780       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  1281      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  462       \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  22        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  780       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1281      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  462       \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  176       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 5,244\n",
      "Trainable params: 5,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 2000\n",
    "n_step_size = 16\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary8.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Entropy reduced to 1900.0 \n",
      "Agent 0 : Entropy reduced to 1805.0 \n",
      "Agent 0 : Entropy reduced to 1714.75 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -16.0, -17.0, -19.0, -19.0, -11.0, -9.0, -10.0, -19.0, -12.0] \n",
      " [-1282.0, -1466.0, -1467.0, -1512.0, -1508.0, -1245.0, -1169.0, -1294.0, -1508.0, -1324.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.16, 0.0, 0.31, 0.0, 0.42, 0.06, 0.04, 0.0], [0.25, 0.0, 0.1, 0.03, 0.43, 0.13, 0.06, 0.0], [0.23, 0.0, 0.14, 0.05, 0.28, 0.2, 0.09, 0.0], [0.14, 0.0, 0.28, 0.1, 0.1, 0.26, 0.11, 0.0], [0.14, 0.0, 0.35, 0.11, 0.08, 0.22, 0.1, 0.0], [0.26, 0.0, 0.15, 0.01, 0.45, 0.08, 0.05, 0.0], [0.19, 0.0, 0.39, 0.01, 0.16, 0.15, 0.1, 0.0], [0.14, 0.0, 0.21, 0.0, 0.57, 0.05, 0.04, 0.0], [0.14, 0.0, 0.35, 0.11, 0.08, 0.22, 0.1, 0.0], [0.16, 0.0, 0.07, 0.0, 0.68, 0.06, 0.03, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.13 0.13 0.12 0.12 0.12 0.13 0.13 0.13]\n",
      "Agent 0 : Entropy reduced to 1629.0124999999998 \n",
      "Agent 0 : Entropy reduced to 1547.5618749999996 \n",
      "Agent 0 : Entropy reduced to 1470.1837812499996 \n",
      "Agent 0 : Entropy reduced to 1396.6745921874995 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -25.0, -34.0, -33.0, -33.0, -43.0, -9.0, -45.0, -36.0, -12.0] \n",
      " [-1340.0, -1264.0, -1340.0, -1297.0, -1392.0, -1554.0, -525.0, -1498.0, -1353.0, -359.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.11, 0.36, 0.22, 0.0, 0.24, 0.0, 0.01, 0.06], [0.15, 0.26, 0.2, 0.0, 0.21, 0.01, 0.03, 0.14], [0.11, 0.36, 0.22, 0.0, 0.24, 0.0, 0.01, 0.06], [0.14, 0.27, 0.28, 0.0, 0.19, 0.0, 0.01, 0.11], [0.14, 0.35, 0.17, 0.0, 0.25, 0.0, 0.01, 0.07], [0.1, 0.52, 0.07, 0.0, 0.29, 0.0, 0.0, 0.02], [0.17, 0.15, 0.13, 0.05, 0.15, 0.08, 0.12, 0.15], [0.1, 0.63, 0.07, 0.0, 0.18, 0.0, 0.0, 0.01], [0.11, 0.35, 0.24, 0.0, 0.22, 0.0, 0.01, 0.07], [0.18, 0.15, 0.12, 0.04, 0.17, 0.07, 0.12, 0.15]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.12 0.12 0.12 0.13 0.13]\n",
      "Agent 0 : Entropy reduced to 1326.8408625781244 \n",
      "Agent 0 : Entropy reduced to 1260.4988194492182 \n",
      "Agent 0 : Entropy reduced to 1197.4738784767574 \n",
      "Agent 0 : Entropy reduced to 1137.6001845529195 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-42.0, -15.0, -44.0, -49.0, -29.0, -41.0, -7.0, -41.0, -45.0, -43.0] \n",
      " [-1310.0, -282.0, -1575.0, -1586.0, -1153.0, -1285.0, -317.0, -1513.0, -1577.0, -1577.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.13, 0.12, 0.11, 0.12, 0.11, 0.18, 0.14, 0.1], [0.11, 0.11, 0.1, 0.14, 0.11, 0.14, 0.16, 0.14], [0.1, 0.16, 0.13, 0.08, 0.13, 0.15, 0.15, 0.1], [0.12, 0.12, 0.11, 0.15, 0.15, 0.09, 0.12, 0.15], [0.13, 0.13, 0.11, 0.11, 0.1, 0.15, 0.14, 0.14], [0.11, 0.12, 0.12, 0.16, 0.14, 0.08, 0.11, 0.15], [0.1, 0.14, 0.16, 0.11, 0.1, 0.13, 0.14, 0.12], [0.14, 0.11, 0.11, 0.14, 0.11, 0.14, 0.14, 0.1], [0.12, 0.1, 0.11, 0.17, 0.14, 0.09, 0.12, 0.14], [0.1, 0.16, 0.14, 0.08, 0.14, 0.14, 0.14, 0.1]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.12 0.13 0.12 0.13 0.12 0.13 0.13 0.13]\n",
      "Agent 0 : Entropy reduced to 1080.7201753252734 \n",
      "Agent 0 : Entropy reduced to 1026.6841665590098 \n",
      "Agent 0 : Entropy reduced to 975.3499582310592 \n",
      "Agent 0 : Entropy reduced to 926.5824603195061 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-84.0, -77.0, -79.0, -68.0, -88.0, -82.0, -44.0, -78.0, -89.0, -85.0] \n",
      " [-1293.0, -1384.0, -1567.0, -431.0, -1054.0, -689.0, -242.0, -1577.0, -1559.0, -1259.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.15, 0.11, 0.11, 0.14, 0.12, 0.12, 0.12, 0.12], [0.14, 0.1, 0.11, 0.15, 0.12, 0.13, 0.12, 0.13], [0.13, 0.12, 0.12, 0.11, 0.12, 0.14, 0.13, 0.12], [0.13, 0.12, 0.11, 0.13, 0.13, 0.13, 0.13, 0.12], [0.15, 0.11, 0.11, 0.13, 0.12, 0.14, 0.12, 0.13], [0.09, 0.17, 0.17, 0.06, 0.13, 0.13, 0.12, 0.12], [0.14, 0.14, 0.1, 0.1, 0.09, 0.13, 0.13, 0.17], [0.14, 0.11, 0.1, 0.16, 0.12, 0.14, 0.12, 0.11], [0.11, 0.14, 0.13, 0.1, 0.14, 0.13, 0.13, 0.11], [0.14, 0.1, 0.11, 0.15, 0.12, 0.13, 0.12, 0.12]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.11 0.14 0.13 0.13 0.12 0.13 0.13 0.13]\n",
      "Agent 0 : Entropy reduced to 880.2533373035308 \n",
      "Agent 0 : Entropy reduced to 836.2406704383542 \n",
      "Agent 0 : Entropy reduced to 794.4286369164364 \n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-129.0, -67.0, -151.0, -136.0, -126.0, -131.0, -132.0, -134.0, -9.0, -145.0] \n",
      " [-1156.0, -889.0, -1486.0, -1409.0, -1328.0, -1455.0, -1394.0, -1550.0, -171.0, -1299.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.15, 0.12, 0.13, 0.12, 0.1, 0.13, 0.14, 0.11], [0.15, 0.1, 0.11, 0.17, 0.1, 0.12, 0.14, 0.12], [0.11, 0.14, 0.13, 0.13, 0.13, 0.1, 0.13, 0.12], [0.14, 0.13, 0.14, 0.13, 0.1, 0.14, 0.14, 0.08], [0.13, 0.13, 0.12, 0.16, 0.11, 0.14, 0.13, 0.08], [0.13, 0.14, 0.12, 0.15, 0.11, 0.13, 0.14, 0.08], [0.14, 0.11, 0.13, 0.14, 0.1, 0.12, 0.13, 0.12], [0.09, 0.14, 0.14, 0.11, 0.15, 0.11, 0.14, 0.12], [0.12, 0.13, 0.12, 0.15, 0.13, 0.11, 0.11, 0.13], [0.15, 0.13, 0.14, 0.1, 0.1, 0.11, 0.14, 0.13]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.11 0.13 0.13 0.13 0.12 0.13 0.13 0.12]\n",
      "Agent 0 : Entropy reduced to 754.7072050706146 \n",
      "Agent 0 : Entropy reduced to 716.9718448170838 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7938735d5891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[1;31m#print(actions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Agent_class.py\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Agent_class.py\u001b[0m in \u001b[0;36maction_value\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;31m# executes call() under the hood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;31m# a simpler option, will become clear later why we don't use it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;31m# Setup work for each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    830\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_output_loss_metrics'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_loss_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m         \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m     \u001b[1;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \"\"\"\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3054\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3055\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3056\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3057\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3058\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m   1145\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m-> 1147\u001b[1;33m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[0;32m   1148\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"AssignVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         value)\n\u001b[0m\u001b[0;32m    183\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for i in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                Agents[idx].reduce_entropy()\n",
    "                print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
