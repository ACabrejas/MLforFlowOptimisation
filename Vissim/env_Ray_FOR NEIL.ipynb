{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_critic_class import ACAgent\n",
    "from MasterAC_Agent import MasterAC_Agent\n",
    "from MasterDQN_Agent import MasterDQN_Agent\n",
    "\n",
    "# Network Specific Libraries\n",
    "from Balance_Functions import balance_dictionary\n",
    "\n",
    "# General Libraries\n",
    "import numpy as np \n",
    "import pylab as plt\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = \"E:\\Backup - Onedrive\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\"\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "agent_type = 'AC'\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Agent hyperparameters\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "timesteps_per_second = 1\n",
    "\n",
    "\n",
    "# for the monitoring only for AC\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiAc_Agents.train(1000)\n",
    "\n",
    "Balance_MultiAc_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n",
    "\n",
    "Balance_MultiAc_Agents.load(best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Agents = []\n",
    "for idx, info in Balance_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        \n",
    "        \n",
    "        # Only for AC\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance DQN Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "## Initialization Parameters ##\n",
    "###############################\n",
    "\n",
    "intersection = \"1_2_4\"\n",
    "map_name  = 'Balance_int'+str(intersection)\n",
    "model_name = map_name\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = \"E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "\n",
    "## Simulation Parameters\n",
    "Random_Seed = 44\n",
    "sim_length = 9001\n",
    "timesteps_per_second = 1\n",
    "agent_type = \"DDQN\"\n",
    "#actions_set = 'default_actions'     # 'default_actions' or 'all_actions'\n",
    "actions_set = 'all_actions'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 500\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 5000\n",
    "batch_size = 128\n",
    "batches_per_episode = 10\n",
    "\n",
    "alpha = 0.00005\n",
    "gamma = 0.95\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "if intersection == \"1_2_4\":\n",
    "    intersection = 1\n",
    "elif intersection == \"11_12\":\n",
    "    intersection = 11\n",
    "partial_dictionary = {\"junctions\": { (intersection-1) : Balance_dictionary[\"junctions\"][intersection-1]},\\\n",
    "                      \"demand\": Balance_dictionary[\"demand\"]}\n",
    "\n",
    "Session_ID = map_name + \"_\" + actions_set + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type\n",
    "print(\"Current simulation: {}\".format(Session_ID))\n",
    "\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Deploy Agents\n",
    "Balance_int_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, partial_dictionary, actions_set,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, batches_per_episode, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)\n",
    "Balance_int_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Deploy Environment\n",
    "env = None\n",
    "env = environment(model_name, vissim_working_directory, sim_length, partial_dictionary, actions_set,\\\n",
    "                  Random_Seed = Random_Seed, timesteps_per_second = timesteps_per_second, mode = 'debug', delete_results = True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test to ensure correct deployment of agents\n",
    "\n",
    "# Test 1: Check number of agents\n",
    "print(env.SCUs.items())\n",
    "\n",
    "# Test 2: Check Dictionary for each agent\n",
    "agent = 0\n",
    "print(\"state_type: \" + env.SCUs[agent].state_type)\n",
    "print(\"state_size: \")\n",
    "print(env.SCUs[agent].state_size)\n",
    "print(\"reward_type: \")\n",
    "print(env.SCUs[agent].reward_type)\n",
    "print(\"compatible_actions: \")\n",
    "print(env.SCUs[agent].compatible_actions)\n",
    "print(\"all_actions: \")\n",
    "print(env.SCUs[agent].all_actions)\n",
    "print(\"Lanes_names: \" )\n",
    "print(env.SCUs[agent].Lanes_names)\n",
    "print(\"Links_names: \")\n",
    "print(env.SCUs[agent].Links_names)\n",
    "print(\"time_steps_per_second: \" + str(env.SCUs[agent].time_steps_per_second))\n",
    "print(\"queues_counter_ID: \" )\n",
    "print(env.SCUs[agent].queues_counter_ID)\n",
    "print(\"queues_counters: \")\n",
    "print(env.SCUs[agent].queues_counters)\n",
    "print(\"signal_controller: \")\n",
    "print(env.SCUs[agent].signal_controller)\n",
    "print(\"Signal_Groups: \" )\n",
    "print(env.SCUs[agent].signal_groups)\n",
    "print(\"Node: \" + str(env.SCUs[agent].Node))\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Tests to ensure correct STATE READING\n",
    "timesteps = 1\n",
    "for i in range(timesteps):\n",
    "    env.Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "## Test 3: Correct Reading of queues from QUEUE COUNTERS\n",
    "print(\"queues_counter_ID: \" )\n",
    "print(env.SCUs[0].queues_counter_ID)\n",
    "print([env.Vissim.Net.QueueCounters.ItemByKey(i).AttValue('QLen(Current, Last)') for i in env.SCUs[0].queues_counter_ID])\n",
    "    \n",
    "# Test 4: Correct Reading of Aggregated Queues by SCU\n",
    "print(env.SCUs[0].calculate_queues())\n",
    "\n",
    "## Test 5: Correct Reading of Global Queues by ENVIRONMENT\n",
    "print(env.get_queues())\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 6: Correct Reading of Initial State, and Generation of according actions\n",
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "print(\"Dict([(Agent_ID, array(state))])\")\n",
    "print(start_state.items())\n",
    "print(\"\")\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = Balance_int_MultiDQN_Agents.Agents[idx].choose_action(s)\n",
    "print(\"{Agent_ID : Chosen_Action}\")\n",
    "print(actions)\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 7: Correct Reading of General State from SCU and Generation of according actions\n",
    "SARSDs = env.step_to_next_action(actions)\n",
    "actions = dict()\n",
    "for idx , sarsd in SARSDs.items():\n",
    "    s,a,r,ns,d = sarsd\n",
    "    \n",
    "print(\"Agent_ID: \" + str(SARSDs.keys()))\n",
    "print(\"Agent_State:\")\n",
    "print(SARSDs[0][0][0])\n",
    "print(\"Agent_Action: \" + str(SARSDs[0][1]))\n",
    "print(\"Agent_Reward: \" + str(SARSDs[0][2]))\n",
    "print(\"Agent_Next_State:\")\n",
    "print(SARSDs[0][3][0])\n",
    "print(\"Done: \" + str(SARSDs[0][4]))\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 8: Correct operation of signal groups\n",
    "signal_group = 5\n",
    "env.SCUs[0].signal_groups[signal_group].SetAttValue(\"SigState\", \"GREEN\")\n",
    "env.Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###### Test 9-1: Correct implementation of actions (SETUP)\n",
    "for idx, agent in Balance_int_MultiDQN_Agents.Agents.items():\n",
    "    agent.reset()\n",
    "\n",
    "start_state = env.get_state()\n",
    "print(\"Initial State: {Agent_ID: initual queues}\")\n",
    "print(start_state)\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = Balance_int_MultiDQN_Agents.Agents[idx].choose_action(s)\n",
    "print(\"Initial Choice of Actions: {Agent_ID: action}\")    \n",
    "print(actions)\n",
    "\n",
    "# That is not a clean way to do this\n",
    "def to_dictionary(dictionary,idx,value):\n",
    "    \"\"\"\n",
    "    Assign a value to an index in a dictionary\n",
    "    \"\"\"\n",
    "    dictionary[idx] = value\n",
    "    \n",
    "## CORRECT - No apparent issues from this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Test 9-2: Correct implementation of actions (EXECUTION)\n",
    "##\n",
    "## ATTENTION: If an \"index out of range\" is requested, the system will break an will\\\n",
    "##            require a reset. This does not affect normal simulation.\n",
    "\n",
    "actions[0] = 4\n",
    "# This is step_to_next_action() function\n",
    "while not env.action_required:\n",
    "    \n",
    "    # This is the step() function\n",
    "    Sarsd = dict()\n",
    "    \n",
    "    # The default position is that no action is required, only a step of simulator\n",
    "    env.action_required = False\n",
    "    #print(\"false 1\")\n",
    "    \n",
    "    [scu.action_update(actions[0] , green_time = 5 ) for idx, scu in env.SCUs.items() if scu.action_required]\n",
    "    \n",
    "    [scu.update() for idx,scu in env.SCUs.items()]\n",
    "    \n",
    "    env.Vissim.Simulation.RunSingleStep()\n",
    "    \n",
    "    [to_dictionary(Sarsd,idx,scu.sars()+[env.done]) for idx,scu in env.SCUs.items() if scu.action_required ]\n",
    "    \n",
    "    if len(Sarsd) > 0 or env.done :\n",
    "        env.action_required = True\n",
    "        #print(\"TRUE\")\n",
    "    \n",
    "    print(Sarsd)\n",
    "        \n",
    "env.action_required = False\n",
    "#print(\"false 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 10: Correct changing of phases based on actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Test 11: Correct calculation of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL DQN Partial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current simulation: Balance_int14_all_actions_500_10800_DuellingDDQN_Queues_rework\n"
     ]
    }
   ],
   "source": [
    "intersection = 14\n",
    "map_name  = 'Balance_int'+str(intersection)\n",
    "model_name = map_name\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = \"E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "\n",
    "## Simulation Parameters\n",
    "Random_Seed = 10\n",
    "sim_length = 10801\n",
    "timesteps_per_second = 1\n",
    "agent_type = \"DuellingDDQN\"\n",
    "actions = 'all_actions'     # 'default_actions' or 'all_actions'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 500\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "batches_per_episode = 10\n",
    "\n",
    "alpha = 0.0005\n",
    "gamma = 0.95\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "if intersection == \"1_2_4\":\n",
    "    intersection = 1\n",
    "elif intersection == \"2_4\":\n",
    "    intersection = 2\n",
    "elif intersection == \"11_12\":\n",
    "    intersection = 11\n",
    "partial_dictionary = {\"junctions\": { (intersection-1) : Balance_dictionary[\"junctions\"][intersection-1]},\\\n",
    "                      \"demand\": Balance_dictionary[\"demand\"]}\n",
    "\n",
    "Session_ID = map_name + \"_\" + actions + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type + \"_Queues_rework\"\n",
    "print(\"Current simulation: {}\".format(Session_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"linear\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 13: SETTING UP AGENT\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 24)           168         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 24)           600         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 24)           600         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 24)           600         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            25          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            75          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_5[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,068\n",
      "Trainable params: 2,068\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Deep Q Learning Agent(s) at Intersection 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, partial_dictionary, actions,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, batches_per_episode, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience file not found. Generating now...\n",
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int8.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 10801 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 10\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.1 seconds.\n",
      "\n",
      "After 0 actions taken by the Agents,  Agent 0 memory is 0.0 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 0 memory is 10.0 percent full\n",
      "Random Seed Set to 11\n",
      "After 2000 actions taken by the Agents,  Agent 0 memory is 20.0 percent full\n",
      "Random Seed Set to 12\n",
      "After 3000 actions taken by the Agents,  Agent 0 memory is 30.0 percent full\n",
      "Random Seed Set to 13\n",
      "After 4000 actions taken by the Agents,  Agent 0 memory is 40.0 percent full\n",
      "Random Seed Set to 14\n",
      "After 5000 actions taken by the Agents,  Agent 0 memory is 50.0 percent full\n",
      "Random Seed Set to 15\n",
      "After 6000 actions taken by the Agents,  Agent 0 memory is 60.0 percent full\n",
      "Random Seed Set to 16\n",
      "After 7000 actions taken by the Agents,  Agent 0 memory is 70.0 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 0 memory is 80.0 percent full\n",
      "Random Seed Set to 17\n",
      "After 9000 actions taken by the Agents,  Agent 0 memory is 90.0 percent full\n",
      "Random Seed Set to 18\n",
      "Memory filled. Saving as:C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Balance_int8\\Agents_Results\\DuellingDDQN\\Balance_int8_all_actions_500_10800_DuellingDDQN_Queues_rework\\Agent0_PERPre_10000.p\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int14.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 10801 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 10\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.08 seconds.\n",
      "\n",
      "start\n",
      "Random Seed Set to 11\n",
      "Episode 281: Finished running.\n",
      "Agent 0, Average Reward: -453.94\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13250612.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12687389.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11679804.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10574019.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9672270.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8384280.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7467679.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6584453.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5786766.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5008594.0000\n",
      "Reducing exploration for all agents to 0.331\n",
      "\n",
      "Episode 282: Starting computation.\n",
      "Random Seed Set to 12\n",
      "Episode 282: Finished running.\n",
      "Agent 0, Average Reward: -447.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4250178.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3644369.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3121030.2500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2682619.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2342646.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1996439.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1932506.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1461671.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1339379.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1137613.2500\n",
      "Reducing exploration for all agents to 0.3286\n",
      "\n",
      "Episode 283: Starting computation.\n",
      "Random Seed Set to 13\n",
      "Episode 283: Finished running.\n",
      "Agent 0, Average Reward: -454.48\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1002171.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 815087.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 755997.4375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 646774.4375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 628781.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 549609.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 508906.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 472701.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 402165.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 408362.2500\n",
      "Reducing exploration for all agents to 0.3262\n",
      "\n",
      "Episode 284: Starting computation.\n",
      "Random Seed Set to 14\n",
      "Episode 284: Finished running.\n",
      "Agent 0, Average Reward: -454.59\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 411645.2812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 464331.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 337622.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 322749.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 295567.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 372183.0938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 282334.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 271856.9062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 244876.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 282791.3438\n",
      "Reducing exploration for all agents to 0.3239\n",
      "\n",
      "Episode 285: Starting computation.\n",
      "Random Seed Set to 15\n",
      "Episode 285: Finished running.\n",
      "Agent 0, Average Reward: -438.12\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 221250.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 244750.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 232055.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 202697.8281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 242700.7031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 193248.8281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 181427.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 201405.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 190058.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 176845.5469\n",
      "Reducing exploration for all agents to 0.3215\n",
      "\n",
      "Episode 286: Starting computation.\n",
      "Random Seed Set to 16\n",
      "Episode 286: Finished running.\n",
      "Agent 0, Average Reward: -452.15\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 156336.0469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 142662.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 151454.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 123432.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 132150.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 165027.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 159474.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 129973.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 152804.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 140347.6719\n",
      "Reducing exploration for all agents to 0.3191\n",
      "\n",
      "Episode 287: Starting computation.\n",
      "Random Seed Set to 17\n",
      "Episode 287: Finished running.\n",
      "Agent 0, Average Reward: -454.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 156350.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 137131.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 143543.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 143455.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 148911.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 138135.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 123279.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 97630.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 105439.0625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 91882.2500\n",
      "Reducing exploration for all agents to 0.3167\n",
      "\n",
      "Episode 288: Starting computation.\n",
      "Random Seed Set to 18\n",
      "Episode 288: Finished running.\n",
      "Agent 0, Average Reward: -456.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 98454.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 96020.7578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 86619.3594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 96215.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 80801.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 67207.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 104601.2812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 102861.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 97530.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 82953.7031\n",
      "Reducing exploration for all agents to 0.3143\n",
      "\n",
      "Episode 289: Starting computation.\n",
      "Random Seed Set to 19\n",
      "Episode 289: Finished running.\n",
      "Agent 0, Average Reward: -453.59\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 94065.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81369.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 80413.0234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78222.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 75945.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59398.0195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 82445.8281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 72832.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 83025.3750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68047.8281\n",
      "Reducing exploration for all agents to 0.312\n",
      "\n",
      "Episode 290: Starting computation.\n",
      "Random Seed Set to 20\n",
      "Episode 290: Finished running.\n",
      "Agent 0, Average Reward: -449.66\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66883.3906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 70690.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 73222.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 73046.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62684.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58798.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60511.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59427.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 63187.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60417.5391\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.3096\n",
      "\n",
      "Episode 291: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 21\n",
      "Episode 291: Finished running.\n",
      "Agent 0, Average Reward: -449.57\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 79525.4688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78672.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 73064.4375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 75663.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 71775.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 70579.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 72333.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66563.4688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 67633.1016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 61264.8711\n",
      "Reducing exploration for all agents to 0.3072\n",
      "\n",
      "Episode 292: Starting computation.\n",
      "Random Seed Set to 22\n",
      "Episode 292: Finished running.\n",
      "Agent 0, Average Reward: -475.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62881.9414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59447.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57381.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52149.5195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56397.0117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50564.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46773.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47306.6992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44096.9023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41570.3242\n",
      "Reducing exploration for all agents to 0.3048\n",
      "\n",
      "Episode 293: Starting computation.\n",
      "Random Seed Set to 23\n",
      "Episode 293: Finished running.\n",
      "Agent 0, Average Reward: -454.22\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41037.5820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41984.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43446.7383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37117.9141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39888.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39051.4727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41507.6328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37611.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35275.0430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30005.0703\n",
      "Reducing exploration for all agents to 0.3024\n",
      "\n",
      "Episode 294: Starting computation.\n",
      "Random Seed Set to 24\n",
      "Episode 294: Finished running.\n",
      "Agent 0, Average Reward: -460.25\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32617.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30460.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33752.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33585.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26993.6934\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28027.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33201.2305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26978.9336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28569.6465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33176.7227\n",
      "Reducing exploration for all agents to 0.3001\n",
      "\n",
      "Episode 295: Starting computation.\n",
      "Random Seed Set to 25\n",
      "Episode 295: Finished running.\n",
      "Agent 0, Average Reward: -461.27\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26930.9590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25839.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26488.2871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22320.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25811.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25716.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25867.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24838.1680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22933.9941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21668.0781\n",
      "Reducing exploration for all agents to 0.2977\n",
      "\n",
      "Episode 296: Starting computation.\n",
      "Random Seed Set to 26\n",
      "Episode 296: Finished running.\n",
      "Agent 0, Average Reward: -457.03\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21432.4727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20297.3906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26906.5371\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23397.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23686.6816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22455.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17001.7324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22451.9238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21391.7598\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18269.4863\n",
      "Reducing exploration for all agents to 0.2953\n",
      "\n",
      "Episode 297: Starting computation.\n",
      "Random Seed Set to 27\n",
      "Episode 297: Finished running.\n",
      "Agent 0, Average Reward: -451.85\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21312.8184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20186.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20119.6777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17494.3594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12796.5566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17982.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17624.5254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19237.9980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15676.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17054.2656\n",
      "Reducing exploration for all agents to 0.2929\n",
      "\n",
      "Episode 298: Starting computation.\n",
      "Random Seed Set to 28\n",
      "Episode 298: Finished running.\n",
      "Agent 0, Average Reward: -455.91\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18549.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18925.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16982.6504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19027.4551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17773.3184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16452.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18587.0859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15488.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17476.7168\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14851.5156\n",
      "Reducing exploration for all agents to 0.2905\n",
      "\n",
      "Episode 299: Starting computation.\n",
      "Random Seed Set to 29\n",
      "Episode 299: Finished running.\n",
      "Agent 0, Average Reward: -458.46\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13818.4160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15043.9736\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13077.2598\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16032.8945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14889.7803\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14352.8652\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18922.5996\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16746.3359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9269.1152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15432.4629\n",
      "Reducing exploration for all agents to 0.2882\n",
      "\n",
      "Episode 300: Starting computation.\n",
      "Random Seed Set to 30\n",
      "Episode 300: Finished running.\n",
      "Agent 0, Average Reward: -453.47\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16872.8262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11955.4648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13222.2480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13006.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13627.3291\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10304.0566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12202.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11558.8604\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11697.4688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15680.8027\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.2858\n",
      "\n",
      "Episode 301: Starting computation.\n",
      "Random Seed Set to 31\n",
      "Episode 301: Finished running.\n",
      "Agent 0, Average Reward: -472.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52715.4961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52958.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51889.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51254.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49966.1367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49166.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47507.1133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43575.4023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47413.5742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45457.2266\n",
      "Reducing exploration for all agents to 0.2834\n",
      "\n",
      "Episode 302: Starting computation.\n",
      "Random Seed Set to 32\n",
      "Episode 302: Finished running.\n",
      "Agent 0, Average Reward: -478.63\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40573.1758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38292.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37369.1836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34651.0586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34105.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32460.9785\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31046.4473\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27252.6113\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26960.4551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26073.0391\n",
      "Reducing exploration for all agents to 0.281\n",
      "\n",
      "Episode 303: Starting computation.\n",
      "Random Seed Set to 33\n",
      "Episode 303: Finished running.\n",
      "Agent 0, Average Reward: -457.76\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22106.6582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22279.9043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21600.6113\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19847.9121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17759.6543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17907.1543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18013.5957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14924.0723\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14471.7441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14469.6514\n",
      "Reducing exploration for all agents to 0.2786\n",
      "\n",
      "Episode 304: Starting computation.\n",
      "Random Seed Set to 34\n",
      "Episode 304: Finished running.\n",
      "Agent 0, Average Reward: -463.5\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11914.1631\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13024.2842\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13568.1465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11045.3135\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10739.2949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10770.4824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9275.9395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9842.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7442.3081\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9178.5684\n",
      "Reducing exploration for all agents to 0.2762\n",
      "\n",
      "Episode 305: Starting computation.\n",
      "Random Seed Set to 35\n",
      "Episode 305: Finished running.\n",
      "Agent 0, Average Reward: -463.49\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8294.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7340.8130\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6960.4888\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6831.0171\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6905.8296\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5700.6235\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6544.8369\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7588.9326\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6262.7749\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5817.6172\n",
      "Reducing exploration for all agents to 0.2739\n",
      "\n",
      "Episode 306: Starting computation.\n",
      "Random Seed Set to 36\n",
      "Episode 306: Finished running.\n",
      "Agent 0, Average Reward: -453.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5302.0220\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5288.3447\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4457.2861\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6002.8613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5259.5112\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5175.5068\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5273.1416\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4494.8701\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5670.8325\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4448.6055\n",
      "Reducing exploration for all agents to 0.2715\n",
      "\n",
      "Episode 307: Starting computation.\n",
      "Random Seed Set to 37\n",
      "Episode 307: Finished running.\n",
      "Agent 0, Average Reward: -470.69\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4603.3271\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4712.5508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5215.8359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5692.0088\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5547.9082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4202.6338\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4179.1128\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5792.4775\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6594.4678\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4438.2666\n",
      "Reducing exploration for all agents to 0.2691\n",
      "\n",
      "Episode 308: Starting computation.\n",
      "Random Seed Set to 38\n",
      "Episode 308: Finished running.\n",
      "Agent 0, Average Reward: -455.67\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5244.6377\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4566.5957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3389.2771\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4129.9189\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3286.0654\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3626.1262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3503.1125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4899.9521\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4449.7061\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3466.3391\n",
      "Reducing exploration for all agents to 0.2667\n",
      "\n",
      "Episode 309: Starting computation.\n",
      "Random Seed Set to 39\n",
      "Episode 309: Finished running.\n",
      "Agent 0, Average Reward: -466.49\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4162.8638\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4277.0137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4462.1333\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3908.4700\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3270.2554\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4343.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4053.0100\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5156.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3515.2615\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4228.4658\n",
      "Reducing exploration for all agents to 0.2643\n",
      "\n",
      "Episode 310: Starting computation.\n",
      "Random Seed Set to 40\n",
      "Episode 310: Finished running.\n",
      "Agent 0, Average Reward: -461.48\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3598.9578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3062.4221\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4134.0498\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3179.4099\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3222.5471\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3014.0474\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3656.1345\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3121.6138\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2977.3157\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3255.7976\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.262\n",
      "\n",
      "Episode 311: Starting computation.\n",
      "Random Seed Set to 41\n",
      "Episode 311: Finished running.\n",
      "Agent 0, Average Reward: -453.6\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53571.0469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52822.0117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52396.4766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50999.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49130.8477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48221.7773\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47675.9414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45424.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43057.7031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42789.0664\n",
      "Reducing exploration for all agents to 0.2596\n",
      "\n",
      "Episode 312: Starting computation.\n",
      "Random Seed Set to 42\n",
      "Episode 312: Finished running.\n",
      "Agent 0, Average Reward: -474.35\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40890.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41058.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38376.4570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34999.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33606.5117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30996.8340\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30004.1836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28810.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26729.4082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24795.4180\n",
      "Reducing exploration for all agents to 0.2572\n",
      "\n",
      "Episode 313: Starting computation.\n",
      "Random Seed Set to 43\n",
      "Episode 313: Finished running.\n",
      "Agent 0, Average Reward: -565.32\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27630.3691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24871.3457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24457.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21713.7246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20615.3965\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19969.5410\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19869.0879\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17312.8301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17260.5371\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16264.7363\n",
      "Reducing exploration for all agents to 0.2548\n",
      "\n",
      "Episode 314: Starting computation.\n",
      "Random Seed Set to 44\n",
      "Episode 314: Finished running.\n",
      "Agent 0, Average Reward: -918.11\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21396.1230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19888.8340\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22019.1465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17878.8613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15337.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15333.0059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12717.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12139.7324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11092.4834\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9629.1211\n",
      "Reducing exploration for all agents to 0.2524\n",
      "\n",
      "Episode 315: Starting computation.\n",
      "Random Seed Set to 45\n",
      "Episode 315: Finished running.\n",
      "Agent 0, Average Reward: -630.3\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9892.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15237.3945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11299.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8340.5918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8368.0713\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9696.9502\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10962.2607\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9734.7412\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8866.8389\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6679.5117\n",
      "Reducing exploration for all agents to 0.2501\n",
      "\n",
      "Episode 316: Starting computation.\n",
      "Random Seed Set to 46\n",
      "Episode 316: Finished running.\n",
      "Agent 0, Average Reward: -599.56\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8130.9917\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7755.5674\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8219.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7247.3472\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12314.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7094.1084\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7162.0557\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6260.2466\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7622.2842\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8169.5078\n",
      "Reducing exploration for all agents to 0.2477\n",
      "\n",
      "Episode 317: Starting computation.\n",
      "Random Seed Set to 47\n",
      "Episode 317: Finished running.\n",
      "Agent 0, Average Reward: -576.1\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11790.3047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5356.4946\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5759.7671\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11608.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6206.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9605.0332\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5677.0576\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6151.9434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6119.5518\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4245.0938\n",
      "Reducing exploration for all agents to 0.2453\n",
      "\n",
      "Episode 318: Starting computation.\n",
      "Random Seed Set to 48\n",
      "Episode 318: Finished running.\n",
      "Agent 0, Average Reward: -533.37\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8822.9521\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11201.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9973.2998\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5680.8828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7940.3311\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4044.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4376.6396\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5838.8994\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6110.0010\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11805.2246\n",
      "Reducing exploration for all agents to 0.2429\n",
      "\n",
      "Episode 319: Starting computation.\n",
      "Random Seed Set to 49\n",
      "Episode 319: Finished running.\n",
      "Agent 0, Average Reward: -512.68\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4280.2490\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7256.9741\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4091.0845\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5703.7783\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6623.1543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4333.1792\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8460.2959\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4265.4941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4548.0645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5782.7627\n",
      "Reducing exploration for all agents to 0.2405\n",
      "\n",
      "Episode 320: Starting computation.\n",
      "Random Seed Set to 50\n",
      "Episode 320: Finished running.\n",
      "Agent 0, Average Reward: -497.08\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4456.7295\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5369.2900\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3413.7612\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5515.3350\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8507.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3299.3098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6785.9355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3711.7886\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6274.5088\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5313.2285\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.2382\n",
      "\n",
      "Episode 321: Starting computation.\n",
      "Random Seed Set to 51\n",
      "Episode 321: Finished running.\n",
      "Agent 0, Average Reward: -475.33\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78547.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 76361.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 71747.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 70328.4453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 67989.3750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59231.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56600.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47670.8242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43900.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40390.3438\n",
      "Reducing exploration for all agents to 0.2358\n",
      "\n",
      "Episode 322: Starting computation.\n",
      "Random Seed Set to 52\n",
      "Episode 322: Finished running.\n",
      "Agent 0, Average Reward: -480.34\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34021.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32971.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28350.1211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24240.4707\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20325.2891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15301.0732\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15499.0254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13705.2939\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10076.3623\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8989.7939\n",
      "Reducing exploration for all agents to 0.2334\n",
      "\n",
      "Episode 323: Starting computation.\n",
      "Random Seed Set to 53\n",
      "Episode 323: Finished running.\n",
      "Agent 0, Average Reward: -472.25\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10138.8027\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6902.4390\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6489.0894\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4585.2554\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4968.5210\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4052.9360\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4057.9104\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6064.1543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6963.2534\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3805.1572\n",
      "Reducing exploration for all agents to 0.231\n",
      "\n",
      "Episode 324: Starting computation.\n",
      "Random Seed Set to 54\n",
      "Episode 324: Finished running.\n",
      "Agent 0, Average Reward: -454.48\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5102.5249\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4411.7725\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4528.3477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3455.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3682.1895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7353.2446\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3760.5867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3890.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2401.0745\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4304.8296\n",
      "Reducing exploration for all agents to 0.2286\n",
      "\n",
      "Episode 325: Starting computation.\n",
      "Random Seed Set to 55\n",
      "Episode 325: Finished running.\n",
      "Agent 0, Average Reward: -474.83\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2821.1533\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3160.5862\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2125.8779\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2669.0571\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2411.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1603.2075\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2102.9165\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2042.7632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3638.8503\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2215.8872\n",
      "Reducing exploration for all agents to 0.2263\n",
      "\n",
      "Episode 326: Starting computation.\n",
      "Random Seed Set to 56\n",
      "Episode 326: Finished running.\n",
      "Agent 0, Average Reward: -451.97\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1256.0546\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1645.5659\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1581.5387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1398.5730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1451.7605\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1129.4626\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1403.7688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2457.1804\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2169.4102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1471.4410\n",
      "Reducing exploration for all agents to 0.2239\n",
      "\n",
      "Episode 327: Starting computation.\n",
      "Random Seed Set to 57\n",
      "Episode 327: Finished running.\n",
      "Agent 0, Average Reward: -408.42\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1921.3965\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2412.4163\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2065.5981\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1544.9420\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1263.3472\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2652.0010\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2451.4233\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1365.9536\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1338.3804\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1215.6561\n",
      "Reducing exploration for all agents to 0.2215\n",
      "\n",
      "Episode 328: Starting computation.\n",
      "Random Seed Set to 58\n",
      "Episode 328: Finished running.\n",
      "Agent 0, Average Reward: -414.11\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3110.0686\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4024.2791\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2311.2671\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2456.1343\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2063.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3542.6587\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1944.2675\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1668.2032\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2280.7505\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4235.5063\n",
      "Reducing exploration for all agents to 0.2191\n",
      "\n",
      "Episode 329: Starting computation.\n",
      "Random Seed Set to 59\n",
      "Episode 329: Finished running.\n",
      "Agent 0, Average Reward: -407.04\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3391.7454\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3176.5564\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3366.1824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4474.3638\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3398.0232\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3281.8271\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6028.3164\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 1746.4259\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4014.5776\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3662.1284\n",
      "Reducing exploration for all agents to 0.2167\n",
      "\n",
      "Episode 330: Starting computation.\n",
      "Random Seed Set to 60\n",
      "Episode 330: Finished running.\n",
      "Agent 0, Average Reward: -434.46\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4072.8960\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3773.4141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3959.9712\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3616.8667\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4597.9824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4663.9062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3081.0588\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3864.9443\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4145.3652\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3713.9607\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.2143\n",
      "\n",
      "Episode 331: Starting computation.\n",
      "Random Seed Set to 61\n",
      "Episode 331: Finished running.\n",
      "Agent 0, Average Reward: -440.3\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51759.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51323.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48659.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48583.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40694.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43408.8711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41261.2930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38028.2227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32972.2266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32219.5312\n",
      "Reducing exploration for all agents to 0.212\n",
      "\n",
      "Episode 332: Starting computation.\n",
      "Random Seed Set to 62\n",
      "Episode 332: Finished running.\n",
      "Agent 0, Average Reward: -1136.04\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47387.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40699.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32589.0215\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24616.1895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18665.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14924.2197\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17306.4434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14647.2119\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19803.7168\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19875.3262\n",
      "Reducing exploration for all agents to 0.2096\n",
      "\n",
      "Episode 333: Starting computation.\n",
      "Random Seed Set to 63\n",
      "Episode 333: Finished running.\n",
      "Agent 0, Average Reward: -877.04\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25693.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25516.2793\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17892.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18232.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11342.1152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12348.4922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12677.5127\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14008.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15544.0166\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16293.5967\n",
      "Reducing exploration for all agents to 0.2072\n",
      "\n",
      "Episode 334: Starting computation.\n",
      "Random Seed Set to 64\n",
      "Episode 334: Finished running.\n",
      "Agent 0, Average Reward: -848.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14022.7705\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14614.4834\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16273.6396\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10472.2910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10620.7256\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15517.4365\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13650.0576\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12522.8135\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15600.1006\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13828.9727\n",
      "Reducing exploration for all agents to 0.2048\n",
      "\n",
      "Episode 335: Starting computation.\n",
      "Random Seed Set to 65\n",
      "Episode 335: Finished running.\n",
      "Agent 0, Average Reward: -1120.6\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15503.8545\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12602.7979\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16443.6582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10125.4229\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8854.4961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8659.4072\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10990.1279\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11460.5850\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9623.9756\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10170.9775\n",
      "Reducing exploration for all agents to 0.2024\n",
      "\n",
      "Episode 336: Starting computation.\n",
      "Random Seed Set to 66\n",
      "Episode 336: Finished running.\n",
      "Agent 0, Average Reward: -790.86\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6779.7207\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9606.6904\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9048.6797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6896.7065\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9390.6064\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7086.5610\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19293.0293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8176.1851\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12677.0420\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15676.6182\n",
      "Reducing exploration for all agents to 0.2001\n",
      "\n",
      "Episode 337: Starting computation.\n",
      "Random Seed Set to 67\n",
      "Episode 337: Finished running.\n",
      "Agent 0, Average Reward: -1167.6\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10093.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8259.8115\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13903.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10837.0117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14760.5566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6531.3374\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13918.2949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6084.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19254.7207\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15571.5742\n",
      "Reducing exploration for all agents to 0.1977\n",
      "\n",
      "Episode 338: Starting computation.\n",
      "Random Seed Set to 68\n",
      "Episode 338: Finished running.\n",
      "Agent 0, Average Reward: -1026.43\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10113.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11578.2646\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13438.2129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7409.9917\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8173.7817\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25407.8457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4711.9082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10974.2803\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14990.5488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10991.9189\n",
      "Reducing exploration for all agents to 0.1953\n",
      "\n",
      "Episode 339: Starting computation.\n",
      "Random Seed Set to 69\n",
      "Episode 339: Finished running.\n",
      "Agent 0, Average Reward: -851.7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7282.7397\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6535.9043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12541.3037\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16004.4131\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10518.7061\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12972.1475\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10862.1064\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10138.4941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11871.7207\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8607.5215\n",
      "Reducing exploration for all agents to 0.1929\n",
      "\n",
      "Episode 340: Starting computation.\n",
      "Random Seed Set to 70\n",
      "Episode 340: Finished running.\n",
      "Agent 0, Average Reward: -1138.53\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6240.8315\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6455.7046\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11517.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10183.2764\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8439.6104\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15858.1631\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7450.2524\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5357.4292\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13365.2568\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10887.1934\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.1905\n",
      "\n",
      "Episode 341: Starting computation.\n",
      "Random Seed Set to 71\n",
      "Episode 341: Finished running.\n",
      "Agent 0, Average Reward: -850.72\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 207266.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 153392.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 99208.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45521.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16740.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11121.4512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37347.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52576.0352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 75590.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 77693.5781\n",
      "Reducing exploration for all agents to 0.1882\n",
      "\n",
      "Episode 342: Starting computation.\n",
      "Random Seed Set to 72\n",
      "Episode 342: Finished running.\n",
      "Agent 0, Average Reward: -671.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57427.8359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51949.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46077.4180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28060.4531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26481.3027\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25382.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20615.2109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25760.5898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36606.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37100.6406\n",
      "Reducing exploration for all agents to 0.1858\n",
      "\n",
      "Episode 343: Starting computation.\n",
      "Random Seed Set to 73\n",
      "Episode 343: Finished running.\n",
      "Agent 0, Average Reward: -937.62\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52060.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53069.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31070.8398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25900.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29318.1367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19403.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15264.3701\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55939.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21452.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22424.7969\n",
      "Reducing exploration for all agents to 0.1834\n",
      "\n",
      "Episode 344: Starting computation.\n",
      "Random Seed Set to 74\n",
      "Episode 344: Finished running.\n",
      "Agent 0, Average Reward: -474.51\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23276.9180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27344.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32490.4766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23734.3906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38280.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26919.6270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16450.3008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25423.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26217.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16936.2969\n",
      "Reducing exploration for all agents to 0.181\n",
      "\n",
      "Episode 345: Starting computation.\n",
      "Random Seed Set to 75\n",
      "Episode 345: Finished running.\n",
      "Agent 0, Average Reward: -798.37\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17743.9355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16143.0244\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16439.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13151.9795\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24774.7520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27431.7617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42889.6094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27707.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19842.4023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42556.6094\n",
      "Reducing exploration for all agents to 0.1786\n",
      "\n",
      "Episode 346: Starting computation.\n",
      "Random Seed Set to 76\n",
      "Episode 346: Finished running.\n",
      "Agent 0, Average Reward: -1010.41\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21152.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30415.8223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15177.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16503.0332\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35792.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30566.8457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29677.7871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12055.9209\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12505.0752\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47030.5391\n",
      "Reducing exploration for all agents to 0.1763\n",
      "\n",
      "Episode 347: Starting computation.\n",
      "Random Seed Set to 77\n",
      "Episode 347: Finished running.\n",
      "Agent 0, Average Reward: -776.75\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30047.2070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13066.8379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18924.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60291.3594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30839.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32773.6133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19898.8418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35763.6172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21258.3906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11586.3447\n",
      "Reducing exploration for all agents to 0.1739\n",
      "\n",
      "Episode 348: Starting computation.\n",
      "Random Seed Set to 78\n",
      "Episode 348: Finished running.\n",
      "Agent 0, Average Reward: -773.08\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24353.0586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37267.6914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16515.9863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41409.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12435.5518\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22034.6602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17558.2500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29395.4512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23720.0938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12894.4307\n",
      "Reducing exploration for all agents to 0.1715\n",
      "\n",
      "Episode 349: Starting computation.\n",
      "Random Seed Set to 79\n",
      "Episode 349: Finished running.\n",
      "Agent 0, Average Reward: -1171.93\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19455.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14483.8350\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41927.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32984.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21908.8867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20449.3730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23613.2070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30076.3184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23707.7773\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11112.0225\n",
      "Reducing exploration for all agents to 0.1691\n",
      "\n",
      "Episode 350: Starting computation.\n",
      "Random Seed Set to 80\n",
      "Episode 350: Finished running.\n",
      "Agent 0, Average Reward: -567.66\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27916.0820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17874.7383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15403.6309\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20138.9395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15874.6455\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16149.3447\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24451.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20453.4648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39178.9180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17428.2461\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1667\n",
      "\n",
      "Episode 351: Starting computation.\n",
      "Random Seed Set to 81\n",
      "Episode 351: Finished running.\n",
      "Agent 0, Average Reward: -907.07\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 174584.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 161633.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 133154.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 105534.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 61124.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52772.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29297.4355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16855.7266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43638.6836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25290.2168\n",
      "Reducing exploration for all agents to 0.1644\n",
      "\n",
      "Episode 352: Starting computation.\n",
      "Random Seed Set to 82\n",
      "Episode 352: Finished running.\n",
      "Agent 0, Average Reward: -598.31\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53589.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65129.1133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65853.3203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68384.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51277.7031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55974.4258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57544.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50678.6914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36606.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17674.9688\n",
      "Reducing exploration for all agents to 0.162\n",
      "\n",
      "Episode 353: Starting computation.\n",
      "Random Seed Set to 83\n",
      "Episode 353: Finished running.\n",
      "Agent 0, Average Reward: -549.77\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25726.7031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39704.4805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53139.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41721.9141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40906.9336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62002.7539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49196.1055\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45269.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41153.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47738.9961\n",
      "Reducing exploration for all agents to 0.1596\n",
      "\n",
      "Episode 354: Starting computation.\n",
      "Random Seed Set to 84\n",
      "Episode 354: Finished running.\n",
      "Agent 0, Average Reward: -839.8\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37717.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36477.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35667.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20735.4551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30606.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34695.3516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35167.8359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33264.0078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27806.6621\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35376.5391\n",
      "Reducing exploration for all agents to 0.1572\n",
      "\n",
      "Episode 355: Starting computation.\n",
      "Random Seed Set to 85\n",
      "Episode 355: Finished running.\n",
      "Agent 0, Average Reward: -482.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30498.4707\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38873.5430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17355.4512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25060.8398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30085.8652\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36624.0508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56254.3516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36391.8320\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21983.6348\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35359.7734\n",
      "Reducing exploration for all agents to 0.1548\n",
      "\n",
      "Episode 356: Starting computation.\n",
      "Random Seed Set to 86\n",
      "Episode 356: Finished running.\n",
      "Agent 0, Average Reward: -511.67\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33103.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30790.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21945.7383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57704.7461\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22506.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25616.3379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57226.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31622.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38441.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25296.7109\n",
      "Reducing exploration for all agents to 0.1524\n",
      "\n",
      "Episode 357: Starting computation.\n",
      "Random Seed Set to 87\n",
      "Episode 357: Finished running.\n",
      "Agent 0, Average Reward: -480.91\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33061.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49385.9414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24279.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44095.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57180.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37574.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43583.5586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31870.2441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22618.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47086.5156\n",
      "Reducing exploration for all agents to 0.1501\n",
      "\n",
      "Episode 358: Starting computation.\n",
      "Random Seed Set to 88\n",
      "Episode 358: Finished running.\n",
      "Agent 0, Average Reward: -1018.1\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47224.0039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25369.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44534.4531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56381.6758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53863.2148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48437.3906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31933.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47175.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21851.9746\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49024.8867\n",
      "Reducing exploration for all agents to 0.1477\n",
      "\n",
      "Episode 359: Starting computation.\n",
      "Random Seed Set to 89\n",
      "Episode 359: Finished running.\n",
      "Agent 0, Average Reward: -885.01\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52801.8398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41171.8867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35102.7422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50321.4922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40657.4922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34401.9023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50804.8242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 85897.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48733.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52368.7422\n",
      "Reducing exploration for all agents to 0.1453\n",
      "\n",
      "Episode 360: Starting computation.\n",
      "Random Seed Set to 90\n",
      "Episode 360: Finished running.\n",
      "Agent 0, Average Reward: -444.09\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 63159.4766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 75958.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53729.2266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49432.2500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39495.2148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45048.0625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50932.6992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32295.2812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35926.9609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38720.0859\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.1429\n",
      "\n",
      "Episode 361: Starting computation.\n",
      "Random Seed Set to 91\n",
      "Episode 361: Finished running.\n",
      "Agent 0, Average Reward: -1032.48\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 199824.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 192754.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 153009.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 142605.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 94865.3750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 113897.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 100150.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 67645.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 69714.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78179.1953\n",
      "Reducing exploration for all agents to 0.1405\n",
      "\n",
      "Episode 362: Starting computation.\n",
      "Random Seed Set to 92\n",
      "Episode 362: Finished running.\n",
      "Agent 0, Average Reward: -581.8\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 88964.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78315.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 147987.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 129244.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 119622.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 79664.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 71151.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 67738.3047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62590.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62561.4805\n",
      "Reducing exploration for all agents to 0.1382\n",
      "\n",
      "Episode 363: Starting computation.\n",
      "Random Seed Set to 93\n",
      "Episode 363: Finished running.\n",
      "Agent 0, Average Reward: -564.37\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62569.8398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60232.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66385.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 80877.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 88573.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 89181.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81773.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 95359.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78354.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64716.0000\n",
      "Reducing exploration for all agents to 0.1358\n",
      "\n",
      "Episode 364: Starting computation.\n",
      "Random Seed Set to 94\n",
      "Episode 364: Finished running.\n",
      "Agent 0, Average Reward: -469.63\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64277.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 76037.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 73040.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 94619.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64031.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48512.9141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 76776.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55092.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 72170.6094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 89063.8906\n",
      "Reducing exploration for all agents to 0.1334\n",
      "\n",
      "Episode 365: Starting computation.\n",
      "Random Seed Set to 95\n",
      "Episode 365: Finished running.\n",
      "Agent 0, Average Reward: -597.39\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 84720.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60651.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66442.6094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55231.3359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66141.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 76444.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 71802.0469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 104136.4688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 79325.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81556.1953\n",
      "Reducing exploration for all agents to 0.131\n",
      "\n",
      "Episode 366: Starting computation.\n",
      "Random Seed Set to 96\n",
      "Episode 366: Finished running.\n",
      "Agent 0, Average Reward: -438.75\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55295.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66129.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 84400.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57300.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 77408.6719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64364.0859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55779.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81553.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 113723.6797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 101300.0938\n",
      "Reducing exploration for all agents to 0.1286\n",
      "\n",
      "Episode 367: Starting computation.\n",
      "Random Seed Set to 97\n",
      "Episode 367: Finished running.\n",
      "Agent 0, Average Reward: -410.77\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59338.2930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 79826.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 63027.6719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 72698.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55318.8984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81032.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64399.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 95687.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 97223.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 80452.5000\n",
      "Reducing exploration for all agents to 0.1263\n",
      "\n",
      "Episode 368: Starting computation.\n",
      "Random Seed Set to 98\n",
      "Episode 368: Finished running.\n",
      "Agent 0, Average Reward: -442.7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 98497.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54900.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64217.4141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 67060.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59528.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48091.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45236.6797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 71180.0625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36100.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60223.5352\n",
      "Reducing exploration for all agents to 0.1239\n",
      "\n",
      "Episode 369: Starting computation.\n",
      "Random Seed Set to 99\n",
      "Episode 369: Finished running.\n",
      "Agent 0, Average Reward: -480.77\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 91203.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39411.8359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50095.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37925.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39685.4141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28287.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31563.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51551.0273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45261.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51070.4531\n",
      "Reducing exploration for all agents to 0.1215\n",
      "\n",
      "Episode 370: Starting computation.\n",
      "Random Seed Set to 100\n",
      "Episode 370: Finished running.\n",
      "Agent 0, Average Reward: -502.37\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28885.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29033.6855\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41220.8281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26176.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43369.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30635.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35885.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47203.5430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20289.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35048.8438\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1191\n",
      "\n",
      "Episode 371: Starting computation.\n",
      "Random Seed Set to 101\n",
      "Episode 371: Finished running.\n",
      "Agent 0, Average Reward: -529.24\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65169.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65087.6914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51696.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56364.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59465.4570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53132.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39199.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52391.7266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54166.1719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27202.5918\n",
      "Reducing exploration for all agents to 0.1167\n",
      "\n",
      "Episode 372: Starting computation.\n",
      "Random Seed Set to 102\n",
      "Episode 372: Finished running.\n",
      "Agent 0, Average Reward: -425.58\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30369.1426\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16327.9629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20183.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15844.7119\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24303.0137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22854.4727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25654.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11473.2803\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14463.7480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19393.7812\n",
      "Reducing exploration for all agents to 0.1144\n",
      "\n",
      "Episode 373: Starting computation.\n",
      "Random Seed Set to 103\n",
      "Episode 373: Finished running.\n",
      "Agent 0, Average Reward: -650.72\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11398.0264\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48127.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9257.7041\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27236.5215\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17604.0938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28771.1934\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26640.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24140.3359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10984.7900\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17388.2637\n",
      "Reducing exploration for all agents to 0.112\n",
      "\n",
      "Episode 374: Starting computation.\n",
      "Random Seed Set to 104\n",
      "Episode 374: Finished running.\n",
      "Agent 0, Average Reward: -571.81\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15722.3662\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6526.8179\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9007.1865\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5483.9976\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12003.2686\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6257.5981\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15516.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22144.4023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20665.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6964.6294\n",
      "Reducing exploration for all agents to 0.1096\n",
      "\n",
      "Episode 375: Starting computation.\n",
      "Random Seed Set to 105\n",
      "Episode 375: Finished running.\n",
      "Agent 0, Average Reward: -601.91\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17086.1816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4549.2920\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20207.1016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6233.4956\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18351.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3599.9426\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6223.1509\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15140.0244\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8888.8525\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20148.6855\n",
      "Reducing exploration for all agents to 0.1072\n",
      "\n",
      "Episode 376: Starting computation.\n",
      "Random Seed Set to 106\n",
      "Episode 376: Finished running.\n",
      "Agent 0, Average Reward: -509.83\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13264.4688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6920.0972\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6724.8330\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4753.0347\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9567.0068\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9044.9023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15681.3291\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8418.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31346.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12370.9971\n",
      "Reducing exploration for all agents to 0.1048\n",
      "\n",
      "Episode 377: Starting computation.\n",
      "Random Seed Set to 107\n",
      "Episode 377: Finished running.\n",
      "Agent 0, Average Reward: -520.98\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13415.8096\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12511.9209\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12194.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3663.6797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17285.3320\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8165.1587\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5959.4116\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11661.4023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29743.4668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15951.4980\n",
      "Reducing exploration for all agents to 0.1025\n",
      "\n",
      "Episode 378: Starting computation.\n",
      "Random Seed Set to 108\n",
      "Episode 378: Finished running.\n",
      "Agent 0, Average Reward: -492.98\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17394.6035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7998.6909\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26108.8086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11466.8408\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18519.0566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28810.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6283.5503\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13157.9404\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5033.2114\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7532.0762\n",
      "Reducing exploration for all agents to 0.1001\n",
      "\n",
      "Episode 379: Starting computation.\n",
      "Random Seed Set to 109\n",
      "Episode 379: Finished running.\n",
      "Agent 0, Average Reward: -555.15\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3933.7439\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14002.5732\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10973.7422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18755.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7849.3823\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5115.3491\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7626.6279\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7143.1489\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6933.1519\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2958.6680\n",
      "Reducing exploration for all agents to 0.0977\n",
      "\n",
      "Episode 380: Starting computation.\n",
      "Random Seed Set to 110\n",
      "Episode 380: Finished running.\n",
      "Agent 0, Average Reward: -509.01\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3483.1760\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2359.5759\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4030.4626\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2493.1206\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3524.5134\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4360.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3166.6218\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3522.3325\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3764.4441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4501.7861\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0953\n",
      "\n",
      "Episode 381: Starting computation.\n",
      "Random Seed Set to 111\n",
      "Episode 381: Finished running.\n",
      "Agent 0, Average Reward: -617.48\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42180.9805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42777.0117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36796.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32090.1895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27076.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19245.2266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13024.1592\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8167.0508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6111.5352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3976.4465\n",
      "Reducing exploration for all agents to 0.0929\n",
      "\n",
      "Episode 382: Starting computation.\n",
      "Random Seed Set to 112\n",
      "Episode 382: Finished running.\n",
      "Agent 0, Average Reward: -654.74\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12685.9092\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6771.7886\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9937.1104\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7797.4116\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7032.0132\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10707.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39225.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10561.5645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16274.9756\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11604.7188\n",
      "Reducing exploration for all agents to 0.0905\n",
      "\n",
      "Episode 383: Starting computation.\n",
      "Random Seed Set to 113\n",
      "Episode 383: Finished running.\n",
      "Agent 0, Average Reward: -811.39\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27668.1270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18171.4219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34696.3242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22278.3887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39216.1797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 70808.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65904.9375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68212.5547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18903.2070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21577.2539\n",
      "Reducing exploration for all agents to 0.0882\n",
      "\n",
      "Episode 384: Starting computation.\n",
      "Random Seed Set to 114\n",
      "Episode 384: Finished running.\n",
      "Agent 0, Average Reward: -449.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28310.5742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40845.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20960.4902\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45638.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51066.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18073.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36339.0898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45523.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19316.2969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66191.0781\n",
      "Reducing exploration for all agents to 0.0858\n",
      "\n",
      "Episode 385: Starting computation.\n",
      "Random Seed Set to 115\n",
      "Episode 385: Finished running.\n",
      "Agent 0, Average Reward: -699.22\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49994.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40791.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53994.5508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22343.9180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21264.3887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17084.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30841.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24637.5742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 67776.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 93168.5703\n",
      "Reducing exploration for all agents to 0.0834\n",
      "\n",
      "Episode 386: Starting computation.\n",
      "Random Seed Set to 116\n",
      "Episode 386: Finished running.\n",
      "Agent 0, Average Reward: -542.61\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15169.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44875.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58851.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26564.4434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40964.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22489.6680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27778.1016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42918.9648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15871.6533\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65523.3203\n",
      "Reducing exploration for all agents to 0.081\n",
      "\n",
      "Episode 387: Starting computation.\n",
      "Random Seed Set to 117\n",
      "Episode 387: Finished running.\n",
      "Agent 0, Average Reward: -747.15\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21144.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 76589.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42799.9062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19813.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46765.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44008.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17027.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45199.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 63135.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20446.2148\n",
      "Reducing exploration for all agents to 0.0786\n",
      "\n",
      "Episode 388: Starting computation.\n",
      "Random Seed Set to 118\n",
      "Episode 388: Finished running.\n",
      "Agent 0, Average Reward: -669.13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51095.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16638.2090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 87744.1797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64386.0938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 87969.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51535.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12995.2373\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21315.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14519.6182\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38093.2500\n",
      "Reducing exploration for all agents to 0.0763\n",
      "\n",
      "Episode 389: Starting computation.\n",
      "Random Seed Set to 119\n",
      "Episode 389: Finished running.\n",
      "Agent 0, Average Reward: -514.87\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26748.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28358.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14443.6162\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16830.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10903.2725\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38676.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11973.2061\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15253.8291\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28677.7051\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15249.4600\n",
      "Reducing exploration for all agents to 0.0739\n",
      "\n",
      "Episode 390: Starting computation.\n",
      "Random Seed Set to 120\n",
      "Episode 390: Finished running.\n",
      "Agent 0, Average Reward: -698.03\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10633.2900\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11420.0381\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6147.7910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8323.9355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11571.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19168.2129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20626.9805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9374.8506\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7415.6450\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10941.2568\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0715\n",
      "\n",
      "Episode 391: Starting computation.\n",
      "Random Seed Set to 121\n",
      "Episode 391: Finished running.\n",
      "Agent 0, Average Reward: -604.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57289.6719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53361.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43963.1836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33776.0469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25526.8066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15968.5566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7649.2988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4556.6699\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3072.3650\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5840.4497\n",
      "Reducing exploration for all agents to 0.0691\n",
      "\n",
      "Episode 392: Starting computation.\n",
      "Random Seed Set to 122\n",
      "Episode 392: Finished running.\n",
      "Agent 0, Average Reward: -714.37\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28392.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11531.5439\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15672.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13716.0498\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15537.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19144.3809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16049.1514\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35194.1680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12226.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9447.8467\n",
      "Reducing exploration for all agents to 0.0667\n",
      "\n",
      "Episode 393: Starting computation.\n",
      "Random Seed Set to 123\n",
      "Episode 393: Finished running.\n",
      "Agent 0, Average Reward: -680.94\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16247.8682\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37255.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11536.1123\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15050.8994\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46271.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23466.0371\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11434.5283\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35424.9023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11937.6377\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33657.6562\n",
      "Reducing exploration for all agents to 0.0644\n",
      "\n",
      "Episode 394: Starting computation.\n",
      "Random Seed Set to 124\n",
      "Episode 394: Finished running.\n",
      "Agent 0, Average Reward: -483.98\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15620.7266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12388.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32393.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15630.3945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12672.8760\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21634.4980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14159.8408\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12087.5752\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7833.6846\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26271.4004\n",
      "Reducing exploration for all agents to 0.062\n",
      "\n",
      "Episode 395: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 125\n",
      "Episode 395: Finished running.\n",
      "Agent 0, Average Reward: -695.37\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16763.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32876.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13136.4160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6473.9517\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33038.8945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11298.0947\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6647.0854\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32526.1777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6372.2363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49225.7344\n",
      "Reducing exploration for all agents to 0.0596\n",
      "\n",
      "Episode 396: Starting computation.\n",
      "Random Seed Set to 126\n",
      "Episode 396: Finished running.\n",
      "Agent 0, Average Reward: -649.52\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15483.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9491.3115\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50173.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26003.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7407.0874\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27440.1016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33072.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34736.2070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10908.1924\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15151.5391\n",
      "Reducing exploration for all agents to 0.0572\n",
      "\n",
      "Episode 397: Starting computation.\n",
      "Random Seed Set to 127\n",
      "Episode 397: Finished running.\n",
      "Agent 0, Average Reward: -717.24\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17388.4199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12494.4678\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24622.4082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28734.7461\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22898.9727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13533.4951\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21359.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28095.7129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11435.3779\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22403.4492\n",
      "Reducing exploration for all agents to 0.0548\n",
      "\n",
      "Episode 398: Starting computation.\n",
      "Random Seed Set to 128\n",
      "Episode 398: Finished running.\n",
      "Agent 0, Average Reward: -907.49\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18451.2402\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9295.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10098.6064\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14441.8291\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31148.3184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11054.2148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8687.6787\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10975.5557\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33291.8633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46599.1406\n",
      "Reducing exploration for all agents to 0.0525\n",
      "\n",
      "Episode 399: Starting computation.\n",
      "Random Seed Set to 129\n",
      "Episode 399: Finished running.\n",
      "Agent 0, Average Reward: -732.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7467.8545\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3829.1213\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20200.0352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26432.7422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4694.5088\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4414.8770\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5990.6992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20046.4805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14805.8877\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6066.7588\n",
      "Reducing exploration for all agents to 0.0501\n",
      "\n",
      "Episode 400: Starting computation.\n",
      "Random Seed Set to 130\n",
      "Episode 400: Finished running.\n",
      "Agent 0, Average Reward: -671.68\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2052.8586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3903.9167\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9300.9629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4413.0371\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2695.4065\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3223.5293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3461.0698\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3418.4004\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9819.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3685.1641\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0477\n",
      "\n",
      "Episode 401: Starting computation.\n",
      "Random Seed Set to 131\n",
      "Episode 401: Finished running.\n",
      "Agent 0, Average Reward: -770.11\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 70260.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60636.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49004.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32111.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18371.9297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7919.1655\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3687.3450\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5732.3525\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6811.1860\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13951.8076\n",
      "Reducing exploration for all agents to 0.0453\n",
      "\n",
      "Episode 402: Starting computation.\n",
      "Random Seed Set to 132\n",
      "Episode 402: Finished running.\n",
      "Agent 0, Average Reward: -934.94\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33531.3203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60177.1484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39261.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42852.0859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46542.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32281.8965\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20691.7168\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21632.7324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23964.0762\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23316.0430\n",
      "Reducing exploration for all agents to 0.0429\n",
      "\n",
      "Episode 403: Starting computation.\n",
      "Random Seed Set to 133\n",
      "Episode 403: Finished running.\n",
      "Agent 0, Average Reward: -759.26\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48650.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49416.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34472.7422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 63674.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40612.4805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40747.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44487.3203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17962.9004\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24416.4980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 85681.2109\n",
      "Reducing exploration for all agents to 0.0406\n",
      "\n",
      "Episode 404: Starting computation.\n",
      "Random Seed Set to 134\n",
      "Episode 404: Finished running.\n",
      "Agent 0, Average Reward: -876.97\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68364.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50875.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26613.1426\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 61970.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28357.9590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14661.5918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 85273.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40668.4453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27669.2109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20475.3105\n",
      "Reducing exploration for all agents to 0.0382\n",
      "\n",
      "Episode 405: Starting computation.\n",
      "Random Seed Set to 135\n",
      "Episode 405: Finished running.\n",
      "Agent 0, Average Reward: -906.76\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65405.4141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46849.7812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41595.0664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19231.9746\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16252.0967\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32931.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58323.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20841.4082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38972.3242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58408.7188\n",
      "Reducing exploration for all agents to 0.0358\n",
      "\n",
      "Episode 406: Starting computation.\n",
      "Random Seed Set to 136\n",
      "Episode 406: Finished running.\n",
      "Agent 0, Average Reward: -959.8\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 79586.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40009.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31648.6465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28457.0137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40406.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23694.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57352.3359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47443.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 61468.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 97486.5469\n",
      "Reducing exploration for all agents to 0.0334\n",
      "\n",
      "Episode 407: Starting computation.\n",
      "Random Seed Set to 137\n",
      "Episode 407: Finished running.\n",
      "Agent 0, Average Reward: -879.83\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28398.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50549.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37585.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48830.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81472.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52094.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23841.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 79900.3750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54568.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56814.8164\n",
      "Reducing exploration for all agents to 0.031\n",
      "\n",
      "Episode 408: Starting computation.\n",
      "Random Seed Set to 138\n",
      "Episode 408: Finished running.\n",
      "Agent 0, Average Reward: -874.01\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18935.5254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32723.0645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31641.6074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59447.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41593.5586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58120.8828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27223.6582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65167.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68702.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30952.0312\n",
      "Reducing exploration for all agents to 0.0286\n",
      "\n",
      "Episode 409: Starting computation.\n",
      "Random Seed Set to 139\n",
      "Episode 409: Finished running.\n",
      "Agent 0, Average Reward: -938.61\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22945.9590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20933.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18781.0273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33635.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23603.5801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22731.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42813.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22438.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16548.7012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33497.6250\n",
      "Reducing exploration for all agents to 0.0263\n",
      "\n",
      "Episode 410: Starting computation.\n",
      "Random Seed Set to 140\n",
      "Episode 410: Finished running.\n",
      "Agent 0, Average Reward: -734.25\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14714.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15114.3467\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26302.0176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14027.1738\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8211.9785\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32708.4121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11525.9697\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40086.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6112.8560\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12997.9482\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0239\n",
      "\n",
      "Episode 411: Starting computation.\n",
      "Random Seed Set to 141\n",
      "Episode 411: Finished running.\n",
      "Agent 0, Average Reward: -743.09\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 101599.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 85691.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 61627.9492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37025.6172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17533.8105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14792.8184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11773.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29854.3496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34416.3047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35476.8281\n",
      "Reducing exploration for all agents to 0.0215\n",
      "\n",
      "Episode 412: Starting computation.\n",
      "Random Seed Set to 142\n",
      "Episode 412: Finished running.\n",
      "Agent 0, Average Reward: -1159.04\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54561.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35468.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30484.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25423.4668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 70751.7266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17624.9551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38938.4766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38372.6680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21357.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27159.3379\n",
      "Reducing exploration for all agents to 0.0191\n",
      "\n",
      "Episode 413: Starting computation.\n",
      "Random Seed Set to 143\n",
      "Episode 413: Finished running.\n",
      "Agent 0, Average Reward: -428.4\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29443.4941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35129.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28893.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 71729.4375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33824.7500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25600.9375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60908.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 92814.3750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34136.9141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30795.0586\n",
      "Reducing exploration for all agents to 0.0167\n",
      "\n",
      "Episode 414: Starting computation.\n",
      "Random Seed Set to 144\n",
      "Episode 414: Finished running.\n",
      "Agent 0, Average Reward: -920.96\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32212.0820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18876.2676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46984.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78280.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57563.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42332.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50690.5820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20896.2402\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18419.0684\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22970.3770\n",
      "Reducing exploration for all agents to 0.0144\n",
      "\n",
      "Episode 415: Starting computation.\n",
      "Random Seed Set to 145\n",
      "Episode 415: Finished running.\n",
      "Agent 0, Average Reward: -920.86\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25938.5254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40890.8281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37355.7422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60864.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24564.7910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13265.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38737.7266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56313.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 64653.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29196.9609\n",
      "Reducing exploration for all agents to 0.012\n",
      "\n",
      "Episode 416: Starting computation.\n",
      "Random Seed Set to 146\n",
      "Episode 416: Finished running.\n",
      "Agent 0, Average Reward: -907.09\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51150.6680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58525.4453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23664.4961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17756.9297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37802.1172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33879.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48785.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68016.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41590.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30006.1797\n",
      "Reducing exploration for all agents to 0.01\n",
      "\n",
      "Episode 417: Starting computation.\n",
      "Random Seed Set to 147\n",
      "Episode 417: Finished running.\n",
      "Agent 0, Average Reward: -969.22\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 87815.8516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 75058.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59621.9609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78763.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 125890.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62305.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59908.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 85975.4766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 92308.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38594.3750\n",
      "Reducing exploration for all agents to 0.01\n",
      "\n",
      "Episode 418: Starting computation.\n",
      "Random Seed Set to 148\n",
      "Episode 418: Finished running.\n",
      "Agent 0, Average Reward: -935.9\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45269.9922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23227.6328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68496.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57798.2305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66052.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 147825.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33521.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 73075.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43495.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32874.0820\n",
      "Reducing exploration for all agents to 0.01\n",
      "\n",
      "Episode 419: Starting computation.\n"
     ]
    }
   ],
   "source": [
    "Balance_int_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.save(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## AGENT TRAINING RESULTS\n",
    "# Path to results folder\n",
    "results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "\n",
    "# Loop over each agent\n",
    "for idx , agent in Balance_int_MultiDQN_Agents.Agents.items():\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[0].signal_id + 1\n",
    "    print(\"Intersection \"+str(intersection_number_in_vissim))\n",
    "    \n",
    "    ## SAVE TRAINING DATA TO JSON.\n",
    "    json_filename = \"Agent{}_Loss_average_reward.json\".format(intersection_number_in_vissim)\n",
    "    Loss_reward = dict()   \n",
    "    # Loss dictionary\n",
    "    for epoch, loss in enumerate(agent.loss):\n",
    "        loss_dict = { epoch : loss }\n",
    "    Loss_reward['Agent{} loss'.format(intersection_number_in_vissim)] = loss_dict\n",
    "    # Reward dictionary            \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    Loss_reward['Agent{} Average_Reward'.format(intersection_number_in_vissim)] = agent.reward_storage\n",
    "    # Store as JSON\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Loss_reward, f)\n",
    "    print(\"Agent {}: Training Loss and Average Reward during training successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "    \n",
    "    ## LOADING DATA FROM JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Agent{}_Loss_average_reward.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "    \n",
    "    ## TRAINING PLOTS\n",
    "    loss_plot_filename  = \"Agent{}_Loss.png\".format(intersection_number_in_vissim)\n",
    "    reward_plot_filename  = \"Agent{}_average_reward.png\".format(intersection_number_in_vissim) \n",
    "    \n",
    "    ## Loss Plot\n",
    "    plt.figure('LossAgent'+str(idx),figsize=(16,9))\n",
    "    plt.plot(agent.loss)\n",
    "    #plt.yscale('log')\n",
    "\n",
    "    plt.xlabel('Training Epoch',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent {} Loss over training'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.savefig(results_path + \"/\" + loss_plot_filename)\n",
    "\n",
    "    ## Average Reward Plot\n",
    "    plt.figure('RewardAgent'+str(idx),figsize=(16,9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Training Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent {} average reward over training'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.savefig(results_path + \"/\" + reward_plot_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.load(500, best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_int_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "time = [t for t in range(len(Balance_int_MultiDQN_Agents.Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "########################################\n",
    "## Queues over time for each junction ##\n",
    "########################################\n",
    "for idx, queues in Balance_int_MultiDQN_Agents.Episode_Queues.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[0].signal_id + 1\n",
    "    \n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    number_queues = np.size(queues,0)\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queues = dict()\n",
    "    Queues['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queues[str(i)] = queue.tolist()\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "    \n",
    "    ## Plot the queues\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    filename = \"Junction{}_Queues.png\".format(intersection_number_in_vissim)           \n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Queues.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Queues, f)\n",
    "        \n",
    "    ### LOADING DATA FROM JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #json_filename = \"Junction{}_Queues.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "        \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Queues during Test successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "       \n",
    "        \n",
    "###################################################        \n",
    "## Accumulated delay over time for each junction ##\n",
    "###################################################\n",
    "for idx, delay in Balance_int_MultiDQN_Agents.Cumulative_Episode_Delays.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[idx].signal_id + 1\n",
    "\n",
    "    # Extract and process delay data\n",
    "    Delay = dict()   \n",
    "    Delay['Time'] = time\n",
    "    Delay['Junction {} delay'.format(intersection_number_in_vissim)] = delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Cumulative_Delay.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Delay, f)\n",
    "        \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Delay successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "    \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Junction{}_Cumulative_Delay.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    # Plot the cumulative delay\n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    filename = \"Junction{}_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "    \n",
    "    \n",
    "    \n",
    "########################################################    \n",
    "## Accumulated stop delay over time for each junction ##\n",
    "########################################################\n",
    "for idx, stop_delay in Balance_int_MultiDQN_Agents.Cumulative_Episode_stop_Delays.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[idx].signal_id + 1    \n",
    "    \n",
    "    # Extract and process stop delay data\n",
    "    Stop_delay = dict()   \n",
    "    Stop_delay['Time'] = time\n",
    "    Stop_delay['Junction {} stop delay'.format(intersection_number_in_vissim)] = stop_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Cumulative_Stop_Delay.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Stop_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Stop Delay successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Junction{}_Cumulative_Stop_Delay.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "    \n",
    "    # Plot the cumulative stop delay\n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    filename = \"Junction{}_Cumulative_Stop_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n",
    "    \n",
    "    \n",
    "###############################################\n",
    "## ONLY IF THERE IS MORE THAN ONE CONTROLLER ##\n",
    "##    These are the global network plots     ##\n",
    "###############################################\n",
    "\n",
    "if len(Balance_int_MultiDQN_Agents.Agents) > 1:\n",
    "    ########################################    \n",
    "    ## Global Accumulated delay over time ##\n",
    "    ########################################\n",
    "    \n",
    "    # Process global delay data\n",
    "    Global_delay = dict()   \n",
    "    Global_delay['Time'] = time\n",
    "    Global_delay['Global accumulated Delay'] = Balance_int_MultiDQN_Agents.Cumulative_Totale_network_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Global_Cumulative_Delay.json\"\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Global_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Global Delay successfuly saved to file:\")\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Global_Cumulative_Delay.json\"\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    \n",
    "    # Plot the global delay\n",
    "    plt.figure('4',figsize=(16,9))\n",
    "    plt.plot(Cumulative_Totale_network_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "    plt.title('Global accumulated Delay',fontsize=18)\n",
    "    plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "    filename = \"Global_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    ## Global Accumulated stop delay over time ##\n",
    "    #############################################\n",
    "    \n",
    "    # Process global stop delay data\n",
    "    Global_stop_delay = dict()   \n",
    "    Global_stop_delay['Time'] = time\n",
    "    Global_stop_delay['Global accumulated stop Delay'] = Balance_int_MultiDQN_Agents.Cumulative_Totale_network_stop_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Global_Cumulative_Stop_Delay.json\"\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Global_stop_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Global Stop Delay successfuly saved to file:\")\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Global_Cumulative_Stop_Delay.json\"\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    # Plot the global stop delay\n",
    "    plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "    plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "    plt.gca().legend('Global accumulated stop Delay')\n",
    "    \n",
    "    filename = \"Global_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current simulation: Balance_int14_all_actions_500_10800_DuellingDDQN_Queues_rework\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 13: SETTING UP AGENT\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 24)           168         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 24)           600         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 24)           600         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 24)           600         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            25          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 3)            75          dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_17[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,068\n",
      "Trainable params: 2,068\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Deep Q Learning Agent(s) at Intersection 13\n",
      "\n",
      "Experience file not found. Generating now...\n",
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int14.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 10801 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 10\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.17 seconds.\n",
      "\n",
      "After 0 actions taken by the Agents,  Agent 0 memory is 0.0 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 0 memory is 10.0 percent full\n",
      "Random Seed Set to 11\n",
      "After 2000 actions taken by the Agents,  Agent 0 memory is 20.0 percent full\n",
      "Random Seed Set to 12\n",
      "After 3000 actions taken by the Agents,  Agent 0 memory is 30.0 percent full\n",
      "Random Seed Set to 13\n",
      "After 4000 actions taken by the Agents,  Agent 0 memory is 40.0 percent full\n",
      "Random Seed Set to 14\n",
      "After 5000 actions taken by the Agents,  Agent 0 memory is 50.0 percent full\n",
      "Random Seed Set to 15\n",
      "After 6000 actions taken by the Agents,  Agent 0 memory is 60.0 percent full\n",
      "After 7000 actions taken by the Agents,  Agent 0 memory is 70.0 percent full\n",
      "Random Seed Set to 16\n",
      "After 8000 actions taken by the Agents,  Agent 0 memory is 80.0 percent full\n",
      "Random Seed Set to 17\n",
      "After 9000 actions taken by the Agents,  Agent 0 memory is 90.0 percent full\n",
      "Random Seed Set to 18\n",
      "Memory filled. Saving as:C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\Balance_int14\\Agents_Results\\DuellingDDQN\\Balance_int14_all_actions_500_10800_DuellingDDQN_Queues_rework\\Agent0_PERPre_10000.p\n",
      "Working Directory set to: C:\\Users\\acabrejasegea\\OneDrive - The Alan Turing Institute\\Desktop\\ATI\\0_TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance_int14.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 10801 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 10\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.09 seconds.\n",
      "\n",
      "start\n",
      "Random Seed Set to 11\n",
      "Episode 1: Finished running.\n",
      "Agent 0, Average Reward: -960.0\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 273488.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 261321.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 269397.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 266194.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 263116.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 259934.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 259214.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 252757.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 253609.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 245721.3125\n",
      "Reducing exploration for all agents to 0.9976\n",
      "\n",
      "Episode 2: Starting computation.\n",
      "Random Seed Set to 12\n",
      "Episode 2: Finished running.\n",
      "Agent 0, Average Reward: -827.9\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 242050.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 235920.8281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 228291.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 228514.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 229400.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 226615.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 220453.0938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 220378.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 215277.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 220538.6406\n",
      "Reducing exploration for all agents to 0.9952\n",
      "\n",
      "Episode 3: Starting computation.\n",
      "Random Seed Set to 13\n",
      "Episode 3: Finished running.\n",
      "Agent 0, Average Reward: -774.13\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 208949.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 208405.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 207273.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 204127.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 202186.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 201862.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 200556.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 192178.6719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 196562.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 192403.6719\n",
      "Reducing exploration for all agents to 0.9929\n",
      "\n",
      "Episode 4: Starting computation.\n",
      "Random Seed Set to 14\n",
      "Episode 4: Finished running.\n",
      "Agent 0, Average Reward: -799.42\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 197124.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 194733.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 190711.5156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 187653.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 184105.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 185486.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 174997.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 179753.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 176836.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 171620.7656\n",
      "Reducing exploration for all agents to 0.9905\n",
      "\n",
      "Episode 5: Starting computation.\n",
      "Random Seed Set to 15\n",
      "Episode 5: Finished running.\n",
      "Agent 0, Average Reward: -803.36\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 178657.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 173613.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 170873.7500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 171871.6094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 168860.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 161218.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 160240.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 159855.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 156018.7500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 153283.4062\n",
      "Reducing exploration for all agents to 0.9881\n",
      "\n",
      "Episode 6: Starting computation.\n",
      "Random Seed Set to 16\n",
      "Episode 6: Finished running.\n",
      "Agent 0, Average Reward: -779.0\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 153590.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 147820.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 145308.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 141151.2812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 143990.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 135801.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 134005.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 131254.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 126940.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 124612.6484\n",
      "Reducing exploration for all agents to 0.9857\n",
      "\n",
      "Episode 7: Starting computation.\n",
      "Random Seed Set to 17\n",
      "Episode 7: Finished running.\n",
      "Agent 0, Average Reward: -910.68\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 124913.4766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 124343.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 119659.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 117083.1797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 114152.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 109993.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 108475.3516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 106478.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 102372.4688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 99594.2891\n",
      "Reducing exploration for all agents to 0.9833\n",
      "\n",
      "Episode 8: Starting computation.\n",
      "Random Seed Set to 18\n",
      "Episode 8: Finished running.\n",
      "Agent 0, Average Reward: -676.22\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 88514.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 84023.1797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 85020.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81439.9141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 80562.1172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 76313.4531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 74284.3047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68402.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66376.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62616.0508\n",
      "Reducing exploration for all agents to 0.981\n",
      "\n",
      "Episode 9: Starting computation.\n",
      "Random Seed Set to 19\n",
      "Episode 9: Finished running.\n",
      "Agent 0, Average Reward: -836.16\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 55577.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50080.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46586.6836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45902.0938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43721.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39901.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37625.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35422.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33708.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31615.9336\n",
      "Reducing exploration for all agents to 0.9786\n",
      "\n",
      "Episode 10: Starting computation.\n",
      "Random Seed Set to 20\n",
      "Episode 10: Finished running.\n",
      "Agent 0, Average Reward: -684.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28254.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27657.4297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24855.8066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23542.3945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22090.9414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20202.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18368.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16750.7754\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15751.0254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15556.7490\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.9762\n",
      "\n",
      "Episode 11: Starting computation.\n",
      "Random Seed Set to 21\n",
      "Episode 11: Finished running.\n",
      "Agent 0, Average Reward: -793.31\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 242780.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 238507.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 231128.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 222631.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 217201.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 206426.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 202604.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 192181.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 183675.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 166338.0000\n",
      "Reducing exploration for all agents to 0.9738\n",
      "\n",
      "Episode 12: Starting computation.\n",
      "Random Seed Set to 22\n",
      "Episode 12: Finished running.\n",
      "Agent 0, Average Reward: -807.29\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 150593.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 146248.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 133567.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 126375.8984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 117058.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 109617.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 99576.4453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 88654.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81428.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 71186.0703\n",
      "Reducing exploration for all agents to 0.9714\n",
      "\n",
      "Episode 13: Starting computation.\n",
      "Random Seed Set to 23\n",
      "Episode 13: Finished running.\n",
      "Agent 0, Average Reward: -870.94\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66397.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 61925.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54159.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47187.4648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39990.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35021.7031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31719.3027\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27234.0078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20931.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17906.2324\n",
      "Reducing exploration for all agents to 0.9691\n",
      "\n",
      "Episode 14: Starting computation.\n",
      "Random Seed Set to 24\n",
      "Episode 14: Finished running.\n",
      "Agent 0, Average Reward: -727.46\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15274.6982\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12474.1162\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10201.6182\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8234.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7493.0811\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5236.8032\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6927.7095\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4684.3823\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4120.6089\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5893.6953\n",
      "Reducing exploration for all agents to 0.9667\n",
      "\n",
      "Episode 15: Starting computation.\n",
      "Random Seed Set to 25\n",
      "Episode 15: Finished running.\n",
      "Agent 0, Average Reward: -819.75\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3659.8740\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3712.8840\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6275.3687\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6318.2046\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4962.0571\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4895.0776\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6153.7827\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6769.0386\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4974.5210\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5961.9653\n",
      "Reducing exploration for all agents to 0.9643\n",
      "\n",
      "Episode 16: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 26\n",
      "Episode 16: Finished running.\n",
      "Agent 0, Average Reward: -759.83\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6048.6826\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5042.8667\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6715.4194\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5568.0483\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5602.8955\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6112.4741\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6208.6538\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6435.8101\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5192.5918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5950.2422\n",
      "Reducing exploration for all agents to 0.9619\n",
      "\n",
      "Episode 17: Starting computation.\n",
      "Random Seed Set to 27\n",
      "Episode 17: Finished running.\n",
      "Agent 0, Average Reward: -784.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3968.0222\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3828.2415\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3618.2371\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3947.2290\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4340.0566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4378.7061\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4340.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3322.0459\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2697.2236\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3957.3674\n",
      "Reducing exploration for all agents to 0.9595\n",
      "\n",
      "Episode 18: Starting computation.\n",
      "Random Seed Set to 28\n",
      "Episode 18: Finished running.\n",
      "Agent 0, Average Reward: -850.16\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3576.6626\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4265.4897\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4646.5913\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3021.6465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3580.4409\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3264.1104\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3255.3523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4047.4177\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3173.0137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2946.7314\n",
      "Reducing exploration for all agents to 0.9571\n",
      "\n",
      "Episode 19: Starting computation.\n",
      "Random Seed Set to 29\n",
      "Episode 19: Finished running.\n",
      "Agent 0, Average Reward: -736.5\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3553.1765\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4085.7861\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3719.3337\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3048.4661\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3875.1707\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3920.9197\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4451.0269\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3877.9583\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3410.3054\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3353.1582\n",
      "Reducing exploration for all agents to 0.9548\n",
      "\n",
      "Episode 20: Starting computation.\n",
      "Random Seed Set to 30\n",
      "Episode 20: Finished running.\n",
      "Agent 0, Average Reward: -722.76\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2666.7756\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2859.3157\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2258.7058\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2941.5991\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3409.1729\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3807.5610\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 2777.8586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3760.5415\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3605.9829\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3131.3098\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.9524\n",
      "\n",
      "Episode 21: Starting computation.\n",
      "Random Seed Set to 31\n",
      "Episode 21: Finished running.\n",
      "Agent 0, Average Reward: -646.9\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 196639.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 197772.7031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 192897.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 178239.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 174758.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 163508.4531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 144265.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 131458.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 116084.4922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 101384.8516\n",
      "Reducing exploration for all agents to 0.95\n",
      "\n",
      "Episode 22: Starting computation.\n",
      "Random Seed Set to 32\n",
      "Episode 22: Finished running.\n",
      "Agent 0, Average Reward: -685.31\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 81152.9609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 72340.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59346.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50093.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37104.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31210.6738\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22771.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19137.1387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12387.1514\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9112.6553\n",
      "Reducing exploration for all agents to 0.9476\n",
      "\n",
      "Episode 23: Starting computation.\n",
      "Random Seed Set to 33\n",
      "Episode 23: Finished running.\n",
      "Agent 0, Average Reward: -591.16\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6608.4404\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7474.9849\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6454.4722\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9055.9121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5237.1782\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8213.1318\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13883.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13706.2217\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11596.8154\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10944.4385\n",
      "Reducing exploration for all agents to 0.9452\n",
      "\n",
      "Episode 24: Starting computation.\n",
      "Random Seed Set to 34\n",
      "Episode 24: Finished running.\n",
      "Agent 0, Average Reward: -793.66\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10917.0107\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16915.5801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16527.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11746.8721\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11054.5537\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10745.4385\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8759.3320\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10635.8154\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11642.8965\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6050.0728\n",
      "Reducing exploration for all agents to 0.9429\n",
      "\n",
      "Episode 25: Starting computation.\n",
      "Random Seed Set to 35\n",
      "Episode 25: Finished running.\n",
      "Agent 0, Average Reward: -608.52\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8727.4775\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9356.5400\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5585.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4775.3975\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4692.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7707.8340\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4737.4409\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4808.5044\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5437.2378\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8925.4072\n",
      "Reducing exploration for all agents to 0.9405\n",
      "\n",
      "Episode 26: Starting computation.\n",
      "Random Seed Set to 36\n",
      "Episode 26: Finished running.\n",
      "Agent 0, Average Reward: -758.6\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6085.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7236.6221\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5157.5820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5862.9570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6391.4761\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6655.4717\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8061.9150\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4222.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8253.8818\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5478.7217\n",
      "Reducing exploration for all agents to 0.9381\n",
      "\n",
      "Episode 27: Starting computation.\n",
      "Random Seed Set to 37\n",
      "Episode 27: Finished running.\n",
      "Agent 0, Average Reward: -821.8\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6230.3809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6216.9727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7049.8887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6776.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5116.6318\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6448.4824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5973.7739\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4341.5903\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5474.1343\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5379.4888\n",
      "Reducing exploration for all agents to 0.9357\n",
      "\n",
      "Episode 28: Starting computation.\n",
      "Random Seed Set to 38\n",
      "Episode 28: Finished running.\n",
      "Agent 0, Average Reward: -632.9\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7249.6157\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7063.2544\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4162.3872\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5463.7676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4535.9517\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5533.6284\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8315.3096\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4833.6602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5433.1226\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5069.2046\n",
      "Reducing exploration for all agents to 0.9333\n",
      "\n",
      "Episode 29: Starting computation.\n",
      "Random Seed Set to 39\n",
      "Episode 29: Finished running.\n",
      "Agent 0, Average Reward: -679.28\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7472.1050\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6991.5283\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5700.8081\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4371.4683\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6301.3047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5683.3418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6104.5059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4669.6064\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6344.8408\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6274.5454\n",
      "Reducing exploration for all agents to 0.931\n",
      "\n",
      "Episode 30: Starting computation.\n",
      "Random Seed Set to 40\n",
      "Episode 30: Finished running.\n",
      "Agent 0, Average Reward: -759.74\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3675.5840\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 5668.3955\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6654.8003\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6504.6504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 4372.5977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6302.4165\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6492.6943\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6384.2700\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 3142.3396\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6895.0044\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.9286\n",
      "\n",
      "Episode 31: Starting computation.\n",
      "Random Seed Set to 41\n",
      "Episode 31: Finished running.\n",
      "Agent 0, Average Reward: -613.68\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 158191.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 158493.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 153454.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 141564.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 126274.0859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 111370.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 104043.7812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 89397.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68187.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56829.5234\n",
      "Reducing exploration for all agents to 0.9262\n",
      "\n",
      "Episode 32: Starting computation.\n",
      "Random Seed Set to 42\n",
      "Episode 32: Finished running.\n",
      "Agent 0, Average Reward: -784.59\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42942.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35163.9297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26212.3613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22976.8691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17170.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10952.8135\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11154.6494\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11322.0273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9854.5615\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13894.0537\n",
      "Reducing exploration for all agents to 0.9238\n",
      "\n",
      "Episode 33: Starting computation.\n",
      "Random Seed Set to 43\n",
      "Episode 33: Finished running.\n",
      "Agent 0, Average Reward: -872.35\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17485.8770\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25133.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16950.0684\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17806.2520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23667.1504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18549.2812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29613.0605\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21365.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18124.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13684.4893\n",
      "Reducing exploration for all agents to 0.9214\n",
      "\n",
      "Episode 34: Starting computation.\n",
      "Random Seed Set to 44\n",
      "Episode 34: Finished running.\n",
      "Agent 0, Average Reward: -574.41\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13189.5322\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12265.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11823.5029\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11827.4150\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6487.0278\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11314.1484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17769.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7925.8057\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9814.9355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10073.7998\n",
      "Reducing exploration for all agents to 0.9191\n",
      "\n",
      "Episode 35: Starting computation.\n",
      "Random Seed Set to 45\n",
      "Episode 35: Finished running.\n",
      "Agent 0, Average Reward: -705.03\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9077.5635\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13999.5850\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14158.0830\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11998.2324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9566.7334\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9818.7588\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15046.7236\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12058.7764\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10785.7822\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12386.1475\n",
      "Reducing exploration for all agents to 0.9167\n",
      "\n",
      "Episode 36: Starting computation.\n",
      "Random Seed Set to 46\n",
      "Episode 36: Finished running.\n",
      "Agent 0, Average Reward: -779.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7763.7129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10587.8564\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11258.8164\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8790.4004\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9245.4287\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8880.2891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7835.0933\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9940.7939\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9373.7900\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9265.6553\n",
      "Reducing exploration for all agents to 0.9143\n",
      "\n",
      "Episode 37: Starting computation.\n",
      "Random Seed Set to 47\n",
      "Episode 37: Finished running.\n",
      "Agent 0, Average Reward: -477.1\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7792.0127\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9830.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8854.0723\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7852.9263\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6161.6274\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6005.7168\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10662.3789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8655.8857\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6633.7627\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14407.7939\n",
      "Reducing exploration for all agents to 0.9119\n",
      "\n",
      "Episode 38: Starting computation.\n",
      "Random Seed Set to 48\n",
      "Episode 38: Finished running.\n",
      "Agent 0, Average Reward: -582.51\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7372.1211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8674.7725\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11275.9561\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9609.5234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7360.2051\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7412.4751\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6892.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7676.3423\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9054.2998\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8387.0361\n",
      "Reducing exploration for all agents to 0.9095\n",
      "\n",
      "Episode 39: Starting computation.\n",
      "Random Seed Set to 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39: Finished running.\n",
      "Agent 0, Average Reward: -600.08\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12414.9756\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11002.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6763.8330\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9293.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7176.0776\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7425.5405\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7512.6680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7761.9941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6608.3457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7297.7197\n",
      "Reducing exploration for all agents to 0.9072\n",
      "\n",
      "Episode 40: Starting computation.\n",
      "Random Seed Set to 50\n",
      "Episode 40: Finished running.\n",
      "Agent 0, Average Reward: -726.4\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11264.8682\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7013.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6025.9111\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9166.7178\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7773.8789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7246.3457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8121.5825\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8127.1924\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 6589.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9191.9971\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.9048\n",
      "\n",
      "Episode 41: Starting computation.\n",
      "Random Seed Set to 51\n",
      "Episode 41: Finished running.\n",
      "Agent 0, Average Reward: -661.96\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 131320.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 118509.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 118651.0234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 109756.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 91502.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 70207.4766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 67146.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54319.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43754.2148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31852.1777\n",
      "Reducing exploration for all agents to 0.9024\n",
      "\n",
      "Episode 42: Starting computation.\n",
      "Random Seed Set to 52\n",
      "Episode 42: Finished running.\n",
      "Agent 0, Average Reward: -836.54\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25000.6191\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21907.3691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13058.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13707.9668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11599.7451\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16976.6543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13242.6650\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17589.6777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17164.8613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20106.0059\n",
      "Reducing exploration for all agents to 0.9\n",
      "\n",
      "Episode 43: Starting computation.\n",
      "Random Seed Set to 53\n",
      "Episode 43: Finished running.\n",
      "Agent 0, Average Reward: -701.72\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28797.8105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23414.8926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24232.6855\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32157.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20269.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25735.2363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23902.3008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18629.9941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22931.8555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11519.9551\n",
      "Reducing exploration for all agents to 0.8976\n",
      "\n",
      "Episode 44: Starting computation.\n",
      "Random Seed Set to 54\n",
      "Episode 44: Finished running.\n",
      "Agent 0, Average Reward: -537.71\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16890.7324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15014.6436\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11242.6621\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12499.1514\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12808.9805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12854.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12028.8857\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12572.0176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14183.2314\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12117.0898\n",
      "Reducing exploration for all agents to 0.8952\n",
      "\n",
      "Episode 45: Starting computation.\n",
      "Random Seed Set to 55\n",
      "Episode 45: Finished running.\n",
      "Agent 0, Average Reward: -627.61\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13994.9180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13790.9570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16501.4922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13247.8096\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12150.2334\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14600.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17130.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11794.3379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17172.2598\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10260.4229\n",
      "Reducing exploration for all agents to 0.8929\n",
      "\n",
      "Episode 46: Starting computation.\n",
      "Random Seed Set to 56\n",
      "Episode 46: Finished running.\n",
      "Agent 0, Average Reward: -651.1\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13084.5908\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18060.9590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11681.8682\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13493.3447\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11130.5811\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10144.8467\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14209.8145\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9865.3232\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11236.5420\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15808.4541\n",
      "Reducing exploration for all agents to 0.8905\n",
      "\n",
      "Episode 47: Starting computation.\n",
      "Random Seed Set to 57\n",
      "Episode 47: Finished running.\n",
      "Agent 0, Average Reward: -653.4\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17730.4590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12855.7764\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12349.2588\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13328.0029\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11384.6221\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9076.7451\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11386.0439\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12193.8701\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13308.6885\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13122.9658\n",
      "Reducing exploration for all agents to 0.8881\n",
      "\n",
      "Episode 48: Starting computation.\n",
      "Random Seed Set to 58\n",
      "Episode 48: Finished running.\n",
      "Agent 0, Average Reward: -584.25\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17659.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17995.2109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11478.5195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8748.3057\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18116.1855\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10483.6143\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19103.2754\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21176.3594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9909.2871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12315.3594\n",
      "Reducing exploration for all agents to 0.8857\n",
      "\n",
      "Episode 49: Starting computation.\n",
      "Random Seed Set to 59\n",
      "Episode 49: Finished running.\n",
      "Agent 0, Average Reward: -518.59\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11250.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24447.7324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11218.8994\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 7597.4185\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10507.2686\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11390.1670\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11689.9805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22139.3809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12731.6719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14114.6650\n",
      "Reducing exploration for all agents to 0.8833\n",
      "\n",
      "Episode 50: Starting computation.\n",
      "Random Seed Set to 60\n",
      "Episode 50: Finished running.\n",
      "Agent 0, Average Reward: -703.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16657.8730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16252.3672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10689.3682\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19820.8066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10186.6553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31829.8535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16357.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10552.5713\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 8549.7002\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13719.8740\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.881\n",
      "\n",
      "Episode 51: Starting computation.\n",
      "Random Seed Set to 61\n",
      "Episode 51: Finished running.\n",
      "Agent 0, Average Reward: -687.43\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 109249.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 109099.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 106595.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 91012.2422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 83695.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 69811.2969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52923.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42987.0898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37812.3555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45163.6641\n",
      "Reducing exploration for all agents to 0.8786\n",
      "\n",
      "Episode 52: Starting computation.\n",
      "Random Seed Set to 62\n",
      "Episode 52: Finished running.\n",
      "Agent 0, Average Reward: -778.8\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22024.8184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17875.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12989.7051\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17731.9316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36261.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25175.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37225.0195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28396.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29905.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27303.9824\n",
      "Reducing exploration for all agents to 0.8762\n",
      "\n",
      "Episode 53: Starting computation.\n",
      "Random Seed Set to 63\n",
      "Episode 53: Finished running.\n",
      "Agent 0, Average Reward: -550.02\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27611.4453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28616.5566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40917.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21143.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18311.6836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28207.8691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25007.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25044.8613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19550.1387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21568.7344\n",
      "Reducing exploration for all agents to 0.8738\n",
      "\n",
      "Episode 54: Starting computation.\n",
      "Random Seed Set to 64\n",
      "Episode 54: Finished running.\n",
      "Agent 0, Average Reward: -505.3\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29350.9238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22524.2207\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22154.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24348.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18749.4180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25791.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18234.6543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23753.8457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20044.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43849.1875\n",
      "Reducing exploration for all agents to 0.8714\n",
      "\n",
      "Episode 55: Starting computation.\n",
      "Random Seed Set to 65\n",
      "Episode 55: Finished running.\n",
      "Agent 0, Average Reward: -516.83\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20938.6309\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20124.5996\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18212.7949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21958.5898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18202.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22521.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18409.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17641.1973\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13321.5332\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23342.1836\n",
      "Reducing exploration for all agents to 0.8691\n",
      "\n",
      "Episode 56: Starting computation.\n",
      "Random Seed Set to 66\n",
      "Episode 56: Finished running.\n",
      "Agent 0, Average Reward: -604.27\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15469.1777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15620.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16829.0664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15296.0176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20362.7676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20088.2539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23606.9316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21434.2754\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14412.2754\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20130.0215\n",
      "Reducing exploration for all agents to 0.8667\n",
      "\n",
      "Episode 57: Starting computation.\n",
      "Random Seed Set to 67\n",
      "Episode 57: Finished running.\n",
      "Agent 0, Average Reward: -583.89\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19301.9629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19244.0645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20972.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 10675.3369\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24227.1113\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16288.4141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16779.6191\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21244.6973\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21381.5332\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14532.2480\n",
      "Reducing exploration for all agents to 0.8643\n",
      "\n",
      "Episode 58: Starting computation.\n",
      "Random Seed Set to 68\n",
      "Episode 58: Finished running.\n",
      "Agent 0, Average Reward: -561.2\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28663.9648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14789.9463\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11139.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17321.0137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20346.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12488.0049\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18777.2520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24023.8418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20911.7324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26757.9707\n",
      "Reducing exploration for all agents to 0.8619\n",
      "\n",
      "Episode 59: Starting computation.\n",
      "Random Seed Set to 69\n",
      "Episode 59: Finished running.\n",
      "Agent 0, Average Reward: -471.33\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20658.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12941.3027\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19758.2676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11778.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16505.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22439.3652\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14758.2676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15029.3057\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16116.1533\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25801.9668\n",
      "Reducing exploration for all agents to 0.8595\n",
      "\n",
      "Episode 60: Starting computation.\n",
      "Random Seed Set to 70\n",
      "Episode 60: Finished running.\n",
      "Agent 0, Average Reward: -466.4\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20859.4102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11512.3760\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12401.8564\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17238.1934\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13553.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18484.1738\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14187.7598\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13798.6475\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16304.2881\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 11853.7256\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.8572\n",
      "\n",
      "Episode 61: Starting computation.\n",
      "Random Seed Set to 71\n",
      "Episode 61: Finished running.\n",
      "Agent 0, Average Reward: -567.41\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 82595.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 77012.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66399.8516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60480.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66181.8516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50276.7812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38917.1055\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33035.0664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28423.2500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22439.2988\n",
      "Reducing exploration for all agents to 0.8548\n",
      "\n",
      "Episode 62: Starting computation.\n",
      "Random Seed Set to 72\n",
      "Episode 62: Finished running.\n",
      "Agent 0, Average Reward: -449.26\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27549.4512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20010.8965\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15163.1777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14873.0244\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18936.1270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19464.2832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30526.3477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24004.4785\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28205.2793\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36589.1680\n",
      "Reducing exploration for all agents to 0.8524\n",
      "\n",
      "Episode 63: Starting computation.\n",
      "Random Seed Set to 73\n",
      "Episode 63: Finished running.\n",
      "Agent 0, Average Reward: -578.08\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24872.9160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24240.1895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30308.3223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32562.0430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26920.4355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24150.2852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18497.8398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17672.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20951.6152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18393.8340\n",
      "Reducing exploration for all agents to 0.85\n",
      "\n",
      "Episode 64: Starting computation.\n",
      "Random Seed Set to 74\n",
      "Episode 64: Finished running.\n",
      "Agent 0, Average Reward: -592.79\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15697.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20438.4434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20342.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16400.4180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15098.7949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21986.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18227.1191\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22784.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14482.7441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19008.3926\n",
      "Reducing exploration for all agents to 0.8476\n",
      "\n",
      "Episode 65: Starting computation.\n",
      "Random Seed Set to 75\n",
      "Episode 65: Finished running.\n",
      "Agent 0, Average Reward: -511.08\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15753.0195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15106.6377\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17619.8652\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15561.1816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19321.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13775.3506\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20469.6387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27619.6621\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14061.9756\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15642.6377\n",
      "Reducing exploration for all agents to 0.8453\n",
      "\n",
      "Episode 66: Starting computation.\n",
      "Random Seed Set to 76\n",
      "Episode 66: Finished running.\n",
      "Agent 0, Average Reward: -533.01\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17365.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16781.8066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18008.8223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17864.7637\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18182.9043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14564.5059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15844.9092\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 9987.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23813.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18820.5391\n",
      "Reducing exploration for all agents to 0.8429\n",
      "\n",
      "Episode 67: Starting computation.\n",
      "Random Seed Set to 77\n",
      "Episode 67: Finished running.\n",
      "Agent 0, Average Reward: -553.04\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16616.3320\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15951.9600\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16556.0098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24882.2910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24736.7871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17579.2871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20535.5488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20381.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13595.1504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19236.7070\n",
      "Reducing exploration for all agents to 0.8405\n",
      "\n",
      "Episode 68: Starting computation.\n",
      "Random Seed Set to 78\n",
      "Episode 68: Finished running.\n",
      "Agent 0, Average Reward: -473.15\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17676.2598\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14819.5508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24052.4121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20206.7090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17539.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29235.5898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21398.1504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16780.0605\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23536.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21587.4922\n",
      "Reducing exploration for all agents to 0.8381\n",
      "\n",
      "Episode 69: Starting computation.\n",
      "Random Seed Set to 79\n",
      "Episode 69: Finished running.\n",
      "Agent 0, Average Reward: -458.2\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21264.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25993.1973\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14942.7783\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24275.7441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21072.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16492.7090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19389.3848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18509.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18676.3887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23743.0996\n",
      "Reducing exploration for all agents to 0.8357\n",
      "\n",
      "Episode 70: Starting computation.\n",
      "Random Seed Set to 80\n",
      "Episode 70: Finished running.\n",
      "Agent 0, Average Reward: -500.49\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16998.1133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21671.0176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22311.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18081.4199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30892.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25049.9590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19528.9160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22995.2988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13378.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21954.6875\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.8333\n",
      "\n",
      "Episode 71: Starting computation.\n",
      "Random Seed Set to 81\n",
      "Episode 71: Finished running.\n",
      "Agent 0, Average Reward: -478.14\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 68373.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 73946.8203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 69482.9375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56678.2930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53268.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49855.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38733.9023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43326.7617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26051.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24854.8379\n",
      "Reducing exploration for all agents to 0.831\n",
      "\n",
      "Episode 72: Starting computation.\n",
      "Random Seed Set to 82\n",
      "Episode 72: Finished running.\n",
      "Agent 0, Average Reward: -911.25\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24174.4238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41604.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25596.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37703.3945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23119.3145\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27908.6074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39828.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40707.0820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21489.8398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45157.0391\n",
      "Reducing exploration for all agents to 0.8286\n",
      "\n",
      "Episode 73: Starting computation.\n",
      "Random Seed Set to 83\n",
      "Episode 73: Finished running.\n",
      "Agent 0, Average Reward: -515.79\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35800.5820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31208.4473\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30265.2461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33177.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26089.7598\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44873.8945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27014.4199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24660.7793\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34313.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24694.3223\n",
      "Reducing exploration for all agents to 0.8262\n",
      "\n",
      "Episode 74: Starting computation.\n",
      "Random Seed Set to 84\n",
      "Episode 74: Finished running.\n",
      "Agent 0, Average Reward: -503.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25062.6582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31249.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26680.4629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29272.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30497.8770\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28475.2012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23049.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26024.0215\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25747.1777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27459.4297\n",
      "Reducing exploration for all agents to 0.8238\n",
      "\n",
      "Episode 75: Starting computation.\n",
      "Random Seed Set to 85\n",
      "Episode 75: Finished running.\n",
      "Agent 0, Average Reward: -455.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20869.1074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32238.0586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30372.7207\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23969.9746\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25258.5957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20593.1660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38513.8945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19048.4902\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20851.2402\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26956.0293\n",
      "Reducing exploration for all agents to 0.8214\n",
      "\n",
      "Episode 76: Starting computation.\n",
      "Random Seed Set to 86\n",
      "Episode 76: Finished running.\n",
      "Agent 0, Average Reward: -502.1\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18474.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21856.0293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20472.9277\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20930.9238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26299.9492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26787.2949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31778.7129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22890.8516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27303.8984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18388.2402\n",
      "Reducing exploration for all agents to 0.8191\n",
      "\n",
      "Episode 77: Starting computation.\n",
      "Random Seed Set to 87\n",
      "Episode 77: Finished running.\n",
      "Agent 0, Average Reward: -574.0\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26621.1211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27712.8711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25268.8105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21751.7754\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31481.0820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23291.7383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24501.7500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31214.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24383.4570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20520.6055\n",
      "Reducing exploration for all agents to 0.8167\n",
      "\n",
      "Episode 78: Starting computation.\n",
      "Random Seed Set to 88\n",
      "Episode 78: Finished running.\n",
      "Agent 0, Average Reward: -552.41\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25139.5293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24418.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22934.6738\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24515.8691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25471.5352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17366.0859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22412.8730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22193.2852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20072.3516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23427.9531\n",
      "Reducing exploration for all agents to 0.8143\n",
      "\n",
      "Episode 79: Starting computation.\n",
      "Random Seed Set to 89\n",
      "Episode 79: Finished running.\n",
      "Agent 0, Average Reward: -452.14\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33725.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19800.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22595.4121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27239.5430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33065.4453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25457.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20929.7246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24089.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20728.2168\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21388.7305\n",
      "Reducing exploration for all agents to 0.8119\n",
      "\n",
      "Episode 80: Starting computation.\n",
      "Random Seed Set to 90\n",
      "Episode 80: Finished running.\n",
      "Agent 0, Average Reward: -470.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32033.9062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21262.8516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23024.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24276.3203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23310.7090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24705.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29045.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24362.3320\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23619.9570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22978.8125\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.8095\n",
      "\n",
      "Episode 81: Starting computation.\n",
      "Random Seed Set to 91\n",
      "Episode 81: Finished running.\n",
      "Agent 0, Average Reward: -443.71\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57480.8633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 63634.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50536.8984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48197.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41639.1289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34421.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30435.1797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34910.4648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31410.4297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29459.9219\n",
      "Reducing exploration for all agents to 0.8072\n",
      "\n",
      "Episode 82: Starting computation.\n",
      "Random Seed Set to 92\n",
      "Episode 82: Finished running.\n",
      "Agent 0, Average Reward: -803.65\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36434.9805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21396.6504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27983.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35922.6094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21221.8535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30884.9746\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23981.3359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35521.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31805.9570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33843.9258\n",
      "Reducing exploration for all agents to 0.8048\n",
      "\n",
      "Episode 83: Starting computation.\n",
      "Random Seed Set to 93\n",
      "Episode 83: Finished running.\n",
      "Agent 0, Average Reward: -532.75\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30774.2109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30042.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35014.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22829.4668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18482.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30687.9121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22019.0176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22489.9160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33514.9336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26729.8887\n",
      "Reducing exploration for all agents to 0.8024\n",
      "\n",
      "Episode 84: Starting computation.\n",
      "Random Seed Set to 94\n",
      "Episode 84: Finished running.\n",
      "Agent 0, Average Reward: -507.49\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24542.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25705.3691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31053.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30362.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22708.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29755.7988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23072.9844\n",
      "Train on 256 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 - 0s - loss: 31490.4473\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24635.0664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20820.0078\n",
      "Reducing exploration for all agents to 0.8\n",
      "\n",
      "Episode 85: Starting computation.\n",
      "Random Seed Set to 95\n",
      "Episode 85: Finished running.\n",
      "Agent 0, Average Reward: -441.99\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28749.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32376.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21472.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23832.9746\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21626.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25327.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27395.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22741.8789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30274.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21708.7422\n",
      "Reducing exploration for all agents to 0.7976\n",
      "\n",
      "Episode 86: Starting computation.\n",
      "Random Seed Set to 96\n",
      "Episode 86: Finished running.\n",
      "Agent 0, Average Reward: -743.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21371.8770\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22498.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16734.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24176.4805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24002.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26018.9316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25159.4551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21203.2988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28112.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27150.8398\n",
      "Reducing exploration for all agents to 0.7953\n",
      "\n",
      "Episode 87: Starting computation.\n",
      "Random Seed Set to 97\n",
      "Episode 87: Finished running.\n",
      "Agent 0, Average Reward: -439.65\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21786.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32149.1699\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18474.0918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20567.4648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39339.0352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21891.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19038.2871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24209.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17280.8086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22314.8203\n",
      "Reducing exploration for all agents to 0.7929\n",
      "\n",
      "Episode 88: Starting computation.\n",
      "Random Seed Set to 98\n",
      "Episode 88: Finished running.\n",
      "Agent 0, Average Reward: -462.29\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33562.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22748.8359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31193.0430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20975.5586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21550.1543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26200.6934\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19943.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24613.8926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28198.2500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31277.7715\n",
      "Reducing exploration for all agents to 0.7905\n",
      "\n",
      "Episode 89: Starting computation.\n",
      "Random Seed Set to 99\n",
      "Episode 89: Finished running.\n",
      "Agent 0, Average Reward: -499.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27143.4922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29464.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20924.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23547.0117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29537.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28879.9648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29520.1035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20663.6055\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25181.6758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20520.5938\n",
      "Reducing exploration for all agents to 0.7881\n",
      "\n",
      "Episode 90: Starting computation.\n",
      "Random Seed Set to 100\n",
      "Episode 90: Finished running.\n",
      "Agent 0, Average Reward: -482.51\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32177.0625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23451.3457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28094.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29695.4512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23637.8711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30661.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33743.8789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17964.7715\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33283.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24641.3984\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.7857\n",
      "\n",
      "Episode 91: Starting computation.\n",
      "Random Seed Set to 101\n",
      "Episode 91: Finished running.\n",
      "Agent 0, Average Reward: -475.92\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53984.5234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54522.6055\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57873.9102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54654.8555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44481.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46116.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32724.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34813.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28552.5488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47006.1758\n",
      "Reducing exploration for all agents to 0.7834\n",
      "\n",
      "Episode 92: Starting computation.\n",
      "Random Seed Set to 102\n",
      "Episode 92: Finished running.\n",
      "Agent 0, Average Reward: -486.7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35596.1836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33964.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38487.2305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38304.6914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40514.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33900.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34289.4648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40703.4414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59614.6602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30592.2891\n",
      "Reducing exploration for all agents to 0.781\n",
      "\n",
      "Episode 93: Starting computation.\n",
      "Random Seed Set to 103\n",
      "Episode 93: Finished running.\n",
      "Agent 0, Average Reward: -438.87\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26163.9609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34742.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28591.0879\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33757.9141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28389.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24300.2129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29709.8359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48489.5508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27360.8867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22682.9766\n",
      "Reducing exploration for all agents to 0.7786\n",
      "\n",
      "Episode 94: Starting computation.\n",
      "Random Seed Set to 104\n",
      "Episode 94: Finished running.\n",
      "Agent 0, Average Reward: -469.71\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31497.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18193.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24686.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24330.0605\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28253.6328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27803.5879\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31395.4941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30299.0898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27024.4805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31221.8965\n",
      "Reducing exploration for all agents to 0.7762\n",
      "\n",
      "Episode 95: Starting computation.\n",
      "Random Seed Set to 105\n",
      "Episode 95: Finished running.\n",
      "Agent 0, Average Reward: -470.32\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16798.9629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28094.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32186.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24449.6523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24465.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23878.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29837.8281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36230.8984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32881.0664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20438.0742\n",
      "Reducing exploration for all agents to 0.7738\n",
      "\n",
      "Episode 96: Starting computation.\n",
      "Random Seed Set to 106\n",
      "Episode 96: Finished running.\n",
      "Agent 0, Average Reward: -475.5\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33659.9805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26617.5137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22491.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60625.6328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33263.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24429.2109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26286.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22779.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24821.5742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30890.6699\n",
      "Reducing exploration for all agents to 0.7714\n",
      "\n",
      "Episode 97: Starting computation.\n",
      "Random Seed Set to 107\n",
      "Episode 97: Finished running.\n",
      "Agent 0, Average Reward: -475.69\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50167.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46629.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29628.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34441.6328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30654.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25783.2070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37612.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29705.0039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30317.8223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30930.8105\n",
      "Reducing exploration for all agents to 0.7691\n",
      "\n",
      "Episode 98: Starting computation.\n",
      "Random Seed Set to 108\n",
      "Episode 98: Finished running.\n",
      "Agent 0, Average Reward: -581.52\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33686.6914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31856.9375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30704.9727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32245.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20891.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33177.0664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32754.4824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34665.3906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47383.5977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39086.2930\n",
      "Reducing exploration for all agents to 0.7667\n",
      "\n",
      "Episode 99: Starting computation.\n",
      "Random Seed Set to 109\n",
      "Episode 99: Finished running.\n",
      "Agent 0, Average Reward: -478.06\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29976.3496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27808.2988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27621.4297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29691.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30341.2832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24728.7461\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20678.5898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32558.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21768.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24864.0117\n",
      "Reducing exploration for all agents to 0.7643\n",
      "\n",
      "Episode 100: Starting computation.\n",
      "Random Seed Set to 110\n",
      "Episode 100: Finished running.\n",
      "Agent 0, Average Reward: -443.12\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30108.2402\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27383.8555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21431.0254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27683.1914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30712.7754\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26542.4512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21020.4570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22044.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25736.8320\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22905.8281\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.7619\n",
      "\n",
      "Episode 101: Starting computation.\n",
      "Random Seed Set to 111\n",
      "Episode 101: Finished running.\n",
      "Agent 0, Average Reward: -453.23\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58852.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56740.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60255.1758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54321.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40292.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35637.7617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46370.4648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 59618.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30745.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30350.4102\n",
      "Reducing exploration for all agents to 0.7595\n",
      "\n",
      "Episode 102: Starting computation.\n",
      "Random Seed Set to 112\n",
      "Episode 102: Finished running.\n",
      "Agent 0, Average Reward: -886.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24120.8398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26976.3047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35685.3945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29715.4160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30476.3008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50210.6133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 49781.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29631.7012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28642.2891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31764.7383\n",
      "Reducing exploration for all agents to 0.7572\n",
      "\n",
      "Episode 103: Starting computation.\n",
      "Random Seed Set to 113\n",
      "Episode 103: Finished running.\n",
      "Agent 0, Average Reward: -444.45\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24970.6895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27164.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24286.1680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28007.6758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27739.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26273.3594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30241.8086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27002.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30581.2070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22480.0625\n",
      "Reducing exploration for all agents to 0.7548\n",
      "\n",
      "Episode 104: Starting computation.\n",
      "Random Seed Set to 114\n",
      "Episode 104: Finished running.\n",
      "Agent 0, Average Reward: -465.87\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33129.7070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26413.2539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29645.0840\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32546.4043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29698.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31728.2969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31713.3672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42306.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22497.9492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34157.2930\n",
      "Reducing exploration for all agents to 0.7524\n",
      "\n",
      "Episode 105: Starting computation.\n",
      "Random Seed Set to 115\n",
      "Episode 105: Finished running.\n",
      "Agent 0, Average Reward: -473.57\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31947.9297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23775.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35055.8867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22287.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23860.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21575.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27732.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32051.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19115.3555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31107.5938\n",
      "Reducing exploration for all agents to 0.75\n",
      "\n",
      "Episode 106: Starting computation.\n",
      "Random Seed Set to 116\n",
      "Episode 106: Finished running.\n",
      "Agent 0, Average Reward: -515.24\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28680.7637\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36763.5352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23544.6543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32427.5039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35122.5039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20788.2988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25443.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24494.1895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21017.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35081.7305\n",
      "Reducing exploration for all agents to 0.7476\n",
      "\n",
      "Episode 107: Starting computation.\n",
      "Random Seed Set to 117\n",
      "Episode 107: Finished running.\n",
      "Agent 0, Average Reward: -452.98\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18302.8164\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32098.6035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29075.2324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21028.6445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24491.7676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35678.5039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23609.0684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27650.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47438.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25810.9805\n",
      "Reducing exploration for all agents to 0.7453\n",
      "\n",
      "Episode 108: Starting computation.\n",
      "Random Seed Set to 118\n",
      "Episode 108: Finished running.\n",
      "Agent 0, Average Reward: -443.64\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35557.7227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41510.1836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26918.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24696.3516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29297.1289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54473.2305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25587.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29957.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29374.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21743.9629\n",
      "Reducing exploration for all agents to 0.7429\n",
      "\n",
      "Episode 109: Starting computation.\n",
      "Random Seed Set to 119\n",
      "Episode 109: Finished running.\n",
      "Agent 0, Average Reward: -456.34\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26808.3770\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16211.4971\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24397.7988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33395.0820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33729.6914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27687.5508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32477.4375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36425.8633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20198.2832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29843.5586\n",
      "Reducing exploration for all agents to 0.7405\n",
      "\n",
      "Episode 110: Starting computation.\n",
      "Random Seed Set to 120\n",
      "Episode 110: Finished running.\n",
      "Agent 0, Average Reward: -445.68\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27740.9668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29750.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31140.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26097.1035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38413.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26160.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32090.5020\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25557.3105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22696.4238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29844.3320\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.7381\n",
      "\n",
      "Episode 111: Starting computation.\n",
      "Random Seed Set to 121\n",
      "Episode 111: Finished running.\n",
      "Agent 0, Average Reward: -450.47\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43038.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57785.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48819.6367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40308.5586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42973.7539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30147.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37995.5430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29748.3379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37519.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28649.1523\n",
      "Reducing exploration for all agents to 0.7357\n",
      "\n",
      "Episode 112: Starting computation.\n",
      "Random Seed Set to 122\n",
      "Episode 112: Finished running.\n",
      "Agent 0, Average Reward: -779.22\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42750.0195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36470.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28536.4043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33334.8477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34481.4414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29054.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34120.6367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25677.3301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39628.9414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41717.1797\n",
      "Reducing exploration for all agents to 0.7334\n",
      "\n",
      "Episode 113: Starting computation.\n",
      "Random Seed Set to 123\n",
      "Episode 113: Finished running.\n",
      "Agent 0, Average Reward: -452.2\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27764.1504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41833.5234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33342.6719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41800.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25279.8887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41733.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29137.4199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35730.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26624.7910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24672.3105\n",
      "Reducing exploration for all agents to 0.731\n",
      "\n",
      "Episode 114: Starting computation.\n",
      "Random Seed Set to 124\n",
      "Episode 114: Finished running.\n",
      "Agent 0, Average Reward: -450.47\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30851.6387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37102.8633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41562.4727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32552.0000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57588.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33834.9180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37191.4258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24690.7227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34418.3555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27787.0840\n",
      "Reducing exploration for all agents to 0.7286\n",
      "\n",
      "Episode 115: Starting computation.\n",
      "Random Seed Set to 125\n",
      "Episode 115: Finished running.\n",
      "Agent 0, Average Reward: -484.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25445.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21537.9316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26662.1973\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21039.6777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29960.1035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48879.2969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32073.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28847.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23046.2871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24535.0840\n",
      "Reducing exploration for all agents to 0.7262\n",
      "\n",
      "Episode 116: Starting computation.\n",
      "Random Seed Set to 126\n",
      "Episode 116: Finished running.\n",
      "Agent 0, Average Reward: -469.7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29845.2422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32915.2227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31779.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28795.7480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42728.4727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19699.5449\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46263.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23821.3613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31344.4238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28066.4004\n",
      "Reducing exploration for all agents to 0.7238\n",
      "\n",
      "Episode 117: Starting computation.\n",
      "Random Seed Set to 127\n",
      "Episode 117: Finished running.\n",
      "Agent 0, Average Reward: -466.11\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44161.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28832.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31144.7285\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18848.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35739.7617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34405.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56371.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35426.7070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51838.1133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25149.6777\n",
      "Reducing exploration for all agents to 0.7215\n",
      "\n",
      "Episode 118: Starting computation.\n",
      "Random Seed Set to 128\n",
      "Episode 118: Finished running.\n",
      "Agent 0, Average Reward: -460.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42424.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25138.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36156.7227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26526.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24181.2676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33001.8164\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34306.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30971.5723\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33690.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31677.0410\n",
      "Reducing exploration for all agents to 0.7191\n",
      "\n",
      "Episode 119: Starting computation.\n",
      "Random Seed Set to 129\n",
      "Episode 119: Finished running.\n",
      "Agent 0, Average Reward: -442.16\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36700.9375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27991.6621\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23829.6973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29293.0449\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 57127.1758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29830.5449\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30237.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33929.2070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25740.2129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29381.5430\n",
      "Reducing exploration for all agents to 0.7167\n",
      "\n",
      "Episode 120: Starting computation.\n",
      "Random Seed Set to 130\n",
      "Episode 120: Finished running.\n",
      "Agent 0, Average Reward: -487.48\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28972.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28001.1582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28661.3184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30455.0117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29199.2051\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41593.9023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30535.4395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28266.1895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34602.6992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32146.5840\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.7143\n",
      "\n",
      "Episode 121: Starting computation.\n",
      "Random Seed Set to 131\n",
      "Episode 121: Finished running.\n",
      "Agent 0, Average Reward: -452.48\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39501.1055\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52369.2812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41615.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52752.1680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39667.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31499.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41170.3555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31616.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39966.6992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24067.3555\n",
      "Reducing exploration for all agents to 0.7119\n",
      "\n",
      "Episode 122: Starting computation.\n",
      "Random Seed Set to 132\n",
      "Episode 122: Finished running.\n",
      "Agent 0, Average Reward: -932.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38490.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24300.0840\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33113.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27536.7676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33179.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32012.2012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56841.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28611.0430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35528.9336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25250.4121\n",
      "Reducing exploration for all agents to 0.7095\n",
      "\n",
      "Episode 123: Starting computation.\n",
      "Random Seed Set to 133\n",
      "Episode 123: Finished running.\n",
      "Agent 0, Average Reward: -446.36\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29974.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30739.9277\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26787.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29448.8066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34860.6914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30893.6699\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38387.3359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34563.7500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24170.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48200.7734\n",
      "Reducing exploration for all agents to 0.7072\n",
      "\n",
      "Episode 124: Starting computation.\n",
      "Random Seed Set to 134\n",
      "Episode 124: Finished running.\n",
      "Agent 0, Average Reward: -460.2\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26435.6895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35355.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21805.5215\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22097.8926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16798.4902\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34290.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26119.1621\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34635.3672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31099.7793\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26982.7051\n",
      "Reducing exploration for all agents to 0.7048\n",
      "\n",
      "Episode 125: Starting computation.\n",
      "Random Seed Set to 135\n",
      "Episode 125: Finished running.\n",
      "Agent 0, Average Reward: -442.77\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31240.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21030.9082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39491.9180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36425.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24144.3848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31276.2520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32856.0625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24367.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27483.2012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34301.3125\n",
      "Reducing exploration for all agents to 0.7024\n",
      "\n",
      "Episode 126: Starting computation.\n",
      "Random Seed Set to 136\n",
      "Episode 126: Finished running.\n",
      "Agent 0, Average Reward: -437.84\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23124.4355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26643.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23087.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35824.0273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33857.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23634.6738\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24314.8066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27913.3652\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37425.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31246.3906\n",
      "Reducing exploration for all agents to 0.7\n",
      "\n",
      "Episode 127: Starting computation.\n",
      "Random Seed Set to 137\n",
      "Episode 127: Finished running.\n",
      "Agent 0, Average Reward: -441.56\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32548.5176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26741.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26092.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31895.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29225.8945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33827.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30454.4199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32965.5742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29106.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27429.6406\n",
      "Reducing exploration for all agents to 0.6976\n",
      "\n",
      "Episode 128: Starting computation.\n",
      "Random Seed Set to 138\n",
      "Episode 128: Finished running.\n",
      "Agent 0, Average Reward: -432.57\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22325.9902\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26347.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31778.6543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28068.1855\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29209.9121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34976.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35824.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28018.1465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23229.6738\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23053.0000\n",
      "Reducing exploration for all agents to 0.6953\n",
      "\n",
      "Episode 129: Starting computation.\n",
      "Random Seed Set to 139\n",
      "Episode 129: Finished running.\n",
      "Agent 0, Average Reward: -439.41\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40114.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20129.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23479.4395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31749.8965\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23754.0938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23823.1367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25598.5137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20688.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19962.5449\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24013.5254\n",
      "Reducing exploration for all agents to 0.6929\n",
      "\n",
      "Episode 130: Starting computation.\n",
      "Random Seed Set to 140\n",
      "Episode 130: Finished running.\n",
      "Agent 0, Average Reward: -451.07\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26264.9316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37309.0625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20133.6895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31866.1289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21081.9492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28501.2285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24198.5254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22255.1484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34132.7500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23057.4473\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.6905\n",
      "\n",
      "Episode 131: Starting computation.\n",
      "Random Seed Set to 141\n",
      "Episode 131: Finished running.\n",
      "Agent 0, Average Reward: -443.18\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47273.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34806.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34199.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34590.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35652.0898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29350.7441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23100.4355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35695.2422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25639.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29195.6035\n",
      "Reducing exploration for all agents to 0.6881\n",
      "\n",
      "Episode 132: Starting computation.\n",
      "Random Seed Set to 142\n",
      "Episode 132: Finished running.\n",
      "Agent 0, Average Reward: -855.65\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53265.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36940.2266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33158.9648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33273.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26853.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27223.4902\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37256.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36778.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21459.7246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29600.6250\n",
      "Reducing exploration for all agents to 0.6857\n",
      "\n",
      "Episode 133: Starting computation.\n",
      "Random Seed Set to 143\n",
      "Episode 133: Finished running.\n",
      "Agent 0, Average Reward: -438.18\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29524.5352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28971.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29195.8262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50070.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30662.8496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36039.0938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32083.3496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34575.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25245.9902\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27726.1133\n",
      "Reducing exploration for all agents to 0.6834\n",
      "\n",
      "Episode 134: Starting computation.\n",
      "Random Seed Set to 144\n",
      "Episode 134: Finished running.\n",
      "Agent 0, Average Reward: -470.85\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32973.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25883.5684\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33613.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39091.6602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30266.2129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44744.1172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29733.1465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40870.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31871.9512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25944.1680\n",
      "Reducing exploration for all agents to 0.681\n",
      "\n",
      "Episode 135: Starting computation.\n",
      "Random Seed Set to 145\n",
      "Episode 135: Finished running.\n",
      "Agent 0, Average Reward: -486.64\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33724.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26076.2441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34774.4648\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37818.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44265.3555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27597.1855\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35049.8203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28011.4180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34993.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25689.8457\n",
      "Reducing exploration for all agents to 0.6786\n",
      "\n",
      "Episode 136: Starting computation.\n",
      "Random Seed Set to 146\n",
      "Episode 136: Finished running.\n",
      "Agent 0, Average Reward: -460.22\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35205.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27881.3418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27237.3809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45970.9297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38254.3906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34499.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26739.9434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38062.7578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34908.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27628.5957\n",
      "Reducing exploration for all agents to 0.6762\n",
      "\n",
      "Episode 137: Starting computation.\n",
      "Random Seed Set to 147\n",
      "Episode 137: Finished running.\n",
      "Agent 0, Average Reward: -442.31\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29557.5723\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34320.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32056.4199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28445.2793\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44544.2500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39384.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29036.4590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38801.8359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35855.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29701.2129\n",
      "Reducing exploration for all agents to 0.6738\n",
      "\n",
      "Episode 138: Starting computation.\n",
      "Random Seed Set to 148\n",
      "Episode 138: Finished running.\n",
      "Agent 0, Average Reward: -445.18\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29574.8926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31584.9238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36346.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33783.1758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37032.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35201.8828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24822.4961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25075.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42038.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34530.3281\n",
      "Reducing exploration for all agents to 0.6715\n",
      "\n",
      "Episode 139: Starting computation.\n",
      "Random Seed Set to 149\n",
      "Episode 139: Finished running.\n",
      "Agent 0, Average Reward: -443.2\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38028.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31022.2129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33019.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35079.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38825.5273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41449.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38188.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33688.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34984.6172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26210.6152\n",
      "Reducing exploration for all agents to 0.6691\n",
      "\n",
      "Episode 140: Starting computation.\n",
      "Random Seed Set to 150\n",
      "Episode 140: Finished running.\n",
      "Agent 0, Average Reward: -442.21\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27718.4570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28888.8379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25117.6113\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35178.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25978.7031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34883.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21748.2480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24329.1191\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20806.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25646.2031\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.6667\n",
      "\n",
      "Episode 141: Starting computation.\n",
      "Random Seed Set to 151\n",
      "Episode 141: Finished running.\n",
      "Agent 0, Average Reward: -447.12\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47347.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38403.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37485.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36710.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24363.0566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37699.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22209.8418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28647.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36924.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26938.1777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.6643\n",
      "\n",
      "Episode 142: Starting computation.\n",
      "Random Seed Set to 152\n",
      "Episode 142: Finished running.\n",
      "Agent 0, Average Reward: -691.08\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37607.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35110.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26399.1367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 51301.1016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35850.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30347.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37426.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42313.0938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29093.7168\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34070.8203\n",
      "Reducing exploration for all agents to 0.6619\n",
      "\n",
      "Episode 143: Starting computation.\n",
      "Random Seed Set to 153\n",
      "Episode 143: Finished running.\n",
      "Agent 0, Average Reward: -447.79\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25418.6816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26036.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35643.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39863.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30464.1797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38879.4805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41846.4961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30703.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30150.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39763.3945\n",
      "Reducing exploration for all agents to 0.6596\n",
      "\n",
      "Episode 144: Starting computation.\n",
      "Random Seed Set to 154\n",
      "Episode 144: Finished running.\n",
      "Agent 0, Average Reward: -450.54\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23199.2227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32341.1426\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48959.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28072.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32350.2559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31675.7617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31409.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38688.5898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32630.6172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21142.4629\n",
      "Reducing exploration for all agents to 0.6572\n",
      "\n",
      "Episode 145: Starting computation.\n",
      "Random Seed Set to 155\n",
      "Episode 145: Finished running.\n",
      "Agent 0, Average Reward: -436.86\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30387.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34216.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27406.9297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35409.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24081.9492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42140.4141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30137.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34152.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38695.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36240.3906\n",
      "Reducing exploration for all agents to 0.6548\n",
      "\n",
      "Episode 146: Starting computation.\n",
      "Random Seed Set to 156\n",
      "Episode 146: Finished running.\n",
      "Agent 0, Average Reward: -462.62\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23852.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30779.4180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33510.8203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36298.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33496.0430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41302.4297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31596.6816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29513.5430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35056.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28772.4062\n",
      "Reducing exploration for all agents to 0.6524\n",
      "\n",
      "Episode 147: Starting computation.\n",
      "Random Seed Set to 157\n",
      "Episode 147: Finished running.\n",
      "Agent 0, Average Reward: -440.4\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44978.8164\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19964.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24519.1133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39809.2812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25636.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25907.1172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26530.2539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20996.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27171.9062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32582.5586\n",
      "Reducing exploration for all agents to 0.65\n",
      "\n",
      "Episode 148: Starting computation.\n",
      "Random Seed Set to 158\n",
      "Episode 148: Finished running.\n",
      "Agent 0, Average Reward: -441.86\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56828.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34003.5273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27016.7676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42637.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20518.5059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38322.7422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39561.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20277.6191\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35894.3672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25063.1133\n",
      "Reducing exploration for all agents to 0.6476\n",
      "\n",
      "Episode 149: Starting computation.\n",
      "Random Seed Set to 159\n",
      "Episode 149: Finished running.\n",
      "Agent 0, Average Reward: -430.52\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18246.3164\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18607.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27640.9160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35309.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42397.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23368.1133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62488.8398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29703.2227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24406.6660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32563.1797\n",
      "Reducing exploration for all agents to 0.6453\n",
      "\n",
      "Episode 150: Starting computation.\n",
      "Random Seed Set to 160\n",
      "Episode 150: Finished running.\n",
      "Agent 0, Average Reward: -447.16\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25545.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30524.2891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18660.8281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27582.3008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35274.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25656.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33842.2969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32276.8301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38345.8984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36515.1289\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.6429\n",
      "\n",
      "Episode 151: Starting computation.\n",
      "Random Seed Set to 161\n",
      "Episode 151: Finished running.\n",
      "Agent 0, Average Reward: -442.34\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43770.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41031.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41533.2969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33717.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21609.7773\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33706.8203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22722.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23117.8711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27945.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33803.1094\n",
      "Reducing exploration for all agents to 0.6405\n",
      "\n",
      "Episode 152: Starting computation.\n",
      "Random Seed Set to 162\n",
      "Episode 152: Finished running.\n",
      "Agent 0, Average Reward: -433.62\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28567.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29677.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23862.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32261.3770\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23363.3789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36118.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32856.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38779.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30487.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30601.9805\n",
      "Reducing exploration for all agents to 0.6381\n",
      "\n",
      "Episode 153: Starting computation.\n",
      "Random Seed Set to 163\n",
      "Episode 153: Finished running.\n",
      "Agent 0, Average Reward: -449.59\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33983.0586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37219.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33976.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28775.9238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26628.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28462.9668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34860.2891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28666.4277\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24001.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22780.6484\n",
      "Reducing exploration for all agents to 0.6357\n",
      "\n",
      "Episode 154: Starting computation.\n",
      "Random Seed Set to 164\n",
      "Episode 154: Finished running.\n",
      "Agent 0, Average Reward: -438.14\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27001.9805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27845.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33430.8789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25618.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35840.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27458.0039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18063.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26600.5820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22509.9297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22126.5293\n",
      "Reducing exploration for all agents to 0.6334\n",
      "\n",
      "Episode 155: Starting computation.\n",
      "Random Seed Set to 165\n",
      "Episode 155: Finished running.\n",
      "Agent 0, Average Reward: -471.26\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24323.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31051.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23336.5273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24983.4297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23600.9727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21320.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27492.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27608.3730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27526.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19272.5645\n",
      "Reducing exploration for all agents to 0.631\n",
      "\n",
      "Episode 156: Starting computation.\n",
      "Random Seed Set to 166\n",
      "Episode 156: Finished running.\n",
      "Agent 0, Average Reward: -447.12\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31534.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25966.0137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27295.4355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21197.2285\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19836.1426\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24926.1016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38515.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31411.1738\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20630.6973\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24373.0156\n",
      "Reducing exploration for all agents to 0.6286\n",
      "\n",
      "Episode 157: Starting computation.\n",
      "Random Seed Set to 167\n",
      "Episode 157: Finished running.\n",
      "Agent 0, Average Reward: -455.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24425.0195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33410.2539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23669.7402\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23945.1680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27600.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35658.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25237.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25855.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33737.1875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24358.0742\n",
      "Reducing exploration for all agents to 0.6262\n",
      "\n",
      "Episode 158: Starting computation.\n",
      "Random Seed Set to 168\n",
      "Episode 158: Finished running.\n",
      "Agent 0, Average Reward: -486.27\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27074.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35063.3906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23279.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22422.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19422.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27905.9570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24777.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26594.2422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26644.4570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32481.1055\n",
      "Reducing exploration for all agents to 0.6238\n",
      "\n",
      "Episode 159: Starting computation.\n",
      "Random Seed Set to 169\n",
      "Episode 159: Finished running.\n",
      "Agent 0, Average Reward: -441.6\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27980.9336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31509.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26313.4297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25342.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27563.1152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25522.5918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25530.1289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22511.8945\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34128.5508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30503.4961\n",
      "Reducing exploration for all agents to 0.6215\n",
      "\n",
      "Episode 160: Starting computation.\n",
      "Random Seed Set to 170\n",
      "Episode 160: Finished running.\n",
      "Agent 0, Average Reward: -439.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29244.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25380.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27745.9082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31976.6152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28851.3887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21942.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30712.3340\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34759.3516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26180.8477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19864.1289\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.6191\n",
      "\n",
      "Episode 161: Starting computation.\n",
      "Random Seed Set to 171\n",
      "Episode 161: Finished running.\n",
      "Agent 0, Average Reward: -435.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41765.9297\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40181.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33477.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30879.8242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33765.0859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30887.0273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25540.7422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26269.9102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21111.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25440.3223\n",
      "Reducing exploration for all agents to 0.6167\n",
      "\n",
      "Episode 162: Starting computation.\n",
      "Random Seed Set to 172\n",
      "Episode 162: Finished running.\n",
      "Agent 0, Average Reward: -458.74\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33171.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36901.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31849.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33298.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26832.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34547.0469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26703.8477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35033.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22550.1582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24599.9453\n",
      "Reducing exploration for all agents to 0.6143\n",
      "\n",
      "Episode 163: Starting computation.\n",
      "Random Seed Set to 173\n",
      "Episode 163: Finished running.\n",
      "Agent 0, Average Reward: -446.26\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28023.6992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30221.1074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34579.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29552.8828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24574.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31351.5293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21283.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23238.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24492.2441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30554.5410\n",
      "Reducing exploration for all agents to 0.6119\n",
      "\n",
      "Episode 164: Starting computation.\n",
      "Random Seed Set to 174\n",
      "Episode 164: Finished running.\n",
      "Agent 0, Average Reward: -427.64\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28365.1621\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26463.9805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25551.5762\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27892.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47974.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47310.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28796.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28003.0898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21981.9922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29651.8262\n",
      "Reducing exploration for all agents to 0.6096\n",
      "\n",
      "Episode 165: Starting computation.\n",
      "Random Seed Set to 175\n",
      "Episode 165: Finished running.\n",
      "Agent 0, Average Reward: -440.97\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27092.5762\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24975.1348\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26703.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20994.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26786.4805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36308.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31714.8887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40827.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24441.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42570.3125\n",
      "Reducing exploration for all agents to 0.6072\n",
      "\n",
      "Episode 166: Starting computation.\n",
      "Random Seed Set to 176\n",
      "Episode 166: Finished running.\n",
      "Agent 0, Average Reward: -456.98\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43218.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21209.9863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27520.2969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25341.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17133.8516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27473.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38250.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32571.9004\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26039.9531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26659.7852\n",
      "Reducing exploration for all agents to 0.6048\n",
      "\n",
      "Episode 167: Starting computation.\n",
      "Random Seed Set to 177\n",
      "Episode 167: Finished running.\n",
      "Agent 0, Average Reward: -438.57\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25290.2266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18675.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22152.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34930.0547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22466.4805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22975.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31711.1172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48765.4375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24648.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24938.5898\n",
      "Reducing exploration for all agents to 0.6024\n",
      "\n",
      "Episode 168: Starting computation.\n",
      "Random Seed Set to 178\n",
      "Episode 168: Finished running.\n",
      "Agent 0, Average Reward: -441.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38034.5234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40436.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33623.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21331.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19384.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20932.1582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16585.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24599.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24181.8711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32700.0430\n",
      "Reducing exploration for all agents to 0.6\n",
      "\n",
      "Episode 169: Starting computation.\n",
      "Random Seed Set to 179\n",
      "Episode 169: Finished running.\n",
      "Agent 0, Average Reward: -436.57\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21518.1406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30060.5547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27078.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24451.6797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53422.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23345.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39295.0820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42705.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27123.8867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28311.7598\n",
      "Reducing exploration for all agents to 0.5977\n",
      "\n",
      "Episode 170: Starting computation.\n",
      "Random Seed Set to 180\n",
      "Episode 170: Finished running.\n",
      "Agent 0, Average Reward: -425.61\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26003.3652\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29648.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26077.0254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18294.8105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27476.0664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22434.8418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18751.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18312.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28126.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25196.4805\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.5953\n",
      "\n",
      "Episode 171: Starting computation.\n",
      "Random Seed Set to 181\n",
      "Episode 171: Finished running.\n",
      "Agent 0, Average Reward: -438.34\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34867.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29248.1367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28735.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33902.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34186.2891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29145.0762\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23329.0410\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19668.0625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35232.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28617.0078\n",
      "Reducing exploration for all agents to 0.5929\n",
      "\n",
      "Episode 172: Starting computation.\n",
      "Random Seed Set to 182\n",
      "Episode 172: Finished running.\n",
      "Agent 0, Average Reward: -453.02\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17720.8105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20453.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23151.6387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29118.0430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28357.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33102.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25398.9062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22069.6523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26905.0684\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26675.4707\n",
      "Reducing exploration for all agents to 0.5905\n",
      "\n",
      "Episode 173: Starting computation.\n",
      "Random Seed Set to 183\n",
      "Episode 173: Finished running.\n",
      "Agent 0, Average Reward: -441.85\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27415.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26065.7266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26696.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31179.8574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18885.7988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23931.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26478.7012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25621.9609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16163.9678\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32182.7051\n",
      "Reducing exploration for all agents to 0.5881\n",
      "\n",
      "Episode 174: Starting computation.\n",
      "Random Seed Set to 184\n",
      "Episode 174: Finished running.\n",
      "Agent 0, Average Reward: -438.7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20046.8320\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24109.3359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20912.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19437.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24529.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26537.3496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19995.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31717.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26261.8867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32518.9336\n",
      "Reducing exploration for all agents to 0.5857\n",
      "\n",
      "Episode 175: Starting computation.\n",
      "Random Seed Set to 185\n",
      "Episode 175: Finished running.\n",
      "Agent 0, Average Reward: -446.33\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23989.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22679.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34742.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19347.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22733.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24069.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29687.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26464.5566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25078.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23336.0020\n",
      "Reducing exploration for all agents to 0.5834\n",
      "\n",
      "Episode 176: Starting computation.\n",
      "Random Seed Set to 186\n",
      "Episode 176: Finished running.\n",
      "Agent 0, Average Reward: -441.56\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20449.3555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20922.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24929.9746\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31235.2578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30648.3828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20242.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23711.6113\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25516.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26613.4141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28860.8789\n",
      "Reducing exploration for all agents to 0.581\n",
      "\n",
      "Episode 177: Starting computation.\n",
      "Random Seed Set to 187\n",
      "Episode 177: Finished running.\n",
      "Agent 0, Average Reward: -447.81\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24130.1426\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29676.2773\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23318.7617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28121.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16272.2305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21531.0273\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23461.1387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26268.9160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20607.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25453.3926\n",
      "Reducing exploration for all agents to 0.5786\n",
      "\n",
      "Episode 178: Starting computation.\n",
      "Random Seed Set to 188\n",
      "Episode 178: Finished running.\n",
      "Agent 0, Average Reward: -434.98\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34313.7500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34625.0586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26906.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21521.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19539.7363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21489.2422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39864.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24583.5547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19808.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21026.5410\n",
      "Reducing exploration for all agents to 0.5762\n",
      "\n",
      "Episode 179: Starting computation.\n",
      "Random Seed Set to 189\n",
      "Episode 179: Finished running.\n",
      "Agent 0, Average Reward: -443.36\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26471.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22511.6387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25277.3477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33591.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26533.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27634.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19536.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25145.4199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29236.9668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29079.7520\n",
      "Reducing exploration for all agents to 0.5738\n",
      "\n",
      "Episode 180: Starting computation.\n",
      "Random Seed Set to 190\n",
      "Episode 180: Finished running.\n",
      "Agent 0, Average Reward: -456.85\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18141.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34656.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19151.3711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19372.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26752.9492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33202.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22239.3809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27208.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32298.1230\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36114.6367\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.5715\n",
      "\n",
      "Episode 181: Starting computation.\n",
      "Random Seed Set to 191\n",
      "Episode 181: Finished running.\n",
      "Agent 0, Average Reward: -434.74\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37293.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32827.7773\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30597.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32501.8359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34056.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20932.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27008.2988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32665.4551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28607.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34081.2109\n",
      "Reducing exploration for all agents to 0.5691\n",
      "\n",
      "Episode 182: Starting computation.\n",
      "Random Seed Set to 192\n",
      "Episode 182: Finished running.\n",
      "Agent 0, Average Reward: -494.66\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28961.4980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25843.8027\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23099.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28989.8730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36289.8242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25530.9121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26870.0195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23856.7227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20672.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23543.7500\n",
      "Reducing exploration for all agents to 0.5667\n",
      "\n",
      "Episode 183: Starting computation.\n",
      "Random Seed Set to 193\n",
      "Episode 183: Finished running.\n",
      "Agent 0, Average Reward: -452.54\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26184.4023\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35810.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28074.6777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25056.2676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26172.3262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33351.6328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24180.2109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22273.4277\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24215.8613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25666.6523\n",
      "Reducing exploration for all agents to 0.5643\n",
      "\n",
      "Episode 184: Starting computation.\n",
      "Random Seed Set to 194\n",
      "Episode 184: Finished running.\n",
      "Agent 0, Average Reward: -433.83\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38944.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23255.6035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28335.3594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20593.9688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27271.3457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25053.9414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20857.6797\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18725.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20640.3730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21054.1113\n",
      "Reducing exploration for all agents to 0.5619\n",
      "\n",
      "Episode 185: Starting computation.\n",
      "Random Seed Set to 195\n",
      "Episode 185: Finished running.\n",
      "Agent 0, Average Reward: -456.13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22209.6504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28115.8828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36486.2969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24996.4355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35205.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28213.6777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23866.1816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28000.5586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27239.1660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26758.9512\n",
      "Reducing exploration for all agents to 0.5596\n",
      "\n",
      "Episode 186: Starting computation.\n",
      "Random Seed Set to 196\n",
      "Episode 186: Finished running.\n",
      "Agent 0, Average Reward: -439.66\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28514.4629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21482.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24987.9980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30004.0645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26901.3652\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19356.4980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29755.9238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28528.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28438.4395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23065.7344\n",
      "Reducing exploration for all agents to 0.5572\n",
      "\n",
      "Episode 187: Starting computation.\n",
      "Random Seed Set to 197\n",
      "Episode 187: Finished running.\n",
      "Agent 0, Average Reward: -438.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36331.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31365.5234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27027.1270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21689.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25206.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27653.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21852.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21600.4102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26875.9277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28393.4238\n",
      "Reducing exploration for all agents to 0.5548\n",
      "\n",
      "Episode 188: Starting computation.\n",
      "Random Seed Set to 198\n",
      "Episode 188: Finished running.\n",
      "Agent 0, Average Reward: -436.67\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23135.4707\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30799.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26147.3750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25813.0098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16763.7812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23740.3750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17719.2559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26345.3496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24315.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32891.7188\n",
      "Reducing exploration for all agents to 0.5524\n",
      "\n",
      "Episode 189: Starting computation.\n",
      "Random Seed Set to 199\n",
      "Episode 189: Finished running.\n",
      "Agent 0, Average Reward: -438.87\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25252.9473\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31067.2949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28158.5117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21965.9180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28933.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23530.4316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29141.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23253.3184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31149.4043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26451.3848\n",
      "Reducing exploration for all agents to 0.55\n",
      "\n",
      "Episode 190: Starting computation.\n",
      "Random Seed Set to 200\n",
      "Episode 190: Finished running.\n",
      "Agent 0, Average Reward: -443.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21985.4102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16766.8613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23807.3965\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20516.2012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25006.9277\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27330.0996\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23198.0918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44257.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23924.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24624.1777\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.5477\n",
      "\n",
      "Episode 191: Starting computation.\n",
      "Random Seed Set to 201\n",
      "Episode 191: Finished running.\n",
      "Agent 0, Average Reward: -437.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36688.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38430.2109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32471.4902\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27816.4668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34954.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16794.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27996.3691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18445.2559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23428.5566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19579.8613\n",
      "Reducing exploration for all agents to 0.5453\n",
      "\n",
      "Episode 192: Starting computation.\n",
      "Random Seed Set to 202\n",
      "Episode 192: Finished running.\n",
      "Agent 0, Average Reward: -465.61\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28102.6035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 48539.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46676.2930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31258.5586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65564.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32326.4922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25286.1152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21894.2852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39153.8828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32896.8086\n",
      "Reducing exploration for all agents to 0.5429\n",
      "\n",
      "Episode 193: Starting computation.\n",
      "Random Seed Set to 203\n",
      "Episode 193: Finished running.\n",
      "Agent 0, Average Reward: -454.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24753.0449\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45553.0859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21294.3672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26251.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28901.6504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26232.3418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24144.9980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19500.0449\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30270.6387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24848.2754\n",
      "Reducing exploration for all agents to 0.5405\n",
      "\n",
      "Episode 194: Starting computation.\n",
      "Random Seed Set to 204\n",
      "Episode 194: Finished running.\n",
      "Agent 0, Average Reward: -440.24\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26183.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20626.4121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21408.2852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18422.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35903.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27602.2227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26623.7871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20929.1543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21991.3965\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24315.6641\n",
      "Reducing exploration for all agents to 0.5381\n",
      "\n",
      "Episode 195: Starting computation.\n",
      "Random Seed Set to 205\n",
      "Episode 195: Finished running.\n",
      "Agent 0, Average Reward: -444.86\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45349.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30557.1113\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29900.0039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23735.4902\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30201.6816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18173.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22090.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12019.0596\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34671.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40383.0547\n",
      "Reducing exploration for all agents to 0.5358\n",
      "\n",
      "Episode 196: Starting computation.\n",
      "Random Seed Set to 206\n",
      "Episode 196: Finished running.\n",
      "Agent 0, Average Reward: -458.68\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25374.3262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29133.0586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28457.4160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29034.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28406.9629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24261.2402\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23481.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24637.9062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19944.6270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38346.2656\n",
      "Reducing exploration for all agents to 0.5334\n",
      "\n",
      "Episode 197: Starting computation.\n",
      "Random Seed Set to 207\n",
      "Episode 197: Finished running.\n",
      "Agent 0, Average Reward: -442.21\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18185.4785\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23604.3789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27619.7520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33981.8047\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21717.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25636.4414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27528.3027\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21763.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21236.0664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28505.8008\n",
      "Reducing exploration for all agents to 0.531\n",
      "\n",
      "Episode 198: Starting computation.\n",
      "Random Seed Set to 208\n",
      "Episode 198: Finished running.\n",
      "Agent 0, Average Reward: -437.85\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23010.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24435.3418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22680.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22335.5723\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20762.5605\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22557.6309\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21598.0879\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20978.0332\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29770.1504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23020.7773\n",
      "Reducing exploration for all agents to 0.5286\n",
      "\n",
      "Episode 199: Starting computation.\n",
      "Random Seed Set to 209\n",
      "Episode 199: Finished running.\n",
      "Agent 0, Average Reward: -441.44\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23290.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28032.4590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22611.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30829.8340\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21618.2871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28227.2090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16493.2676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25222.0625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32883.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24863.7949\n",
      "Reducing exploration for all agents to 0.5262\n",
      "\n",
      "Episode 200: Starting computation.\n",
      "Random Seed Set to 210\n",
      "Episode 200: Finished running.\n",
      "Agent 0, Average Reward: -437.73\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28128.7070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23130.3340\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20417.8457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24828.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19325.5059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24751.3418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25181.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23438.2637\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28754.9082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22975.6191\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.5238\n",
      "\n",
      "Episode 201: Starting computation.\n",
      "Random Seed Set to 211\n",
      "Episode 201: Finished running.\n",
      "Agent 0, Average Reward: -443.76\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34792.4453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38619.5938\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32619.2832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24777.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19444.6543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24333.8535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27933.9238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24499.4355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29005.2227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37342.3516\n",
      "Reducing exploration for all agents to 0.5215\n",
      "\n",
      "Episode 202: Starting computation.\n",
      "Random Seed Set to 212\n",
      "Episode 202: Finished running.\n",
      "Agent 0, Average Reward: -446.67\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29263.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29983.4277\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17722.3730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18744.0762\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29286.6621\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42912.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21954.5508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23802.1504\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29434.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19626.1699\n",
      "Reducing exploration for all agents to 0.5191\n",
      "\n",
      "Episode 203: Starting computation.\n",
      "Random Seed Set to 213\n",
      "Episode 203: Finished running.\n",
      "Agent 0, Average Reward: -438.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30814.2363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23732.1152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28305.5996\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25737.2598\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33472.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33376.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22884.3223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19221.8301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40378.3672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18554.4688\n",
      "Reducing exploration for all agents to 0.5167\n",
      "\n",
      "Episode 204: Starting computation.\n",
      "Random Seed Set to 214\n",
      "Episode 204: Finished running.\n",
      "Agent 0, Average Reward: -436.72\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28847.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27859.8496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23020.8516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25079.4414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33162.3594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17562.7676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25239.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26727.2949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28354.3027\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19309.8809\n",
      "Reducing exploration for all agents to 0.5143\n",
      "\n",
      "Episode 205: Starting computation.\n",
      "Random Seed Set to 215\n",
      "Episode 205: Finished running.\n",
      "Agent 0, Average Reward: -436.73\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23740.3730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31656.2559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22614.7793\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32066.9277\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24743.4727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39257.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22269.5996\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18538.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18462.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21895.3574\n",
      "Reducing exploration for all agents to 0.5119\n",
      "\n",
      "Episode 206: Starting computation.\n",
      "Random Seed Set to 216\n",
      "Episode 206: Finished running.\n",
      "Agent 0, Average Reward: -460.78\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24621.1582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18054.9980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27858.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27344.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32522.1895\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28293.6426\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17120.0137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25089.7441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20314.1230\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21797.0176\n",
      "Reducing exploration for all agents to 0.5096\n",
      "\n",
      "Episode 207: Starting computation.\n",
      "Random Seed Set to 217\n",
      "Episode 207: Finished running.\n",
      "Agent 0, Average Reward: -429.75\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22677.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26581.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30258.5742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25465.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22731.2051\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26464.4395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23231.8574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23942.7988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30102.2285\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24507.5137\n",
      "Reducing exploration for all agents to 0.5072\n",
      "\n",
      "Episode 208: Starting computation.\n",
      "Random Seed Set to 218\n",
      "Episode 208: Finished running.\n",
      "Agent 0, Average Reward: -447.5\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33479.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20692.7012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23483.8867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16998.3789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28338.6191\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25698.3184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32160.1855\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27966.0020\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20642.4004\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22539.0215\n",
      "Reducing exploration for all agents to 0.5048\n",
      "\n",
      "Episode 209: Starting computation.\n",
      "Random Seed Set to 219\n",
      "Episode 209: Finished running.\n",
      "Agent 0, Average Reward: -432.9\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36413.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28067.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24442.4707\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18219.7129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27745.7324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28471.4590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29166.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28715.9492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22545.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24625.4277\n",
      "Reducing exploration for all agents to 0.5024\n",
      "\n",
      "Episode 210: Starting computation.\n",
      "Random Seed Set to 220\n",
      "Episode 210: Finished running.\n",
      "Agent 0, Average Reward: -471.38\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22714.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15637.9658\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19280.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21925.9941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17954.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19313.3848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31183.1152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28257.9004\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23489.8691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22082.3496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.5\n",
      "\n",
      "Episode 211: Starting computation.\n",
      "Random Seed Set to 221\n",
      "Episode 211: Finished running.\n",
      "Agent 0, Average Reward: -462.03\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39219.1211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34700.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29742.2832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28548.2734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31769.3730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27740.3418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25064.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29770.6035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29728.6699\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24170.9922\n",
      "Reducing exploration for all agents to 0.4977\n",
      "\n",
      "Episode 212: Starting computation.\n",
      "Random Seed Set to 222\n",
      "Episode 212: Finished running.\n",
      "Agent 0, Average Reward: -429.92\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39625.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38753.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22104.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36347.3008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19126.6328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34679.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33843.0820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30545.7949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30010.7090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26769.3555\n",
      "Reducing exploration for all agents to 0.4953\n",
      "\n",
      "Episode 213: Starting computation.\n",
      "Random Seed Set to 223\n",
      "Episode 213: Finished running.\n",
      "Agent 0, Average Reward: -448.89\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38858.4102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39432.7188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40359.5430\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39401.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52067.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25599.2656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31745.0098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38229.6094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26889.3223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25972.7266\n",
      "Reducing exploration for all agents to 0.4929\n",
      "\n",
      "Episode 214: Starting computation.\n",
      "Random Seed Set to 224\n",
      "Episode 214: Finished running.\n",
      "Agent 0, Average Reward: -437.95\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28026.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47854.4102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25690.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28064.8496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45728.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26317.5723\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29110.6582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32094.1660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17478.3066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32830.1250\n",
      "Reducing exploration for all agents to 0.4905\n",
      "\n",
      "Episode 215: Starting computation.\n",
      "Random Seed Set to 225\n",
      "Episode 215: Finished running.\n",
      "Agent 0, Average Reward: -438.16\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36079.2031\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34171.5547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31288.0801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29344.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65462.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53862.1641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31794.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32961.2852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24777.2383\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37163.1016\n",
      "Reducing exploration for all agents to 0.4881\n",
      "\n",
      "Episode 216: Starting computation.\n",
      "Random Seed Set to 226\n",
      "Episode 216: Finished running.\n",
      "Agent 0, Average Reward: -445.87\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27130.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27795.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29708.9766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26108.1035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24320.6914\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29587.8887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22560.8672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37240.2539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29889.6777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28526.6152\n",
      "Reducing exploration for all agents to 0.4858\n",
      "\n",
      "Episode 217: Starting computation.\n",
      "Random Seed Set to 227\n",
      "Episode 217: Finished running.\n",
      "Agent 0, Average Reward: -435.0\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25808.4746\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24938.1035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38709.4258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 83896.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50917.6328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40030.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 56572.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32052.0840\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 62499.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32049.2461\n",
      "Reducing exploration for all agents to 0.4834\n",
      "\n",
      "Episode 218: Starting computation.\n",
      "Random Seed Set to 228\n",
      "Episode 218: Finished running.\n",
      "Agent 0, Average Reward: -445.84\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30452.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28248.0879\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30829.7598\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25028.0293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18226.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38128.9609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34509.3984\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25992.0234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39500.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 65395.0000\n",
      "Reducing exploration for all agents to 0.481\n",
      "\n",
      "Episode 219: Starting computation.\n",
      "Random Seed Set to 229\n",
      "Episode 219: Finished running.\n",
      "Agent 0, Average Reward: -447.64\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43678.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29496.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40760.5547\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35287.7891\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20114.0449\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20328.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30361.9863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19201.0879\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33281.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20700.6484\n",
      "Reducing exploration for all agents to 0.4786\n",
      "\n",
      "Episode 220: Starting computation.\n",
      "Random Seed Set to 230\n",
      "Episode 220: Finished running.\n",
      "Agent 0, Average Reward: -434.84\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20352.0156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31098.8926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20064.2402\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17373.5723\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38019.8516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28195.5957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23034.4941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24715.0801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19417.3184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34530.9375\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.4762\n",
      "\n",
      "Episode 221: Starting computation.\n",
      "Random Seed Set to 231\n",
      "Episode 221: Finished running.\n",
      "Agent 0, Average Reward: -448.93\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29088.8477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28848.1465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21591.9336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22133.4238\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22452.7637\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19424.4551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24147.1387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24417.4590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20831.0723\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30423.5547\n",
      "Reducing exploration for all agents to 0.4739\n",
      "\n",
      "Episode 222: Starting computation.\n",
      "Random Seed Set to 232\n",
      "Episode 222: Finished running.\n",
      "Agent 0, Average Reward: -478.81\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22198.5527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28625.8066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23124.7637\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30210.6289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35730.8789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24471.0410\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23131.8574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21068.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23712.0996\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26362.4180\n",
      "Reducing exploration for all agents to 0.4715\n",
      "\n",
      "Episode 223: Starting computation.\n",
      "Random Seed Set to 233\n",
      "Episode 223: Finished running.\n",
      "Agent 0, Average Reward: -468.02\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18945.0391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16345.3477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22452.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24462.8262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24206.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14961.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21686.4219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18701.4785\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18935.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28418.6621\n",
      "Reducing exploration for all agents to 0.4691\n",
      "\n",
      "Episode 224: Starting computation.\n",
      "Random Seed Set to 234\n",
      "Episode 224: Finished running.\n",
      "Agent 0, Average Reward: -450.76\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20625.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21701.6875\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22235.0566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34420.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15659.2559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29052.9590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25675.4492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21488.3301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24097.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23793.1367\n",
      "Reducing exploration for all agents to 0.4667\n",
      "\n",
      "Episode 225: Starting computation.\n",
      "Random Seed Set to 235\n",
      "Episode 225: Finished running.\n",
      "Agent 0, Average Reward: -439.3\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18989.6387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26240.9707\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23797.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23151.4102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19680.9844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19627.9199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21542.7070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35527.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26047.2520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44531.1367\n",
      "Reducing exploration for all agents to 0.4643\n",
      "\n",
      "Episode 226: Starting computation.\n",
      "Random Seed Set to 236\n",
      "Episode 226: Finished running.\n",
      "Agent 0, Average Reward: -443.2\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26227.1777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26753.6191\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22534.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18609.4590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37921.9492\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24896.6230\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31039.3184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20113.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25830.7363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29393.4941\n",
      "Reducing exploration for all agents to 0.4619\n",
      "\n",
      "Episode 227: Starting computation.\n",
      "Random Seed Set to 237\n",
      "Episode 227: Finished running.\n",
      "Agent 0, Average Reward: -451.93\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31264.4609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19846.9785\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20893.5059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20349.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31106.2949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20245.5605\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23000.1270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20543.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21993.6309\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21408.4102\n",
      "Reducing exploration for all agents to 0.4596\n",
      "\n",
      "Episode 228: Starting computation.\n",
      "Random Seed Set to 238\n",
      "Episode 228: Finished running.\n",
      "Agent 0, Average Reward: -445.05\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19634.2793\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26014.0059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25302.6152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24080.8828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19252.5801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20049.7363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28022.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22514.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21691.1660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36460.8867\n",
      "Reducing exploration for all agents to 0.4572\n",
      "\n",
      "Episode 229: Starting computation.\n",
      "Random Seed Set to 239\n",
      "Episode 229: Finished running.\n",
      "Agent 0, Average Reward: -459.79\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26895.5625\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18683.1699\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20587.6387\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23918.9316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22803.7520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33711.9727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26583.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17271.9863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31471.8535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20140.1621\n",
      "Reducing exploration for all agents to 0.4548\n",
      "\n",
      "Episode 230: Starting computation.\n",
      "Random Seed Set to 240\n",
      "Episode 230: Finished running.\n",
      "Agent 0, Average Reward: -434.78\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19442.9062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25031.0801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28098.8887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17973.5410\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16922.9551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18236.3379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14683.9434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17821.5059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25558.4434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25711.7754\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.4524\n",
      "\n",
      "Episode 231: Starting computation.\n",
      "Random Seed Set to 241\n",
      "Episode 231: Finished running.\n",
      "Agent 0, Average Reward: -445.63\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30012.9082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26029.3301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28306.5566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30353.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22136.2578\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21890.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29813.8730\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26274.4473\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27309.1465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16823.4355\n",
      "Reducing exploration for all agents to 0.45\n",
      "\n",
      "Episode 232: Starting computation.\n",
      "Random Seed Set to 242\n",
      "Episode 232: Finished running.\n",
      "Agent 0, Average Reward: -448.89\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29717.9824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29704.7656\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26671.7910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20335.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26977.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29949.0176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21597.0918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 76391.1484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33765.6602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24335.4355\n",
      "Reducing exploration for all agents to 0.4477\n",
      "\n",
      "Episode 233: Starting computation.\n",
      "Random Seed Set to 243\n",
      "Episode 233: Finished running.\n",
      "Agent 0, Average Reward: -455.28\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27985.5684\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34803.1289\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22051.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31375.6152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21253.3496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 70471.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39815.6602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 75614.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26233.4414\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28471.9707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.4453\n",
      "\n",
      "Episode 234: Starting computation.\n",
      "Random Seed Set to 244\n",
      "Episode 234: Finished running.\n",
      "Agent 0, Average Reward: -435.97\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 73005.3438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21361.3262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33822.7539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36293.8906\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26569.3301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22674.5137\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22364.0840\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24014.9980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30609.0801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27327.4512\n",
      "Reducing exploration for all agents to 0.4429\n",
      "\n",
      "Episode 235: Starting computation.\n",
      "Random Seed Set to 245\n",
      "Episode 235: Finished running.\n",
      "Agent 0, Average Reward: -447.51\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 78303.1484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23791.6074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39244.0039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 41577.1328\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33101.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27518.1934\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28357.5059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28035.6348\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21602.9785\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32934.5117\n",
      "Reducing exploration for all agents to 0.4405\n",
      "\n",
      "Episode 236: Starting computation.\n",
      "Random Seed Set to 246\n",
      "Episode 236: Finished running.\n",
      "Agent 0, Average Reward: -464.07\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24332.5801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23499.1777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24171.5898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28309.2363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22120.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25007.7285\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21754.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26653.6094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40462.0742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27727.9141\n",
      "Reducing exploration for all agents to 0.4381\n",
      "\n",
      "Episode 237: Starting computation.\n",
      "Random Seed Set to 247\n",
      "Episode 237: Finished running.\n",
      "Agent 0, Average Reward: -444.36\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25886.3145\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24597.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24874.6172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33946.3359\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22199.2539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29392.2461\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30611.3887\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34525.2305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30930.1777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25695.4746\n",
      "Reducing exploration for all agents to 0.4358\n",
      "\n",
      "Episode 238: Starting computation.\n",
      "Random Seed Set to 248\n",
      "Episode 238: Finished running.\n",
      "Agent 0, Average Reward: -451.31\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33046.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28518.2480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31886.7520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28945.4043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34067.0352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33209.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22454.9941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16156.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37803.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20958.8965\n",
      "Reducing exploration for all agents to 0.4334\n",
      "\n",
      "Episode 239: Starting computation.\n",
      "Random Seed Set to 249\n",
      "Episode 239: Finished running.\n",
      "Agent 0, Average Reward: -454.57\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22693.2227\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19017.2363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43038.8750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 42142.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 76195.3281\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32089.4395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36797.0195\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40662.5117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29146.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30445.3496\n",
      "Reducing exploration for all agents to 0.431\n",
      "\n",
      "Episode 240: Starting computation.\n",
      "Random Seed Set to 250\n",
      "Episode 240: Finished running.\n",
      "Agent 0, Average Reward: -439.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20606.1094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20036.5117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17307.2090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27572.2832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23777.7480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18343.2266\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24398.2344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20333.0059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21272.5293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28152.5586\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.4286\n",
      "\n",
      "Episode 241: Starting computation.\n",
      "Random Seed Set to 251\n",
      "Episode 241: Finished running.\n",
      "Agent 0, Average Reward: -451.17\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25905.9004\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28416.3516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20173.9805\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19806.9512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16289.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23954.5078\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25067.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29321.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36552.1133\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33001.5039\n",
      "Reducing exploration for all agents to 0.4262\n",
      "\n",
      "Episode 242: Starting computation.\n",
      "Random Seed Set to 252\n",
      "Episode 242: Finished running.\n",
      "Agent 0, Average Reward: -454.05\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22896.4570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19693.7129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23946.6758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28533.3262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24756.9434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21942.3223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35087.8477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26146.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22678.6660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32534.9434\n",
      "Reducing exploration for all agents to 0.4239\n",
      "\n",
      "Episode 243: Starting computation.\n",
      "Random Seed Set to 253\n",
      "Episode 243: Finished running.\n",
      "Agent 0, Average Reward: -455.33\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24894.1152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16762.7754\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28866.3418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27592.4824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25174.7676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24960.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22449.0918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22740.2188\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17307.7520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23326.3027\n",
      "Reducing exploration for all agents to 0.4215\n",
      "\n",
      "Episode 244: Starting computation.\n",
      "Random Seed Set to 254\n",
      "Episode 244: Finished running.\n",
      "Agent 0, Average Reward: -435.69\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16390.8594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18567.1582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31510.3691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23515.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21425.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29094.8223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25279.2207\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19580.1758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19393.1934\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21542.3613\n",
      "Reducing exploration for all agents to 0.4191\n",
      "\n",
      "Episode 245: Starting computation.\n",
      "Random Seed Set to 255\n",
      "Episode 245: Finished running.\n",
      "Agent 0, Average Reward: -454.85\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21958.4590\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23454.0703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19714.2520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22835.0176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20209.0254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22613.8066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24539.9121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22087.7969\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21406.5215\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17570.3770\n",
      "Reducing exploration for all agents to 0.4167\n",
      "\n",
      "Episode 246: Starting computation.\n",
      "Random Seed Set to 256\n",
      "Episode 246: Finished running.\n",
      "Agent 0, Average Reward: -443.99\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23678.9570\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18393.8848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19567.1465\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23457.8926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19725.5645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19895.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15584.3242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18015.4961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26058.5566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19496.0625\n",
      "Reducing exploration for all agents to 0.4143\n",
      "\n",
      "Episode 247: Starting computation.\n",
      "Random Seed Set to 257\n",
      "Episode 247: Finished running.\n",
      "Agent 0, Average Reward: -448.74\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26620.2617\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23902.7480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15612.4834\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18066.7793\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20709.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23172.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15268.0879\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22700.4688\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21394.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23020.1113\n",
      "Reducing exploration for all agents to 0.412\n",
      "\n",
      "Episode 248: Starting computation.\n",
      "Random Seed Set to 258\n",
      "Episode 248: Finished running.\n",
      "Agent 0, Average Reward: -449.67\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14859.4141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20687.7754\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22363.0059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22682.9609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34465.5000\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15761.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13737.4697\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25186.7559\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18672.5254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16629.0469\n",
      "Reducing exploration for all agents to 0.4096\n",
      "\n",
      "Episode 249: Starting computation.\n",
      "Random Seed Set to 259\n",
      "Episode 249: Finished running.\n",
      "Agent 0, Average Reward: -450.4\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30028.0977\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17344.1816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20336.2637\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26050.8184\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19045.1973\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21979.2051\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13728.5635\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28568.9746\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20428.5234\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18475.1074\n",
      "Reducing exploration for all agents to 0.4072\n",
      "\n",
      "Episode 250: Starting computation.\n",
      "Random Seed Set to 260\n",
      "Episode 250: Finished running.\n",
      "Agent 0, Average Reward: -433.25\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28388.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21321.0254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17551.5645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23740.8613\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26907.2871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23058.5957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28977.7910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18449.9980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17457.5801\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16863.0957\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.4048\n",
      "\n",
      "Episode 251: Starting computation.\n",
      "Random Seed Set to 261\n",
      "Episode 251: Finished running.\n",
      "Agent 0, Average Reward: -442.25\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33138.7734\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24702.1270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25590.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14280.4482\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23535.8105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19960.6758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22554.3691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26284.2715\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27684.0566\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15078.6201\n",
      "Reducing exploration for all agents to 0.4024\n",
      "\n",
      "Episode 252: Starting computation.\n",
      "Random Seed Set to 262\n",
      "Episode 252: Finished running.\n",
      "Agent 0, Average Reward: -484.7\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22323.7090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17012.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23670.5918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28276.5996\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18908.9863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23091.8379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24050.7246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20984.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22720.7246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24186.6465\n",
      "Reducing exploration for all agents to 0.4\n",
      "\n",
      "Episode 253: Starting computation.\n",
      "Random Seed Set to 263\n",
      "Episode 253: Finished running.\n",
      "Agent 0, Average Reward: -454.07\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16363.7715\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29301.8496\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27508.6699\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26764.2109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28861.9883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20793.1543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 12697.1963\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21478.2949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20564.7871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23398.8613\n",
      "Reducing exploration for all agents to 0.3977\n",
      "\n",
      "Episode 254: Starting computation.\n",
      "Random Seed Set to 264\n",
      "Episode 254: Finished running.\n",
      "Agent 0, Average Reward: -439.28\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23001.8711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23530.3340\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24201.9004\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20271.8145\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29636.0293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17267.1660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17583.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17080.0371\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22388.8262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17555.6699\n",
      "Reducing exploration for all agents to 0.3953\n",
      "\n",
      "Episode 255: Starting computation.\n",
      "Random Seed Set to 265\n",
      "Episode 255: Finished running.\n",
      "Agent 0, Average Reward: -446.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20933.1992\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31389.3203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15107.6025\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18687.8418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21108.1562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22456.7676\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21940.9434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18121.6816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18571.3203\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36426.9180\n",
      "Reducing exploration for all agents to 0.3929\n",
      "\n",
      "Episode 256: Starting computation.\n",
      "Random Seed Set to 266\n",
      "Episode 256: Finished running.\n",
      "Agent 0, Average Reward: -451.16\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17182.5645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33632.3633\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23894.4980\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18628.6602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20016.0645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20118.1602\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31757.5820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18508.4863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28872.2949\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21641.0918\n",
      "Reducing exploration for all agents to 0.3905\n",
      "\n",
      "Episode 257: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 267\n",
      "Episode 257: Finished running.\n",
      "Agent 0, Average Reward: -441.69\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23175.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15189.2520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22170.9355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20613.5918\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24196.2480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32995.7070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24888.9512\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13277.2314\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19468.9316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22244.7695\n",
      "Reducing exploration for all agents to 0.3881\n",
      "\n",
      "Episode 258: Starting computation.\n",
      "Random Seed Set to 268\n",
      "Episode 258: Finished running.\n",
      "Agent 0, Average Reward: -436.77\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17698.7637\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16883.6250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19347.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19324.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26282.4531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27718.3574\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14370.5645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18792.0762\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26741.8535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18146.4785\n",
      "Reducing exploration for all agents to 0.3858\n",
      "\n",
      "Episode 259: Starting computation.\n",
      "Random Seed Set to 269\n",
      "Episode 259: Finished running.\n",
      "Agent 0, Average Reward: -437.21\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29413.1270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23796.1172\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22846.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28026.9395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16219.1553\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16632.0645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14576.8506\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19324.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27390.1152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18960.6973\n",
      "Reducing exploration for all agents to 0.3834\n",
      "\n",
      "Episode 260: Starting computation.\n",
      "Random Seed Set to 270\n",
      "Episode 260: Finished running.\n",
      "Agent 0, Average Reward: -462.33\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16754.6152\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16825.1230\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21135.4199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23884.5059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15765.2080\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19331.0410\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21956.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18343.4434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22518.6211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25824.1738\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.381\n",
      "\n",
      "Episode 261: Starting computation.\n",
      "Random Seed Set to 271\n",
      "Episode 261: Finished running.\n",
      "Agent 0, Average Reward: -438.87\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23847.9043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18486.3848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22360.7930\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33027.1836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20714.0898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14552.5957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20140.4258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22720.8691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20073.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21584.1953\n",
      "Reducing exploration for all agents to 0.3786\n",
      "\n",
      "Episode 262: Starting computation.\n",
      "Random Seed Set to 272\n",
      "Episode 262: Finished running.\n",
      "Agent 0, Average Reward: -461.98\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20105.1074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20486.5527\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20313.7539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24670.7500\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18297.6309\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21873.6348\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23284.5742\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17856.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16261.4180\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23556.5371\n",
      "Reducing exploration for all agents to 0.3762\n",
      "\n",
      "Episode 263: Starting computation.\n",
      "Random Seed Set to 273\n",
      "Episode 263: Finished running.\n",
      "Agent 0, Average Reward: -441.14\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14233.4775\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19088.0840\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19111.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16111.6309\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18739.2520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16098.5420\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20188.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28291.8105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16652.6113\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19650.3281\n",
      "Reducing exploration for all agents to 0.3739\n",
      "\n",
      "Episode 264: Starting computation.\n",
      "Random Seed Set to 274\n",
      "Episode 264: Finished running.\n",
      "Agent 0, Average Reward: -447.99\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23126.7871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25826.6074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20421.9941\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30021.2910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25352.6406\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18379.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18609.1426\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24486.5684\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15938.0576\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19148.7559\n",
      "Reducing exploration for all agents to 0.3715\n",
      "\n",
      "Episode 265: Starting computation.\n",
      "Random Seed Set to 275\n",
      "Episode 265: Finished running.\n",
      "Agent 0, Average Reward: -450.57\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22767.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21299.9121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18663.5488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24039.1230\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22177.9434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13461.3975\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27813.9043\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25862.9863\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23935.8301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20071.2832\n",
      "Reducing exploration for all agents to 0.3691\n",
      "\n",
      "Episode 266: Starting computation.\n",
      "Random Seed Set to 276\n",
      "Episode 266: Finished running.\n",
      "Agent 0, Average Reward: -445.33\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16321.2988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16161.0908\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15688.1025\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25088.9473\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20075.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22270.6035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23780.8438\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30687.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16021.1221\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19928.4980\n",
      "Reducing exploration for all agents to 0.3667\n",
      "\n",
      "Episode 267: Starting computation.\n",
      "Random Seed Set to 277\n",
      "Episode 267: Finished running.\n",
      "Agent 0, Average Reward: -447.3\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20997.6367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26517.9277\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19498.0840\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15665.6533\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23310.1934\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19783.2441\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20527.6270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27150.2305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16658.3105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23469.1387\n",
      "Reducing exploration for all agents to 0.3643\n",
      "\n",
      "Episode 268: Starting computation.\n",
      "Random Seed Set to 278\n",
      "Episode 268: Finished running.\n",
      "Agent 0, Average Reward: -442.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24449.1738\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29544.3379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24872.4355\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15048.7324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22877.1543\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24848.5371\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16073.2568\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14704.2920\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17373.0059\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23106.2754\n",
      "Reducing exploration for all agents to 0.362\n",
      "\n",
      "Episode 269: Starting computation.\n",
      "Random Seed Set to 279\n",
      "Episode 269: Finished running.\n",
      "Agent 0, Average Reward: -463.9\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20410.3457\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18712.1016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18032.7695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21289.6230\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23007.6758\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20046.5762\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21071.6035\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24799.2422\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27785.5645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20732.7051\n",
      "Reducing exploration for all agents to 0.3596\n",
      "\n",
      "Episode 270: Starting computation.\n",
      "Random Seed Set to 280\n",
      "Episode 270: Finished running.\n",
      "Agent 0, Average Reward: -433.07\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19777.4629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20615.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24656.9824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26434.6230\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23675.9609\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23579.2012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20030.1816\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20028.6074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25138.6719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21642.7617\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.3572\n",
      "\n",
      "Episode 271: Starting computation.\n",
      "Random Seed Set to 281\n",
      "Episode 271: Finished running.\n",
      "Agent 0, Average Reward: -466.88\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27663.9453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30110.1660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29323.5156\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19004.0957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24717.4551\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27209.8691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20934.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27324.2012\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22426.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25495.2207\n",
      "Reducing exploration for all agents to 0.3548\n",
      "\n",
      "Episode 272: Starting computation.\n",
      "Random Seed Set to 282\n",
      "Episode 272: Finished running.\n",
      "Agent 0, Average Reward: -424.73\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13904.6074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27452.1934\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22304.5859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37454.3477\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23719.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35883.6484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38306.1211\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27996.1113\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24117.4434\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37072.9141\n",
      "Reducing exploration for all agents to 0.3524\n",
      "\n",
      "Episode 273: Starting computation.\n",
      "Random Seed Set to 283\n",
      "Episode 273: Finished running.\n",
      "Agent 0, Average Reward: -442.63\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23698.3145\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23825.3672\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23627.5098\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16446.2988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32657.5996\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23093.0332\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22697.2090\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17787.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23913.0703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27605.3906\n",
      "Reducing exploration for all agents to 0.3501\n",
      "\n",
      "Episode 274: Starting computation.\n",
      "Random Seed Set to 284\n",
      "Episode 274: Finished running.\n",
      "Agent 0, Average Reward: -455.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32288.5605\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20682.2480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16585.0684\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21480.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26354.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27402.1074\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18453.8223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31790.8262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18450.0645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19890.6250\n",
      "Reducing exploration for all agents to 0.3477\n",
      "\n",
      "Episode 275: Starting computation.\n",
      "Random Seed Set to 285\n",
      "Episode 275: Finished running.\n",
      "Agent 0, Average Reward: -454.05\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18981.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28670.5820\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24593.5488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23025.9199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13993.9922\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20272.9824\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17774.3418\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19293.9316\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25684.9199\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15243.2822\n",
      "Reducing exploration for all agents to 0.3453\n",
      "\n",
      "Episode 276: Starting computation.\n",
      "Random Seed Set to 286\n",
      "Episode 276: Finished running.\n",
      "Agent 0, Average Reward: -449.21\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24795.3926\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17358.1270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23624.8828\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14125.2451\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23124.7520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18548.6953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18447.8789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17467.9395\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22797.7129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21749.9102\n",
      "Reducing exploration for all agents to 0.3429\n",
      "\n",
      "Episode 277: Starting computation.\n",
      "Random Seed Set to 287\n",
      "Episode 277: Finished running.\n",
      "Agent 0, Average Reward: -461.74\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25067.0859\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36308.8125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19958.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19452.7363\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21360.5176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17817.3848\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 13534.6836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17834.6016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23457.0586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24957.3516\n",
      "Reducing exploration for all agents to 0.3405\n",
      "\n",
      "Episode 278: Starting computation.\n",
      "Random Seed Set to 288\n",
      "Episode 278: Finished running.\n",
      "Agent 0, Average Reward: -444.65\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24698.3379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27437.5488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39944.9961\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20793.8555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24917.0508\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27918.7148\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22897.2910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24208.5469\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17445.7988\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23304.1504\n",
      "Reducing exploration for all agents to 0.3381\n",
      "\n",
      "Episode 279: Starting computation.\n",
      "Random Seed Set to 289\n",
      "Episode 279: Finished running.\n",
      "Agent 0, Average Reward: -434.6\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21206.8691\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23049.7129\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 14834.1064\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16283.1318\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17637.1484\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16164.0400\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28950.9668\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20874.6230\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23415.8262\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22079.8418\n",
      "Reducing exploration for all agents to 0.3358\n",
      "\n",
      "Episode 280: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 290\n",
      "Episode 280: Finished running.\n",
      "Agent 0, Average Reward: -452.42\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15593.8740\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17777.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22115.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16782.4453\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17159.6855\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22547.3516\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16861.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24238.7539\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19528.4883\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16793.1582\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Saving architecture, weights and optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.3334\n",
      "\n",
      "Episode 281: Starting computation.\n",
      "Random Seed Set to 291\n",
      "Episode 281: Finished running.\n",
      "Agent 0, Average Reward: -435.52\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21008.6660\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28762.9629\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15793.9893\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37405.9141\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32822.9219\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 15390.4160\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19297.4062\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19661.7852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20288.0488\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 16428.0977\n",
      "Reducing exploration for all agents to 0.331\n",
      "\n",
      "Episode 282: Starting computation.\n",
      "Random Seed Set to 292\n",
      "Episode 282: Finished running.\n",
      "Agent 0, Average Reward: -416.36\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27990.3066\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43499.7070\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21791.0176\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21497.3535\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22332.2852\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28819.1973\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38163.3242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21978.8008\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54078.1016\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29335.2441\n",
      "Reducing exploration for all agents to 0.3286\n",
      "\n",
      "Episode 283: Starting computation.\n",
      "Random Seed Set to 293\n",
      "Episode 283: Finished running.\n",
      "Agent 0, Average Reward: -467.67\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37585.8320\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29978.5586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 58029.5352\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18245.6836\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52233.3125\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17134.3379\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22239.3145\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31080.4102\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23917.0898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 44965.0430\n",
      "Reducing exploration for all agents to 0.3262\n",
      "\n",
      "Episode 284: Starting computation.\n",
      "Random Seed Set to 294\n",
      "Episode 284: Finished running.\n",
      "Agent 0, Average Reward: -469.2\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17470.2246\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33692.7109\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28329.3398\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20250.1270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22278.0645\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 52475.4531\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21144.6641\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43022.0117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25576.9082\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25358.6875\n",
      "Reducing exploration for all agents to 0.3239\n",
      "\n",
      "Episode 285: Starting computation.\n",
      "Random Seed Set to 295\n",
      "Episode 285: Finished running.\n",
      "Agent 0, Average Reward: -469.29\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20846.2324\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34796.3164\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 20040.0586\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 21717.5332\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40429.5664\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22001.3594\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31697.5781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40038.8711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 47755.1367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 53372.6680\n",
      "Reducing exploration for all agents to 0.3215\n",
      "\n",
      "Episode 286: Starting computation.\n",
      "Random Seed Set to 296\n",
      "Episode 286: Finished running.\n",
      "Agent 0, Average Reward: -458.82\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33728.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35068.7344\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30543.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34337.3242\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24240.1250\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34062.5898\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 18019.1777\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34446.1055\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30010.2812\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23623.0957\n",
      "Reducing exploration for all agents to 0.3191\n",
      "\n",
      "Episode 287: Starting computation.\n",
      "Random Seed Set to 297\n",
      "Episode 287: Finished running.\n",
      "Agent 0, Average Reward: -411.67\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30092.6309\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 66117.6094\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 29460.0254\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26115.2715\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35177.1953\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32315.0312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 54339.9375\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32542.6270\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36122.5703\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 60330.4023\n",
      "Reducing exploration for all agents to 0.3167\n",
      "\n",
      "Episode 288: Starting computation.\n",
      "Random Seed Set to 298\n",
      "Episode 288: Finished running.\n",
      "Agent 0, Average Reward: -444.75\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22661.0293\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 40245.9258\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30526.8340\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32565.8105\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26117.4844\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32476.3223\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 30386.4336\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27684.1309\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43219.9727\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25132.7305\n",
      "Reducing exploration for all agents to 0.3143\n",
      "\n",
      "Episode 289: Starting computation.\n",
      "Random Seed Set to 299\n",
      "Episode 289: Finished running.\n",
      "Agent 0, Average Reward: -438.71\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35628.3086\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33550.4766\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 28597.8145\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 37202.0781\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34464.7305\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38576.8555\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24819.4121\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 38467.1719\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 45588.5312\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27797.4844\n",
      "Reducing exploration for all agents to 0.312\n",
      "\n",
      "Episode 290: Starting computation.\n",
      "Random Seed Set to 300\n",
      "Episode 290: Finished running.\n",
      "Agent 0, Average Reward: -446.19\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25310.6582\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25375.8301\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17412.8711\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 36875.6445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 24872.2480\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 19124.2871\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26689.8809\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 23512.2910\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 35872.3867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17802.9414\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.3096\n",
      "\n",
      "Episode 291: Starting computation.\n",
      "Random Seed Set to 301\n",
      "Episode 291: Finished running.\n",
      "Agent 0, Average Reward: -440.57\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26144.2988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26549.3750\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26049.6348\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 27222.0117\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32719.1445\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 43708.1680\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 22984.5449\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 26839.5957\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32862.6367\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33334.0352\n",
      "Reducing exploration for all agents to 0.3072\n",
      "\n",
      "Episode 292: Starting computation.\n",
      "Random Seed Set to 302\n",
      "Episode 292: Finished running.\n",
      "Agent 0, Average Reward: -414.86\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 33989.2695\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 50210.6562\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 31674.8867\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 17767.2520\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 34335.8789\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39799.5039\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 46195.5391\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 39221.1523\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 25724.7832\n",
      "Train on 256 samples\n",
      "256/256 - 0s - loss: 32537.5312\n",
      "Reducing exploration for all agents to 0.3048\n",
      "\n",
      "Episode 293: Starting computation.\n"
     ]
    }
   ],
   "source": [
    "intersection = 14\n",
    "map_name  = 'Balance_int'+str(intersection)\n",
    "model_name = map_name\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = \"E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "\n",
    "## Simulation Parameters\n",
    "Random_Seed = 10\n",
    "sim_length = 10801\n",
    "timesteps_per_second = 1\n",
    "agent_type = \"DuellingDDQN\"\n",
    "actions = 'all_actions'     # 'default_actions' or 'all_actions'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 500\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "batches_per_episode = 10\n",
    "\n",
    "alpha = 0.0005\n",
    "gamma = 0.95\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "if intersection == \"1_2_4\":\n",
    "    intersection = 1\n",
    "elif intersection == \"2_4\":\n",
    "    intersection = 2\n",
    "elif intersection == \"11_12\":\n",
    "    intersection = 11\n",
    "partial_dictionary = {\"junctions\": { (intersection-1) : Balance_dictionary[\"junctions\"][intersection-1]},\\\n",
    "                      \"demand\": Balance_dictionary[\"demand\"]}\n",
    "\n",
    "Session_ID = map_name + \"_\" + actions + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type + \"_Queues_rework\"\n",
    "print(\"Current simulation: {}\".format(Session_ID))\n",
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"linear\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.01\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0.01 if entry < 0.01 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n",
    "Balance_int_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, partial_dictionary, actions,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, batches_per_episode, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)\n",
    "Balance_int_MultiDQN_Agents.prepopulate_memory()\n",
    "Balance_int_MultiDQN_Agents.train(episodes)\n",
    "Balance_int_MultiDQN_Agents.save(episodes)\n",
    "\n",
    "## AGENT TRAINING RESULTS\n",
    "# Path to results folder\n",
    "results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "\n",
    "# Loop over each agent\n",
    "for idx , agent in Balance_int_MultiDQN_Agents.Agents.items():\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[0].signal_id + 1\n",
    "    print(\"Intersection \"+str(intersection_number_in_vissim))\n",
    "    \n",
    "    ## SAVE TRAINING DATA TO JSON.\n",
    "    json_filename = \"Agent{}_Loss_average_reward.json\".format(intersection_number_in_vissim)\n",
    "    Loss_reward = dict()   \n",
    "    # Loss dictionary\n",
    "    for epoch, loss in enumerate(agent.loss):\n",
    "        loss_dict = { epoch : loss }\n",
    "    Loss_reward['Agent{} loss'.format(intersection_number_in_vissim)] = loss_dict\n",
    "    # Reward dictionary            \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    Loss_reward['Agent{} Average_Reward'.format(intersection_number_in_vissim)] = agent.reward_storage\n",
    "    # Store as JSON\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Loss_reward, f)\n",
    "    print(\"Agent {}: Training Loss and Average Reward during training successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "    \n",
    "    ## LOADING DATA FROM JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Agent{}_Loss_average_reward.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "    \n",
    "    ## TRAINING PLOTS\n",
    "    loss_plot_filename  = \"Agent{}_Loss.png\".format(intersection_number_in_vissim)\n",
    "    reward_plot_filename  = \"Agent{}_average_reward.png\".format(intersection_number_in_vissim) \n",
    "    \n",
    "    ## Loss Plot\n",
    "    plt.figure('LossAgent'+str(idx),figsize=(16,9))\n",
    "    plt.plot(agent.loss)\n",
    "    #plt.yscale('log')\n",
    "\n",
    "    plt.xlabel('Training Epoch',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent {} Loss over training'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.savefig(results_path + \"/\" + loss_plot_filename)\n",
    "\n",
    "    ## Average Reward Plot\n",
    "    plt.figure('RewardAgent'+str(idx),figsize=(16,9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Training Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent {} average reward over training'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.savefig(results_path + \"/\" + reward_plot_filename)\n",
    "    \n",
    "Balance_int_MultiDQN_Agents.load(500, best = True)\n",
    "Balance_int_MultiDQN_Agents.test()\n",
    "results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "time = [t for t in range(len(Balance_int_MultiDQN_Agents.Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "########################################\n",
    "## Queues over time for each junction ##\n",
    "########################################\n",
    "for idx, queues in Balance_int_MultiDQN_Agents.Episode_Queues.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[0].signal_id + 1\n",
    "    \n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    number_queues = np.size(queues,0)\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queues = dict()\n",
    "    Queues['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queues[str(i)] = queue.tolist()\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "    \n",
    "    ## Plot the queues\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    filename = \"Junction{}_Queues.png\".format(intersection_number_in_vissim)           \n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Queues.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Queues, f)\n",
    "        \n",
    "    ### LOADING DATA FROM JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #json_filename = \"Junction{}_Queues.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "        \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Queues during Test successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "       \n",
    "        \n",
    "###################################################        \n",
    "## Accumulated delay over time for each junction ##\n",
    "###################################################\n",
    "for idx, delay in Balance_int_MultiDQN_Agents.Cumulative_Episode_Delays.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[idx].signal_id + 1\n",
    "\n",
    "    # Extract and process delay data\n",
    "    Delay = dict()   \n",
    "    Delay['Time'] = time\n",
    "    Delay['Junction {} delay'.format(intersection_number_in_vissim)] = delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Cumulative_Delay.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Delay, f)\n",
    "        \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Delay successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "    \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Junction{}_Cumulative_Delay.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    # Plot the cumulative delay\n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    filename = \"Junction{}_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "    \n",
    "    \n",
    "    \n",
    "########################################################    \n",
    "## Accumulated stop delay over time for each junction ##\n",
    "########################################################\n",
    "for idx, stop_delay in Balance_int_MultiDQN_Agents.Cumulative_Episode_stop_Delays.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = Balance_int_MultiDQN_Agents.Agents[idx].signal_id + 1    \n",
    "    \n",
    "    # Extract and process stop delay data\n",
    "    Stop_delay = dict()   \n",
    "    Stop_delay['Time'] = time\n",
    "    Stop_delay['Junction {} stop delay'.format(intersection_number_in_vissim)] = stop_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Cumulative_Stop_Delay.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Stop_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Stop Delay successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Junction{}_Cumulative_Stop_Delay.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "    \n",
    "    # Plot the cumulative stop delay\n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    filename = \"Junction{}_Cumulative_Stop_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n",
    "    \n",
    "    \n",
    "###############################################\n",
    "## ONLY IF THERE IS MORE THAN ONE CONTROLLER ##\n",
    "##    These are the global network plots     ##\n",
    "###############################################\n",
    "\n",
    "if len(Balance_int_MultiDQN_Agents.Agents) > 1:\n",
    "    ########################################    \n",
    "    ## Global Accumulated delay over time ##\n",
    "    ########################################\n",
    "    \n",
    "    # Process global delay data\n",
    "    Global_delay = dict()   \n",
    "    Global_delay['Time'] = time\n",
    "    Global_delay['Global accumulated Delay'] = Balance_int_MultiDQN_Agents.Cumulative_Totale_network_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Global_Cumulative_Delay.json\"\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Global_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Global Delay successfuly saved to file:\")\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Global_Cumulative_Delay.json\"\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    \n",
    "    # Plot the global delay\n",
    "    plt.figure('4',figsize=(16,9))\n",
    "    plt.plot(Cumulative_Totale_network_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "    plt.title('Global accumulated Delay',fontsize=18)\n",
    "    plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "    filename = \"Global_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    ## Global Accumulated stop delay over time ##\n",
    "    #############################################\n",
    "    \n",
    "    # Process global stop delay data\n",
    "    Global_stop_delay = dict()   \n",
    "    Global_stop_delay['Time'] = time\n",
    "    Global_stop_delay['Global accumulated stop Delay'] = Balance_int_MultiDQN_Agents.Cumulative_Totale_network_stop_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Global_Cumulative_Stop_Delay.json\"\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Global_stop_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Global Stop Delay successfuly saved to file:\")\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Global_Cumulative_Stop_Delay.json\"\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    # Plot the global stop delay\n",
    "    plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "    plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "    plt.gca().legend('Global accumulated stop Delay')\n",
    "    \n",
    "    filename = \"Global_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current simulation: Balance_default_actions_500_3600_DQN\n"
     ]
    }
   ],
   "source": [
    "map_name  = 'Balance'\n",
    "model_name = map_name\n",
    "\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = \"E:\\\\OneDrive - University of Warwick\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\"\n",
    "\n",
    "## Simulation Parameters\n",
    "Random_Seed = 44\n",
    "sim_length = 3601\n",
    "timesteps_per_second = 1\n",
    "agent_type = \"DQN\"\n",
    "actions = 'default_actions'     # 'default_actions' or 'all_actions'\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 500\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 5000\n",
    "batch_size = 128\n",
    "batches_per_episode = 10\n",
    "\n",
    "alpha = 0.00005\n",
    "gamma = 0.95\n",
    "\n",
    "# Load and partition balance dictionary\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "\n",
    "Session_ID = map_name + \"_\" + actions + \"_\" + str(episodes) + \"_\" + str(sim_length-1) + \"_\" + agent_type\n",
    "print(\"Current simulation: {}\".format(Session_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEyCAYAAADjpUkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xm8XePd///XOyc5aAYxxJgQY00Vw2nQaqkhQhApRSrmcqvx7t3hRs3t/Sv1VdW7KKrGlqoxar5jatUUxBRTGiERJUhEUJL4/P641pHt5AzrJGeftYf38/FYj73X2mut/TnXQ3z2da1rUERgZmZm1a9H0QGYmZlZ13BSNzMzqxFO6mZmZjXCSd3MzKxGOKmbmZnVCCd1MzOzGuGkblYhJF0u6efd+H13SDqwu76vPZLul/S9LrrXaZKu7upzzaqBk7pZJ0maIuljSXNKtt8WHVd7WkteEbFzRFxRVExm1vV6Fh2AWZXaLSL+r+ggACT1jIh5RcdhZsVzTd2sC0m6UNL1JftnSRqnZFtJ0ySdKOmdrMa/Xzv3OkzSJEnvSRoraZWSz0LSUZJeAV7Jjp0naaqk2ZKekPSN7Phw4ERgn6xV4ens+OdN3pJ6SDpJ0muS3pZ0paSls88GZ993oKTXs9h/2k7cu0iaKOkDSW9I+lHJZyMlTchi/GcWW7PVJT2UXXe3pOVLrttS0j8kzZL0tKRtSz5bQ9ID2XX3AKXXbStpWov4pkjaoY3Y2/wes2rgpG7WtX4IbCzpoCypHgocGAvmY16JlHRWBQ4ELpb05ZY3kbQd8Atgb2Bl4DXg2han7QFsAWyQ7T8ObAIsC/wJ+IukJSPiTuD/A/4cEX0iYkgrcR+Ubd8C1gT6AC0fKWwNfBnYHjhF0vptlMGlwH9ERF9gI+De7G8aClwJ/BjoD3wTmFJy3XeBg4EVgEbgR9l1qwK3AT/P/rYfATdIGpBd9yfgCVK5/oxUrp2W43vMKp6TutmiuTmrzTVvhwFExEfAGOBXwNXAMRExrcW1J0fEJxHxACmJ7N3K/fcD/hART0bEJ8AJwFaSBpec84uIeC8iPs6+++qIeDci5kXEOcASpCScx37AryJickTMyb5vX0mlj+hOj4iPI+Jp4GmgtR8HAHOBDST1i4iZEfFkdvzQ7G+6JyI+i4g3IuLFkusui4iXs7/nOtIPFEjleXtE3J5ddw8wHthF0mrAV1lQpg8Ct+b8m1tq83sW8X5m3c5J3WzR7BER/Uu2S5o/iIjHgMmASMmp1MyI+LBk/zVgFRa2SvZZ8z3nAO+SavjNppZeIOmHkl6Q9L6kWcDSlDRFd+AL35e97wmsWHLsXyXvPyLV5luzJykRvpY1i2+VHR8E/LOdGNq6/+rAd0p/RJFaDVbO4m6tTBdFe99jVhWc1M26mKSjSLXk6cBPWny8jKTeJfurZee1NJ2UZJrv2RtYDnij5Jwo+fwbwH+Tav3LRER/4H3SD4svnNuGL3xfFtc84K0OrltIRDweESNJzeg3s+CHzVRgrc7eL7vuqhY/onpHxJnAm7Reps0+BL7UvCOpAWirOb297zGrCk7qZl1I0rqkZ7JjgP2Bn0japMVpp0tqzBLxrsBfWrnVn4CDJW0iaQnSM/FHI2JKG1/dl5SEZwA9JZ0C9Cv5/C1gsKS2/s1fA/wg63TWhwXP4DvVqz77u/aTtHREzAVmA/Ozjy/N/qbts455q0paL8dtrwZ2k7STpAZJS2Yd4AZGxGukJvLmMt0a2K3k2peBJSWNkNQLOIn0g6tT39OZMjArkpO62aK5VV8cp35T9vz5auCsiHg6Il4h9Tq/KkvMkJqYZ5Jqxn8EjmjxXBmAiBgHnAzcQKqNrgXs2048dwF3kJLYa8C/+WLzfPMPh3clPcnC/gBcBTwIvJpdf0xHhdCG/YEpkmYDR5B+4DQ/ljgYOJfUivAAX2wdaFVETAVGkspyBunv+jEL/v/1XVKHwfeAU0md8ZqvfR84Evg9qZXjQ6BlH4e832NW8bSgU66ZlVM2POrqiHDNz8zKwr9AzczMaoSTupmZWY1w87uZmVmNcE3dzMysRjipm5mZ1YiqW6Vt+eWXj8GDBxcdhpmZWbd44okn3omIXGsQVF1SHzx4MOPHjy86DDMzs24hKffUx25+NzMzqxFO6mZmZjXCSd3MzKxGOKmbmZnVCCd1MzOzGlG2pC7pD5LelvRcG59L0m8kTZL0jKTNyhWLmZlZPShnTf1yYHg7n+8MrJNthwMXljEWMzOzmle2pB4RD5LWN27LSODKSB4B+ktauVzxmJmZ1boin6mvCkwt2Z+WHes2EyfCGWfAZ59157eamZmVR5FJXa0ca3XJOEmHSxovafyMGTO6LIAnn4RTT4UJE7rslmZmZoUpMqlPAwaV7A8Eprd2YkRcHBFNEdE0YECu6W9z2WGH9Hr33V12SzMzs8IUmdTHAgdkveC3BN6PiDe7M4CVVoIhQ5zUzcysNpRtQRdJ1wDbAstLmgacCvQCiIjfAbcDuwCTgI+Ag8sVS3uGDYNf/xo+/BB69y4iAjMzs65RtqQeEaM7+DyAo8r1/XkNGwZnnw0PPAC77FJ0NGZmZouu7meU23prWHJJN8GbmVn1q/ukvuSSsM02TupmZlb96j6pQ2qCf+EFmDat6EjMzMwWnZM6KakD3HNPsXGYmZktDid1YMMNYeWV3QRvZmbVzUkdkGDHHVNN3VPGmplZtXJSzwwbBu++C089VXQkZmZmi8ZJPeMpY83MrNo5qWdWXBE22cRJ3czMqpeTeolhw+Chh2DOnKIjMTMz6zwn9RI77QRz58K99xYdiZmZWec5qZfYemvo2xduu63oSMzMzDrPSb1EY2Ma2nb77RBRdDRmZmad46TewogRabrYZ54pOhIzM7POcVJvoXn5VTfBm5lZtXFSb2GllWDzzZ3Uzcys+jipt2LECHjkkTTDnJmZWbVwUm/FiBFpDvi77io6EjMzs/yc1FvR1AQrrOAmeDMzqy5O6q3o0QN23hnuvBPmzy86GjMzs3yc1NswYgS89156tm5mZlYNnNTbMGwY9OzpJngzM6seTuptWHrpNG2sk7qZmVULJ/V2jBiRZpabOrXoSMzMzDrmpN6OXXdNr7feWmwcZmZmeTipt2O99eDLX4abby46EjMzs445qXdg1Ci47z6YObPoSMzMzNrnpN6BPfaAefPScqxmZmaVzEm9A1/9Kqy8spvgzcys8jmpd6BHDxg5Eu64Az7+uOhozMzM2uaknsOoUfDhhzBuXNGRmJmZtc1JPYdtt4V+/dwEb2Zmlc1JPYfGxjQRzdixXuDFzMwql5N6TqNGwYwZ8I9/FB2JmZlZ65zUcxo+PNXY3QRvZmaVykk9p759YYcd4KabIKLoaMzMzBZW1qQuabiklyRNknR8K5+vJuk+SU9JekbSLuWMZ3GNGgWvvpoWeTEzM6s0ZUvqkhqA84GdgQ2A0ZI2aHHaScB1EbEpsC9wQbni6Qq7757Grd9wQ9GRmJmZLazDpC7p25JekfS+pNmSPpA0O8e9hwKTImJyRHwKXAuMbHFOAP2y90sD0zsTfHdbYYU0vO2669wEb2ZmlSdPTf2XwO4RsXRE9IuIvhHRr8OrYFWgdCXyadmxUqcBYyRNA24Hjslx30LtvTe89BI8+2zRkZiZmX1RnqT+VkS8sAj3VivHWtZvRwOXR8RAYBfgKkkLxSTpcEnjJY2fMWPGIoTSdb797dQEf911hYZhZma2kDxJfbykP0sanTXFf1vSt3NcNw0YVLI/kIWb1w8FrgOIiIeBJYHlW94oIi6OiKaIaBowYECOry6fAQNgu+3gz392E7yZmVWWPEm9H/ARMAzYLdt2zXHd48A6ktaQ1EjqCDe2xTmvA9sDSFqflNSLrYrnsPfeMGkSTJhQdCRmZmYL9OzohIg4eFFuHBHzJB0N3AU0AH+IiOclnQGMj4ixwA+BSyT9gNQ0f1BE5dd/R42C738/NcFvumnR0ZiZmSXqKIdKGgj8L/B1UuL9O3BcREwrf3gLa2pqivHjxxfx1V+w006ptj5pEqi13gNmZmZdQNITEdGU59w8ze+XkZrNVyH1Xr81O1bX9t4bJk+GJ58sOhIzM7MkT1IfEBGXRcS8bLscKLa3WgUYNQp69nQveDMzqxx5kvo7ksZIasi2McC75Q6s0i27LOy4oyeiMTOzypEnqR8C7A38C3gT2Cs7Vvf23humTIHHHy86EjMzs3y9318Hdu+GWKrOyJFpOdZrroGhQ4uOxszM6l2bSV3STyLil5L+l4VngiMiji1rZFVgmWVgxIiU1M8+Oz1jNzMzK0p7ze/NU8OOB55oZTNg//3hrbdg3LiiIzEzs3rXZt0yIm7N3n4UEX8p/UzSd8oaVRXZZRfo3x+uuiqNXTczMytKno5yJ+Q8VpeWWCJ1mLvpJpgzp+hozMysnrWZ1CXtnD1PX1XSb0q2y4F53RZhFdh/f/joI7j55qIjMTOzetZeTX066Xn6v/nis/SxgBuaS3ztazB4MFx9ddGRmJlZPWvvmfrTwNOS/hQRc7sxpqrTowfstx/84hfwr3/BSisVHZGZmdWjPM/UB0u6XtJESZObt7JHVmXGjIHPPoNrry06EjMzq1d5F3S5kPQc/VvAlcBV5QyqGq23HjQ1pV7wZmZmRciT1JeKiHGkZVpfi4jTgO3KG1Z1GjMmrdo2cWLRkZiZWT3Kk9T/LakH8IqkoyWNAlYoc1xVad99oaEBrryy6EjMzKwe5Unq/wl8CTgW2BwYAxxYzqCq1Yorpmljr7gC5nnQn5mZdbN2k7qkBmDviJgTEdMi4uCI2DMiHumm+KrOoYemHvB33FF0JGZmVm/aTeoRMR/YXJK6KZ6qt/POqcZ+6aVFR2JmZvUmz7piTwG3SPoL8GHzwYi4sWxRVbFeveDAA+Gcczxm3czMuleeZ+rLAu+Serzvlm27ljOoanfIITB/vjvMmZlZ91LEQkulV7SmpqYYP3580WF06BvfgLffhhdfBD+8MDOzRSXpiYhoynNuhzV1SQMl3STpbUlvSbpB0sDFD7O2HXoovPwyPPRQ0ZGYmVm9yDuj3FhgFWBV4NbsmLVjr72gTx93mDMzs+6TJ6kPiIjLImJetl0ODChzXFWvT580Gc1118Hs2UVHY2Zm9SBPUn9H0hhJDdk2htRxzjpw6KFpnfU//7noSMzMrB7kSeqHAHsD/wLeBPbKjlkHttgCNtoILrwQqqw/opmZVaEOk3pEvB4Ru0fEgIhYISL2iIjXuiO4aifBkUfCU0/BY48VHY2ZmdW6NiefkfS/QJv1y4g4tiwR1ZgxY+AnP4ELLkg1dzMzs3Jpb0a5yh8MXgX69oUDDki94M85B5ZfvuiIzMysVrWZ1CPiitJ9Sf3S4fig7FHVmO9/P9XUL7sMfvzjoqMxM7NalWfymSZJzwLPAM9JelrS5uUPrXZstBF885vwu9/BZ58VHY2ZmdWqPL3f/wAcGRGDI2J14Cg8+UynHXkkTJ4Md99ddCRmZlar8iT1DyLib807EfF3wE3wnTRqVFqS9YILio7EzMxqVZ6k/pikiyRtK2kbSRcA90vaTNJm5Q6wVjQ2wmGHwV//ClOmFB2NmZnVojxJfRNgXeBU4DRgfeBrwDnA/2vvQknDJb0kaZKk49s4Z29JEyU9L+lPnYq+yhx+eBq7/rvfFR2JmZnVorItvSqpAXgZ2BGYBjwOjI6IiSXnrANcB2wXETMlrRARb7d332pZerUte+0F994LU6dC795FR2NmZpWuq5devUrS0iX7q0sal+PeQ4FJETE5Ij4FrgVGtjjnMOD8iJgJ0FFCrwU/+AHMnAlXXNHxuWZmZp2Rp/n978CjknaRdBhwD/DrHNetCkwt2Z+WHSu1LrCupIckPSJpeJ6gq9nXvgZDh8Kvf+3hbWZm1rXam1EOgIi4SNLzwH3AO8CmEfGvHPdWa7dr5fvXAbYFBgJ/k7RRRMz6wo2kw4HDAVZbbbUcX125pFRbHz0abrsNdtut6IjMzKxW5Gl+3580Vv0A4HLgdklDctx7GjCoZH8gML2Vc26JiLkR8SrwEinJf0FEXBwRTRHRNGBA9S/lvueeMGgQnHtu0ZGYmVktydP8viewdURcExEnAEcAeZ4IPw6sI2kNSY3AvsDYFufcDHwLQNLypOb4yXmDr1a9esExx8B998GECUVHY2ZmtSLP0qt7lHZgi4jHSJ3gOrpuHnA0cBfwAnBdRDwv6QxJu2en3QW8K2kiqXn/xxHx7iL8HVXnsMNS73fX1s3MrKt0OKRN0rrAhcCKEbGRpI2B3SPi590RYEvVPqSt1LHHpjHrr70GK69cdDRmZlaJunRIG3AJcAIwFyAiniE1pdtiOu44mDcPfvvboiMxM7NakCepfylrci81rxzB1Ju11kpzwp9/PsyeXXQ0ZmZW7fIk9XckrUU2HE3SXsCbZY2qjpxwArz/vqeONTOzxZcnqR8FXASsJ+kN4D9JPeCtCzQ1wY47wq9+BR9/XHQ0ZmZWzfL0fp8cETsAA4D1ImLriHit/KHVjxNPhLfegsu8Sr2ZmS2GPDV1ACLiw4jwOuplsM02sNVWcPbZMHdu0dGYmVm1yp3UrXyk9Gx9yhS49tqiozEzs2rlpF4hRoyAr3wFfvELL/RiZmaLpsMFXbJ10UcAg0vPj4hflS+s+tOjBxx/POy3H4wdC3vsUXREZmZWbfLU1G8FDgKWA/qWbNbF9t4b1lwTfvYz6GCiPzMzs4V0WFMHBkbExmWPxOjZE046CQ45JNXWR44sOiIzM6smeWrqd0gaVvZIDID994e114ZTTvGzdTMz65w8Sf0R4CZJH0uaLekDSZ7UtEx69oRTT4VnnoEbbyw6GjMzqyZ5kvo5wFakOeD7RUTfiOhX5rjq2ujRsN56KbnPn190NGZmVi3yJPVXgOeiozVarcs0NMBpp8HEiXDddUVHY2Zm1SLPeuqXA2sCdwCfNB8vakhbLa2n3p7PPoMhQ+DTT+H551OzvJmZ1Z+uXk/9VWAc0IiHtHWbHj3g9NPh5ZfhmmuKjsbMzKpBhzX1z0+U+gIREXPKG1L76qWmDmms+uabw6xZ8MILsMQSRUdkZmbdrUtr6pI2kvQU8BzwvKQnJG24uEFax6Q0beyrr3q9dTMz61ie5veLgf+KiNUjYnXgh8Al5Q3Lmg0bBjvskGaZe//9oqMxM7NKliep946I+5p3IuJ+oHfZIrIvkOCXv4R334Wzzio6GjMzq2R5kvpkSSdLGpxtJ5E6z1k32XTTtNDLuefCtGlFR2NmZpUqT1I/BBgA3AjclL0/uJxB2cJ+/vM0zO2UU4qOxMzMKlWHST0iZkbEsRGxWURsGhHHRcTM7gjOFhg8GI4+Gq64Ap59tuhozMysErU5pE3SrUCb490iYvdyBdWeehrS1tJ778Faa8GWW8IddxQdjZmZdYeuGtL2/0jzvr8KfEzq8X4JMIc0vM262bLLpqVZ77wTbrut6GjMzKzS5Jkm9sGI+GZHx7pLPdfUIU0b+5WvpIlpnn3WE9KYmdW6rp4mdoCkNUtuvgaps5wVoLERzjsPXnklvZqZmTXLk9R/ANwv6X5J9wP3Af9Z1qisXcOHw267pQlp3nyz6GjMzKxS5On9fiewDnBctn05Iu4qd2DWvl/9KjXFH3980ZGYmVmlyFNTB9gc2BAYAuwj6YDyhWR5rL02/PCHcOWV8MgjRUdjZmaVIM+CLleResJvDXw123I9sLfyOvFEWGWVNH59/vyiozEzs6L1zHFOE7BB5F2j1bpNnz5wzjkwejRccAEcc0zREZmZWZHyNL8/B6xU7kBs0eyzD+y0E/z0p54X3sys3uVJ6ssDEyXdJWls81buwCwfKdXS582DY48tOhozMytSnub308odhC2eNdeEU09NPeFvuQVGjiw6IjMzK0KHM8ot1s2l4cB5QAPw+4g4s43z9gL+Anw1ItqdLq7eZ5Rry9y5sNlmMGsWTJwIffsWHZGZmXWFLp1RTtKWkh6XNEfSp5LmS5qd47oG4HxgZ2ADYLSkDVo5ry9wLPBonoCtdb16wcUXwxtveHlWM7N6leeZ+m+B0cArwFLA97JjHRkKTIqIyRHxKXAt0FrD8M+AXwL/zhWxtWmrreCII+A3v4GHHy46GjMz6265Jp+JiElAQ0TMj4jLgG1zXLYqMLVkf1p27HOSNgUGRcRf84VrHTnrLBg0CA46CD7+uOhozMysO+VJ6h9JagQmSPqlpB8AvXNcp1aOff4AX1IP4Fzghx3eSDpc0nhJ42fMmJHjq+tX375w6aXw8stpmVYzM6sfeZL6/tl5RwMfAoOAPXNcNy07t9lAYHrJfl9gI9JiMVOALYGxkhbqDBARF0dEU0Q0DRjgBeI6sv32cOSRcO658Pe/Fx2NmZl1l3Z7v2ed3a6IiDGdvrHUE3gZ2B54A3gc+G5EPN/G+fcDP3Lv964xZw5svDE0NMCECdA7T9uKmZlVnC7r/R4R80nrqTd2NoiImEeq3d8FvABcFxHPSzpD0u6dvZ91Tp8+cNllMGlSmiPezMxqX57JZ6YAD2WzyH3YfDAiftXRhRFxO3B7i2OtDriKiG1zxGKdsM02aZa53/wGdt0Vdtyx6IjMzKyc8jxTnw78NTu3b8lmVeDMM2GDDeCAA8B9DM3MaluHNfWIOL07ArHyWGopuOYaGDoUDj4Ybr01zRdvZma1J9c4datuG28MZ58Nt90G559fdDRmZlYuTup14uijYZdd4Ec/gmefLToaMzMrhzaTuqSzstfvdF84Vi5S6g3fvz+MHg0ffVR0RGZm1tXaq6nvIqkXcEJ3BWPltcIKcNVVaRW3738fyrhAn5mZFaC9pH4n8A6wsaTZkj4ofe2m+KyL7bhjWsXtyivhkkuKjsbMzLpSm0k9In4cEUsDt0VEv4joW/rajTFaFzv5ZNhpJzjmGPDkfGZmtaPDjnIRMVLSipJ2zTZPvl7lGhrg6qthxRVhr73gvfeKjsjMzLpCh0k96yj3GPAdYG/gMUl7lTswK6/ll4frr4fp02H//eGzz4qOyMzMFleeIW0nAV+NiAMj4gBgKHByecOy7jB0KJx3Htx+e3rObmZm1S3P3O89IuLtkv138fj2mnHEEfDkk/A//5Omk/3ud4uOyMzMFlWepH6npLuAa7L9fWixSItVLynNMvfSS3DIIbD22qkGb2Zm1SdPR7kfAxcBGwNDgIsj4r/LHZh1n8ZGuOEGWHll2GMPeOONoiMyM7NFkaemTkTcCNxY5lisQAMGwNix8LWvpcT+wAPwpS8VHZWZmXWGn43b577yFfjjH+GJJ2C//WD+/KIjMjOzznBSty/YfffUI/7mm+HYYz2VrJlZNcnV/C6pEVg3230pIuaWLyQr2jHHwNSpabnWQYPg+OOLjsjMzPLoMKlL2ha4ApgCCBgk6cCIeLC8oVmRzjwzJfYTToBVV00T1JiZWWXLU1M/BxgWES8BSFqXNLxt83IGZsXq0QMuvxzeeisNdVthhTRfvJmZVa48z9R7NSd0gIh4GehVvpCsUiyxBNx0E2y0EYwaBQ+6bcbMrKLlSerjJV0qadtsuwR4otyBWWVYemm46y5YfXXYdVd4/PGiIzIzs7bkSerfB54HjgWOAyYCR5QzKKssK6wA99wDyy0Hw4fDc88VHZGZmbVGUWVjlpqammK8FwEvxOTJ8I1vpPHrDz4I667b8TVmZrZ4JD0REU15zm2zpi7puuz1WUnPtNy6KlirHmuuCf/3f2mZ1m23hRdfLDoiMzMr1V7v9+Oy1127IxCrDuuvD/ffD9ttlxL7uHGw4YZFR2VmZtBOTT0i3szeHhkRr5VuwJHdE55Vog02SIm9Rw/41rfgGbfbmJlVhDwd5XZs5djOXR2IVZf11kuLvjQ2plr7k08WHZGZmbX3TP37kp4FvtziefqrgOtmxjrrpMTeu3dqir///qIjMjOrb+3V1P8E7AaMzV6bt80jYkw3xGZVYK214KGH0hzxw4enhWDMzKwY7T1Tfz8ipkTE6Ow5+sdAAH0krdZtEVrFGzgwDXHbdFPYc0+49NKiIzIzq08dPlOXtJukV4BXgQdIC7vcUea4rMost1wa7rbjjvC978HPfuZlW83MuluejnI/B7YEXo6INYDtgYfKGpVVpd69YezYtKLbKafAgQfCJ58UHZWZWf3Ik9TnRsS7QA9JPSLiPmCTMsdlVaqxEa64As44A666KtXc33mn6KjMzOpDnqQ+S1If4EHgj5LOA+aVNyyrZhKcfDJccw089hhsuSW89FLH15mZ2eLJk9RHAh8BPwDuBP5J6gXfIUnDJb0kaZKk41v5/L8kTcyGyo2TtHpngrfKtu++cN99MHt2Sux33110RGZmta3DpB4RH0bEZxExLyKuAM4Hhnd0naSG7NydgQ2A0ZI2aHHaU0BTRGwMXA/8srN/gFW2rbaCRx9NPeSHD4f/+Z80d7yZmXW99iaf6SfpBEm/lTRMydHAZGDvHPceCkyKiMkR8SlwLanW/7mIuC8iPsp2HwEGLtqfYZVsjTXgkUdSzf2kk2DUKJg1q+iozMxqT3s19auALwPPAt8D7ga+A4yMiJHtXNdsVWBqyf607FhbDsVD5WpW797wxz/CeefB7bfDV78Kzz5bdFRmZrWlvaS+ZkQcFBEXAaOBJmDXiJiQ895q5VirI5cljcnuf3Ybnx8uabyk8TNmzMj59VZpJDj22PScfc4c2GKLNFGNx7ObmXWN9pL63OY3ETEfeDUiPujEvacBg0r2BwLTW54kaQfgp8DuEdHqqOaIuDgimiKiacCAAZ0IwSrR1lvDU0+l5+3f+x7ssw/MnFl0VGZm1a+9pD5E0uxs+wDYuPm9pNk57v04sI6kNSQ1AvuS5pH/nKRNgYtICf3tRf0jrPqstFLqDX/mmXDTTbDJJvC3vxUdlZlZdWtv7veGiOiXbX0jomfJ+34d3Tgi5gFHA3cBLwDXRcTzks6QtHt22tlAH+AvkiZIGtvG7awGNTTAf/83/OMf0KtXWuntlFNg7twOLzUzs1YoquyBZlNTU4wfP77oMKyLffABHH3wAD7yAAAOVElEQVQ0XHllWhjmsstgyJCiozIzK56kJyKiKc+5eSafMSu7vn3T9LI33gjTp0NTE5x2Gnz6adGRmZlVDyd1qyijRsHzz6fOc6efnoa+Pflk0VGZmVUHJ3WrOMstB1dfDbfcAm+/DUOHwn/9V2qiNzOztjmpW8XafXeYODENe/v1r2G99eC66zyu3cysLU7qVtGWWQZ+97vUQ37FFVOz/E47wcsvFx2ZmVnlcVK3qrDllmkZ19/8Ji0Qs9FGqUnek9aYmS3gpG5Vo2dPOOaYtDb7AQekJvm1106J3mPbzcyc1K0KrbQS/P73aarZzTaD446DDTdMM9P5ebuZ1TMndataQ4akqWZvuy3V4r/97TQE7vbbndzNrD45qVtVk2CXXeCZZ9IsdO+9ByNGwNe/DuPGObmbWX1xUrea0LMnHHQQvPgiXHQRTJ0KO+wA3/oW3Huvk7uZ1QcndaspjY1w+OHwyiupA91LL8H226cJbK6/HubPLzpCM7PycVK3mrTkkqmn/Kuvppr7rFnwne/A+uvDxRfDv/9ddIRmZl3PSd1q2pJLppr7iy+m2ej69YP/+A8YPDgtGDN9etERmpl1HSd1qwsNDamm/vjjqQPdZpulBWNWXx1Gj4aHHvJzdzOrfk7qVlck2G67NOztlVfg2GPhzjth661Tor/kEpg9u+gozcwWjZO61a2114ZzzoFp09Jz9vnzU1P9yiunnvR/+5tr72ZWXZzUre717g2HHQZPPw2PPAJjxsCNN8I3vwnrrgu/+EUaImdmVumc1M0yEmyxReot/+abcMUVsMoqcOKJsNpq8I1vwAUXpDXezcwqkZO6WSt6906LxjzwAEyaBD/7WZqt7qijUqLfaac0g92sWUVHama2gKLKHho2NTXF+PHjiw7D6tSzz8K116Zt8mTo1SvNWjdyJOy+OwwcWHSEZlZrJD0REU25znVSN+u8iDQ87i9/gVtuST3pAZqaUoIfOTKt+S4VG6eZVT8ndbNuFJEmt7nlFrj5Znj00XR8tdVg2LC07bADLLNMsXGaWXVyUjcr0Jtvwq23pvHv48alce89eqRlYXfaKSX5LbZIi9CYmXXESd2sQsybl2rud9+dtsceg88+gz590vKw22yThs41NcESSxQdrZlVIid1swr13nup9n7fffDgg/D88+n4kkvCllumBP/Nb6Zafb9+xcZqZpXBSd2sSrzzDvz97ynBP/ggPPVUqslLsMEGacnYoUNTc/1GG6Xe9mZWX5zUzarU7Nnw8MOpyf6xx9LrO++kz5ZaKs1PP3QobLopbLIJrLeeE71ZretMUndXHbMK0q9f6ky3005pPwKmTEnJvTnRX3jhgvXgGxthww1hyJCU5IcMSZt72pvVJ9fUzarMvHnw0ktprvoJExa8lk5fu8oqsP76C28rruix82bVxs3vZnXoX/9akOQnToQXXkjbnDkLzunff0GCX2cdWGutBdvSSxcXu5m1zc3vZnVopZVg+PC0NYuAN95YkOBfeCEl/L/+deGFaZZb7otJvnlbbbVU8/eze7PK56RuVsOkNB/9wIGw445f/Gz27DR//T//+cXt4Yfhz39OvfCb9eiR1pkfNCgl+dZeBwxw075Z0ZzUzepUv36pc90mmyz82aefwmuvpaQ/dSq8/vqC16eegrFjF3TWa9arV2otWHnl9Fr6vvR1xRU90Y5ZuTipm9lCGhvTM/d11mn984g01K450b/+Okyfnp7rv/lm6rH/8MMwY0br1y+zDCy/fP6tf//UWmBm7XNSN7NOk1Jz+4ABaex8W+bOTc/um5N96eu776YfBq+/Dk8+md5/8knr9+nRA5ZdNiX3/v1Tp77W3re137evfxRYfShrUpc0HDgPaAB+HxFntvh8CeBKYHPgXWCfiJhSzpjMrPv06gWrrpq2jkTARx+l5N7W9v77aZs1K/04mDUr7X/4Ycf37907zbm/qNtSS6XpfJdaauHNPxisUpQtqUtqAM4HdgSmAY9LGhsRE0tOOxSYGRFrS9oXOAvYp1wxmVnlklLi7d0bVl+9c9fOnfvFhN+c7GfNgpkz4YMP0tC+ltusWTBtWnrffM6nn3Y+9sbG1pN9a9uSS6Y+BY2NaWt+39qxznze2JhW/uvZM/3IcKfF+lTOmvpQYFJETAaQdC0wEihN6iOB07L31wO/laSotsHzZlaoXr0WPH9fXJ9+mmr+LX8AfPzxom2zZ8Nbby3Y/+ST9B2ffJK2cv3frmfPVC7Nib55W5xjpcd79ICGhnyvnTl3Ua6RFrx21fvFuUdDQ3GzOpYzqa8KTC3ZnwZs0dY5ETFP0vvAcsA7ZYzLzKxNzbXe7vqf8vz5X0z0n37a9vv2Pp87N8022Ly13O/MsY8/Ti0XbZ03d24a8vjZZyn+9l7rUf/+qYWoCOVM6q01/rT8TZrnHCQdDhwOsNpqqy1+ZGZmFaKhAb70pbTVoubkn+cHQPNrZ86dPz+1dkSk/a58v6jXNTYWV97lTOrTgEEl+wOB6W2cM01ST2Bp4L2WN4qIi4GLIU0TW5ZozcysyzU3pVv3KGdRPw6sI2kNSY3AvsDYFueMBQ7M3u8F3Ovn6WZmZoumbDX17Bn50cBdpCFtf4iI5yWdAYyPiLHApcBVkiaRauj7liseMzOzWlfWceoRcTtwe4tjp5S8/zfwnXLGYGZmVi/8pMPMzKxGOKmbmZnVCCd1MzOzGuGkbmZmViOc1M3MzGqEk7qZmVmNULXN9SJpBvBaF95yeTzX/OJyGS4+l2HXcDkuPpfh4uvqMlw9IgbkObHqknpXkzQ+IpqKjqOauQwXn8uwa7gcF5/LcPEVWYZufjczM6sRTupmZmY1wkk9W/3NFovLcPG5DLuGy3HxuQwXX2FlWPfP1M3MzGqFa+pmZmY1om6TuqThkl6SNEnS8UXHU8kk/UHS25KeKzm2rKR7JL2SvS6THZek32Tl+oykzYqLvHJIGiTpPkkvSHpe0nHZcZdjTpKWlPSYpKezMjw9O76GpEezMvyzpMbs+BLZ/qTs88FFxl9JJDVIekrSX7N9l2EnSZoi6VlJEySNz44V/u+5LpO6pAbgfGBnYANgtKQNio2qol0ODG9x7HhgXESsA4zL9iGV6TrZdjhwYTfFWOnmAT+MiPWBLYGjsv/mXI75fQJsFxFDgE2A4ZK2BM4Czs3KcCZwaHb+ocDMiFgbODc7z5LjgBdK9l2Gi+ZbEbFJyfC1wv8912VSB4YCkyJickR8ClwLjCw4pooVEQ8C77U4PBK4Int/BbBHyfErI3kE6C9p5e6JtHJFxJsR8WT2/gPS/1BXxeWYW1YWc7LdXtkWwHbA9dnxlmXYXLbXA9tLUjeFW7EkDQRGAL/P9oXLsKsU/u+5XpP6qsDUkv1p2THLb8WIeBNSwgJWyI67bDuQNWFuCjyKy7FTsmbjCcDbwD3AP4FZETEvO6W0nD4vw+zz94HlujfiivRr4CfAZ9n+crgMF0UAd0t6QtLh2bHC/z33LMdNq0BrvzQ9DKBruGzbIakPcAPwnxExu51Kj8uxFRExH9hEUn/gJmD91k7LXl2GLUjaFXg7Ip6QtG3z4VZOdRl27OsRMV3SCsA9kl5s59xuK8d6ralPAwaV7A8EphcUS7V6q7n5KHt9Ozvusm2DpF6khP7HiLgxO+xyXAQRMQu4n9Q/ob+k5gpKaTl9XobZ50uz8GOkevN1YHdJU0iPHbcj1dxdhp0UEdOz17dJPzCHUgH/nus1qT8OrJP1+GwE9gXGFhxTtRkLHJi9PxC4peT4AVlvzy2B95ubo+pZ9hzyUuCFiPhVyUcux5wkDchq6EhaCtiB1DfhPmCv7LSWZdhctnsB90adT8wRESdExMCIGEz6/969EbEfLsNOkdRbUt/m98Aw4Dkq4d9zRNTlBuwCvEx6JvfTouOp5A24BngTmEv6xXko6bnaOOCV7HXZ7FyRRhb8E3gWaCo6/krYgK1JzW3PABOybReXY6fKcGPgqawMnwNOyY6vCTwGTAL+AiyRHV8y25+Ufb5m0X9DJW3AtsBfXYaLVHZrAk9n2/PNOaQS/j17RjkzM7MaUa/N72ZmZjXHSd3MzKxGOKmbmZnVCCd1MzOzGuGkbmZmViOc1M1qkKT52epRzVu7KxFKOkLSAV3wvVMkLb+49zGzReMhbWY1SNKciOhTwPdOIY3Bfae7v9vMXFM3qytZTfqsbF3yxyStnR0/TdKPsvfHSpqYrft8bXZsWUk3Z8cekbRxdnw5SXdna3NfRMkc15LGZN8xQdJF2WIsDZIul/Rcthb1DwooBrOa5aRuVpuWatH8vk/JZ7MjYijwW9K83y0dD2waERsDR2THTgeeyo6dCFyZHT8V+HtEbEqaCnM1AEnrA/uQFr3YBJgP7EdaB33ViNgoIr4CXNaFf7NZ3avXVdrMat3HWTJtzTUlr+e28vkzwB8l3QzcnB3bGtgTICLuzWroSwPfBL6dHb9N0szs/O2BzYHHs5XoliItbnErsKak/wVuA+5e9D/RzFpyTd2s/kQb75uNIM1TvTnwRLY6V3tLR7Z2DwFXRMQm2fbliDgtImYCQ0grrB0F/H4R/wYza4WTuln92afk9eHSDyT1AAZFxH3AT4D+QB/gQVLzOdk63O9ExOwWx3cGlsluNQ7YK1truvmZ/OpZz/geEXEDcDKwWbn+SLN65OZ3s9q0lKQJJft3RkTzsLYlJD1K+lE/usV1DcDVWdO6gHMjYpak04DLJD0DfMSC5SVPB66R9CTwAPA6QERMlHQScHf2Q2EuqWb+cXaf5grFCV33J5uZh7SZ1REPOTOrbW5+NzMzqxGuqZuZmdUI19TNzMxqhJO6mZlZjXBSNzMzqxFO6mZmZjXCSd3MzKxGOKmbmZnViP8fs6NV8ITfzlsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 0: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 48)                432       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,283\n",
      "Trainable params: 5,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 0\n",
      "\n",
      "INTERSECTION 1: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 48)                432       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,283\n",
      "Trainable params: 5,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 1\n",
      "\n",
      "INTERSECTION 2: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 48)                720       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4)                 196       \n",
      "=================================================================\n",
      "Total params: 5,620\n",
      "Trainable params: 5,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 2\n",
      "\n",
      "INTERSECTION 3: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 48)                432       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,283\n",
      "Trainable params: 5,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 3\n",
      "\n",
      "INTERSECTION 4: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 48)                336       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,187\n",
      "Trainable params: 5,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 4\n",
      "\n",
      "INTERSECTION 5: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 4,994\n",
      "Trainable params: 4,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 5\n",
      "\n",
      "INTERSECTION 6: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 48)                336       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 5,138\n",
      "Trainable params: 5,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 6\n",
      "\n",
      "INTERSECTION 7: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 48)                384       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,235\n",
      "Trainable params: 5,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 7\n",
      "\n",
      "INTERSECTION 8: SETTING UP AGENT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 4,994\n",
      "Trainable params: 4,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 8\n",
      "\n",
      "INTERSECTION 9: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 48)                240       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 9\n",
      "\n",
      "INTERSECTION 10: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 48)                240       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 10\n",
      "\n",
      "INTERSECTION 11: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 48)                240       \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 2)                 98        \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 11\n",
      "\n",
      "INTERSECTION 12: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 48)                384       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 4)                 196       \n",
      "=================================================================\n",
      "Total params: 5,284\n",
      "Trainable params: 5,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 12\n",
      "\n",
      "INTERSECTION 13: SETTING UP AGENT\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 48)                336       \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 5,187\n",
      "Trainable params: 5,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Deployed instance of Standard Deep Q Learning Agent(s) at Intersection 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, actions,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, batches_per_episode, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.save(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For the agent training\n",
    "ploty = 1\n",
    "for idx , agent in Balance_MultiDQN_Agents.Agents.items():\n",
    "    print(\"Agent \"+str(idx))\n",
    "    #print(ploty)\n",
    "    #plt.subplot(14, 2, ploty)\n",
    "\n",
    "    plt.figure('6'+str(idx),figsize=(4.5, 3))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"DQN\", \\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    ploty+=1\n",
    "    #print(ploty)\n",
    "\n",
    "    \n",
    "    #plt.subplot(14, 2, ploty)\n",
    "    plt.figure('7'+str(idx),figsize=(4.5, 3))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"DQN\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    #plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    #Loss_rewarddf.to_csv(csv_Path,index=False)\n",
    "    ploty+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Balance_MultiDQN_Agents.Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Balance_MultiDQN_Agents.Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Balance_MultiDQN_Agents.Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Balance_MultiDQN_Agents.Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Balance_MultiDQN_Agents.Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.load(498, best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight AC\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Straigth_AC\"\n",
    "\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.00001\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 0.5\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.train(200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.save(401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.load(200, best = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay = Single_Cross_Straight_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in  Single_Cross_Straight_MultiAC_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.Agents[0].Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDQN\"\n",
    "Session_ID = \"Single_Cross_Straigth_DuelingDQN20c0\"\n",
    "\n",
    "# all controller actions\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4]  }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 300\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.load(300 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Single_Cross_Straight_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(Episode_Queues[0])\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue)\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in  Single_Cross_Straight_MultiDQN_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3600\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_TripleAC4test1\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "    },\n",
    "   'demand' : {\"default\" : [400,400,400,400] }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 5000\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 action DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "Session_ID = \"Single_Cross_Triple4_actions\"\n",
    "#Session_ID = \"DQN\"\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{ 'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' :    {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    "         \n",
    "         'all_actions' :       {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    "         \n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         },\n",
    "        },\n",
    "     'demand' : { 'default' : [400, 400, 400, 400]}\n",
    "                  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 5\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'copy_weights_frequency' and 'epsilon_sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b327b6f6c10f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m Single_Cross_Triple4_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n\u001b[0;32m      2\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPER_activated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_weights_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_sequence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                 Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'copy_weights_frequency' and 'epsilon_sequence'"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, \n",
    "                                                       Single_Cross_Triple_dictionary4,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.train(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.load(best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple4_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To be arranged for multy agents\n",
    "\n",
    "queues = np.array(Episode_Queues[0])\n",
    "queues = queues.T\n",
    "\n",
    "delay = Cumulative_Episode_Delays[0]\n",
    "\n",
    "# Plot the queues\n",
    "plt.figure(1)\n",
    "for queue in queues:\n",
    "    plt.plot(queue)\n",
    "\n",
    "# plot the junctions delays\n",
    "plt.figure(2)\n",
    "plt.plot(delay)\n",
    "\n",
    "#plot the total delays \n",
    "plt.figure(3)\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "\n",
    "# Dont freak out the 2 delays are not the same because the node is not covering all the junction\n",
    "\n",
    "\"\"\"\n",
    "Because the cars never leave the nodes the delay is not computed correctly (when the agent doesn't work) \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].loss)\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)\n",
    "print(Single_Cross_Triple4_MultiDQN_Agents.Agents[0].reward_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple4_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_AC10\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]             \n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [300,300,300,300],\n",
    "             1 : [600,600,600,600],\n",
    "             2 : [1350,750,1350,750],\n",
    "             3 : [1500,750,1500,750],\n",
    "             4 : [1050,750,1050,750],\n",
    "             5 : [750,1050,750,1050],\n",
    "             6 : [750,1500,750,1500],\n",
    "             7 : [750,1350,750,1350],\n",
    "             8 : [600,600,600,600],\n",
    "             9 : [300,300,300,300]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.000001\n",
    "\n",
    "\n",
    "value = 5\n",
    "entropy = 500\n",
    "n_step_size = 4\n",
    "state_size = [13]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.load(50, best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays, Cumulative_Totale_network_delay = Single_Cross_Triple8_MultiAC_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Single_Cross_Triple8_MultiAC_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\nwalton\\\\Dropbox (The University of Manchester)\\\\ACTIVE\\\\TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "Session_ID = \"Single_Cross_Triple8_actions_DuelingDDQN20c10\"\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]             \n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [300,300,300,300],\n",
    "             1 : [600,600,600,600],\n",
    "             2 : [1350,750,1350,750],\n",
    "             3 : [1500,750,1500,750],\n",
    "             4 : [1050,750,1050,750],\n",
    "             5 : [750,1050,750,1050],\n",
    "             6 : [750,1500,750,1500],\n",
    "             7 : [750,1350,750,1350],\n",
    "             8 : [600,600,600,600],\n",
    "             9 : [300,300,300,300]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 400 \n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of tensorflow.python.keras.layers.core failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nwalton\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 244, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\nwalton\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 376, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\nwalton\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\nwalton\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\importlib\\__init__.py\", line 147, in reload\n",
      "    raise ImportError(msg.format(name), name=name)\n",
      "ImportError: module DQNAgents not in sys.modules\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 0: SETTING UP AGENT\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, 24)           336         input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 24)           600         dense_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, 24)           600         dense_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_138 (Dense)               (None, 24)           600         dense_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_141 (Dense)               (None, 1)            25          dense_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_139 (Dense)               (None, 8)            200         dense_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_141[0][0]                  \n",
      "                                                                 dense_139[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,361\n",
      "Trainable params: 2,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Double Deep Q Learning Agent(s) at Intersection 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents =\\\n",
    "MasterDQN_Agent(model_name, \n",
    "                vissim_working_directory, \n",
    "                sim_length, \n",
    "                Single_Cross_Triple_dictionary8,\n",
    "                'default_actions',\n",
    "                gamma, alpha, \n",
    "                agent_type,\n",
    "                memory_size,\n",
    "                PER_activated,\n",
    "                batch_size, \n",
    "                1,\n",
    "                copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Experience: Found. Loading into agents\n",
      "Previous Experience: Successfully loaded file from:\n",
      "C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Triple\\Agents_Results\\DuelingDDQN\\Single_Cross_Triple8_actions_DuelingDDQN20c10\\Agent0_PERPre_1000.p\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.56 seconds.\n",
      "\n",
      "start\n",
      "Random Seed Set to 101\n",
      "Episode 1: Finished running.\n",
      "Agent 0, Average Reward: -1195.29\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 3s - loss: 64041.5703\n",
      "Reducing exploration for all agents to 0.9828\n",
      "\n",
      "Episode 2: Starting computation.\n",
      "Random Seed Set to 102\n",
      "Episode 2: Finished running.\n",
      "Agent 0, Average Reward: -702.17\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 76660.6953\n",
      "Reducing exploration for all agents to 0.966\n",
      "\n",
      "Episode 3: Starting computation.\n",
      "Random Seed Set to 103\n",
      "Episode 3: Finished running.\n",
      "Agent 0, Average Reward: -1039.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 100402.4453\n",
      "Reducing exploration for all agents to 0.9494\n",
      "\n",
      "Episode 4: Starting computation.\n",
      "Random Seed Set to 104\n",
      "Episode 4: Finished running.\n",
      "Agent 0, Average Reward: -1115.43\n",
      "Epoch 1/1\n",
      " - 0s - loss: 90571.0234\n",
      "Reducing exploration for all agents to 0.9331\n",
      "\n",
      "Episode 5: Starting computation.\n",
      "Random Seed Set to 105\n",
      "Episode 5: Finished running.\n",
      "Agent 0, Average Reward: -1121.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 97024.3672\n",
      "Reducing exploration for all agents to 0.9171\n",
      "\n",
      "Episode 6: Starting computation.\n",
      "Random Seed Set to 106\n",
      "Episode 6: Finished running.\n",
      "Agent 0, Average Reward: -1046.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 82406.2891\n",
      "Reducing exploration for all agents to 0.9013\n",
      "\n",
      "Episode 7: Starting computation.\n",
      "Random Seed Set to 107\n",
      "Episode 7: Finished running.\n",
      "Agent 0, Average Reward: -1189.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 62380.2344\n",
      "Reducing exploration for all agents to 0.8859\n",
      "\n",
      "Episode 8: Starting computation.\n",
      "Random Seed Set to 108\n",
      "Episode 8: Finished running.\n",
      "Agent 0, Average Reward: -1258.4\n",
      "Epoch 1/1\n",
      " - 0s - loss: 53972.3828\n",
      "Reducing exploration for all agents to 0.8707\n",
      "\n",
      "Episode 9: Starting computation.\n",
      "Random Seed Set to 109\n",
      "Episode 9: Finished running.\n",
      "Agent 0, Average Reward: -1239.17\n",
      "Epoch 1/1\n",
      " - 0s - loss: 38728.5703\n",
      "Reducing exploration for all agents to 0.8557\n",
      "\n",
      "Episode 10: Starting computation.\n",
      "Random Seed Set to 110\n",
      "Episode 10: Finished running.\n",
      "Agent 0, Average Reward: -1384.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25387.2578\n",
      "Reducing exploration for all agents to 0.841\n",
      "\n",
      "Episode 11: Starting computation.\n",
      "Random Seed Set to 111\n",
      "Episode 11: Finished running.\n",
      "Agent 0, Average Reward: -1390.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11471.5605\n",
      "Reducing exploration for all agents to 0.8266\n",
      "\n",
      "Episode 12: Starting computation.\n",
      "Random Seed Set to 112\n",
      "Episode 12: Finished running.\n",
      "Agent 0, Average Reward: -1171.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4570.0713\n",
      "Reducing exploration for all agents to 0.8124\n",
      "\n",
      "Episode 13: Starting computation.\n",
      "Random Seed Set to 113\n",
      "Episode 13: Finished running.\n",
      "Agent 0, Average Reward: -1357.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3425.6392\n",
      "Reducing exploration for all agents to 0.7985\n",
      "\n",
      "Episode 14: Starting computation.\n",
      "Random Seed Set to 114\n",
      "Episode 14: Finished running.\n",
      "Agent 0, Average Reward: -1333.01\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12653.4736\n",
      "Reducing exploration for all agents to 0.7848\n",
      "\n",
      "Episode 15: Starting computation.\n",
      "Random Seed Set to 115\n",
      "Episode 15: Finished running.\n",
      "Agent 0, Average Reward: -914.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22989.1016\n",
      "Reducing exploration for all agents to 0.7713\n",
      "\n",
      "Episode 16: Starting computation.\n",
      "Random Seed Set to 116\n",
      "Episode 16: Finished running.\n",
      "Agent 0, Average Reward: -1371.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 23084.8145\n",
      "Reducing exploration for all agents to 0.7581\n",
      "\n",
      "Episode 17: Starting computation.\n",
      "Random Seed Set to 117\n",
      "Episode 17: Finished running.\n",
      "Agent 0, Average Reward: -1379.4\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25444.9023\n",
      "Reducing exploration for all agents to 0.745\n",
      "\n",
      "Episode 18: Starting computation.\n",
      "Random Seed Set to 118\n",
      "Episode 18: Finished running.\n",
      "Agent 0, Average Reward: -1470.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 24967.6914\n",
      "Reducing exploration for all agents to 0.7323\n",
      "\n",
      "Episode 19: Starting computation.\n",
      "Random Seed Set to 119\n",
      "Episode 19: Finished running.\n",
      "Agent 0, Average Reward: -1535.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 13352.9248\n",
      "Reducing exploration for all agents to 0.7197\n",
      "\n",
      "Episode 20: Starting computation.\n",
      "Random Seed Set to 120\n",
      "Episode 20: Finished running.\n",
      "Agent 0, Average Reward: -1516.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9514.8057\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.7073\n",
      "\n",
      "Episode 21: Starting computation.\n",
      "Random Seed Set to 121\n",
      "Episode 21: Finished running.\n",
      "Agent 0, Average Reward: -1633.65\n",
      "Epoch 1/1\n",
      " - 0s - loss: 348204.8750\n",
      "Reducing exploration for all agents to 0.6952\n",
      "\n",
      "Episode 22: Starting computation.\n",
      "Random Seed Set to 122\n",
      "Episode 22: Finished running.\n",
      "Agent 0, Average Reward: -1216.49\n",
      "Epoch 1/1\n",
      " - 0s - loss: 273112.7812\n",
      "Reducing exploration for all agents to 0.6833\n",
      "\n",
      "Episode 23: Starting computation.\n",
      "Random Seed Set to 123\n",
      "Episode 23: Finished running.\n",
      "Agent 0, Average Reward: -1714.06\n",
      "Epoch 1/1\n",
      " - 0s - loss: 243007.2656\n",
      "Reducing exploration for all agents to 0.6715\n",
      "\n",
      "Episode 24: Starting computation.\n",
      "Random Seed Set to 124\n",
      "Episode 24: Finished running.\n",
      "Agent 0, Average Reward: -1522.83\n",
      "Epoch 1/1\n",
      " - 0s - loss: 209747.0938\n",
      "Reducing exploration for all agents to 0.66\n",
      "\n",
      "Episode 25: Starting computation.\n",
      "Random Seed Set to 125\n",
      "Episode 25: Finished running.\n",
      "Agent 0, Average Reward: -1295.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 142212.0625\n",
      "Reducing exploration for all agents to 0.6487\n",
      "\n",
      "Episode 26: Starting computation.\n",
      "Random Seed Set to 126\n",
      "Episode 26: Finished running.\n",
      "Agent 0, Average Reward: -1299.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 69943.1562\n",
      "Reducing exploration for all agents to 0.6375\n",
      "\n",
      "Episode 27: Starting computation.\n",
      "Random Seed Set to 127\n",
      "Episode 27: Finished running.\n",
      "Agent 0, Average Reward: -1049.96\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15248.6914\n",
      "Reducing exploration for all agents to 0.6266\n",
      "\n",
      "Episode 28: Starting computation.\n",
      "Random Seed Set to 128\n",
      "Episode 28: Finished running.\n",
      "Agent 0, Average Reward: -1110.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7082.8813\n",
      "Reducing exploration for all agents to 0.6158\n",
      "\n",
      "Episode 29: Starting computation.\n",
      "Random Seed Set to 129\n",
      "Episode 29: Finished running.\n",
      "Agent 0, Average Reward: -1215.97\n",
      "Epoch 1/1\n",
      " - 0s - loss: 34927.3516\n",
      "Reducing exploration for all agents to 0.6053\n",
      "\n",
      "Episode 30: Starting computation.\n",
      "Random Seed Set to 130\n",
      "Episode 30: Finished running.\n",
      "Agent 0, Average Reward: -999.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 69020.8047\n",
      "Reducing exploration for all agents to 0.5949\n",
      "\n",
      "Episode 31: Starting computation.\n",
      "Random Seed Set to 131\n",
      "Episode 31: Finished running.\n",
      "Agent 0, Average Reward: -1437.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 102127.0938\n",
      "Reducing exploration for all agents to 0.5847\n",
      "\n",
      "Episode 32: Starting computation.\n",
      "Random Seed Set to 132\n",
      "Episode 32: Finished running.\n",
      "Agent 0, Average Reward: -1277.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 94163.0156\n",
      "Reducing exploration for all agents to 0.5746\n",
      "\n",
      "Episode 33: Starting computation.\n",
      "Random Seed Set to 133\n",
      "Episode 33: Finished running.\n",
      "Agent 0, Average Reward: -1267.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 75563.5859\n",
      "Reducing exploration for all agents to 0.5648\n",
      "\n",
      "Episode 34: Starting computation.\n",
      "Random Seed Set to 134\n",
      "Episode 34: Finished running.\n",
      "Agent 0, Average Reward: -1229.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 37167.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.5551\n",
      "\n",
      "Episode 35: Starting computation.\n",
      "Random Seed Set to 135\n",
      "Episode 35: Finished running.\n",
      "Agent 0, Average Reward: -1438.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 19326.8789\n",
      "Reducing exploration for all agents to 0.5456\n",
      "\n",
      "Episode 36: Starting computation.\n",
      "Random Seed Set to 136\n",
      "Episode 36: Finished running.\n",
      "Agent 0, Average Reward: -1456.11\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20311.9883\n",
      "Reducing exploration for all agents to 0.5362\n",
      "\n",
      "Episode 37: Starting computation.\n",
      "Random Seed Set to 137\n",
      "Episode 37: Finished running.\n",
      "Agent 0, Average Reward: -1265.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 31464.3809\n",
      "Reducing exploration for all agents to 0.527\n",
      "\n",
      "Episode 38: Starting computation.\n",
      "Random Seed Set to 138\n",
      "Episode 38: Finished running.\n",
      "Agent 0, Average Reward: -1495.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41942.4883\n",
      "Reducing exploration for all agents to 0.5179\n",
      "\n",
      "Episode 39: Starting computation.\n",
      "Random Seed Set to 139\n",
      "Episode 39: Finished running.\n",
      "Agent 0, Average Reward: -1397.02\n",
      "Epoch 1/1\n",
      " - 0s - loss: 38553.4570\n",
      "Reducing exploration for all agents to 0.5091\n",
      "\n",
      "Episode 40: Starting computation.\n",
      "Random Seed Set to 140\n",
      "Episode 40: Finished running.\n",
      "Agent 0, Average Reward: -1301.21\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29063.5332\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.5003\n",
      "\n",
      "Episode 41: Starting computation.\n",
      "Random Seed Set to 141\n",
      "Episode 41: Finished running.\n",
      "Agent 0, Average Reward: -1511.36\n",
      "Epoch 1/1\n",
      " - 0s - loss: 231786.8125\n",
      "Reducing exploration for all agents to 0.4917\n",
      "\n",
      "Episode 42: Starting computation.\n",
      "Random Seed Set to 142\n",
      "Episode 42: Finished running.\n",
      "Agent 0, Average Reward: -1457.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 185516.6562\n",
      "Reducing exploration for all agents to 0.4833\n",
      "\n",
      "Episode 43: Starting computation.\n",
      "Random Seed Set to 143\n",
      "Episode 43: Finished running.\n",
      "Agent 0, Average Reward: -1553.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 117160.2344\n",
      "Reducing exploration for all agents to 0.475\n",
      "\n",
      "Episode 44: Starting computation.\n",
      "Random Seed Set to 144\n",
      "Episode 44: Finished running.\n",
      "Agent 0, Average Reward: -1621.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 67804.0703\n",
      "Reducing exploration for all agents to 0.4668\n",
      "\n",
      "Episode 45: Starting computation.\n",
      "Random Seed Set to 145\n",
      "Episode 45: Finished running.\n",
      "Agent 0, Average Reward: -1741.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 40020.8320\n",
      "Reducing exploration for all agents to 0.4588\n",
      "\n",
      "Episode 46: Starting computation.\n",
      "Random Seed Set to 146\n",
      "Episode 46: Finished running.\n",
      "Agent 0, Average Reward: -1826.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16362.5947\n",
      "Reducing exploration for all agents to 0.451\n",
      "\n",
      "Episode 47: Starting computation.\n",
      "Random Seed Set to 147\n",
      "Episode 47: Finished running.\n",
      "Agent 0, Average Reward: -1757.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16134.1689\n",
      "Reducing exploration for all agents to 0.4432\n",
      "\n",
      "Episode 48: Starting computation.\n",
      "Random Seed Set to 148\n",
      "Episode 48: Finished running.\n",
      "Agent 0, Average Reward: -1637.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25579.8691\n",
      "Reducing exploration for all agents to 0.4356\n",
      "\n",
      "Episode 49: Starting computation.\n",
      "Random Seed Set to 149\n",
      "Episode 49: Finished running.\n",
      "Agent 0, Average Reward: -1801.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 80096.6797\n",
      "Reducing exploration for all agents to 0.4281\n",
      "\n",
      "Episode 50: Starting computation.\n",
      "Random Seed Set to 150\n",
      "Episode 50: Finished running.\n",
      "Agent 0, Average Reward: -1795.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 106776.8906\n",
      "Reducing exploration for all agents to 0.4208\n",
      "\n",
      "Episode 51: Starting computation.\n",
      "Random Seed Set to 151\n",
      "Episode 51: Finished running.\n",
      "Agent 0, Average Reward: -1898.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 97603.2891\n",
      "Reducing exploration for all agents to 0.4136\n",
      "\n",
      "Episode 52: Starting computation.\n",
      "Random Seed Set to 152\n",
      "Episode 52: Finished running.\n",
      "Agent 0, Average Reward: -1894.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 35308.7305\n",
      "Reducing exploration for all agents to 0.4065\n",
      "\n",
      "Episode 53: Starting computation.\n",
      "Random Seed Set to 153\n",
      "Episode 53: Finished running.\n",
      "Agent 0, Average Reward: -2288.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 42366.4375\n",
      "Reducing exploration for all agents to 0.3995\n",
      "\n",
      "Episode 54: Starting computation.\n",
      "Random Seed Set to 154\n",
      "Episode 54: Finished running.\n",
      "Agent 0, Average Reward: -1868.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 61418.2461\n",
      "Reducing exploration for all agents to 0.3926\n",
      "\n",
      "Episode 55: Starting computation.\n",
      "Random Seed Set to 155\n",
      "Episode 55: Finished running.\n",
      "Agent 0, Average Reward: -1755.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 56147.1289\n",
      "Reducing exploration for all agents to 0.3859\n",
      "\n",
      "Episode 56: Starting computation.\n",
      "Random Seed Set to 156\n",
      "Episode 56: Finished running.\n",
      "Agent 0, Average Reward: -2326.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 43468.5781\n",
      "Reducing exploration for all agents to 0.3793\n",
      "\n",
      "Episode 57: Starting computation.\n",
      "Random Seed Set to 157\n",
      "Episode 57: Finished running.\n",
      "Agent 0, Average Reward: -1921.43\n",
      "Epoch 1/1\n",
      " - 0s - loss: 32745.3086\n",
      "Reducing exploration for all agents to 0.3728\n",
      "\n",
      "Episode 58: Starting computation.\n",
      "Random Seed Set to 158\n",
      "Episode 58: Finished running.\n",
      "Agent 0, Average Reward: -1967.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22982.1016\n",
      "Reducing exploration for all agents to 0.3664\n",
      "\n",
      "Episode 59: Starting computation.\n",
      "Random Seed Set to 159\n",
      "Episode 59: Finished running.\n",
      "Agent 0, Average Reward: -2008.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 14793.5068\n",
      "Reducing exploration for all agents to 0.3601\n",
      "\n",
      "Episode 60: Starting computation.\n",
      "Random Seed Set to 160\n",
      "Episode 60: Finished running.\n",
      "Agent 0, Average Reward: -1777.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11812.1895\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.3539\n",
      "\n",
      "Episode 61: Starting computation.\n",
      "Random Seed Set to 161\n",
      "Episode 61: Finished running.\n",
      "Agent 0, Average Reward: -1452.49\n",
      "Epoch 1/1\n",
      " - 0s - loss: 325045.5625\n",
      "Reducing exploration for all agents to 0.3478\n",
      "\n",
      "Episode 62: Starting computation.\n",
      "Random Seed Set to 162\n",
      "Episode 62: Finished running.\n",
      "Agent 0, Average Reward: -2084.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 236457.7812\n",
      "Reducing exploration for all agents to 0.3418\n",
      "\n",
      "Episode 63: Starting computation.\n",
      "Random Seed Set to 163\n",
      "Episode 63: Finished running.\n",
      "Agent 0, Average Reward: -1935.95\n",
      "Epoch 1/1\n",
      " - 0s - loss: 108417.9844\n",
      "Reducing exploration for all agents to 0.336\n",
      "\n",
      "Episode 64: Starting computation.\n",
      "Random Seed Set to 164\n",
      "Episode 64: Finished running.\n",
      "Agent 0, Average Reward: -2052.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11294.2656\n",
      "Reducing exploration for all agents to 0.3302\n",
      "\n",
      "Episode 65: Starting computation.\n",
      "Random Seed Set to 165\n",
      "Episode 65: Finished running.\n",
      "Agent 0, Average Reward: -1190.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 64339.2070\n",
      "Reducing exploration for all agents to 0.3245\n",
      "\n",
      "Episode 66: Starting computation.\n",
      "Random Seed Set to 166\n",
      "Episode 66: Finished running.\n",
      "Agent 0, Average Reward: -885.74\n",
      "Epoch 1/1\n",
      " - 0s - loss: 111571.4766\n",
      "Reducing exploration for all agents to 0.319\n",
      "\n",
      "Episode 67: Starting computation.\n",
      "Random Seed Set to 167\n",
      "Episode 67: Finished running.\n",
      "Agent 0, Average Reward: -1248.42\n",
      "Epoch 1/1\n",
      " - 0s - loss: 66589.3672\n",
      "Reducing exploration for all agents to 0.3135\n",
      "\n",
      "Episode 68: Starting computation.\n",
      "Random Seed Set to 168\n",
      "Episode 68: Finished running.\n",
      "Agent 0, Average Reward: -1948.46\n",
      "Epoch 1/1\n",
      " - 0s - loss: 75944.7266\n",
      "Reducing exploration for all agents to 0.3081\n",
      "\n",
      "Episode 69: Starting computation.\n",
      "Random Seed Set to 169\n",
      "Episode 69: Finished running.\n",
      "Agent 0, Average Reward: -1795.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 40181.1484\n",
      "Reducing exploration for all agents to 0.3028\n",
      "\n",
      "Episode 70: Starting computation.\n",
      "Random Seed Set to 170\n",
      "Episode 70: Finished running.\n",
      "Agent 0, Average Reward: -1923.0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22984.0020\n",
      "Reducing exploration for all agents to 0.2976\n",
      "\n",
      "Episode 71: Starting computation.\n",
      "Random Seed Set to 171\n",
      "Episode 71: Finished running.\n",
      "Agent 0, Average Reward: -1905.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41302.1133\n",
      "Reducing exploration for all agents to 0.2925\n",
      "\n",
      "Episode 72: Starting computation.\n",
      "Random Seed Set to 172\n",
      "Episode 72: Finished running.\n",
      "Agent 0, Average Reward: -1759.22\n",
      "Epoch 1/1\n",
      " - 0s - loss: 54951.2461\n",
      "Reducing exploration for all agents to 0.2875\n",
      "\n",
      "Episode 73: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 173\n",
      "Episode 73: Finished running.\n",
      "Agent 0, Average Reward: -1854.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 43667.6289\n",
      "Reducing exploration for all agents to 0.2826\n",
      "\n",
      "Episode 74: Starting computation.\n",
      "Random Seed Set to 174\n",
      "Episode 74: Finished running.\n",
      "Agent 0, Average Reward: -2128.77\n",
      "Epoch 1/1\n",
      " - 0s - loss: 89723.5469\n",
      "Reducing exploration for all agents to 0.2777\n",
      "\n",
      "Episode 75: Starting computation.\n",
      "Random Seed Set to 175\n",
      "Episode 75: Finished running.\n",
      "Agent 0, Average Reward: -1909.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 79454.7344\n",
      "Reducing exploration for all agents to 0.273\n",
      "\n",
      "Episode 76: Starting computation.\n",
      "Random Seed Set to 176\n",
      "Episode 76: Finished running.\n",
      "Agent 0, Average Reward: -1158.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 68806.2266\n",
      "Reducing exploration for all agents to 0.2683\n",
      "\n",
      "Episode 77: Starting computation.\n",
      "Random Seed Set to 177\n",
      "Episode 77: Finished running.\n",
      "Agent 0, Average Reward: -1178.59\n",
      "Epoch 1/1\n",
      " - 0s - loss: 36593.2812\n",
      "Reducing exploration for all agents to 0.2637\n",
      "\n",
      "Episode 78: Starting computation.\n",
      "Random Seed Set to 178\n",
      "Episode 78: Finished running.\n",
      "Agent 0, Average Reward: -1545.71\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15524.0850\n",
      "Reducing exploration for all agents to 0.2591\n",
      "\n",
      "Episode 79: Starting computation.\n",
      "Random Seed Set to 179\n",
      "Episode 79: Finished running.\n",
      "Agent 0, Average Reward: -1288.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20383.2969\n",
      "Reducing exploration for all agents to 0.2547\n",
      "\n",
      "Episode 80: Starting computation.\n",
      "Random Seed Set to 180\n",
      "Episode 80: Finished running.\n",
      "Agent 0, Average Reward: -1522.96\n",
      "Epoch 1/1\n",
      " - 0s - loss: 37279.0898\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.2503\n",
      "\n",
      "Episode 81: Starting computation.\n",
      "Random Seed Set to 181\n",
      "Episode 81: Finished running.\n",
      "Agent 0, Average Reward: -1635.85\n",
      "Epoch 1/1\n",
      " - 0s - loss: 260599.3438\n",
      "Reducing exploration for all agents to 0.246\n",
      "\n",
      "Episode 82: Starting computation.\n",
      "Random Seed Set to 182\n",
      "Episode 82: Finished running.\n",
      "Agent 0, Average Reward: -1504.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 146680.4062\n",
      "Reducing exploration for all agents to 0.2418\n",
      "\n",
      "Episode 83: Starting computation.\n",
      "Random Seed Set to 183\n",
      "Episode 83: Finished running.\n",
      "Agent 0, Average Reward: -1690.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 24127.5039\n",
      "Reducing exploration for all agents to 0.2377\n",
      "\n",
      "Episode 84: Starting computation.\n",
      "Random Seed Set to 184\n",
      "Episode 84: Finished running.\n",
      "Agent 0, Average Reward: -953.63\n",
      "Epoch 1/1\n",
      " - 0s - loss: 19276.3887\n",
      "Reducing exploration for all agents to 0.2336\n",
      "\n",
      "Episode 85: Starting computation.\n",
      "Random Seed Set to 185\n",
      "Episode 85: Finished running.\n",
      "Agent 0, Average Reward: -1406.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 93923.4453\n",
      "Reducing exploration for all agents to 0.2296\n",
      "\n",
      "Episode 86: Starting computation.\n",
      "Random Seed Set to 186\n",
      "Episode 86: Finished running.\n",
      "Agent 0, Average Reward: -1487.98\n",
      "Epoch 1/1\n",
      " - 0s - loss: 87093.6641\n",
      "Reducing exploration for all agents to 0.2256\n",
      "\n",
      "Episode 87: Starting computation.\n",
      "Random Seed Set to 187\n",
      "Episode 87: Finished running.\n",
      "Agent 0, Average Reward: -1599.31\n",
      "Epoch 1/1\n",
      " - 0s - loss: 99254.0469\n",
      "Reducing exploration for all agents to 0.2218\n",
      "\n",
      "Episode 88: Starting computation.\n",
      "Random Seed Set to 188\n",
      "Episode 88: Finished running.\n",
      "Agent 0, Average Reward: -1538.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 45840.8789\n",
      "Reducing exploration for all agents to 0.2179\n",
      "\n",
      "Episode 89: Starting computation.\n",
      "Random Seed Set to 189\n",
      "Episode 89: Finished running.\n",
      "Agent 0, Average Reward: -1970.1\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28946.7422\n",
      "Reducing exploration for all agents to 0.2142\n",
      "\n",
      "Episode 90: Starting computation.\n",
      "Random Seed Set to 190\n",
      "Episode 90: Finished running.\n",
      "Agent 0, Average Reward: -1819.21\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20202.8770\n",
      "Reducing exploration for all agents to 0.2105\n",
      "\n",
      "Episode 91: Starting computation.\n",
      "Random Seed Set to 191\n",
      "Episode 91: Finished running.\n",
      "Agent 0, Average Reward: -1877.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 44785.0273\n",
      "Reducing exploration for all agents to 0.2069\n",
      "\n",
      "Episode 92: Starting computation.\n",
      "Random Seed Set to 192\n",
      "Episode 92: Finished running.\n",
      "Agent 0, Average Reward: -1901.29\n",
      "Epoch 1/1\n",
      " - 0s - loss: 52908.0742\n",
      "Reducing exploration for all agents to 0.2034\n",
      "\n",
      "Episode 93: Starting computation.\n",
      "Random Seed Set to 193\n",
      "Episode 93: Finished running.\n",
      "Agent 0, Average Reward: -1532.4\n",
      "Epoch 1/1\n",
      " - 0s - loss: 59645.2344\n",
      "Reducing exploration for all agents to 0.1999\n",
      "\n",
      "Episode 94: Starting computation.\n",
      "Random Seed Set to 194\n",
      "Episode 94: Finished running.\n",
      "Agent 0, Average Reward: -1242.02\n",
      "Epoch 1/1\n",
      " - 0s - loss: 99179.6094\n",
      "Reducing exploration for all agents to 0.1964\n",
      "\n",
      "Episode 95: Starting computation.\n",
      "Random Seed Set to 195\n",
      "Episode 95: Finished running.\n",
      "Agent 0, Average Reward: -1459.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 95688.3516\n",
      "Reducing exploration for all agents to 0.1931\n",
      "\n",
      "Episode 96: Starting computation.\n",
      "Random Seed Set to 196\n",
      "Episode 96: Finished running.\n",
      "Agent 0, Average Reward: -1419.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 35360.3242\n",
      "Reducing exploration for all agents to 0.1898\n",
      "\n",
      "Episode 97: Starting computation.\n",
      "Random Seed Set to 197\n",
      "Episode 97: Finished running.\n",
      "Agent 0, Average Reward: -1306.02\n",
      "Epoch 1/1\n",
      " - 0s - loss: 81332.7578\n",
      "Reducing exploration for all agents to 0.1865\n",
      "\n",
      "Episode 98: Starting computation.\n",
      "Random Seed Set to 198\n",
      "Episode 98: Finished running.\n",
      "Agent 0, Average Reward: -1768.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 150477.7812\n",
      "Reducing exploration for all agents to 0.1833\n",
      "\n",
      "Episode 99: Starting computation.\n",
      "Random Seed Set to 199\n",
      "Episode 99: Finished running.\n",
      "Agent 0, Average Reward: -2634.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 310446.3438\n",
      "Reducing exploration for all agents to 0.1802\n",
      "\n",
      "Episode 100: Starting computation.\n",
      "Random Seed Set to 200\n",
      "Episode 100: Finished running.\n",
      "Agent 0, Average Reward: -2630.93\n",
      "Epoch 1/1\n",
      " - 0s - loss: 164743.1562\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.1771\n",
      "\n",
      "Episode 101: Starting computation.\n",
      "Random Seed Set to 201\n",
      "Episode 101: Finished running.\n",
      "Agent 0, Average Reward: -2634.8\n",
      "Epoch 1/1\n",
      " - 0s - loss: 513309.0938\n",
      "Reducing exploration for all agents to 0.174\n",
      "\n",
      "Episode 102: Starting computation.\n",
      "Random Seed Set to 202\n",
      "Episode 102: Finished running.\n",
      "Agent 0, Average Reward: -2545.62\n",
      "Epoch 1/1\n",
      " - 0s - loss: 458272.5312\n",
      "Reducing exploration for all agents to 0.171\n",
      "\n",
      "Episode 103: Starting computation.\n",
      "Random Seed Set to 203\n",
      "Episode 103: Finished running.\n",
      "Agent 0, Average Reward: -2611.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 130267.5469\n",
      "Reducing exploration for all agents to 0.1681\n",
      "\n",
      "Episode 104: Starting computation.\n",
      "Random Seed Set to 204\n",
      "Episode 104: Finished running.\n",
      "Agent 0, Average Reward: -1420.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 35903.1602\n",
      "Reducing exploration for all agents to 0.1652\n",
      "\n",
      "Episode 105: Starting computation.\n",
      "Random Seed Set to 205\n",
      "Episode 105: Finished running.\n",
      "Agent 0, Average Reward: -1900.69\n",
      "Epoch 1/1\n",
      " - 0s - loss: 43360.2891\n",
      "Reducing exploration for all agents to 0.1624\n",
      "\n",
      "Episode 106: Starting computation.\n",
      "Random Seed Set to 206\n",
      "Episode 106: Finished running.\n",
      "Agent 0, Average Reward: -2004.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 68926.3516\n",
      "Reducing exploration for all agents to 0.1596\n",
      "\n",
      "Episode 107: Starting computation.\n",
      "Random Seed Set to 207\n",
      "Episode 107: Finished running.\n",
      "Agent 0, Average Reward: -1895.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 178637.2969\n",
      "Reducing exploration for all agents to 0.1569\n",
      "\n",
      "Episode 108: Starting computation.\n",
      "Random Seed Set to 208\n",
      "Episode 108: Finished running.\n",
      "Agent 0, Average Reward: -1839.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 74992.5234\n",
      "Reducing exploration for all agents to 0.1542\n",
      "\n",
      "Episode 109: Starting computation.\n",
      "Random Seed Set to 209\n",
      "Episode 109: Finished running.\n",
      "Agent 0, Average Reward: -1827.77\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33753.2344\n",
      "Reducing exploration for all agents to 0.1515\n",
      "\n",
      "Episode 110: Starting computation.\n",
      "Random Seed Set to 210\n",
      "Episode 110: Finished running.\n",
      "Agent 0, Average Reward: -1939.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 68847.0234\n",
      "Reducing exploration for all agents to 0.1489\n",
      "\n",
      "Episode 111: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 211\n",
      "Episode 111: Finished running.\n",
      "Agent 0, Average Reward: -2009.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 131481.4688\n",
      "Reducing exploration for all agents to 0.1464\n",
      "\n",
      "Episode 112: Starting computation.\n",
      "Random Seed Set to 212\n",
      "Episode 112: Finished running.\n",
      "Agent 0, Average Reward: -1879.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 95189.7734\n",
      "Reducing exploration for all agents to 0.1438\n",
      "\n",
      "Episode 113: Starting computation.\n",
      "Random Seed Set to 213\n",
      "Episode 113: Finished running.\n",
      "Agent 0, Average Reward: -2140.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41357.3359\n",
      "Reducing exploration for all agents to 0.1414\n",
      "\n",
      "Episode 114: Starting computation.\n",
      "Random Seed Set to 214\n",
      "Episode 114: Finished running.\n",
      "Agent 0, Average Reward: -1986.81\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33898.8164\n",
      "Reducing exploration for all agents to 0.1389\n",
      "\n",
      "Episode 115: Starting computation.\n",
      "Random Seed Set to 215\n",
      "Episode 115: Finished running.\n",
      "Agent 0, Average Reward: -2011.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 66595.6250\n",
      "Reducing exploration for all agents to 0.1366\n",
      "\n",
      "Episode 116: Starting computation.\n",
      "Random Seed Set to 216\n",
      "Episode 116: Finished running.\n",
      "Agent 0, Average Reward: -2199.71\n",
      "Epoch 1/1\n",
      " - 0s - loss: 119838.1250\n",
      "Reducing exploration for all agents to 0.1342\n",
      "\n",
      "Episode 117: Starting computation.\n",
      "Random Seed Set to 217\n",
      "Episode 117: Finished running.\n",
      "Agent 0, Average Reward: -2076.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25903.8516\n",
      "Reducing exploration for all agents to 0.1319\n",
      "\n",
      "Episode 118: Starting computation.\n",
      "Random Seed Set to 218\n",
      "Episode 118: Finished running.\n",
      "Agent 0, Average Reward: -2027.41\n",
      "Epoch 1/1\n",
      " - 0s - loss: 52337.7227\n",
      "Reducing exploration for all agents to 0.1297\n",
      "\n",
      "Episode 119: Starting computation.\n",
      "Random Seed Set to 219\n",
      "Episode 119: Finished running.\n",
      "Agent 0, Average Reward: -2004.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 44863.6055\n",
      "Reducing exploration for all agents to 0.1274\n",
      "\n",
      "Episode 120: Starting computation.\n",
      "Random Seed Set to 220\n",
      "Episode 120: Finished running.\n",
      "Agent 0, Average Reward: -2010.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 45406.2539\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.1252\n",
      "\n",
      "Episode 121: Starting computation.\n",
      "Random Seed Set to 221\n",
      "Episode 121: Finished running.\n",
      "Agent 0, Average Reward: -2010.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 261774.8906\n",
      "Reducing exploration for all agents to 0.1231\n",
      "\n",
      "Episode 122: Starting computation.\n",
      "Random Seed Set to 222\n",
      "Episode 122: Finished running.\n",
      "Agent 0, Average Reward: -2244.93\n",
      "Epoch 1/1\n",
      " - 0s - loss: 101558.8828\n",
      "Reducing exploration for all agents to 0.121\n",
      "\n",
      "Episode 123: Starting computation.\n",
      "Random Seed Set to 223\n",
      "Episode 123: Finished running.\n",
      "Agent 0, Average Reward: -2092.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6697.7495\n",
      "Reducing exploration for all agents to 0.1189\n",
      "\n",
      "Episode 124: Starting computation.\n",
      "Random Seed Set to 224\n",
      "Episode 124: Finished running.\n",
      "Agent 0, Average Reward: -1647.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 39705.9062\n",
      "Reducing exploration for all agents to 0.1169\n",
      "\n",
      "Episode 125: Starting computation.\n",
      "Random Seed Set to 225\n",
      "Episode 125: Finished running.\n",
      "Agent 0, Average Reward: -2191.26\n",
      "Epoch 1/1\n",
      " - 0s - loss: 81310.6953\n",
      "Reducing exploration for all agents to 0.1149\n",
      "\n",
      "Episode 126: Starting computation.\n",
      "Random Seed Set to 226\n",
      "Episode 126: Finished running.\n",
      "Agent 0, Average Reward: -2000.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25179.6348\n",
      "Reducing exploration for all agents to 0.1129\n",
      "\n",
      "Episode 127: Starting computation.\n",
      "Random Seed Set to 227\n",
      "Episode 127: Finished running.\n",
      "Agent 0, Average Reward: -2001.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12128.0879\n",
      "Reducing exploration for all agents to 0.1109\n",
      "\n",
      "Episode 128: Starting computation.\n",
      "Random Seed Set to 228\n",
      "Episode 128: Finished running.\n",
      "Agent 0, Average Reward: -2183.53\n",
      "Epoch 1/1\n",
      " - 0s - loss: 14227.4355\n",
      "Reducing exploration for all agents to 0.109\n",
      "\n",
      "Episode 129: Starting computation.\n",
      "Random Seed Set to 229\n",
      "Episode 129: Finished running.\n",
      "Agent 0, Average Reward: -2029.31\n",
      "Epoch 1/1\n",
      " - 0s - loss: 10949.7822\n",
      "Reducing exploration for all agents to 0.1072\n",
      "\n",
      "Episode 130: Starting computation.\n",
      "Random Seed Set to 230\n",
      "Episode 130: Finished running.\n",
      "Agent 0, Average Reward: -1929.42\n",
      "Epoch 1/1\n",
      " - 0s - loss: 13788.2451\n",
      "Reducing exploration for all agents to 0.1053\n",
      "\n",
      "Episode 131: Starting computation.\n",
      "Random Seed Set to 231\n",
      "Episode 131: Finished running.\n",
      "Agent 0, Average Reward: -2207.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20566.8984\n",
      "Reducing exploration for all agents to 0.1035\n",
      "\n",
      "Episode 132: Starting computation.\n",
      "Random Seed Set to 232\n",
      "Episode 132: Finished running.\n",
      "Agent 0, Average Reward: -2258.97\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22702.0371\n",
      "Reducing exploration for all agents to 0.1017\n",
      "\n",
      "Episode 133: Starting computation.\n",
      "Random Seed Set to 233\n",
      "Episode 133: Finished running.\n",
      "Agent 0, Average Reward: -2173.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7560.4102\n",
      "Reducing exploration for all agents to 0.1\n",
      "\n",
      "Episode 134: Starting computation.\n",
      "Random Seed Set to 234\n",
      "Episode 134: Finished running.\n",
      "Agent 0, Average Reward: -2084.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20999.3750\n",
      "Reducing exploration for all agents to 0.0983\n",
      "\n",
      "Episode 135: Starting computation.\n",
      "Random Seed Set to 235\n",
      "Episode 135: Finished running.\n",
      "Agent 0, Average Reward: -2172.99\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20510.7324\n",
      "Reducing exploration for all agents to 0.0966\n",
      "\n",
      "Episode 136: Starting computation.\n",
      "Random Seed Set to 236\n",
      "Episode 136: Finished running.\n",
      "Agent 0, Average Reward: -2076.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16671.5176\n",
      "Reducing exploration for all agents to 0.0949\n",
      "\n",
      "Episode 137: Starting computation.\n",
      "Random Seed Set to 237\n",
      "Episode 137: Finished running.\n",
      "Agent 0, Average Reward: -1954.98\n",
      "Epoch 1/1\n",
      " - 0s - loss: 10108.4131\n",
      "Reducing exploration for all agents to 0.0933\n",
      "\n",
      "Episode 138: Starting computation.\n",
      "Random Seed Set to 238\n",
      "Episode 138: Finished running.\n",
      "Agent 0, Average Reward: -2010.76\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12015.7188\n",
      "Reducing exploration for all agents to 0.0917\n",
      "\n",
      "Episode 139: Starting computation.\n",
      "Random Seed Set to 239\n",
      "Episode 139: Finished running.\n",
      "Agent 0, Average Reward: -2018.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22235.3223\n",
      "Reducing exploration for all agents to 0.0901\n",
      "\n",
      "Episode 140: Starting computation.\n",
      "Random Seed Set to 240\n",
      "Episode 140: Finished running.\n",
      "Agent 0, Average Reward: -2355.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28551.1582\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0886\n",
      "\n",
      "Episode 141: Starting computation.\n",
      "Random Seed Set to 241\n",
      "Episode 141: Finished running.\n",
      "Agent 0, Average Reward: -2273.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 311590.1562\n",
      "Reducing exploration for all agents to 0.0871\n",
      "\n",
      "Episode 142: Starting computation.\n",
      "Random Seed Set to 242\n",
      "Episode 142: Finished running.\n",
      "Agent 0, Average Reward: -2218.36\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41124.4414\n",
      "Reducing exploration for all agents to 0.0856\n",
      "\n",
      "Episode 143: Starting computation.\n",
      "Random Seed Set to 243\n",
      "Episode 143: Finished running.\n",
      "Agent 0, Average Reward: -2898.74\n",
      "Epoch 1/1\n",
      " - 0s - loss: 93580.2969\n",
      "Reducing exploration for all agents to 0.0841\n",
      "\n",
      "Episode 144: Starting computation.\n",
      "Random Seed Set to 244\n",
      "Episode 144: Finished running.\n",
      "Agent 0, Average Reward: -2632.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 134505.1094\n",
      "Reducing exploration for all agents to 0.0827\n",
      "\n",
      "Episode 145: Starting computation.\n",
      "Random Seed Set to 245\n",
      "Episode 145: Finished running.\n",
      "Agent 0, Average Reward: -2001.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 45007.1953\n",
      "Reducing exploration for all agents to 0.0812\n",
      "\n",
      "Episode 146: Starting computation.\n",
      "Random Seed Set to 246\n",
      "Episode 146: Finished running.\n",
      "Agent 0, Average Reward: -1762.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 35699.6445\n",
      "Reducing exploration for all agents to 0.0798\n",
      "\n",
      "Episode 147: Starting computation.\n",
      "Random Seed Set to 247\n",
      "Episode 147: Finished running.\n",
      "Agent 0, Average Reward: -1645.78\n",
      "Epoch 1/1\n",
      " - 0s - loss: 67234.0156\n",
      "Reducing exploration for all agents to 0.0785\n",
      "\n",
      "Episode 148: Starting computation.\n",
      "Random Seed Set to 248\n",
      "Episode 148: Finished running.\n",
      "Agent 0, Average Reward: -1628.11\n",
      "Epoch 1/1\n",
      " - 0s - loss: 82206.0469\n",
      "Reducing exploration for all agents to 0.0771\n",
      "\n",
      "Episode 149: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 249\n",
      "Episode 149: Finished running.\n",
      "Agent 0, Average Reward: -2143.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 58693.1484\n",
      "Reducing exploration for all agents to 0.0758\n",
      "\n",
      "Episode 150: Starting computation.\n",
      "Random Seed Set to 250\n",
      "Episode 150: Finished running.\n",
      "Agent 0, Average Reward: -1679.29\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9610.3916\n",
      "Reducing exploration for all agents to 0.0745\n",
      "\n",
      "Episode 151: Starting computation.\n",
      "Random Seed Set to 251\n",
      "Episode 151: Finished running.\n",
      "Agent 0, Average Reward: -2082.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 65707.0781\n",
      "Reducing exploration for all agents to 0.0732\n",
      "\n",
      "Episode 152: Starting computation.\n",
      "Random Seed Set to 252\n",
      "Episode 152: Finished running.\n",
      "Agent 0, Average Reward: -1055.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 97591.5703\n",
      "Reducing exploration for all agents to 0.072\n",
      "\n",
      "Episode 153: Starting computation.\n",
      "Random Seed Set to 253\n",
      "Episode 153: Finished running.\n",
      "Agent 0, Average Reward: -1275.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 46433.9180\n",
      "Reducing exploration for all agents to 0.0707\n",
      "\n",
      "Episode 154: Starting computation.\n",
      "Random Seed Set to 254\n",
      "Episode 154: Finished running.\n",
      "Agent 0, Average Reward: -1360.3\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11873.9746\n",
      "Reducing exploration for all agents to 0.0695\n",
      "\n",
      "Episode 155: Starting computation.\n",
      "Random Seed Set to 255\n",
      "Episode 155: Finished running.\n",
      "Agent 0, Average Reward: -2031.22\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15826.9473\n",
      "Reducing exploration for all agents to 0.0683\n",
      "\n",
      "Episode 156: Starting computation.\n",
      "Random Seed Set to 256\n",
      "Episode 156: Finished running.\n",
      "Agent 0, Average Reward: -1891.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 39765.2148\n",
      "Reducing exploration for all agents to 0.0672\n",
      "\n",
      "Episode 157: Starting computation.\n",
      "Random Seed Set to 257\n",
      "Episode 157: Finished running.\n",
      "Agent 0, Average Reward: -2063.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28986.5762\n",
      "Reducing exploration for all agents to 0.066\n",
      "\n",
      "Episode 158: Starting computation.\n",
      "Random Seed Set to 258\n",
      "Episode 158: Finished running.\n",
      "Agent 0, Average Reward: -1487.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 17349.3652\n",
      "Reducing exploration for all agents to 0.0649\n",
      "\n",
      "Episode 159: Starting computation.\n",
      "Random Seed Set to 259\n",
      "Episode 159: Finished running.\n",
      "Agent 0, Average Reward: -1367.55\n",
      "Epoch 1/1\n",
      " - 0s - loss: 55110.6992\n",
      "Reducing exploration for all agents to 0.0638\n",
      "\n",
      "Episode 160: Starting computation.\n",
      "Random Seed Set to 260\n",
      "Episode 160: Finished running.\n",
      "Agent 0, Average Reward: -1883.3\n",
      "Epoch 1/1\n",
      " - 0s - loss: 44541.7148\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0627\n",
      "\n",
      "Episode 161: Starting computation.\n",
      "Random Seed Set to 261\n",
      "Episode 161: Finished running.\n",
      "Agent 0, Average Reward: -2075.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 280512.8438\n",
      "Reducing exploration for all agents to 0.0616\n",
      "\n",
      "Episode 162: Starting computation.\n",
      "Random Seed Set to 262\n",
      "Episode 162: Finished running.\n",
      "Agent 0, Average Reward: -1954.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 133641.6562\n",
      "Reducing exploration for all agents to 0.0605\n",
      "\n",
      "Episode 163: Starting computation.\n",
      "Random Seed Set to 263\n",
      "Episode 163: Finished running.\n",
      "Agent 0, Average Reward: -1408.55\n",
      "Epoch 1/1\n",
      " - 0s - loss: 47105.1953\n",
      "Reducing exploration for all agents to 0.0595\n",
      "\n",
      "Episode 164: Starting computation.\n",
      "Random Seed Set to 264\n",
      "Episode 164: Finished running.\n",
      "Agent 0, Average Reward: -2270.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 74086.3203\n",
      "Reducing exploration for all agents to 0.0585\n",
      "\n",
      "Episode 165: Starting computation.\n",
      "Random Seed Set to 265\n",
      "Episode 165: Finished running.\n",
      "Agent 0, Average Reward: -1862.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 148327.5312\n",
      "Reducing exploration for all agents to 0.0575\n",
      "\n",
      "Episode 166: Starting computation.\n",
      "Random Seed Set to 266\n",
      "Episode 166: Finished running.\n",
      "Agent 0, Average Reward: -2279.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 90199.1172\n",
      "Reducing exploration for all agents to 0.0565\n",
      "\n",
      "Episode 167: Starting computation.\n",
      "Random Seed Set to 267\n",
      "Episode 167: Finished running.\n",
      "Agent 0, Average Reward: -2026.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22520.6348\n",
      "Reducing exploration for all agents to 0.0555\n",
      "\n",
      "Episode 168: Starting computation.\n",
      "Random Seed Set to 268\n",
      "Episode 168: Finished running.\n",
      "Agent 0, Average Reward: -1882.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29261.9648\n",
      "Reducing exploration for all agents to 0.0546\n",
      "\n",
      "Episode 169: Starting computation.\n",
      "Random Seed Set to 269\n",
      "Episode 169: Finished running.\n",
      "Agent 0, Average Reward: -1345.77\n",
      "Epoch 1/1\n",
      " - 0s - loss: 59386.5586\n",
      "Reducing exploration for all agents to 0.0536\n",
      "\n",
      "Episode 170: Starting computation.\n",
      "Random Seed Set to 270\n",
      "Episode 170: Finished running.\n",
      "Agent 0, Average Reward: -2132.39\n",
      "Epoch 1/1\n",
      " - 0s - loss: 53967.6875\n",
      "Reducing exploration for all agents to 0.0527\n",
      "\n",
      "Episode 171: Starting computation.\n",
      "Random Seed Set to 271\n",
      "Episode 171: Finished running.\n",
      "Agent 0, Average Reward: -2002.62\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29630.4238\n",
      "Reducing exploration for all agents to 0.0518\n",
      "\n",
      "Episode 172: Starting computation.\n",
      "Random Seed Set to 272\n",
      "Episode 172: Finished running.\n",
      "Agent 0, Average Reward: -2089.41\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5636.9253\n",
      "Reducing exploration for all agents to 0.0509\n",
      "\n",
      "Episode 173: Starting computation.\n",
      "Random Seed Set to 273\n",
      "Episode 173: Finished running.\n",
      "Agent 0, Average Reward: -1871.87\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28497.2422\n",
      "Reducing exploration for all agents to 0.05\n",
      "\n",
      "Episode 174: Starting computation.\n",
      "Random Seed Set to 274\n",
      "Episode 174: Finished running.\n",
      "Agent 0, Average Reward: -2000.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41103.5195\n",
      "Reducing exploration for all agents to 0.0492\n",
      "\n",
      "Episode 175: Starting computation.\n",
      "Random Seed Set to 275\n",
      "Episode 175: Finished running.\n",
      "Agent 0, Average Reward: -2077.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20460.1504\n",
      "Reducing exploration for all agents to 0.0483\n",
      "\n",
      "Episode 176: Starting computation.\n",
      "Random Seed Set to 276\n",
      "Episode 176: Finished running.\n",
      "Agent 0, Average Reward: -2053.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8070.7959\n",
      "Reducing exploration for all agents to 0.0475\n",
      "\n",
      "Episode 177: Starting computation.\n",
      "Random Seed Set to 277\n",
      "Episode 177: Finished running.\n",
      "Agent 0, Average Reward: -1978.92\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16792.9531\n",
      "Reducing exploration for all agents to 0.0467\n",
      "\n",
      "Episode 178: Starting computation.\n",
      "Random Seed Set to 278\n",
      "Episode 178: Finished running.\n",
      "Agent 0, Average Reward: -2035.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29321.6309\n",
      "Reducing exploration for all agents to 0.0459\n",
      "\n",
      "Episode 179: Starting computation.\n",
      "Random Seed Set to 279\n",
      "Episode 179: Finished running.\n",
      "Agent 0, Average Reward: -1974.02\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16327.2949\n",
      "Reducing exploration for all agents to 0.0451\n",
      "\n",
      "Episode 180: Starting computation.\n",
      "Random Seed Set to 280\n",
      "Episode 180: Finished running.\n",
      "Agent 0, Average Reward: -2078.46\n",
      "Epoch 1/1\n",
      " - 0s - loss: 6287.0288\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0443\n",
      "\n",
      "Episode 181: Starting computation.\n",
      "Random Seed Set to 281\n",
      "Episode 181: Finished running.\n",
      "Agent 0, Average Reward: -2005.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 208940.7656\n",
      "Reducing exploration for all agents to 0.0436\n",
      "\n",
      "Episode 182: Starting computation.\n",
      "Random Seed Set to 282\n",
      "Episode 182: Finished running.\n",
      "Agent 0, Average Reward: -2305.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27605.1973\n",
      "Reducing exploration for all agents to 0.0428\n",
      "\n",
      "Episode 183: Starting computation.\n",
      "Random Seed Set to 283\n",
      "Episode 183: Finished running.\n",
      "Agent 0, Average Reward: -1436.59\n",
      "Epoch 1/1\n",
      " - 0s - loss: 43425.2422\n",
      "Reducing exploration for all agents to 0.0421\n",
      "\n",
      "Episode 184: Starting computation.\n",
      "Random Seed Set to 284\n",
      "Episode 184: Finished running.\n",
      "Agent 0, Average Reward: -1896.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 74416.2344\n",
      "Reducing exploration for all agents to 0.0414\n",
      "\n",
      "Episode 185: Starting computation.\n",
      "Random Seed Set to 285\n",
      "Episode 185: Finished running.\n",
      "Agent 0, Average Reward: -2150.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 116214.2422\n",
      "Reducing exploration for all agents to 0.0406\n",
      "\n",
      "Episode 186: Starting computation.\n",
      "Random Seed Set to 286\n",
      "Episode 186: Finished running.\n",
      "Agent 0, Average Reward: -2556.75\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41780.4141\n",
      "Reducing exploration for all agents to 0.0399\n",
      "\n",
      "Episode 187: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 287\n",
      "Episode 187: Finished running.\n",
      "Agent 0, Average Reward: -2676.24\n",
      "Epoch 1/1\n",
      " - 0s - loss: 26354.7539\n",
      "Reducing exploration for all agents to 0.0393\n",
      "\n",
      "Episode 188: Starting computation.\n",
      "Random Seed Set to 288\n",
      "Episode 188: Finished running.\n",
      "Agent 0, Average Reward: -2692.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 76970.7109\n",
      "Reducing exploration for all agents to 0.0386\n",
      "\n",
      "Episode 189: Starting computation.\n",
      "Random Seed Set to 289\n",
      "Episode 189: Finished running.\n",
      "Agent 0, Average Reward: -2564.24\n",
      "Epoch 1/1\n",
      " - 0s - loss: 36843.9297\n",
      "Reducing exploration for all agents to 0.0379\n",
      "\n",
      "Episode 190: Starting computation.\n",
      "Random Seed Set to 290\n",
      "Episode 190: Finished running.\n",
      "Agent 0, Average Reward: -2780.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11266.4717\n",
      "Reducing exploration for all agents to 0.0373\n",
      "\n",
      "Episode 191: Starting computation.\n",
      "Random Seed Set to 291\n",
      "Episode 191: Finished running.\n",
      "Agent 0, Average Reward: -2742.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 61189.3398\n",
      "Reducing exploration for all agents to 0.0366\n",
      "\n",
      "Episode 192: Starting computation.\n",
      "Random Seed Set to 292\n",
      "Episode 192: Finished running.\n",
      "Agent 0, Average Reward: -2934.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 32463.7461\n",
      "Reducing exploration for all agents to 0.036\n",
      "\n",
      "Episode 193: Starting computation.\n",
      "Random Seed Set to 293\n",
      "Episode 193: Finished running.\n",
      "Agent 0, Average Reward: -2770.53\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7933.8208\n",
      "Reducing exploration for all agents to 0.0354\n",
      "\n",
      "Episode 194: Starting computation.\n",
      "Random Seed Set to 294\n",
      "Episode 194: Finished running.\n",
      "Agent 0, Average Reward: -2529.68\n",
      "Epoch 1/1\n",
      " - 0s - loss: 49272.8125\n",
      "Reducing exploration for all agents to 0.0348\n",
      "\n",
      "Episode 195: Starting computation.\n",
      "Random Seed Set to 295\n",
      "Episode 195: Finished running.\n",
      "Agent 0, Average Reward: -2480.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 23257.4453\n",
      "Reducing exploration for all agents to 0.0342\n",
      "\n",
      "Episode 196: Starting computation.\n",
      "Random Seed Set to 296\n",
      "Episode 196: Finished running.\n",
      "Agent 0, Average Reward: -2631.57\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3718.4602\n",
      "Reducing exploration for all agents to 0.0336\n",
      "\n",
      "Episode 197: Starting computation.\n",
      "Random Seed Set to 297\n",
      "Episode 197: Finished running.\n",
      "Agent 0, Average Reward: -2839.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 32353.1133\n",
      "Reducing exploration for all agents to 0.033\n",
      "\n",
      "Episode 198: Starting computation.\n",
      "Random Seed Set to 298\n",
      "Episode 198: Finished running.\n",
      "Agent 0, Average Reward: -2922.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33191.1094\n",
      "Reducing exploration for all agents to 0.0325\n",
      "\n",
      "Episode 199: Starting computation.\n",
      "Random Seed Set to 299\n",
      "Episode 199: Finished running.\n",
      "Agent 0, Average Reward: -2693.62\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12237.8389\n",
      "Reducing exploration for all agents to 0.0319\n",
      "\n",
      "Episode 200: Starting computation.\n",
      "Random Seed Set to 300\n",
      "Episode 200: Finished running.\n",
      "Agent 0, Average Reward: -2545.96\n",
      "Epoch 1/1\n",
      " - 0s - loss: 24252.7695\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0314\n",
      "\n",
      "Episode 201: Starting computation.\n",
      "Random Seed Set to 301\n",
      "Episode 201: Finished running.\n",
      "Agent 0, Average Reward: -2632.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 299539.2500\n",
      "Reducing exploration for all agents to 0.0308\n",
      "\n",
      "Episode 202: Starting computation.\n",
      "Random Seed Set to 302\n",
      "Episode 202: Finished running.\n",
      "Agent 0, Average Reward: -2792.75\n",
      "Epoch 1/1\n",
      " - 0s - loss: 42608.5156\n",
      "Reducing exploration for all agents to 0.0303\n",
      "\n",
      "Episode 203: Starting computation.\n",
      "Random Seed Set to 303\n",
      "Episode 203: Finished running.\n",
      "Agent 0, Average Reward: -2614.65\n",
      "Epoch 1/1\n",
      " - 0s - loss: 103448.1719\n",
      "Reducing exploration for all agents to 0.0298\n",
      "\n",
      "Episode 204: Starting computation.\n",
      "Random Seed Set to 304\n",
      "Episode 204: Finished running.\n",
      "Agent 0, Average Reward: -2259.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 248031.2031\n",
      "Reducing exploration for all agents to 0.0293\n",
      "\n",
      "Episode 205: Starting computation.\n",
      "Random Seed Set to 305\n",
      "Episode 205: Finished running.\n",
      "Agent 0, Average Reward: -2492.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 102124.2188\n",
      "Reducing exploration for all agents to 0.0288\n",
      "\n",
      "Episode 206: Starting computation.\n",
      "Random Seed Set to 306\n",
      "Episode 206: Finished running.\n",
      "Agent 0, Average Reward: -2472.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 26143.5176\n",
      "Reducing exploration for all agents to 0.0283\n",
      "\n",
      "Episode 207: Starting computation.\n",
      "Random Seed Set to 307\n",
      "Episode 207: Finished running.\n",
      "Agent 0, Average Reward: -2539.92\n",
      "Epoch 1/1\n",
      " - 0s - loss: 136940.9531\n",
      "Reducing exploration for all agents to 0.0278\n",
      "\n",
      "Episode 208: Starting computation.\n",
      "Random Seed Set to 308\n",
      "Episode 208: Finished running.\n",
      "Agent 0, Average Reward: -2610.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 136154.6250\n",
      "Reducing exploration for all agents to 0.0273\n",
      "\n",
      "Episode 209: Starting computation.\n",
      "Random Seed Set to 309\n",
      "Episode 209: Finished running.\n",
      "Agent 0, Average Reward: -2527.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 62379.2266\n",
      "Reducing exploration for all agents to 0.0268\n",
      "\n",
      "Episode 210: Starting computation.\n",
      "Random Seed Set to 310\n",
      "Episode 210: Finished running.\n",
      "Agent 0, Average Reward: -1712.17\n",
      "Epoch 1/1\n",
      " - 0s - loss: 68955.0156\n",
      "Reducing exploration for all agents to 0.0264\n",
      "\n",
      "Episode 211: Starting computation.\n",
      "Random Seed Set to 311\n",
      "Episode 211: Finished running.\n",
      "Agent 0, Average Reward: -1526.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 156007.2344\n",
      "Reducing exploration for all agents to 0.0259\n",
      "\n",
      "Episode 212: Starting computation.\n",
      "Random Seed Set to 312\n",
      "Episode 212: Finished running.\n",
      "Agent 0, Average Reward: -1392.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 75900.2266\n",
      "Reducing exploration for all agents to 0.0255\n",
      "\n",
      "Episode 213: Starting computation.\n",
      "Random Seed Set to 313\n",
      "Episode 213: Finished running.\n",
      "Agent 0, Average Reward: -1249.96\n",
      "Epoch 1/1\n",
      " - 0s - loss: 66117.0078\n",
      "Reducing exploration for all agents to 0.025\n",
      "\n",
      "Episode 214: Starting computation.\n",
      "Random Seed Set to 314\n",
      "Episode 214: Finished running.\n",
      "Agent 0, Average Reward: -1800.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 70980.2188\n",
      "Reducing exploration for all agents to 0.0246\n",
      "\n",
      "Episode 215: Starting computation.\n",
      "Random Seed Set to 315\n",
      "Episode 215: Finished running.\n",
      "Agent 0, Average Reward: -1952.29\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28638.8691\n",
      "Reducing exploration for all agents to 0.0242\n",
      "\n",
      "Episode 216: Starting computation.\n",
      "Random Seed Set to 316\n",
      "Episode 216: Finished running.\n",
      "Agent 0, Average Reward: -1761.31\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33547.3555\n",
      "Reducing exploration for all agents to 0.0238\n",
      "\n",
      "Episode 217: Starting computation.\n",
      "Random Seed Set to 317\n",
      "Episode 217: Finished running.\n",
      "Agent 0, Average Reward: -1846.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29092.8418\n",
      "Reducing exploration for all agents to 0.0234\n",
      "\n",
      "Episode 218: Starting computation.\n",
      "Random Seed Set to 318\n",
      "Episode 218: Finished running.\n",
      "Agent 0, Average Reward: -1814.59\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27494.9375\n",
      "Reducing exploration for all agents to 0.023\n",
      "\n",
      "Episode 219: Starting computation.\n",
      "Random Seed Set to 319\n",
      "Episode 219: Finished running.\n",
      "Agent 0, Average Reward: -1817.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18066.4961\n",
      "Reducing exploration for all agents to 0.0226\n",
      "\n",
      "Episode 220: Starting computation.\n",
      "Random Seed Set to 320\n",
      "Episode 220: Finished running.\n",
      "Agent 0, Average Reward: -1978.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 26378.9277\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0222\n",
      "\n",
      "Episode 221: Starting computation.\n",
      "Random Seed Set to 321\n",
      "Episode 221: Finished running.\n",
      "Agent 0, Average Reward: -2014.36\n",
      "Epoch 1/1\n",
      " - 0s - loss: 154196.6562\n",
      "Reducing exploration for all agents to 0.0218\n",
      "\n",
      "Episode 222: Starting computation.\n",
      "Random Seed Set to 322\n",
      "Episode 222: Finished running.\n",
      "Agent 0, Average Reward: -2052.22\n",
      "Epoch 1/1\n",
      " - 0s - loss: 23022.5547\n",
      "Reducing exploration for all agents to 0.0214\n",
      "\n",
      "Episode 223: Starting computation.\n",
      "Random Seed Set to 323\n",
      "Episode 223: Finished running.\n",
      "Agent 0, Average Reward: -1975.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 63800.9062\n",
      "Reducing exploration for all agents to 0.0211\n",
      "\n",
      "Episode 224: Starting computation.\n",
      "Random Seed Set to 324\n",
      "Episode 224: Finished running.\n",
      "Agent 0, Average Reward: -1630.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 165387.3125\n",
      "Reducing exploration for all agents to 0.0207\n",
      "\n",
      "Episode 225: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 325\n",
      "Episode 225: Finished running.\n",
      "Agent 0, Average Reward: -2045.92\n",
      "Epoch 1/1\n",
      " - 0s - loss: 91589.6797\n",
      "Reducing exploration for all agents to 0.0203\n",
      "\n",
      "Episode 226: Starting computation.\n",
      "Random Seed Set to 326\n",
      "Episode 226: Finished running.\n",
      "Agent 0, Average Reward: -2475.46\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29055.0605\n",
      "Reducing exploration for all agents to 0.02\n",
      "\n",
      "Episode 227: Starting computation.\n",
      "Random Seed Set to 327\n",
      "Episode 227: Finished running.\n",
      "Agent 0, Average Reward: -2409.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 110290.5469\n",
      "Reducing exploration for all agents to 0.0196\n",
      "\n",
      "Episode 228: Starting computation.\n",
      "Random Seed Set to 328\n",
      "Episode 228: Finished running.\n",
      "Agent 0, Average Reward: -2397.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 108385.5703\n",
      "Reducing exploration for all agents to 0.0193\n",
      "\n",
      "Episode 229: Starting computation.\n",
      "Random Seed Set to 329\n",
      "Episode 229: Finished running.\n",
      "Agent 0, Average Reward: -2642.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 32678.9609\n",
      "Reducing exploration for all agents to 0.019\n",
      "\n",
      "Episode 230: Starting computation.\n",
      "Random Seed Set to 330\n",
      "Episode 230: Finished running.\n",
      "Agent 0, Average Reward: -2642.16\n",
      "Epoch 1/1\n",
      " - 0s - loss: 79208.8516\n",
      "Reducing exploration for all agents to 0.0186\n",
      "\n",
      "Episode 231: Starting computation.\n",
      "Random Seed Set to 331\n",
      "Episode 231: Finished running.\n",
      "Agent 0, Average Reward: -2928.36\n",
      "Epoch 1/1\n",
      " - 0s - loss: 58896.0586\n",
      "Reducing exploration for all agents to 0.0183\n",
      "\n",
      "Episode 232: Starting computation.\n",
      "Random Seed Set to 332\n",
      "Episode 232: Finished running.\n",
      "Agent 0, Average Reward: -2794.65\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15519.7510\n",
      "Reducing exploration for all agents to 0.018\n",
      "\n",
      "Episode 233: Starting computation.\n",
      "Random Seed Set to 333\n",
      "Episode 233: Finished running.\n",
      "Agent 0, Average Reward: -2940.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 132035.3594\n",
      "Reducing exploration for all agents to 0.0177\n",
      "\n",
      "Episode 234: Starting computation.\n",
      "Random Seed Set to 334\n",
      "Episode 234: Finished running.\n",
      "Agent 0, Average Reward: -2751.31\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7898.9526\n",
      "Reducing exploration for all agents to 0.0174\n",
      "\n",
      "Episode 235: Starting computation.\n",
      "Random Seed Set to 335\n",
      "Episode 235: Finished running.\n",
      "Agent 0, Average Reward: -2887.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16857.8398\n",
      "Reducing exploration for all agents to 0.0171\n",
      "\n",
      "Episode 236: Starting computation.\n",
      "Random Seed Set to 336\n",
      "Episode 236: Finished running.\n",
      "Agent 0, Average Reward: -2843.07\n",
      "Epoch 1/1\n",
      " - 0s - loss: 19473.3613\n",
      "Reducing exploration for all agents to 0.0168\n",
      "\n",
      "Episode 237: Starting computation.\n",
      "Random Seed Set to 337\n",
      "Episode 237: Finished running.\n",
      "Agent 0, Average Reward: -2812.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8308.6279\n",
      "Reducing exploration for all agents to 0.0165\n",
      "\n",
      "Episode 238: Starting computation.\n",
      "Random Seed Set to 338\n",
      "Episode 238: Finished running.\n",
      "Agent 0, Average Reward: -2627.08\n",
      "Epoch 1/1\n",
      " - 0s - loss: 13601.2598\n",
      "Reducing exploration for all agents to 0.0162\n",
      "\n",
      "Episode 239: Starting computation.\n",
      "Random Seed Set to 339\n",
      "Episode 239: Finished running.\n",
      "Agent 0, Average Reward: -2867.87\n",
      "Epoch 1/1\n",
      " - 0s - loss: 8397.2061\n",
      "Reducing exploration for all agents to 0.016\n",
      "\n",
      "Episode 240: Starting computation.\n",
      "Random Seed Set to 340\n",
      "Episode 240: Finished running.\n",
      "Agent 0, Average Reward: -2836.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 7319.0459\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0157\n",
      "\n",
      "Episode 241: Starting computation.\n",
      "Random Seed Set to 341\n",
      "Episode 241: Finished running.\n",
      "Agent 0, Average Reward: -2598.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 164476.1094\n",
      "Reducing exploration for all agents to 0.0154\n",
      "\n",
      "Episode 242: Starting computation.\n",
      "Random Seed Set to 342\n",
      "Episode 242: Finished running.\n",
      "Agent 0, Average Reward: -2800.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 19808.2871\n",
      "Reducing exploration for all agents to 0.0152\n",
      "\n",
      "Episode 243: Starting computation.\n",
      "Random Seed Set to 343\n",
      "Episode 243: Finished running.\n",
      "Agent 0, Average Reward: -2774.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 188835.1406\n",
      "Reducing exploration for all agents to 0.0149\n",
      "\n",
      "Episode 244: Starting computation.\n",
      "Random Seed Set to 344\n",
      "Episode 244: Finished running.\n",
      "Agent 0, Average Reward: -2861.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 45447.4688\n",
      "Reducing exploration for all agents to 0.0146\n",
      "\n",
      "Episode 245: Starting computation.\n",
      "Random Seed Set to 345\n",
      "Episode 245: Finished running.\n",
      "Agent 0, Average Reward: -2869.77\n",
      "Epoch 1/1\n",
      " - 0s - loss: 61335.4531\n",
      "Reducing exploration for all agents to 0.0144\n",
      "\n",
      "Episode 246: Starting computation.\n",
      "Random Seed Set to 346\n",
      "Episode 246: Finished running.\n",
      "Agent 0, Average Reward: -2855.59\n",
      "Epoch 1/1\n",
      " - 0s - loss: 112569.7578\n",
      "Reducing exploration for all agents to 0.0141\n",
      "\n",
      "Episode 247: Starting computation.\n",
      "Random Seed Set to 347\n",
      "Episode 247: Finished running.\n",
      "Agent 0, Average Reward: -2790.68\n",
      "Epoch 1/1\n",
      " - 0s - loss: 4411.9702\n",
      "Reducing exploration for all agents to 0.0139\n",
      "\n",
      "Episode 248: Starting computation.\n",
      "Random Seed Set to 348\n",
      "Episode 248: Finished running.\n",
      "Agent 0, Average Reward: -2874.01\n",
      "Epoch 1/1\n",
      " - 0s - loss: 80086.9375\n",
      "Reducing exploration for all agents to 0.0137\n",
      "\n",
      "Episode 249: Starting computation.\n",
      "Random Seed Set to 349\n",
      "Episode 249: Finished running.\n",
      "Agent 0, Average Reward: -2733.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 57900.7383\n",
      "Reducing exploration for all agents to 0.0134\n",
      "\n",
      "Episode 250: Starting computation.\n",
      "Random Seed Set to 350\n",
      "Episode 250: Finished running.\n",
      "Agent 0, Average Reward: -2934.31\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9690.3066\n",
      "Reducing exploration for all agents to 0.0132\n",
      "\n",
      "Episode 251: Starting computation.\n",
      "Random Seed Set to 351\n",
      "Episode 251: Finished running.\n",
      "Agent 0, Average Reward: -2561.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 53473.2969\n",
      "Reducing exploration for all agents to 0.013\n",
      "\n",
      "Episode 252: Starting computation.\n",
      "Random Seed Set to 352\n",
      "Episode 252: Finished running.\n",
      "Agent 0, Average Reward: -2811.53\n",
      "Epoch 1/1\n",
      " - 0s - loss: 23424.7461\n",
      "Reducing exploration for all agents to 0.0127\n",
      "\n",
      "Episode 253: Starting computation.\n",
      "Random Seed Set to 353\n",
      "Episode 253: Finished running.\n",
      "Agent 0, Average Reward: -2921.74\n",
      "Epoch 1/1\n",
      " - 0s - loss: 5751.7603\n",
      "Reducing exploration for all agents to 0.0125\n",
      "\n",
      "Episode 254: Starting computation.\n",
      "Random Seed Set to 354\n",
      "Episode 254: Finished running.\n",
      "Agent 0, Average Reward: -2922.77\n",
      "Epoch 1/1\n",
      " - 0s - loss: 40851.2734\n",
      "Reducing exploration for all agents to 0.0123\n",
      "\n",
      "Episode 255: Starting computation.\n",
      "Random Seed Set to 355\n",
      "Episode 255: Finished running.\n",
      "Agent 0, Average Reward: -2652.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 13105.5029\n",
      "Reducing exploration for all agents to 0.0121\n",
      "\n",
      "Episode 256: Starting computation.\n",
      "Random Seed Set to 356\n",
      "Episode 256: Finished running.\n",
      "Agent 0, Average Reward: -2837.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12254.8506\n",
      "Reducing exploration for all agents to 0.0119\n",
      "\n",
      "Episode 257: Starting computation.\n",
      "Random Seed Set to 357\n",
      "Episode 257: Finished running.\n",
      "Agent 0, Average Reward: -2857.31\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20511.4492\n",
      "Reducing exploration for all agents to 0.0117\n",
      "\n",
      "Episode 258: Starting computation.\n",
      "Random Seed Set to 358\n",
      "Episode 258: Finished running.\n",
      "Agent 0, Average Reward: -2881.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 11955.6455\n",
      "Reducing exploration for all agents to 0.0115\n",
      "\n",
      "Episode 259: Starting computation.\n",
      "Random Seed Set to 359\n",
      "Episode 259: Finished running.\n",
      "Agent 0, Average Reward: -2810.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 16325.7793\n",
      "Reducing exploration for all agents to 0.0113\n",
      "\n",
      "Episode 260: Starting computation.\n",
      "Random Seed Set to 360\n",
      "Episode 260: Finished running.\n",
      "Agent 0, Average Reward: -2846.75\n",
      "Epoch 1/1\n",
      " - 0s - loss: 108048.4922\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0111\n",
      "\n",
      "Episode 261: Starting computation.\n",
      "Random Seed Set to 361\n",
      "Episode 261: Finished running.\n",
      "Agent 0, Average Reward: -2818.29\n",
      "Epoch 1/1\n",
      " - 0s - loss: 182067.8281\n",
      "Reducing exploration for all agents to 0.0109\n",
      "\n",
      "Episode 262: Starting computation.\n",
      "Random Seed Set to 362\n",
      "Episode 262: Finished running.\n",
      "Agent 0, Average Reward: -2849.24\n",
      "Epoch 1/1\n",
      " - 0s - loss: 9342.3086\n",
      "Reducing exploration for all agents to 0.0107\n",
      "\n",
      "Episode 263: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 363\n",
      "Episode 263: Finished running.\n",
      "Agent 0, Average Reward: -2072.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 152795.0625\n",
      "Reducing exploration for all agents to 0.0105\n",
      "\n",
      "Episode 264: Starting computation.\n",
      "Random Seed Set to 364\n",
      "Episode 264: Finished running.\n",
      "Agent 0, Average Reward: -2098.51\n",
      "Epoch 1/1\n",
      " - 0s - loss: 124207.4453\n",
      "Reducing exploration for all agents to 0.0104\n",
      "\n",
      "Episode 265: Starting computation.\n",
      "Random Seed Set to 365\n",
      "Episode 265: Finished running.\n",
      "Agent 0, Average Reward: -2095.02\n",
      "Epoch 1/1\n",
      " - 0s - loss: 156032.8125\n",
      "Reducing exploration for all agents to 0.0102\n",
      "\n",
      "Episode 266: Starting computation.\n",
      "Random Seed Set to 366\n",
      "Episode 266: Finished running.\n",
      "Agent 0, Average Reward: -2155.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 182725.4219\n",
      "Reducing exploration for all agents to 0.01\n",
      "\n",
      "Episode 267: Starting computation.\n",
      "Random Seed Set to 367\n",
      "Episode 267: Finished running.\n",
      "Agent 0, Average Reward: -2082.32\n",
      "Epoch 1/1\n",
      " - 0s - loss: 183697.5938\n",
      "Reducing exploration for all agents to 0.0098\n",
      "\n",
      "Episode 268: Starting computation.\n",
      "Random Seed Set to 368\n",
      "Episode 268: Finished running.\n",
      "Agent 0, Average Reward: -1995.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 145112.9844\n",
      "Reducing exploration for all agents to 0.0097\n",
      "\n",
      "Episode 269: Starting computation.\n",
      "Random Seed Set to 369\n",
      "Episode 269: Finished running.\n",
      "Agent 0, Average Reward: -1909.58\n",
      "Epoch 1/1\n",
      " - 0s - loss: 90058.1484\n",
      "Reducing exploration for all agents to 0.0095\n",
      "\n",
      "Episode 270: Starting computation.\n",
      "Random Seed Set to 370\n",
      "Episode 270: Finished running.\n",
      "Agent 0, Average Reward: -1014.51\n",
      "Epoch 1/1\n",
      " - 0s - loss: 26044.9785\n",
      "Reducing exploration for all agents to 0.0093\n",
      "\n",
      "Episode 271: Starting computation.\n",
      "Random Seed Set to 371\n",
      "Episode 271: Finished running.\n",
      "Agent 0, Average Reward: -2007.4\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28720.3926\n",
      "Reducing exploration for all agents to 0.0092\n",
      "\n",
      "Episode 272: Starting computation.\n",
      "Random Seed Set to 372\n",
      "Episode 272: Finished running.\n",
      "Agent 0, Average Reward: -2348.92\n",
      "Epoch 1/1\n",
      " - 0s - loss: 92028.5078\n",
      "Reducing exploration for all agents to 0.009\n",
      "\n",
      "Episode 273: Starting computation.\n",
      "Random Seed Set to 373\n",
      "Episode 273: Finished running.\n",
      "Agent 0, Average Reward: -2603.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 175883.8125\n",
      "Reducing exploration for all agents to 0.0089\n",
      "\n",
      "Episode 274: Starting computation.\n",
      "Random Seed Set to 374\n",
      "Episode 274: Finished running.\n",
      "Agent 0, Average Reward: -2423.63\n",
      "Epoch 1/1\n",
      " - 0s - loss: 75603.8281\n",
      "Reducing exploration for all agents to 0.0087\n",
      "\n",
      "Episode 275: Starting computation.\n",
      "Random Seed Set to 375\n",
      "Episode 275: Finished running.\n",
      "Agent 0, Average Reward: -2446.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 93384.9609\n",
      "Reducing exploration for all agents to 0.0086\n",
      "\n",
      "Episode 276: Starting computation.\n",
      "Random Seed Set to 376\n",
      "Episode 276: Finished running.\n",
      "Agent 0, Average Reward: -2262.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 168132.6719\n",
      "Reducing exploration for all agents to 0.0084\n",
      "\n",
      "Episode 277: Starting computation.\n",
      "Random Seed Set to 377\n",
      "Episode 277: Finished running.\n",
      "Agent 0, Average Reward: -1943.28\n",
      "Epoch 1/1\n",
      " - 0s - loss: 55455.0664\n",
      "Reducing exploration for all agents to 0.0083\n",
      "\n",
      "Episode 278: Starting computation.\n",
      "Random Seed Set to 378\n",
      "Episode 278: Finished running.\n",
      "Agent 0, Average Reward: -1915.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27583.7461\n",
      "Reducing exploration for all agents to 0.0081\n",
      "\n",
      "Episode 279: Starting computation.\n",
      "Random Seed Set to 379\n",
      "Episode 279: Finished running.\n",
      "Agent 0, Average Reward: -1856.49\n",
      "Epoch 1/1\n",
      " - 0s - loss: 35942.0820\n",
      "Reducing exploration for all agents to 0.008\n",
      "\n",
      "Episode 280: Starting computation.\n",
      "Random Seed Set to 380\n",
      "Episode 280: Finished running.\n",
      "Agent 0, Average Reward: -1611.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 64013.9688\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0078\n",
      "\n",
      "Episode 281: Starting computation.\n",
      "Random Seed Set to 381\n",
      "Episode 281: Finished running.\n",
      "Agent 0, Average Reward: -1791.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 120457.1328\n",
      "Reducing exploration for all agents to 0.0077\n",
      "\n",
      "Episode 282: Starting computation.\n",
      "Random Seed Set to 382\n",
      "Episode 282: Finished running.\n",
      "Agent 0, Average Reward: -1599.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 49306.0391\n",
      "Reducing exploration for all agents to 0.0076\n",
      "\n",
      "Episode 283: Starting computation.\n",
      "Random Seed Set to 383\n",
      "Episode 283: Finished running.\n",
      "Agent 0, Average Reward: -1548.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 32497.8770\n",
      "Reducing exploration for all agents to 0.0075\n",
      "\n",
      "Episode 284: Starting computation.\n",
      "Random Seed Set to 384\n",
      "Episode 284: Finished running.\n",
      "Agent 0, Average Reward: -1663.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 58181.8086\n",
      "Reducing exploration for all agents to 0.0073\n",
      "\n",
      "Episode 285: Starting computation.\n",
      "Random Seed Set to 385\n",
      "Episode 285: Finished running.\n",
      "Agent 0, Average Reward: -1752.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 83440.1484\n",
      "Reducing exploration for all agents to 0.0072\n",
      "\n",
      "Episode 286: Starting computation.\n",
      "Random Seed Set to 386\n",
      "Episode 286: Finished running.\n",
      "Agent 0, Average Reward: -1708.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 60213.4844\n",
      "Reducing exploration for all agents to 0.0071\n",
      "\n",
      "Episode 287: Starting computation.\n",
      "Random Seed Set to 387\n",
      "Episode 287: Finished running.\n",
      "Agent 0, Average Reward: -1582.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 44746.4180\n",
      "Reducing exploration for all agents to 0.007\n",
      "\n",
      "Episode 288: Starting computation.\n",
      "Random Seed Set to 388\n",
      "Episode 288: Finished running.\n",
      "Agent 0, Average Reward: -1706.46\n",
      "Epoch 1/1\n",
      " - 0s - loss: 51366.2305\n",
      "Reducing exploration for all agents to 0.0068\n",
      "\n",
      "Episode 289: Starting computation.\n",
      "Random Seed Set to 389\n",
      "Episode 289: Finished running.\n",
      "Agent 0, Average Reward: -1971.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 65665.6562\n",
      "Reducing exploration for all agents to 0.0067\n",
      "\n",
      "Episode 290: Starting computation.\n",
      "Random Seed Set to 390\n",
      "Episode 290: Finished running.\n",
      "Agent 0, Average Reward: -1964.04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 38572.7344\n",
      "Reducing exploration for all agents to 0.0066\n",
      "\n",
      "Episode 291: Starting computation.\n",
      "Random Seed Set to 391\n",
      "Episode 291: Finished running.\n",
      "Agent 0, Average Reward: -1842.42\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41141.9375\n",
      "Reducing exploration for all agents to 0.0065\n",
      "\n",
      "Episode 292: Starting computation.\n",
      "Random Seed Set to 392\n",
      "Episode 292: Finished running.\n",
      "Agent 0, Average Reward: -1549.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 35631.5664\n",
      "Reducing exploration for all agents to 0.0064\n",
      "\n",
      "Episode 293: Starting computation.\n",
      "Random Seed Set to 393\n",
      "Episode 293: Finished running.\n",
      "Agent 0, Average Reward: -1669.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22829.8516\n",
      "Reducing exploration for all agents to 0.0063\n",
      "\n",
      "Episode 294: Starting computation.\n",
      "Random Seed Set to 394\n",
      "Episode 294: Finished running.\n",
      "Agent 0, Average Reward: -1634.74\n",
      "Epoch 1/1\n",
      " - 0s - loss: 32898.3203\n",
      "Reducing exploration for all agents to 0.0062\n",
      "\n",
      "Episode 295: Starting computation.\n",
      "Random Seed Set to 395\n",
      "Episode 295: Finished running.\n",
      "Agent 0, Average Reward: -1667.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28135.2910\n",
      "Reducing exploration for all agents to 0.0061\n",
      "\n",
      "Episode 296: Starting computation.\n",
      "Random Seed Set to 396\n",
      "Episode 296: Finished running.\n",
      "Agent 0, Average Reward: -1809.57\n",
      "Epoch 1/1\n",
      " - 0s - loss: 24355.3945\n",
      "Reducing exploration for all agents to 0.0059\n",
      "\n",
      "Episode 297: Starting computation.\n",
      "Random Seed Set to 397\n",
      "Episode 297: Finished running.\n",
      "Agent 0, Average Reward: -2188.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27517.3086\n",
      "Reducing exploration for all agents to 0.0058\n",
      "\n",
      "Episode 298: Starting computation.\n",
      "Random Seed Set to 398\n",
      "Episode 298: Finished running.\n",
      "Agent 0, Average Reward: -1931.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 37267.0117\n",
      "Reducing exploration for all agents to 0.0057\n",
      "\n",
      "Episode 299: Starting computation.\n",
      "Random Seed Set to 399\n",
      "Episode 299: Finished running.\n",
      "Agent 0, Average Reward: -2166.16\n",
      "Epoch 1/1\n",
      " - 0s - loss: 51930.7539\n",
      "Reducing exploration for all agents to 0.0056\n",
      "\n",
      "Episode 300: Starting computation.\n",
      "Random Seed Set to 400\n",
      "Episode 300: Finished running.\n",
      "Agent 0, Average Reward: -2515.38\n",
      "Epoch 1/1\n",
      " - 0s - loss: 71561.9922\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0056\n",
      "\n",
      "Episode 301: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 401\n",
      "Episode 301: Finished running.\n",
      "Agent 0, Average Reward: -2491.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 161032.0000\n",
      "Reducing exploration for all agents to 0.0055\n",
      "\n",
      "Episode 302: Starting computation.\n",
      "Random Seed Set to 402\n",
      "Episode 302: Finished running.\n",
      "Agent 0, Average Reward: -2261.4\n",
      "Epoch 1/1\n",
      " - 0s - loss: 26126.2891\n",
      "Reducing exploration for all agents to 0.0054\n",
      "\n",
      "Episode 303: Starting computation.\n",
      "Random Seed Set to 403\n",
      "Episode 303: Finished running.\n",
      "Agent 0, Average Reward: -2318.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 126538.3125\n",
      "Reducing exploration for all agents to 0.0053\n",
      "\n",
      "Episode 304: Starting computation.\n",
      "Random Seed Set to 404\n",
      "Episode 304: Finished running.\n",
      "Agent 0, Average Reward: -2436.85\n",
      "Epoch 1/1\n",
      " - 0s - loss: 186096.5469\n",
      "Reducing exploration for all agents to 0.0052\n",
      "\n",
      "Episode 305: Starting computation.\n",
      "Random Seed Set to 405\n",
      "Episode 305: Finished running.\n",
      "Agent 0, Average Reward: -2606.65\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25148.9551\n",
      "Reducing exploration for all agents to 0.0051\n",
      "\n",
      "Episode 306: Starting computation.\n",
      "Random Seed Set to 406\n",
      "Episode 306: Finished running.\n",
      "Agent 0, Average Reward: -2601.51\n",
      "Epoch 1/1\n",
      " - 0s - loss: 105269.1250\n",
      "Reducing exploration for all agents to 0.005\n",
      "\n",
      "Episode 307: Starting computation.\n",
      "Random Seed Set to 407\n",
      "Episode 307: Finished running.\n",
      "Agent 0, Average Reward: -2648.62\n",
      "Epoch 1/1\n",
      " - 0s - loss: 118000.8203\n",
      "Reducing exploration for all agents to 0.0049\n",
      "\n",
      "Episode 308: Starting computation.\n",
      "Random Seed Set to 408\n",
      "Episode 308: Finished running.\n",
      "Agent 0, Average Reward: -2323.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29900.4473\n",
      "Reducing exploration for all agents to 0.0048\n",
      "\n",
      "Episode 309: Starting computation.\n",
      "Random Seed Set to 409\n",
      "Episode 309: Finished running.\n",
      "Agent 0, Average Reward: -2634.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 98482.5547\n",
      "Reducing exploration for all agents to 0.0047\n",
      "\n",
      "Episode 310: Starting computation.\n",
      "Random Seed Set to 410\n",
      "Episode 310: Finished running.\n",
      "Agent 0, Average Reward: -2381.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 89135.6641\n",
      "Reducing exploration for all agents to 0.0047\n",
      "\n",
      "Episode 311: Starting computation.\n",
      "Random Seed Set to 411\n",
      "Episode 311: Finished running.\n",
      "Agent 0, Average Reward: -1929.98\n",
      "Epoch 1/1\n",
      " - 0s - loss: 30383.6973\n",
      "Reducing exploration for all agents to 0.0046\n",
      "\n",
      "Episode 312: Starting computation.\n",
      "Random Seed Set to 412\n",
      "Episode 312: Finished running.\n",
      "Agent 0, Average Reward: -1879.91\n",
      "Epoch 1/1\n",
      " - 0s - loss: 54189.3047\n",
      "Reducing exploration for all agents to 0.0045\n",
      "\n",
      "Episode 313: Starting computation.\n",
      "Random Seed Set to 413\n",
      "Episode 313: Finished running.\n",
      "Agent 0, Average Reward: -1936.68\n",
      "Epoch 1/1\n",
      " - 0s - loss: 125780.8516\n",
      "Reducing exploration for all agents to 0.0044\n",
      "\n",
      "Episode 314: Starting computation.\n",
      "Random Seed Set to 414\n",
      "Episode 314: Finished running.\n",
      "Agent 0, Average Reward: -1797.96\n",
      "Epoch 1/1\n",
      " - 0s - loss: 69130.4297\n",
      "Reducing exploration for all agents to 0.0044\n",
      "\n",
      "Episode 315: Starting computation.\n",
      "Random Seed Set to 415\n",
      "Episode 315: Finished running.\n",
      "Agent 0, Average Reward: -1869.46\n",
      "Epoch 1/1\n",
      " - 0s - loss: 50263.1914\n",
      "Reducing exploration for all agents to 0.0043\n",
      "\n",
      "Episode 316: Starting computation.\n",
      "Random Seed Set to 416\n",
      "Episode 316: Finished running.\n",
      "Agent 0, Average Reward: -1671.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28204.2676\n",
      "Reducing exploration for all agents to 0.0042\n",
      "\n",
      "Episode 317: Starting computation.\n",
      "Random Seed Set to 417\n",
      "Episode 317: Finished running.\n",
      "Agent 0, Average Reward: -1647.01\n",
      "Epoch 1/1\n",
      " - 0s - loss: 56666.6406\n",
      "Reducing exploration for all agents to 0.0041\n",
      "\n",
      "Episode 318: Starting computation.\n",
      "Random Seed Set to 418\n",
      "Episode 318: Finished running.\n",
      "Agent 0, Average Reward: -1477.06\n",
      "Epoch 1/1\n",
      " - 0s - loss: 63496.8750\n",
      "Reducing exploration for all agents to 0.0041\n",
      "\n",
      "Episode 319: Starting computation.\n",
      "Random Seed Set to 419\n",
      "Episode 319: Finished running.\n",
      "Agent 0, Average Reward: -1565.8\n",
      "Epoch 1/1\n",
      " - 0s - loss: 32162.6113\n",
      "Reducing exploration for all agents to 0.004\n",
      "\n",
      "Episode 320: Starting computation.\n",
      "Random Seed Set to 420\n",
      "Episode 320: Finished running.\n",
      "Agent 0, Average Reward: -1593.53\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20620.2695\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0039\n",
      "\n",
      "Episode 321: Starting computation.\n",
      "Random Seed Set to 421\n",
      "Episode 321: Finished running.\n",
      "Agent 0, Average Reward: -1537.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 89651.2891\n",
      "Reducing exploration for all agents to 0.0039\n",
      "\n",
      "Episode 322: Starting computation.\n",
      "Random Seed Set to 422\n",
      "Episode 322: Finished running.\n",
      "Agent 0, Average Reward: -1478.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 79502.8516\n",
      "Reducing exploration for all agents to 0.0038\n",
      "\n",
      "Episode 323: Starting computation.\n",
      "Random Seed Set to 423\n",
      "Episode 323: Finished running.\n",
      "Agent 0, Average Reward: -1601.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 42697.5898\n",
      "Reducing exploration for all agents to 0.0037\n",
      "\n",
      "Episode 324: Starting computation.\n",
      "Random Seed Set to 424\n",
      "Episode 324: Finished running.\n",
      "Agent 0, Average Reward: -1495.04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12948.4873\n",
      "Reducing exploration for all agents to 0.0037\n",
      "\n",
      "Episode 325: Starting computation.\n",
      "Random Seed Set to 425\n",
      "Episode 325: Finished running.\n",
      "Agent 0, Average Reward: -1454.57\n",
      "Epoch 1/1\n",
      " - 0s - loss: 21251.8379\n",
      "Reducing exploration for all agents to 0.0036\n",
      "\n",
      "Episode 326: Starting computation.\n",
      "Random Seed Set to 426\n",
      "Episode 326: Finished running.\n",
      "Agent 0, Average Reward: -1685.39\n",
      "Epoch 1/1\n",
      " - 0s - loss: 65509.4922\n",
      "Reducing exploration for all agents to 0.0035\n",
      "\n",
      "Episode 327: Starting computation.\n",
      "Random Seed Set to 427\n",
      "Episode 327: Finished running.\n",
      "Agent 0, Average Reward: -1735.92\n",
      "Epoch 1/1\n",
      " - 0s - loss: 64755.9922\n",
      "Reducing exploration for all agents to 0.0035\n",
      "\n",
      "Episode 328: Starting computation.\n",
      "Random Seed Set to 428\n",
      "Episode 328: Finished running.\n",
      "Agent 0, Average Reward: -1707.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 12592.4658\n",
      "Reducing exploration for all agents to 0.0034\n",
      "\n",
      "Episode 329: Starting computation.\n",
      "Random Seed Set to 429\n",
      "Episode 329: Finished running.\n",
      "Agent 0, Average Reward: -1658.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 39972.0781\n",
      "Reducing exploration for all agents to 0.0034\n",
      "\n",
      "Episode 330: Starting computation.\n",
      "Random Seed Set to 430\n",
      "Episode 330: Finished running.\n",
      "Agent 0, Average Reward: -1659.02\n",
      "Epoch 1/1\n",
      " - 0s - loss: 48502.4805\n",
      "Reducing exploration for all agents to 0.0033\n",
      "\n",
      "Episode 331: Starting computation.\n",
      "Random Seed Set to 431\n",
      "Episode 331: Finished running.\n",
      "Agent 0, Average Reward: -1718.42\n",
      "Epoch 1/1\n",
      " - 0s - loss: 64426.9336\n",
      "Reducing exploration for all agents to 0.0032\n",
      "\n",
      "Episode 332: Starting computation.\n",
      "Random Seed Set to 432\n",
      "Episode 332: Finished running.\n",
      "Agent 0, Average Reward: -1863.38\n",
      "Epoch 1/1\n",
      " - 0s - loss: 61667.8086\n",
      "Reducing exploration for all agents to 0.0032\n",
      "\n",
      "Episode 333: Starting computation.\n",
      "Random Seed Set to 433\n",
      "Episode 333: Finished running.\n",
      "Agent 0, Average Reward: -1480.29\n",
      "Epoch 1/1\n",
      " - 0s - loss: 17129.2559\n",
      "Reducing exploration for all agents to 0.0031\n",
      "\n",
      "Episode 334: Starting computation.\n",
      "Random Seed Set to 434\n",
      "Episode 334: Finished running.\n",
      "Agent 0, Average Reward: -1625.49\n",
      "Epoch 1/1\n",
      " - 0s - loss: 23103.6816\n",
      "Reducing exploration for all agents to 0.0031\n",
      "\n",
      "Episode 335: Starting computation.\n",
      "Random Seed Set to 435\n",
      "Episode 335: Finished running.\n",
      "Agent 0, Average Reward: -1545.74\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29974.2012\n",
      "Reducing exploration for all agents to 0.003\n",
      "\n",
      "Episode 336: Starting computation.\n",
      "Random Seed Set to 436\n",
      "Episode 336: Finished running.\n",
      "Agent 0, Average Reward: -1538.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 22802.8535\n",
      "Reducing exploration for all agents to 0.003\n",
      "\n",
      "Episode 337: Starting computation.\n",
      "Random Seed Set to 437\n",
      "Episode 337: Finished running.\n",
      "Agent 0, Average Reward: -1615.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15006.8975\n",
      "Reducing exploration for all agents to 0.0029\n",
      "\n",
      "Episode 338: Starting computation.\n",
      "Random Seed Set to 438\n",
      "Episode 338: Finished running.\n",
      "Agent 0, Average Reward: -1639.56\n",
      "Epoch 1/1\n",
      " - 0s - loss: 21354.3555\n",
      "Reducing exploration for all agents to 0.0029\n",
      "\n",
      "Episode 339: Starting computation.\n",
      "Random Seed Set to 439\n",
      "Episode 339: Finished running.\n",
      "Agent 0, Average Reward: -1659.83\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29407.6777\n",
      "Reducing exploration for all agents to 0.0028\n",
      "\n",
      "Episode 340: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 440\n",
      "Episode 340: Finished running.\n",
      "Agent 0, Average Reward: -1781.87\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25678.7754\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0028\n",
      "\n",
      "Episode 341: Starting computation.\n",
      "Random Seed Set to 441\n",
      "Episode 341: Finished running.\n",
      "Agent 0, Average Reward: -1948.1\n",
      "Epoch 1/1\n",
      " - 0s - loss: 118511.9297\n",
      "Reducing exploration for all agents to 0.0027\n",
      "\n",
      "Episode 342: Starting computation.\n",
      "Random Seed Set to 442\n",
      "Episode 342: Finished running.\n",
      "Agent 0, Average Reward: -1681.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 32022.9590\n",
      "Reducing exploration for all agents to 0.0027\n",
      "\n",
      "Episode 343: Starting computation.\n",
      "Random Seed Set to 443\n",
      "Episode 343: Finished running.\n",
      "Agent 0, Average Reward: -1730.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 25826.7441\n",
      "Reducing exploration for all agents to 0.0026\n",
      "\n",
      "Episode 344: Starting computation.\n",
      "Random Seed Set to 444\n",
      "Episode 344: Finished running.\n",
      "Agent 0, Average Reward: -1467.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 61474.4922\n",
      "Reducing exploration for all agents to 0.0026\n",
      "\n",
      "Episode 345: Starting computation.\n",
      "Random Seed Set to 445\n",
      "Episode 345: Finished running.\n",
      "Agent 0, Average Reward: -1438.57\n",
      "Epoch 1/1\n",
      " - 0s - loss: 72597.6484\n",
      "Reducing exploration for all agents to 0.0025\n",
      "\n",
      "Episode 346: Starting computation.\n",
      "Random Seed Set to 446\n",
      "Episode 346: Finished running.\n",
      "Agent 0, Average Reward: -1621.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 40020.0156\n",
      "Reducing exploration for all agents to 0.0025\n",
      "\n",
      "Episode 347: Starting computation.\n",
      "Random Seed Set to 447\n",
      "Episode 347: Finished running.\n",
      "Agent 0, Average Reward: -1679.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18348.0762\n",
      "Reducing exploration for all agents to 0.0025\n",
      "\n",
      "Episode 348: Starting computation.\n",
      "Random Seed Set to 448\n",
      "Episode 348: Finished running.\n",
      "Agent 0, Average Reward: -1491.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 26787.3418\n",
      "Reducing exploration for all agents to 0.0024\n",
      "\n",
      "Episode 349: Starting computation.\n",
      "Random Seed Set to 449\n",
      "Episode 349: Finished running.\n",
      "Agent 0, Average Reward: -1527.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 37896.1523\n",
      "Reducing exploration for all agents to 0.0024\n",
      "\n",
      "Episode 350: Starting computation.\n",
      "Random Seed Set to 450\n",
      "Episode 350: Finished running.\n",
      "Agent 0, Average Reward: -1574.62\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41650.8477\n",
      "Reducing exploration for all agents to 0.0023\n",
      "\n",
      "Episode 351: Starting computation.\n",
      "Random Seed Set to 451\n",
      "Episode 351: Finished running.\n",
      "Agent 0, Average Reward: -1440.63\n",
      "Epoch 1/1\n",
      " - 0s - loss: 30493.4688\n",
      "Reducing exploration for all agents to 0.0023\n",
      "\n",
      "Episode 352: Starting computation.\n",
      "Random Seed Set to 452\n",
      "Episode 352: Finished running.\n",
      "Agent 0, Average Reward: -1612.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20104.4336\n",
      "Reducing exploration for all agents to 0.0023\n",
      "\n",
      "Episode 353: Starting computation.\n",
      "Random Seed Set to 453\n",
      "Episode 353: Finished running.\n",
      "Agent 0, Average Reward: -1624.28\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18180.7422\n",
      "Reducing exploration for all agents to 0.0022\n",
      "\n",
      "Episode 354: Starting computation.\n",
      "Random Seed Set to 454\n",
      "Episode 354: Finished running.\n",
      "Agent 0, Average Reward: -1505.41\n",
      "Epoch 1/1\n",
      " - 0s - loss: 29647.6230\n",
      "Reducing exploration for all agents to 0.0022\n",
      "\n",
      "Episode 355: Starting computation.\n",
      "Random Seed Set to 455\n",
      "Episode 355: Finished running.\n",
      "Agent 0, Average Reward: -1216.21\n",
      "Epoch 1/1\n",
      " - 0s - loss: 30722.4082\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 356: Starting computation.\n",
      "Random Seed Set to 456\n",
      "Episode 356: Finished running.\n",
      "Agent 0, Average Reward: -1413.59\n",
      "Epoch 1/1\n",
      " - 0s - loss: 35067.5664\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 357: Starting computation.\n",
      "Random Seed Set to 457\n",
      "Episode 357: Finished running.\n",
      "Agent 0, Average Reward: -1663.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 18791.5117\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 358: Starting computation.\n",
      "Random Seed Set to 458\n",
      "Episode 358: Finished running.\n",
      "Agent 0, Average Reward: -1782.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 41315.0703\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 359: Starting computation.\n",
      "Random Seed Set to 459\n",
      "Episode 359: Finished running.\n",
      "Agent 0, Average Reward: -1737.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 28615.1934\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 360: Starting computation.\n",
      "Random Seed Set to 460\n",
      "Episode 360: Finished running.\n",
      "Agent 0, Average Reward: -1781.39\n",
      "Epoch 1/1\n",
      " - 0s - loss: 21891.6270\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 361: Starting computation.\n",
      "Random Seed Set to 461\n",
      "Episode 361: Finished running.\n",
      "Agent 0, Average Reward: -1620.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 79904.1016\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 362: Starting computation.\n",
      "Random Seed Set to 462\n",
      "Episode 362: Finished running.\n",
      "Agent 0, Average Reward: -1610.81\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27641.7656\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 363: Starting computation.\n",
      "Random Seed Set to 463\n",
      "Episode 363: Finished running.\n",
      "Agent 0, Average Reward: -1053.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 40588.4570\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 364: Starting computation.\n",
      "Random Seed Set to 464\n",
      "Episode 364: Finished running.\n",
      "Agent 0, Average Reward: -1571.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 50406.5312\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 365: Starting computation.\n",
      "Random Seed Set to 465\n",
      "Episode 365: Finished running.\n",
      "Agent 0, Average Reward: -1455.8\n",
      "Epoch 1/1\n",
      " - 0s - loss: 96404.3203\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 366: Starting computation.\n",
      "Random Seed Set to 466\n",
      "Episode 366: Finished running.\n",
      "Agent 0, Average Reward: -1825.57\n",
      "Epoch 1/1\n",
      " - 0s - loss: 74343.6250\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 367: Starting computation.\n",
      "Random Seed Set to 467\n",
      "Episode 367: Finished running.\n",
      "Agent 0, Average Reward: -1909.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 24593.9043\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 368: Starting computation.\n",
      "Random Seed Set to 468\n",
      "Episode 368: Finished running.\n",
      "Agent 0, Average Reward: -2268.95\n",
      "Epoch 1/1\n",
      " - 0s - loss: 36834.0430\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 369: Starting computation.\n",
      "Random Seed Set to 469\n",
      "Episode 369: Finished running.\n",
      "Agent 0, Average Reward: -2211.26\n",
      "Epoch 1/1\n",
      " - 0s - loss: 46823.9023\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 370: Starting computation.\n",
      "Random Seed Set to 470\n",
      "Episode 370: Finished running.\n",
      "Agent 0, Average Reward: -2136.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 38389.8242\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 371: Starting computation.\n",
      "Random Seed Set to 471\n",
      "Episode 371: Finished running.\n",
      "Agent 0, Average Reward: -1839.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 21959.3027\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 372: Starting computation.\n",
      "Random Seed Set to 472\n",
      "Episode 372: Finished running.\n",
      "Agent 0, Average Reward: -1826.08\n",
      "Epoch 1/1\n",
      " - 0s - loss: 42124.9102\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 373: Starting computation.\n",
      "Random Seed Set to 473\n",
      "Episode 373: Finished running.\n",
      "Agent 0, Average Reward: -1875.32\n",
      "Epoch 1/1\n",
      " - 0s - loss: 23037.2441\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 374: Starting computation.\n",
      "Random Seed Set to 474\n",
      "Episode 374: Finished running.\n",
      "Agent 0, Average Reward: -1963.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 24356.0957\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 375: Starting computation.\n",
      "Random Seed Set to 475\n",
      "Episode 375: Finished running.\n",
      "Agent 0, Average Reward: -1907.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 39661.6289\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 376: Starting computation.\n",
      "Random Seed Set to 476\n",
      "Episode 376: Finished running.\n",
      "Agent 0, Average Reward: -2032.11\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33147.9258\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 377: Starting computation.\n",
      "Random Seed Set to 477\n",
      "Episode 377: Finished running.\n",
      "Agent 0, Average Reward: -1664.31\n",
      "Epoch 1/1\n",
      " - 0s - loss: 36224.7734\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 378: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 478\n",
      "Episode 378: Finished running.\n",
      "Agent 0, Average Reward: -1486.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33346.8750\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 379: Starting computation.\n",
      "Random Seed Set to 479\n",
      "Episode 379: Finished running.\n",
      "Agent 0, Average Reward: -1459.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 27317.7148\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 380: Starting computation.\n",
      "Random Seed Set to 480\n",
      "Episode 380: Finished running.\n",
      "Agent 0, Average Reward: -1085.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 37312.3438\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 381: Starting computation.\n",
      "Random Seed Set to 481\n",
      "Episode 381: Finished running.\n",
      "Agent 0, Average Reward: -1669.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 57201.7383\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 382: Starting computation.\n",
      "Random Seed Set to 482\n",
      "Episode 382: Finished running.\n",
      "Agent 0, Average Reward: -1607.65\n",
      "Epoch 1/1\n",
      " - 0s - loss: 71034.1328\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 383: Starting computation.\n",
      "Random Seed Set to 483\n",
      "Episode 383: Finished running.\n",
      "Agent 0, Average Reward: -1747.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 58840.0430\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 384: Starting computation.\n",
      "Random Seed Set to 484\n",
      "Episode 384: Finished running.\n",
      "Agent 0, Average Reward: -1343.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 47995.8281\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 385: Starting computation.\n",
      "Random Seed Set to 485\n",
      "Episode 385: Finished running.\n",
      "Agent 0, Average Reward: -1125.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 57297.3867\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 386: Starting computation.\n",
      "Random Seed Set to 486\n",
      "Episode 386: Finished running.\n",
      "Agent 0, Average Reward: -1266.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 55370.7969\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 387: Starting computation.\n",
      "Random Seed Set to 487\n",
      "Episode 387: Finished running.\n",
      "Agent 0, Average Reward: -1222.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 38064.9102\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 388: Starting computation.\n",
      "Random Seed Set to 488\n",
      "Episode 388: Finished running.\n",
      "Agent 0, Average Reward: -1596.77\n",
      "Epoch 1/1\n",
      " - 0s - loss: 47015.6992\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 389: Starting computation.\n",
      "Random Seed Set to 489\n",
      "Episode 389: Finished running.\n",
      "Agent 0, Average Reward: -1814.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 45152.4102\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 390: Starting computation.\n",
      "Random Seed Set to 490\n",
      "Episode 390: Finished running.\n",
      "Agent 0, Average Reward: -1861.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 72796.9688\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 391: Starting computation.\n",
      "Random Seed Set to 491\n",
      "Episode 391: Finished running.\n",
      "Agent 0, Average Reward: -1798.22\n",
      "Epoch 1/1\n",
      " - 0s - loss: 58039.1211\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 392: Starting computation.\n",
      "Random Seed Set to 492\n",
      "Episode 392: Finished running.\n",
      "Agent 0, Average Reward: -1756.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 59690.9023\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 393: Starting computation.\n",
      "Random Seed Set to 493\n",
      "Episode 393: Finished running.\n",
      "Agent 0, Average Reward: -1454.83\n",
      "Epoch 1/1\n",
      " - 0s - loss: 36313.2031\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 394: Starting computation.\n",
      "Random Seed Set to 494\n",
      "Episode 394: Finished running.\n",
      "Agent 0, Average Reward: -1664.53\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33781.9648\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 395: Starting computation.\n",
      "Random Seed Set to 495\n",
      "Episode 395: Finished running.\n",
      "Agent 0, Average Reward: -1870.43\n",
      "Epoch 1/1\n",
      " - 0s - loss: 31257.7285\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 396: Starting computation.\n",
      "Random Seed Set to 496\n",
      "Episode 396: Finished running.\n",
      "Agent 0, Average Reward: -1755.63\n",
      "Epoch 1/1\n",
      " - 0s - loss: 15092.6699\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 397: Starting computation.\n",
      "Random Seed Set to 497\n",
      "Episode 397: Finished running.\n",
      "Agent 0, Average Reward: -1808.98\n",
      "Epoch 1/1\n",
      " - 0s - loss: 33475.6875\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 398: Starting computation.\n",
      "Random Seed Set to 498\n",
      "Episode 398: Finished running.\n",
      "Agent 0, Average Reward: -1888.39\n",
      "Epoch 1/1\n",
      " - 0s - loss: 58570.6211\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 399: Starting computation.\n",
      "Random Seed Set to 499\n",
      "Episode 399: Finished running.\n",
      "Agent 0, Average Reward: -1807.63\n",
      "Epoch 1/1\n",
      " - 0s - loss: 20752.0137\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 400: Starting computation.\n",
      "Random Seed Set to 500\n",
      "Episode 400: Finished running.\n",
      "Agent 0, Average Reward: -1964.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 51138.3086\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 401: Starting computation.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.train(episodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.save(401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\Single_Cross_Triple\\Agents_Results\\DuelingDDQN\\Single_Cross_Triple8_actions_DuelingDDQN20c10\\BestAgent0.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected keyword argument passed to optimizer: name",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-582111e275ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSingle_Cross_Triple8_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\MasterDQN_Agent.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, episode, best)\u001b[0m\n\u001b[0;32m    402\u001b[0m \t\t\"\"\"\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m                         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_episode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\General_agent.py\u001b[0m in \u001b[0;36mload_agent\u001b[1;34m(self, vissim_working_directory, model_name, agent_type, Session_ID, episode, best)\u001b[0m\n\u001b[0;32m    109\u001b[0m                                 \u001b[0mFilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Agents_Results\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSession_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Episode'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'Agent'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    243\u001b[0m       \u001b[0moptimizer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer_config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       optimizer = optimizers.deserialize(\n\u001b[1;32m--> 245\u001b[1;33m           optimizer_config, custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;31m# Recover loss functions and metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    795\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m       printable_module_name='optimizer')\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 list(custom_objects.items())))\n\u001b[0;32m    174\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m       \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    147\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lr, beta_1, beta_2, epsilon, decay, amsgrad, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m                \u001b[0mamsgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                **kwargs):\n\u001b[1;32m--> 443\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'iterations'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         raise TypeError('Unexpected keyword argument '\n\u001b[1;32m---> 58\u001b[1;33m                         'passed to optimizer: ' + str(k))\n\u001b[0m\u001b[0;32m     59\u001b[0m       \u001b[1;31m# checks that clipnorm >= 0 and clipvalue >= 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected keyword argument passed to optimizer: name"
     ]
    }
   ],
   "source": [
    "#Single_Cross_Triple8_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Episode_Queues, Cumulative_Episode_Delays,Cumulative_Episode_stop_Delays, Cumulative_Totale_network_delay,Cumulative_Totale_network_stop_delay  = Single_Cross_Triple8_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time = [t for t in range(len(Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "# Queues ovzer time for each junction\n",
    "for idx, queues in Episode_Queues.items():\n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queuesdf = pd.DataFrame()\n",
    "    \n",
    "    Queuesdf['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queuesdf[str(i)] = queue\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "        \n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(idx),fontsize=18)\n",
    "    #plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.png\".format(idx))               \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Queues.csv\".format(idx))        \n",
    "   \n",
    "    Queuesdf.to_csv(csv_Path,index=False)\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "# Accumulated delay over time for each junction\n",
    "for idx, delay in Cumulative_Episode_Delays.items():\n",
    "    \n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    delaydf = pd.DataFrame()   \n",
    "    delaydf['Time'] = time\n",
    "    delaydf['Junction {} delay'.format(idx)] = delay\n",
    "    \n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_Delay.csv\".format(idx))   \n",
    "    plt.savefig(Path)\n",
    "\n",
    "    delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "    \n",
    "# Accumulated stop delay over time for each junction\n",
    "for idx, stop_delay in Cumulative_Episode_stop_Delays.items():\n",
    "    \n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    stop_delaydf = pd.DataFrame()   \n",
    "    stop_delaydf['Time'] = time\n",
    "    stop_delaydf['Junction {} stop delay'.format(idx)] = stop_delay\n",
    "\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                            \"Junction{}_Cumulative_stop_Delay.png\".format(idx))\n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Junction{}_Cumulative_stop_Delay.csv\".format(idx))\n",
    "    plt.savefig(Path)\n",
    "    stop_delaydf.to_csv(csv_Path,index=False)\n",
    "    \n",
    "# Global Accumulated delay over time\n",
    "plt.figure('4',figsize=(16,9))\n",
    "\n",
    "\n",
    "Global_delaydf = pd.DataFrame()   \n",
    "Global_delaydf['Time'] = time\n",
    "Global_delaydf['Global accumulated Delay'] = Cumulative_Totale_network_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "plt.title('Global accumulated Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "# Global Accumulated stop delay over time\n",
    "plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "\n",
    "Global_stop_delaydf = pd.DataFrame()   \n",
    "Global_stop_delaydf['Time'] = time\n",
    "Global_stop_delaydf['Global accumulated stop Delay'] = Cumulative_Totale_network_stop_delay\n",
    "\n",
    "plt.plot(Cumulative_Totale_network_stop_delay)\n",
    "plt.xlabel('Time [s]',fontsize=18)\n",
    "plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "plt.gca().legend('Global accumulated stop Delay')\n",
    "\n",
    "Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.png\")\n",
    "csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Total_Cumulative_stop_Delay.csv\")\n",
    "plt.savefig(Path)\n",
    "Global_stop_delaydf.to_csv(csv_Path,index=False)\n",
    "\n",
    "\n",
    "# For the agent training\n",
    "\n",
    "for idx , agent in Single_Cross_Triple8_MultiDQN_Agents.Agents.items():  \n",
    "    plt.figure('6'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Loss_rewarddf = pd.DataFrame()   \n",
    "    \n",
    "    plt.plot(agent.loss)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent{} Loss over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent Loss over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_Loss.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    csv_Path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                           \"Agent{}_Loss_average_reward.csv\".format(idx)) \n",
    "    \n",
    "    \n",
    "    plt.figure('7'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent{} average reward over training'.format(idx),fontsize=18)\n",
    "    plt.gca().legend('Agent reward over training')\n",
    "    \n",
    "    Path  = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", Session_ID,\\\n",
    "                        \"Agent{}_average_reward.png\".format(idx)) \n",
    "    plt.savefig(Path)\n",
    "    \n",
    "    \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    \n",
    "    Loss_rewarddf['episode'] = episode \n",
    "    Loss_rewarddf['Agent{} loss'.format(idx)] = agent.loss\n",
    "    Loss_rewarddf['Agent{} Average_Reward'.format(idx)] = agent.reward_storage\n",
    "    \n",
    "    Loss_rewarddf.to_csv(csv_Path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 500\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.6 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Triple8_MultiDQN_Agents.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five intersection DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Five_intersection'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\nwalton\\\\Dropbox (The University of Manchester)\\\\ACTIVE\\\\TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "timesteps_per_second = 1\n",
    "\n",
    "agent_type = \"DuelingDDQN\"\n",
    "actions = \"all_actions\"\n",
    "Session_ID = \"TEST\"\n",
    "\n",
    "# all controller actions\n",
    "Five_intersection_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "   'junctions' : {0 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['11-1', '11-2', '11-3', '12-1', '12-2', '12-3', '13-1', '13-2', '13-3', '14-1', '14-2', '14-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "         },\n",
    "                  1 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['21-1', '21-2', '21-3', '22-1', '22-2', '22-3', '23-1', '23-2', '23-3', '24-1', '24-2', '24-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "        'queues_counter_ID' : [13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "         },\n",
    "                  2 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['31-1', '31-2', '31-3', '32-1', '32-2', '32-3', '33-1', '33-2', '33-3', '34-1', '34-2', '34-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [25,26,27,28,29,30,31,32,33,34,35,36]\n",
    "         },\n",
    "                  3 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['41-1', '41-2', '41-3', '42-1', '42-2', '42-3', '43-1', '43-2', '43-3', '44-1', '44-2', '44-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "          'queues_counter_ID' : [37,38,39,40,41,42,43,44,45,46,47,48]\n",
    "         },\n",
    "                  4 : {'default_actions' :  {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    "         \n",
    "         'all_actions' :        {            0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                             1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                             2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                             3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                             4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                             5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                             6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                             7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [],\n",
    "         'lane' : ['51-1', '51-2', '51-3', '52-1', '52-2', '52-3', '53-1', '53-2', '53-3', '54-1', '54-2', '54-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'agent_type' : agent_type,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' ,\n",
    "         'queues_counter_ID' : [49,50,51,52,53,54,55,56,57,58,59,60]\n",
    "         }\n",
    "    },\n",
    "   'demand' : { 'default' : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             \n",
    "             0 : [200,200,200,200,200,200,200,200,200,200,200,200],\n",
    "             1 : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             2 : [500,900,500,500,900,500,500,900,500,500,900,500],\n",
    "             3 : [500,1000,500,500,1000,500,500,1000,500,500,1000,500],\n",
    "             4 : [500,700,500,500,700,500,500,700,500,500,700,500],\n",
    "             5 : [500,700,500,500,700,500,500,700,500,500,700,500],\n",
    "             6 : [500,1000,500,500,1000,500,500,1000,500,500,1000,500],\n",
    "             7 : [500,900,500,500,900,500,500,900,500,500,900,500],\n",
    "             8 : [400,400,400,400,400,400,400,400,400,400,400,400],\n",
    "             9 : [200,200,200,200,200,200,200,200,200,200,200,200]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN Hyperaramenters\n",
    "episodes = 5\n",
    "copy_weights_frequency = 20 # On a successfull run I copied the weight every 50\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 5000\n",
    "batch_size = 128\n",
    "batches_per_episode = 5\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"linear\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 0: SETTING UP AGENT\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_208 (Dense)               (None, 24)           336         input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_209 (Dense)               (None, 24)           600         dense_208[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_212 (Dense)               (None, 24)           600         dense_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_210 (Dense)               (None, 24)           600         dense_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_213 (Dense)               (None, 1)            25          dense_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_211 (Dense)               (None, 8)            200         dense_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_213[0][0]                  \n",
      "                                                                 dense_211[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,361\n",
      "Trainable params: 2,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Double Deep Q Learning Agent(s) at Intersection 0\n",
      "\n",
      "INTERSECTION 1: SETTING UP AGENT\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_47 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_220 (Dense)               (None, 24)           336         input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_221 (Dense)               (None, 24)           600         dense_220[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_224 (Dense)               (None, 24)           600         dense_221[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_222 (Dense)               (None, 24)           600         dense_221[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_225 (Dense)               (None, 1)            25          dense_224[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_223 (Dense)               (None, 8)            200         dense_222[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_225[0][0]                  \n",
      "                                                                 dense_223[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,361\n",
      "Trainable params: 2,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Double Deep Q Learning Agent(s) at Intersection 1\n",
      "\n",
      "INTERSECTION 2: SETTING UP AGENT\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_49 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_232 (Dense)               (None, 24)           336         input_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_233 (Dense)               (None, 24)           600         dense_232[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_236 (Dense)               (None, 24)           600         dense_233[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_234 (Dense)               (None, 24)           600         dense_233[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_237 (Dense)               (None, 1)            25          dense_236[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_235 (Dense)               (None, 8)            200         dense_234[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_237[0][0]                  \n",
      "                                                                 dense_235[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,361\n",
      "Trainable params: 2,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Double Deep Q Learning Agent(s) at Intersection 2\n",
      "\n",
      "INTERSECTION 3: SETTING UP AGENT\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_51 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_244 (Dense)               (None, 24)           336         input_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_245 (Dense)               (None, 24)           600         dense_244[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_248 (Dense)               (None, 24)           600         dense_245[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_246 (Dense)               (None, 24)           600         dense_245[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_249 (Dense)               (None, 1)            25          dense_248[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_247 (Dense)               (None, 8)            200         dense_246[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_249[0][0]                  \n",
      "                                                                 dense_247[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,361\n",
      "Trainable params: 2,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Double Deep Q Learning Agent(s) at Intersection 3\n",
      "\n",
      "INTERSECTION 4: SETTING UP AGENT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_53 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_256 (Dense)               (None, 24)           336         input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_257 (Dense)               (None, 24)           600         dense_256[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_260 (Dense)               (None, 24)           600         dense_257[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_258 (Dense)               (None, 24)           600         dense_257[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_261 (Dense)               (None, 1)            25          dense_260[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_259 (Dense)               (None, 8)            200         dense_258[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 8)            0           dense_261[0][0]                  \n",
      "                                                                 dense_259[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,361\n",
      "Trainable params: 2,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Double Deep Q Learning Agent(s) at Intersection 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Five_intersection_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Five_intersection_dictionary, actions,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, batches_per_episode, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed, timesteps_per_second, Session_ID, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Experience: Found. Loading into agents\n",
      "Previous Experience: Successfully loaded file from:\n",
      "C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\Five_intersection\\Agents_Results\\DuelingDDQN\\TEST\\Agent0_PERPre_5000.p\n",
      "Previous Experience: Successfully loaded file from:\n",
      "C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\Five_intersection\\Agents_Results\\DuelingDDQN\\TEST\\Agent1_PERPre_5000.p\n",
      "Previous Experience: Successfully loaded file from:\n",
      "C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\Five_intersection\\Agents_Results\\DuelingDDQN\\TEST\\Agent2_PERPre_5000.p\n",
      "Previous Experience: Successfully loaded file from:\n",
      "C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\Five_intersection\\Agents_Results\\DuelingDDQN\\TEST\\Agent3_PERPre_5000.p\n",
      "Previous Experience: Successfully loaded file from:\n",
      "C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\Five_intersection\\Agents_Results\\DuelingDDQN\\TEST\\Agent4_PERPre_5000.p\n"
     ]
    }
   ],
   "source": [
    "Five_intersection_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.save(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MasterAgent = Five_intersection_MultiDQN_Agents\n",
    "\n",
    "## AGENT TRAINING RESULTS\n",
    "# Path to results folder\n",
    "results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "\n",
    "# Loop over each agent\n",
    "for idx , agent in MasterAgent.Agents.items():\n",
    "    intersection_number_in_vissim = MasterAgent.Agents[0].signal_id + 1\n",
    "    print(\"Intersection \"+str(intersection_number_in_vissim))\n",
    "    \n",
    "    ## SAVE TRAINING DATA TO JSON.\n",
    "    json_filename = \"Agent{}_Loss_average_reward.json\".format(intersection_number_in_vissim)\n",
    "    Loss_reward = dict()   \n",
    "    # Loss dictionary\n",
    "    for epoch, loss in enumerate(agent.loss):\n",
    "        loss_dict = { epoch : loss }\n",
    "    Loss_reward['Agent{} loss'.format(intersection_number_in_vissim)] = loss_dict\n",
    "    # Reward dictionary            \n",
    "    episode = [i for i in range(len(agent.reward_storage))]\n",
    "    Loss_reward['Agent{} Average_Reward'.format(intersection_number_in_vissim)] = agent.reward_storage\n",
    "    # Store as JSON\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Loss_reward, f)\n",
    "    print(\"Agent {}: Training Loss and Average Reward during training successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "    \n",
    "    ## LOADING DATA FROM JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Agent{}_Loss_average_reward.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "    \n",
    "    ## TRAINING PLOTS\n",
    "    loss_plot_filename  = \"Agent{}_Loss.png\".format(intersection_number_in_vissim)\n",
    "    reward_plot_filename  = \"Agent{}_average_reward.png\".format(intersection_number_in_vissim) \n",
    "    \n",
    "    ## Loss Plot\n",
    "    plt.figure('LossAgent'+str(idx),figsize=(16,9))\n",
    "    plt.plot(agent.loss)\n",
    "    #plt.yscale('log')\n",
    "\n",
    "    plt.xlabel('Training Epoch',fontsize=18)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.title('Agent {} Loss over training'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.savefig(results_path + \"/\" + loss_plot_filename)\n",
    "\n",
    "    ## Average Reward Plot\n",
    "    plt.figure('RewardAgent'+str(idx),figsize=(16,9))\n",
    "    plt.plot(agent.reward_storage)\n",
    "    plt.xlabel('Training Episode',fontsize=18)\n",
    "    plt.ylabel('Average reward',fontsize=18)\n",
    "    plt.title('Agent {} average reward over training'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.savefig(results_path + \"/\" + reward_plot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "C:\\Users\\nwalton\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\Five_intersection\\Agents_Results\\DuelingDDQN\\TEST\\BestAgent0.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected keyword argument passed to optimizer: name",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-6dc59efa1994>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mFive_intersection_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\MasterDQN_Agent.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, episode, best)\u001b[0m\n\u001b[0;32m    402\u001b[0m \t\t\"\"\"\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m                         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_episode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox (The University of Manchester)\\ACTIVE\\TMF\\MLforFlowOptimisation\\Vissim\\General_agent.py\u001b[0m in \u001b[0;36mload_agent\u001b[1;34m(self, vissim_working_directory, model_name, agent_type, Session_ID, episode, best)\u001b[0m\n\u001b[0;32m    109\u001b[0m                                 \u001b[0mFilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Agents_Results\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSession_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Episode'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'Agent'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    243\u001b[0m       \u001b[0moptimizer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer_config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       optimizer = optimizers.deserialize(\n\u001b[1;32m--> 245\u001b[1;33m           optimizer_config, custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;31m# Recover loss functions and metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    795\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m       printable_module_name='optimizer')\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 list(custom_objects.items())))\n\u001b[0;32m    174\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m       \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    147\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lr, beta_1, beta_2, epsilon, decay, amsgrad, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m                \u001b[0mamsgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                **kwargs):\n\u001b[1;32m--> 443\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'iterations'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         raise TypeError('Unexpected keyword argument '\n\u001b[1;32m---> 58\u001b[1;33m                         'passed to optimizer: ' + str(k))\n\u001b[0m\u001b[0;32m     59\u001b[0m       \u001b[1;31m# checks that clipnorm >= 0 and clipvalue >= 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected keyword argument passed to optimizer: name"
     ]
    }
   ],
   "source": [
    "Five_intersection_MultiDQN_Agents.load(400,best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to load the same agent 5 times for transfer learning approaches\n",
    "for i in range(5):\n",
    "    Five_intersection_MultiDQN_Agents.Agents[i].load_agent(vissim_working_directory, 'Single_Cross_Triple', \"DuelingDDQN\",'Single_Cross_Triple8_actions',400 , best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MasterAgent = Five_intersection_MultiDQN_Agents\n",
    "\n",
    "\n",
    "results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "time = [t for t in range(len(MasterAgent.Cumulative_Totale_network_stop_delay))]\n",
    "\n",
    "########################################\n",
    "## Queues over time for each junction ##\n",
    "########################################\n",
    "for idx, queues in MasterAgent.Episode_Queues.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = MasterAgent.Agents[0].signal_id + 1\n",
    "    \n",
    "    queues = np.array(queues)\n",
    "    queues = queues.T\n",
    "    number_queues = np.size(queues,0)\n",
    "    \n",
    "    plt.figure('1'+str(idx),figsize=(16, 9))\n",
    "    \n",
    "    Queues = dict()\n",
    "    Queues['Time'] = time\n",
    "    Queues_legend = []\n",
    "    \n",
    "    for i, queue in enumerate(queues):\n",
    "        plt.plot(queue, label = \"Queue\"+str(i))\n",
    "        Queues[str(i)] = queue.tolist()\n",
    "        Queues_legend.append(\"Queue\"+str(i))\n",
    "    \n",
    "    ## Plot the queues\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Queue Length',fontsize=18)\n",
    "    plt.title('Junction {} Queue length'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend(Queues_legend)\n",
    "    \n",
    "    filename = \"Junction{}_Queues.png\".format(intersection_number_in_vissim)           \n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Queues.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Queues, f)\n",
    "        \n",
    "    ### LOADING DATA FROM JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #json_filename = \"Junction{}_Queues.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "        \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Queues during Test successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "       \n",
    "        \n",
    "###################################################        \n",
    "## Accumulated delay over time for each junction ##\n",
    "###################################################\n",
    "for idx, delay in MasterAgent.Cumulative_Episode_Delays.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = MasterAgent.Agents[idx].signal_id + 1\n",
    "\n",
    "    # Extract and process delay data\n",
    "    Delay = dict()   \n",
    "    Delay['Time'] = time\n",
    "    Delay['Junction {} delay'.format(intersection_number_in_vissim)] = delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Cumulative_Delay.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Delay, f)\n",
    "        \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Delay successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "    \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Junction{}_Cumulative_Delay.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    # Plot the cumulative delay\n",
    "    plt.figure('2'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Delay',fontsize=18)\n",
    "    plt.title('Junction {} Delay'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated delay')\n",
    "    \n",
    "    filename = \"Junction{}_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "    \n",
    "    \n",
    "    \n",
    "########################################################    \n",
    "## Accumulated stop delay over time for each junction ##\n",
    "########################################################\n",
    "for idx, stop_delay in MasterAgent.Cumulative_Episode_stop_Delays.items():\n",
    "    # Identify Junction ID in map\n",
    "    intersection_number_in_vissim = MasterAgent.Agents[idx].signal_id + 1    \n",
    "    \n",
    "    # Extract and process stop delay data\n",
    "    Stop_delay = dict()   \n",
    "    Stop_delay['Time'] = time\n",
    "    Stop_delay['Junction {} stop delay'.format(intersection_number_in_vissim)] = stop_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Junction{}_Cumulative_Stop_Delay.json\".format(intersection_number_in_vissim)        \n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Stop_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Stop Delay successfuly saved to file:\".format(intersection_number_in_vissim))\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Junction{}_Cumulative_Stop_Delay.json\".format(intersection_number_in_vissim)\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "    \n",
    "    # Plot the cumulative stop delay\n",
    "    plt.figure('3'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Accumulated Stop Delay',fontsize=18)\n",
    "    plt.title('Junction {} Stop Delay'.format(intersection_number_in_vissim),fontsize=18)\n",
    "    plt.gca().legend('Junction accumulated Stop delay')\n",
    "    \n",
    "    filename = \"Junction{}_Cumulative_Stop_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n",
    "    \n",
    "    \n",
    "###############################################\n",
    "## ONLY IF THERE IS MORE THAN ONE CONTROLLER ##\n",
    "##    These are the global network plots     ##\n",
    "###############################################\n",
    "\n",
    "if len(MasterAgent.Agents) > 1:\n",
    "    ########################################    \n",
    "    ## Global Accumulated delay over time ##\n",
    "    ########################################\n",
    "    \n",
    "    # Process global delay data\n",
    "    Global_delay = dict()   \n",
    "    Global_delay['Time'] = time\n",
    "    Global_delay['Global accumulated Delay'] = MasterAgent.Cumulative_Totale_network_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Global_Cumulative_Delay.json\"\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Global_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Global Delay successfuly saved to file:\")\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Global_Cumulative_Delay.json\"\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    \n",
    "    # Plot the global delay\n",
    "    plt.figure('4',figsize=(16,9))\n",
    "    plt.plot(MasterAgent.Cumulative_Totale_network_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Global accumulated Delay',fontsize=18)\n",
    "    plt.title('Global accumulated Delay',fontsize=18)\n",
    "    plt.gca().legend('Global accumulated delay')\n",
    "\n",
    "    filename = \"Global_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    ## Global Accumulated stop delay over time ##\n",
    "    #############################################\n",
    "    \n",
    "    # Process global stop delay data\n",
    "    Global_stop_delay = dict()   \n",
    "    Global_stop_delay['Time'] = time\n",
    "    Global_stop_delay['Global accumulated stop Delay'] = MasterAgent.Cumulative_Totale_network_stop_delay\n",
    "    \n",
    "    # Store as JSON\n",
    "    json_filename = \"Global_Cumulative_Stop_Delay.json\"\n",
    "    with open(results_path + \"/\" + json_filename, 'w') as f:\n",
    "        json.dump(Global_stop_delay, f)\n",
    "    \n",
    "    # Success Message\n",
    "    print(\"Agent {}: Test Cumulative Global Stop Delay successfuly saved to file:\")\n",
    "    print(results_path + \"/\" + json_filename)\n",
    "        \n",
    "    ### Loading data from JSON\n",
    "    #results_path = os.path.join(vissim_working_directory, model_name, \"Agents_Results\", agent_type, Session_ID)\n",
    "    #dictionary_filename = \"Global_Cumulative_Stop_Delay.json\"\n",
    "    #with open(results_path + \"/\" + json_filename, 'r') as fp:\n",
    "    #    data = json.load(fp)\n",
    "    #print(data)\n",
    "\n",
    "    # Plot the global stop delay\n",
    "    plt.figure('5'+str(idx),figsize=(16, 9))\n",
    "    plt.plot(MasterAgent.Cumulative_Totale_network_stop_delay)\n",
    "    plt.xlabel('Time [s]',fontsize=18)\n",
    "    plt.ylabel('Global accumulated stop Delay',fontsize=18)\n",
    "    plt.title('Global accumulated stop Delay',fontsize=18)\n",
    "    plt.gca().legend('Global accumulated stop Delay')\n",
    "    \n",
    "    filename = \"Global_Cumulative_Delay.png\".format(intersection_number_in_vissim)\n",
    "    plt.savefig(results_path + \"/\" + filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.Agents[2] = Five_intersection_MultiDQN_Agents.Agents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Five_intersection_MultiDQN_Agents.demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
