{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load Vissim\n",
    "# Documents here\n",
    "# C:\\Users\\Public\\Documents\\PTV Vision\\PTV Vissim 11\\Examples Training\n",
    "# Signal Control files here\n",
    "# C:\\Program Files\\PTV Vision\\PTV Vissim 11\\API\\SignalControl_DLLs\n",
    "\n",
    "# for loading client\n",
    "import win32com.client as com\n",
    "import os\n",
    "# standard libraries \n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from IPython import display\n",
    "# For Q-function\n",
    "from collections import defaultdict\n",
    "# For saving\n",
    "import datetime\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "'''\n",
    "This loads up a Vissim instance \n",
    "    -- Required: inpx file , layx file\n",
    "    -- warning: quite flakey loading but once loaded okay\n",
    "'''\n",
    "def Load_Vissim(End_of_simulation = 10000,\\\n",
    "                Quick_Mode=1,\\\n",
    "                Path_to_network = 'C:\\\\Users\\\\Public\\\\Documents\\\\PTV Vision\\\\PTV Vissim 11\\\\Examples Training\\\\COM\\\\Basic Commands\\\\',\\\n",
    "                inpx_Filename = 'COM Basic Commands.inpx',\\\n",
    "                layx_Filename = 'COM Basic Commands.layx'\\\n",
    "               ):\n",
    "    Vissim = None\n",
    "    # Load Vissim\n",
    "    Vissim = com.gencache.EnsureDispatch(\"Vissim.Vissim\")\n",
    "    # Load file\n",
    "    inpx_Filename                = os.path.join(Path_to_network, inpx_Filename)\n",
    "    flag_read_additionally  = False # you can read network(elements) additionally, in this case set \"flag_read_additionally\" to true\n",
    "    Vissim.LoadNet(inpx_Filename, flag_read_additionally)\n",
    "    # Load a Layout:\n",
    "    layx_Filename = os.path.join(Path_to_network, layx_Filename)\n",
    "    Vissim.LoadLayout(layx_Filename)\n",
    "    # Configure non-GUI for training\n",
    "    Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)\n",
    "    Vissim.Simulation.AttValue('UseAllCores')\n",
    "    Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",Quick_Mode)\n",
    "    # Set a long simulation time\n",
    "    Vissim.Simulation.SetAttValue('SimPeriod', End_of_simulation)\n",
    "    return Vissim\n",
    "\n",
    "# '''\n",
    "# Set up Vissim and the parameters for optimizing\n",
    "# (Worth commenting out after set up)\n",
    "# if ERROR like ''has no attribute 'CLSIDToClassMap' ''\n",
    "# DELETE folders:\n",
    "# C:\\Users\\nwalton\\AppData\\Local\\Temp\\gen_py\n",
    "# C:\\Users\\nwalton\\AppData\\Local\\Temp\\VISSIM\n",
    "# Then Restart PC...\n",
    "# '''\n",
    "\n",
    "# We attempt 5 times to load vissim\n",
    "\n",
    "# Attempts = 5 \n",
    "# for _ in range(Attempts):\n",
    "#     try:\n",
    "#         Vissim \\\n",
    "#         = \\\n",
    "#         Load_Vissim(\n",
    "#         Path_to_network = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\Single_Cross_Straight',\\\n",
    "#         inpx_Filename = 'Single_Cross_Straight.inpx',\\\n",
    "#         layx_Filename = 'Single_Cross_Straight.layx'\\\n",
    "#         )\n",
    "#         print(\"Success\")\n",
    "#         break\n",
    "#     except:\n",
    "#         print(\"Fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get simulation parameters\n",
    "'''\n",
    "Set up the parameters of the optimization\n",
    "'''\n",
    "\n",
    "'''\n",
    "makes a dictionary for all signals and thier positions\n",
    "'''\n",
    "def Get_Signal_Positions(Signal_Groups):\n",
    "    Signal_Positions = dict()\n",
    "    for SG in Signal_Groups:\n",
    "        for SH in SG.SigHeads:\n",
    "            Lane = SH.AttValue('Lane')\n",
    "            Position = SH.AttValue('Pos')\n",
    "            Signal_Positions[Lane] = Position\n",
    "    return Signal_Positions\n",
    "\n",
    "# Lanes for detection\n",
    "Lane_List = ['3-1','1-1','7-1','5-1']\n",
    "\n",
    "# which lane signals can be green at the same time\n",
    "actions = [(1,0,1,0),\\\n",
    "            (0,1,0,1)]\n",
    "# Define the Q-function\n",
    "# Q_fn = Q_function(actions)\n",
    "\n",
    "# round the state space\n",
    "rounding = 1.\n",
    "sim_steps = 100 # number of simulation steps before update\n",
    "# set the load to be light\n",
    "number_of_inputs = len(Vissim.Net.VehicleInputs)\n",
    "new_volume = 400\n",
    "for key in range(1,number_of_inputs+1):\n",
    "    Vissim.Net.VehicleInputs.ItemByKey(key).SetAttValue('Volume(1)', new_volume)\n",
    "    \n",
    "# get the list of signal controllers\n",
    "Signal_Controller = Vissim.Net.SignalControllers.GetAll()[0]\n",
    "Signal_Groups = Signal_Controller.SGs.GetAll()\n",
    "Signal_Positions = Get_Signal_Positions(Signal_Groups)\n",
    "\n",
    "# These are states and rewards which are global variables \n",
    "# Assigned None for now\n",
    "\n",
    "Q_Size = None # Queue sizes at junctions\n",
    "delays = dict() # Total delay and change in delay for each vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get state and reward info\n",
    "'''\n",
    "This code gets the average delay from 1000 steps under MaxWeight\n",
    "'''\n",
    "def Get_Q_Size(Lane_List=None, rounding=None):\n",
    "    # Loads globals if variables not specfied\n",
    "    if Lane_List is None :\n",
    "        Lane_List = globals()['Lane_List']\n",
    "    if rounding is None :\n",
    "        rounding = globals()['rounding']\n",
    "        \n",
    "    # initialize with zero queues\n",
    "    Qsum = 0\n",
    "    Q_sizes = dict.fromkeys(Lane_List)\n",
    "    for key in Q_sizes.keys():\n",
    "        Q_sizes[key]=0\n",
    "\n",
    "    # initialize with zero numbers of non-waiting cars\n",
    "    nonQsum = 0\n",
    "    nonQ_sizes = dict.fromkeys(Lane_List)\n",
    "    for key in nonQ_sizes.keys():\n",
    "        nonQ_sizes[key]=0\n",
    "\n",
    "    # get all Q lengths    \n",
    "    All_Vehicles = Vissim.Net.Vehicles.GetAll() \n",
    "    for Veh in All_Vehicles:\n",
    "        lane = Veh.AttValue('Lane')\n",
    "        if lane in Lane_List : \n",
    "            if Veh.AttValue('InQueue') == 1 :\n",
    "                Q_sizes[lane] += 1\n",
    "            else : \n",
    "                nonQ_sizes[lane] += 1\n",
    "\n",
    "    state = []\n",
    "\n",
    "    for lane in Lane_List :\n",
    "        state.append(math.ceil(Q_sizes[lane] / rounding))\n",
    "        \n",
    "    return tuple(state)\n",
    "\n",
    "'''\n",
    "state is now the closest vehicle to the junction\n",
    "reward is now the total delay\n",
    "'''\n",
    "\n",
    "\n",
    "def Get_First_Vehicle(Lane_List=None, rounding=None):\n",
    "    # Loads globals if variables not specfied\n",
    "    if Lane_List is None :\n",
    "        Lane_List = globals()['Lane_List']\n",
    "    if rounding is None :\n",
    "        rounding = globals()['rounding']\n",
    "    \n",
    "    All_Vehicles = Vissim.Net.Vehicles.GetAll()\n",
    "    \n",
    "    lane_state = dict()\n",
    "\n",
    "    for cnt_Veh in range(len(All_Vehicles)):\n",
    "        veh_position = All_Vehicles[cnt_Veh].AttValue('Pos')\n",
    "        veh_lane = All_Vehicles[cnt_Veh].AttValue('Lane')\n",
    "\n",
    "        if veh_lane in Signal_Positions.keys():\n",
    "\n",
    "            rel_position = rounding * math.ceil((Signal_Positions[veh_lane] - veh_position) / rounding) \n",
    "\n",
    "            if  rel_position >= 0 :\n",
    "\n",
    "                if veh_lane in lane_state.keys():    \n",
    "                    if rel_position < lane_state[veh_lane]:\n",
    "                        lane_state[veh_lane] = rel_position\n",
    "                else :\n",
    "                    lane_state[veh_lane] = rel_position\n",
    "    \n",
    "    state = []\n",
    "    for lane in Lane_List:    \n",
    "        if lane in lane_state.keys():\n",
    "            state.append(lane_state[lane])\n",
    "        else:\n",
    "            state.append(np.nan)\n",
    "    return tuple(state)\n",
    "\n",
    "\n",
    "'''\n",
    "Gets the delays of all vehicles in the network:\n",
    "    -- dictionary keys are vehicle numbers\n",
    "    -- 1st entry is delay\n",
    "    -- 2nd entry is change in delay\n",
    "'''\n",
    "state = None\n",
    "\n",
    "def Delay_Dictionary(Current_Dict=None):\n",
    "    # make sure current state is defined\n",
    "    if Current_Dict is None:\n",
    "        try :\n",
    "            Current_Dict = globals()['delays']\n",
    "        except NameError:\n",
    "            Current_Dict = dict()\n",
    "\n",
    "    Delay_Dict= dict()\n",
    "    All_Vehicles = Vissim.Net.Vehicles.GetAll() # get all vehicles in the network at the actual simulation second\n",
    "    for cnt_Veh in range(len(All_Vehicles)):\n",
    "        veh_number      = All_Vehicles[cnt_Veh].AttValue('No')\n",
    "        delay           = All_Vehicles[cnt_Veh].AttValue('DelayTm')  \n",
    "\n",
    "        if veh_number in Current_Dict.keys():\n",
    "            old_delay = Current_Dict[veh_number][0]\n",
    "            Delay_Dict[veh_number] = [delay,delay-old_delay]\n",
    "        else :\n",
    "            Delay_Dict[veh_number] = [delay,0.]\n",
    "    return Delay_Dict\n",
    "\n",
    "'''\n",
    "state is now the closest vehicle to the junction\n",
    "reward is now the total delay\n",
    "'''\n",
    "\n",
    "def Get_Delay(delays=None):\n",
    "    # Use global as default\n",
    "    if delays is None:\n",
    "        delays = globals()['delays']\n",
    "        \n",
    "    total_delay = 0\n",
    "    for key, val in delays.items():\n",
    "        total_delay += val[1]\n",
    "    return -total_delay\n",
    "\n",
    "def Get_Total_Queue(Q_Size=None):\n",
    "    # Use global as default\n",
    "    if Q_Size is None:\n",
    "        Q_Size = Get_Q_Size()\n",
    "        \n",
    "    return -sum(Q_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Actions\n",
    "\n",
    "# Simple RED and GREEN Actions\n",
    "def Do_Action_Easy(action=None,Signal_Groups=None):\n",
    "    # Set global as default\n",
    "    if action is None:\n",
    "        action = globals()['action']\n",
    "    if Signal_Groups is None:\n",
    "        Signal_Groups = globals()['Signal_Groups']\n",
    "    \n",
    "    for i, sg in enumerate(Signal_Groups):\n",
    "        if action[i] == 1:\n",
    "            new_state = \"GREEN\"\n",
    "        else :\n",
    "            new_state = \"RED\"\n",
    "        sg.SetAttValue(\"SigState\", new_state)\n",
    "\n",
    "# GREEN/AMBER/RED/REDAMBER Actions\n",
    "def Do_Action_RGA(action=None,Signal_Groups=None):\n",
    "# Consist of 4 steps: \n",
    "# Greens go Amber\n",
    "# Ambers go Red\n",
    "# Reds go RedAmber\n",
    "# RedAmbers go Green\n",
    "\n",
    "    # Set global as default\n",
    "    if action is None:\n",
    "        action = globals()['action']\n",
    "    if Signal_Groups is None:\n",
    "        Signal_Groups = globals()['Signal_Groups']\n",
    "\n",
    "    # Initial Parameters\n",
    "    Sim_Period = Vissim.Simulation.AttValue('SimPeriod') #End of Simulation\n",
    "    Amber_Time = 4. #One second of Amber\n",
    "    Red_Time = 1.\n",
    "    RedAmber_Time = 1.\n",
    "\n",
    "    # If current_state = 'GREEN' and next_state = 'RED'\n",
    "    # Then go AMBER\n",
    "    for i, sg in enumerate(Signal_Groups):\n",
    "        current_state = sg.AttValue(\"SigState\")\n",
    "        if current_state == \"GREEN\" and action[i] == 0 :\n",
    "            sg.SetAttValue(\"SigState\", \"AMBER\")\n",
    "\n",
    "    # Simulate 4 seconds for Amber\n",
    "    Sim_Time = Vissim.Simulation.AttValue('SimSec')\n",
    "    Amber_Break = min(Sim_Time+Amber_Time,Sim_Period)\n",
    "    Vissim.Simulation.SetAttValue('SimBreakAt', Amber_Break)\n",
    "    Vissim.Simulation.RunContinuous()\n",
    "\n",
    "    # Set the AMBER lights red\n",
    "    for i, sg in enumerate(Signal_Groups):\n",
    "        current_state = sg.AttValue(\"SigState\")\n",
    "        if current_state == \"AMBER\":\n",
    "            sg.SetAttValue(\"SigState\", \"RED\")\n",
    "\n",
    "    # Simulate 1 second for Red\n",
    "    Sim_Time = Vissim.Simulation.AttValue('SimSec')\n",
    "    Red_Break = min(Sim_Time+Red_Time,Sim_Period)\n",
    "    Vissim.Simulation.SetAttValue('SimBreakAt', Red_Break)\n",
    "    Vissim.Simulation.RunContinuous()\n",
    "\n",
    "    # If current state \"RED\" and next_state = \"GREEN\"\n",
    "    # Then go RedAmber\n",
    "    for i, sg in enumerate(Signal_Groups):\n",
    "        current_state = sg.AttValue(\"SigState\")\n",
    "        if current_state == \"RED\" and action[i] == 1 :\n",
    "            sg.SetAttValue(\"SigState\", \"REDAMBER\")\n",
    "\n",
    "    # Simulate 1 second for RedAmber\n",
    "    Sim_Time = Vissim.Simulation.AttValue('SimSec')\n",
    "    RedAmber_Break = min(Sim_Time+RedAmber_Time,Sim_Period)\n",
    "    Vissim.Simulation.SetAttValue('SimBreakAt', RedAmber_Break)\n",
    "    Vissim.Simulation.RunContinuous()\n",
    "    \n",
    "    # Finally set all RedAmbers to Green\n",
    "    for i, sg in enumerate(Signal_Groups):\n",
    "        current_state = sg.AttValue(\"SigState\")\n",
    "        if current_state == \"REDAMBER\":\n",
    "            sg.SetAttValue(\"SigState\", \"GREEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Controllers / Learners\n",
    "'''\n",
    "MaxWeight\n",
    "'''\n",
    "def MaxWeight(state,actions):\n",
    "    opt_val = 0\n",
    "    for action in actions : \n",
    "        val = np.dot(action,state)\n",
    "        if val >= opt_val :\n",
    "            opt_val = val\n",
    "            opt_act = action\n",
    "    return opt_act\n",
    "\n",
    "'''\n",
    "Easy Q_learner Q_Function\n",
    "'''\n",
    "class Q_function():\n",
    "    def __init__(self, actions = actions):\n",
    "        # Q function\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        # number of visits\n",
    "        self.N = defaultdict(lambda: defaultdict(float))\n",
    "        self.actions = actions\n",
    "\n",
    "    def Check(self,state,actions=None):\n",
    "        if actions is None :\n",
    "            actions = self.actions\n",
    "        \n",
    "        if state not in self.Q.keys():\n",
    "            for action in actions:\n",
    "                self.Q[state][action] = 0\n",
    "\n",
    "    def Max(self,state):\n",
    "        Q_maximum = np.max(list(self.Q[state].values()))\n",
    "        return Q_maximum\n",
    "\n",
    "    def Action(self,state,epsilon=0):\n",
    "        if np.random.rand() < epsilon :\n",
    "            idx = np.random.randint(len(actions))\n",
    "            action = actions[idx]\n",
    "        else :\n",
    "            self.Check(state,actions)\n",
    "            action = max(self.Q[state], key=self.Q[state].get)\n",
    "        return action\n",
    "\n",
    "    def Learn(self,state,action,reward,next_state,learning_rate=0.1,discount_factor=0.5):\n",
    "        # Check if state,action and next_state are in Q\n",
    "        self.Check(state)\n",
    "        self.Check(next_state)\n",
    "        self.N_update(state,action)\n",
    "\n",
    "        dQ = reward \\\n",
    "            + discount_factor * self.Max(next_state) \\\n",
    "            - self.Q[state][action]\n",
    "        self.Q[state][action] = self.Q[state][action] + learning_rate * dQ \n",
    "        \n",
    "        return self.Q\n",
    "\n",
    "    def N_update(self,state,action,actions=None):\n",
    "        if actions is None :\n",
    "            actions = self.actions\n",
    "        \n",
    "        if state not in self.N.keys():\n",
    "            for action in actions:\n",
    "                self.N[state][action] = 0 \n",
    "        self.N[state][action] = self.N[state][action] + 1\n",
    "        return self.N[state][action]\n",
    "\n",
    "    def Print(self):\n",
    "        for state in Q_fn.Q.keys():\n",
    "            for action in Q_fn.Q[state].keys():\n",
    "                print(state,action,Q_fn.N[state][action],Q_fn.Q[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.464167868058816 -2.7\n"
     ]
    }
   ],
   "source": [
    "# MaxWeight implementation\n",
    "# Defined above: MaxWeight(state=Q_Size,actions=actions) \n",
    "\n",
    "Get_State = Get_Q_Size\n",
    "Get_Reward = Get_Delay\n",
    "Do_Action = Do_Action_RGA\n",
    "sim_steps = 1\n",
    "sim_length = 100\n",
    "\n",
    "Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",False)\n",
    "Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)\n",
    "Vissim.Simulation.AttValue('UseAllCores')\n",
    "delays = dict()\n",
    "rewards = []\n",
    "Queues = []\n",
    "\n",
    "for _ in range(sim_length):\n",
    "    if Vissim.Simulation.AttValue('SimSec') == 0.0 :\n",
    "        for _ in range(sim_steps):\n",
    "            Vissim.Simulation.RunSingleStep()\n",
    "    Q_Size = Get_Q_Size()\n",
    "    delays = Delay_Dictionary()\n",
    "    state = Get_State()\n",
    "    action = MaxWeight(Q_Size,actions)\n",
    "    Do_Action()\n",
    "    for _ in range(sim_steps):              # Take a few simulation steps\n",
    "        Vissim.Simulation.RunSingleStep()\n",
    "    reward = Get_Reward()              # Get the reward\n",
    "    rewards.append(reward)\n",
    "    Queues.append(Get_Total_Queue())\n",
    "    print(np.mean(rewards),np.mean(Queues))\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Easy Q learner \n",
    "# -- \n",
    "'''\n",
    "Set up the parameters of the optimization\n",
    "'''\n",
    "\n",
    "\n",
    "Get_State = Get_Q_Size\n",
    "Get_Reward = Get_Total_Queue\n",
    "Do_Action = Do_Action_RGA\n",
    "\n",
    "Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",True)\n",
    "#comment below gives lower resolution to simulation\n",
    "#Vissim.Simulation.Stop()\n",
    "#Vissim.Simulation.SetAttValue(\"SimRes\",1)\n",
    "\n",
    "Q_fn = Q_function(actions)\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    sars = []\n",
    "    start_time = time.time()\n",
    "    for iters in range(10000):\n",
    "        # Take a few sets if at the start of simluation \n",
    "        # (This corrects a bug)\n",
    "        if Vissim.Simulation.AttValue('SimSec') == 0.0 :\n",
    "            for _ in range(sim_steps):\n",
    "                Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "        state = Get_State(Lane_List, rounding)  # Get the current state\n",
    "        action = Q_fn.Action(state,0.1)         # Get the current action\n",
    "        Do_Action(action,Signal_Groups)         # Implement the action\n",
    "        for _ in range(sim_steps):              # Take a few simulation steps\n",
    "            Vissim.Simulation.RunSingleStep()\n",
    "        reward = Get_Reward(state)              # Get the reward\n",
    "        next_state = Get_State(Lane_List, rounding)     # Get next state\n",
    "        Q_fn.Learn(state,action,reward,next_state)      # Apply Q-Learning\n",
    "        sars.append([state,action,reward,next_state])    # Save data\n",
    "        Q_fn.N_update(state,action)\n",
    "        print(iters, int(time.time()-start_time))\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Pickle Q_Learner & SARS\n",
    "'''\n",
    "This save the Q-function (and the time of training)\n",
    "'''\n",
    "# now = str(datetime.datetime.now())\n",
    "# now = now.replace(\" \",\"\")\n",
    "# now = now.replace(\":\",\"-\")\n",
    "# pickle.dump(sars, open( \"SARS\"+now+\".p\", \"wb\" ))\n",
    "# dill.dump(Q_fn, open( \"Q\"+now+\".p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Easy Q learner - sars learner \n",
    "sars = pickle.load( open(\"SARS.p\", \"rb\" ))\n",
    "Q_fn_load = Q_function()\n",
    "\n",
    "for state, action, reward, next_state in sars:\n",
    "    Q_fn_load.Learn(state,action,reward,next_state) \n",
    "    Q_fn_load.N_update(state,action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# First Vehicle Q-learner\n",
    "# Good for light load\n",
    "\n",
    "Get_State = Get_First_Vehicle\n",
    "Get_Reward = Get_Delay\n",
    "Do_Action = Do_Action_RGA\n",
    "sim_steps = 20 # needs higher sim steps (I think).\n",
    "rounding = 5.\n",
    "Q_fn = Q_function(actions)\n",
    "\n",
    "Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",True)\n",
    "#comment below gives lower resolution to simulation\n",
    "#Vissim.Simulation.Stop()\n",
    "#Vissim.Simulation.SetAttValue(\"SimRes\",1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    sars = []\n",
    "    rewards = []\n",
    "    start_time = time.time()\n",
    "    delays = dict()\n",
    "    for iters in range(10000):\n",
    "        # Take a few sets if at the start of simluation \n",
    "        # (This corrects a bug)\n",
    "        if Vissim.Simulation.AttValue('SimSec') == 0.0 :\n",
    "            for _ in range(sim_steps):\n",
    "                Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "        state = Get_State(Lane_List, rounding)  # Get the current state\n",
    "        action = Q_fn.Action(state,0.1)         # Get the current action\n",
    "        delays = Delay_Dictionary(delays)\n",
    "        Do_Action(action,Signal_Groups)         # Implement the action\n",
    "        for _ in range(sim_steps):              # Take a few simulation steps\n",
    "            Vissim.Simulation.RunSingleStep()\n",
    "        reward = Get_Reward(state)              # Get the reward\n",
    "        rewards.append(reward)\n",
    "        next_state = Get_State(Lane_List, rounding)     # Get next state\n",
    "        Q_fn.Learn(state,action,reward,next_state)      # Apply Q-Learning\n",
    "        sars.append([state,action,reward,next_state])    # Save data\n",
    "        Q_fn.N_update(state,action)\n",
    "        print(iters, int(time.time()-start_time), np.mean(rewards))\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "# dill.dump(Q_fn, open( \"Q_First_Vehicle_Learner.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Delay based Q-learner  \n",
    "\n",
    "# First Vehicle Q-learner\n",
    "# Good for light load\n",
    "\n",
    "Get_State = Get_Q_Size\n",
    "Get_Reward = Get_Delay\n",
    "\n",
    "Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",True)\n",
    "#comment below gives lower resolution to simulation\n",
    "#Vissim.Simulation.Stop()\n",
    "#Vissim.Simulation.SetAttValue(\"SimRes\",1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    sars = []\n",
    "    rewards = []\n",
    "    start_time = time.time()\n",
    "    delays = dict()\n",
    "    for iters in range(10000):\n",
    "        # Take a few sets if at the start of simluation \n",
    "        # (This corrects a bug)\n",
    "        if Vissim.Simulation.AttValue('SimSec') == 0.0 :\n",
    "            for _ in range(sim_steps):\n",
    "                Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "        state = Get_State(Lane_List, rounding)  # Get the current state\n",
    "        action = Q_fn.Action(state,0.1)         # Get the current action\n",
    "        delays = Delay_Dictionary(delays)\n",
    "        Do_Action(action,Signal_Groups)         # Implement the action\n",
    "        for _ in range(sim_steps):              # Take a few simulation steps\n",
    "            Vissim.Simulation.RunSingleStep()\n",
    "        reward = Get_Reward(state)              # Get the reward\n",
    "        rewards.append(reward)\n",
    "        next_state = Get_State(Lane_List, rounding)     # Get next state\n",
    "        Q_fn.Learn(state,action,reward,next_state)      # Apply Q-Learning\n",
    "        sars.append([state,action,reward,next_state])    # Save data\n",
    "        Q_fn.N_update(state,action)\n",
    "        print(iters, int(time.time()-start_time), np.mean(rewards))\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Show current Q_function\n",
    "\n",
    "Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",False)\n",
    "Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)\n",
    "Vissim.Simulation.AttValue('UseAllCores')\n",
    "\n",
    "for _ in range(50):\n",
    "    if Vissim.Simulation.AttValue('SimSec') == 0.0 :\n",
    "        for _ in range(sim_steps):\n",
    "            Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "    state = Get_State(Lane_List,rounding)\n",
    "    action = Q_fn.Action(state,0.0)\n",
    "    Do_Action(action,Signal_Groups)\n",
    "    for _ in range(10):              # Take a few simulation steps\n",
    "        Vissim.Simulation.RunSingleStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load Alvaro Parameters\n",
    "from keras.models import load_model\n",
    "model = load_model('Single_Cross_Straight_Episodes400_ProgramDQN_Agent0.h5')\n",
    "\n",
    "## Network Model Parameters\n",
    "Random_Seed = 42\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "program = 'DQN' # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'\n",
    "state_type  = 'Queues'\n",
    "PER_activated = True\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "## Load trained model?\n",
    "Demo_Mode = False\n",
    "load_trained = False\n",
    "Quickmode = True\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "## Data handling flags\n",
    "# Flag for restarting the COM Server\n",
    "reset_flag = True\n",
    "#cache_flag = False\n",
    "# If a fresh start is needed, all previous results from simulations are deleted\n",
    "Start_Fresh = True\n",
    "# Debug action\n",
    "debug_action = False\n",
    "\n",
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 100\n",
    "partial_save_at = 100\n",
    "copy_weights_frequency = 5\n",
    "reset_frequency = 101\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = simulation_length*5\n",
    "\n",
    "## State-Action Parameters\n",
    "state_size = 4\n",
    "action_size = 5\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "memory_size = 1000\n",
    "alpha   = 0.0001\n",
    "#alpha   = 0.001\n",
    "\n",
    "gamma   = 0.95\n",
    "\n",
    "# Exploration Schedule\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "#epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes+1)) # Geometric decay\n",
    "\n",
    "# Demand Schedule\n",
    "demands = [100, 200, 400, 600, 800, 1000]\n",
    "# Session ID\n",
    "Session_ID = 'Episodes'+str(episodes)+'_Program'+program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ALvaro DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, ID, state_type, npa, memory_size, gamma, epsilon_start, epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, PER_activated, DoubleDQN, Dueling):\n",
    "        # Agent Junction ID and Controller ID\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        \n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Agent Hyperparameters\n",
    "        self.gamma = gamma                    # discount rate\n",
    "        self.epsilon = epsilon_start          # starting exploration rate\n",
    "        self.epsilon_min = epsilon_end        # final exploration rate\n",
    "        self.epsilon_decay = epsilon_decay    # decay of exploration rate\n",
    "        self.learning_rate = alpha            # learning rate\n",
    "\n",
    "        # Agent Architecture\n",
    "        self.DoubleDQN = DoubleDQN            # Double Deep Q Network Flag\n",
    "        self.Dueling = Dueling                # Dueling Q Networks Flag\n",
    "        self.PER_activated = PER_activated    # Prioritized Experience Replay Flag\n",
    "\n",
    "        # Model and target networks\n",
    "        self.copy_weights_frequency = copy_weights_frequency    # Frequency to copy weights to target network\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # Architecture Debug Messages\n",
    "        if self.DoubleDQN:\n",
    "            if self.Dueling:\n",
    "                print(\"Deploying instance of Dueling Double Deep Q Learning Agent(s)\")\n",
    "            else:\n",
    "                print(\"Deploying instance of Double Deep Q Learning Agent(s)\")\n",
    "        else:\n",
    "            if self.Dueling:\n",
    "                print(\"Deploying instance of Dueling Deep Q Learning Agent(s)\")\n",
    "            else:\n",
    "                print(\"Deploying instance of Standard Deep Q Learning Agent(s)\")\n",
    "\n",
    "        # Initial Setup of S, A, R, S_\n",
    "        self.state = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.newstate = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        # Metrics Storage Initialization\n",
    "        self.episode_reward = []\n",
    "        self.loss = []\n",
    "\n",
    "        if self.PER_activated:\n",
    "            # If PER_activated spawn BinaryTree and Memory object to store priorities and experiences\n",
    "            self.memory = PER.Memory(memory_size)\n",
    "        else:\n",
    "            # Else use the deque structure to only store experiences which will be sampled uniformly\n",
    "            self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "    # Update the Junction IDs for the agent\n",
    "    def update_IDS(self, ID, npa):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "    \n",
    "    # Agent Neural Network definition\n",
    "    def _build_model(self):\n",
    "        if self.Dueling:\n",
    "            # Architecture for the Neural Net in the Dueling Deep Q-Learning Model\n",
    "            #model = Sequential()\n",
    "            input_layer = Input(shape = (self.state_size,))\n",
    "            dense1 = Dense(24, input_dim=self.state_size, activation='relu')(input_layer)\n",
    "            #dense2 = Dense(48, activation='relu')(dense1)\n",
    "            #flatten = Flatten()(dense2)\n",
    "            fc1 = Dense(48)(dense1)\n",
    "            dueling_actions = Dense(self.action_size)(fc1)\n",
    "            fc2 = Dense(48)(dense1)\n",
    "            dueling_values = Dense(1)(fc2)\n",
    "\n",
    "            def dueling_operator(duel_input):\n",
    "                duel_v = duel_input[0]\n",
    "                duel_a = duel_input[1]\n",
    "                return (duel_v + (duel_a - K.mean(duel_a, axis = 1, keepdims = True)))\n",
    "\n",
    "            policy = Lambda(dueling_operator, name = 'policy')([dueling_values, dueling_actions])\n",
    "            model = Model(inputs=[input_layer], outputs=[policy])\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            return(model)\n",
    "        else:\n",
    "            # Architecture for the Neural Net in Deep-Q learning Model (also Double version)\n",
    "            model = Sequential()\n",
    "            model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "            model.add(Dense(48, activation='relu'))\n",
    "            model.add(Dense(self.action_size, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            return model\n",
    "    \n",
    "    # Obtain the state based on different state definitions\n",
    "    def get_state(self, state_type, state_size, Vissim):\n",
    "        if state_type == 'Queues':\n",
    "            #Obtain Queue Values (average value over the last period)\n",
    "            West_Queue  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)')\n",
    "            South_Queue = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)')\n",
    "            East_Queue  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)')\n",
    "            North_Queue = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)')\n",
    "            state = [West_Queue, South_Queue, East_Queue, North_Queue]\n",
    "            state = np.reshape(state, [1,state_size])\n",
    "            return(state)\n",
    "        elif state_type == 'Delay':\n",
    "            # Obtain Delay Values (average delay in lane * nr cars in queue)\n",
    "            West_Delay    = Vissim.Net.DelayMeasurements.ItemByKey(1).AttValue('VehDelay(Current,Last,All)') \n",
    "            West_Stopped  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QStops(Current,Last)')\n",
    "            South_Delay   = Vissim.Net.DelayMeasurements.ItemByKey(2).AttValue('VehDelay(Current,Last,All)') \n",
    "            South_Stopped = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QStops(Current,Last)')\n",
    "            East_Delay    = Vissim.Net.DelayMeasurements.ItemByKey(3).AttValue('VehDelay(Current,Last,All)') \n",
    "            East_Stopped  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QStops(Current,Last)')\n",
    "            North_Delay   = Vissim.Net.DelayMeasurements.ItemByKey(4).AttValue('VehDelay(Current,Last,All)') \n",
    "            North_Stopped = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QStops(Current,Last)')\n",
    "            \n",
    "            pre_state = [West_Delay, South_Delay, East_Delay, North_Delay, West_Stopped, South_Stopped, East_Stopped, North_Stopped]\n",
    "            pre_state = [0 if state is None else state for state in pre_state]\n",
    "            \n",
    "            state = [pre_state[0]*pre_state[4], pre_state[1]*pre_state[5], pre_state[2]*pre_state[6], pre_state[3]*pre_state[7]]\n",
    "            state = np.reshape(state, [1,state_size])\n",
    "            return(state)\n",
    "        elif state_type == 'MaxFlow':\n",
    "            pass\n",
    "        elif state_type == 'FuelConsumption':\n",
    "            pass\n",
    "        elif state_type == 'NOx':\n",
    "            pass\n",
    "        elif state_type == \"COM\":\n",
    "            pass\n",
    "    \n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        if self.PER_activated:\n",
    "            experience = (state, action, reward, next_state)\n",
    "            self.memory.store(experience)\n",
    "        else:\n",
    "            self.memory.append((state, action, reward, next_state))\n",
    "    \n",
    "    # Choosing actions\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size) \n",
    "            self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "            #print('Chosen Random Action {}'.format(action+1))\n",
    "            return action\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            action = np.argmax(act_values[0]) \n",
    "            self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "            #print('Chosen Not-Random Action {}'.format(action+1))\n",
    "            return action  # returns action\n",
    "    \n",
    "    def get_reward(self):\n",
    "        #reward = -np.absolute((self.newstate[0][0]-self.newstate[0][2])-(self.newstate[0][1]-self.newstate[0][3])) - \n",
    "        #reward = -np.sum(Agents[0].newstate[0])\n",
    "        reward = -np.sum([0 if state is None else state for state in self.newstate[0]])\n",
    "        #print(reward)\n",
    "\n",
    "        self.episode_reward.append(reward)\n",
    "        return reward\n",
    "    \n",
    "    def replay_single(self, batch_size, episode, loss):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "\n",
    "            if self.DoubleDQN:\n",
    "                next_action = np.argmax(self.target_model.predict(np.reshape(next_state,(1,self.state_size))), axis=1)\n",
    "                target = reward + self.gamma * self.target_model.predict(np.reshape(next_state,(1,self.state_size)))[0][next_action]\n",
    "            else:\n",
    "                target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "                # No fixed targets version\n",
    "                #target = reward + self.gamma * np.max(self.model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            self.loss.append(self.model.history.history['loss'][0])\n",
    "\n",
    "        # Exploration rate decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon += self.epsilon_decay\n",
    "        # Copy weights every 5 episodes\n",
    "        if (episode+1) % self.copy_weights_frequency == 0 and episode != 0:\n",
    "            self.copy_weights()   \n",
    "   \n",
    "    def replay_batch(self, batch_size, episode, loss):\n",
    "        state_vector = []\n",
    "        target_f_vector = []\n",
    "        absolute_errors = [] \n",
    "\n",
    "        if self.PER_activated:\n",
    "            tree_idx, minibatch, ISWeights_mb = self.memory.sample(batch_size)\n",
    "            minibatch = [item[0] for item in minibatch]\n",
    "            #return(minibatch)\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "            if self.DoubleDQN:\n",
    "                next_action = np.argmax(self.target_model.predict(np.reshape(next_state,(1,self.state_size))), axis=1)\n",
    "                target = reward + self.gamma * self.target_model.predict(np.reshape(next_state,(1,self.state_size)))[0][next_action]\n",
    "            else:\n",
    "                # Fixed Q-Target\n",
    "                target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "                # No fixed targets version\n",
    "                #target = reward + self.gamma * np.max(self.model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "\n",
    "            # This section incorporates the reward into the prediction and calculates the absolute error between old and new\n",
    "            target_f = self.model.predict(state)\n",
    "            absolute_errors.append(abs(target_f[0][action] - target)[0])\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            state_vector.append(state[0])\n",
    "            target_f_vector.append(target_f[0])\n",
    "\n",
    "        state_matrix = np.asarray(state_vector)\n",
    "        target_f_matrix = np.asarray(target_f_vector)\n",
    "\n",
    "        self.model.fit(state_matrix, target_f_matrix, epochs=1, verbose=0)\n",
    "        self.loss.append(self.model.history.history['loss'])\n",
    "\n",
    "        if self.PER_activated:\n",
    "            #Update priority\n",
    "            self.memory.batch_update(tree_idx, absolute_errors)\n",
    "\n",
    "        # Exploration rate decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon += self.epsilon_decay\n",
    "        # Copy weights every 5 episodes\n",
    "        if (episode+1) % self.copy_weights_frequency == 0 and episode != 0:\n",
    "            self.copy_weights()   \n",
    "\n",
    "    # Copy weights function\n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(\"Weights succesfully copied to Target model.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Dueling Deep Q Learning Agent(s)\n"
     ]
    }
   ],
   "source": [
    "# Load Alvaro DQN agent\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from NParser import NetworkParser\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.layers import merge, Dense, Input, Lambda\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import PER\n",
    "\n",
    "npa = NetworkParser(Vissim)\n",
    "\n",
    "Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                   epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, PER_activated, DoubleDQN = False,\\\n",
    "               Dueling = True) for ID in npa.signal_controllers_ids]\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('Single_Cross_Straight_Episodes400_ProgramDQN_Agent0.h5')\n",
    "Agents[0].model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-25.033817739753996, -21.594)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Alvaro DQN agent\n",
    "import Simulator_Functions as SF \n",
    "import random\n",
    "\n",
    "Get_Reward = Get_Delay\n",
    "\n",
    "# Run a Single Episode for a set simulation length\n",
    "def run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, Demo_Mode, PER_activated):\n",
    "    cycle_t = 0\n",
    "    Vissim.Simulation.SetAttValue('SimRes', 1)\n",
    "    #Vissim.Simulation.RunContinuous()\n",
    "    rewards = []\n",
    "    Queues = []\n",
    "    for time_t in range(simulation_length):\n",
    "        if cycle_t == 900:\n",
    "            for agent in Agents:\n",
    "\n",
    "                agent.newstate = agent.get_state(state_type, state_size, Vissim)\n",
    "                agent.reward   = agent.get_reward()\n",
    "                agent.remember(agent.state, agent.action, agent.reward, agent.newstate)\n",
    "                agent.action = agent.act(agent.newstate)\n",
    "                if Demo_Mode:\n",
    "                    print('Agent Reward in this cycle is : {}'.format(round(agent.reward,2)))\n",
    "\n",
    "                agent.state    = agent.newstate\n",
    "\n",
    "            cycle_t = 0\n",
    "        else:\n",
    "            cycle_t += 1\n",
    "        \n",
    "        delays = Delay_Dictionary()\n",
    "        reward = Get_Reward()              # Get the reward\n",
    "        Q_Size = Get_Total_Queue()\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        Queues.append(Q_Size)\n",
    "        \n",
    "        print(np.mean(rewards),np.mean(Queues))\n",
    "        display.clear_output(wait=True)\n",
    "            \n",
    "        # Advance the game to the next frame based on the action.\n",
    "        Vissim.Simulation.RunSingleStep()\n",
    "    # Stop the simulation    \n",
    "    Vissim.Simulation.Stop()\n",
    "\n",
    "    return np.mean(rewards), np.mean(Queues)\n",
    "\n",
    "Vissim.Simulation.Stop()\n",
    "simulation_length = 500\n",
    "run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, Demo_Mode, PER_activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make Actions a dictionary\n",
    "2. No model load functionality in Alvaro code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load Alvaro Parameters\n",
    "from keras.models import load_model\n",
    "model = load_model('Single_Cross_Straight_Episodes400_ProgramDQN_Agent0.h5')\n",
    "\n",
    "## Network Model Parameters\n",
    "Random_Seed = 42\n",
    "model_name  = 'Single_Cross_Straight'\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "program = 'DQN' # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'\n",
    "state_type  = 'Queues'\n",
    "PER_activated = True\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = False\n",
    "## Load trained model?\n",
    "Demo_Mode = False\n",
    "load_trained = False\n",
    "Quickmode = True\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "## Data handling flags\n",
    "# Flag for restarting the COM Server\n",
    "reset_flag = True\n",
    "#cache_flag = False\n",
    "# If a fresh start is needed, all previous results from simulations are deleted\n",
    "Start_Fresh = True\n",
    "# Debug action\n",
    "debug_action = False\n",
    "\n",
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes and copy weights with frequency \"f\"\n",
    "episodes = 100\n",
    "partial_save_at = 100\n",
    "copy_weights_frequency = 5\n",
    "reset_frequency = 101\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "simulation_length = 3600*1 + 1\n",
    "memory_population_length = simulation_length*5\n",
    "\n",
    "## State-Action Parameters\n",
    "state_size = 4\n",
    "action_size = 5\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "memory_size = 1000\n",
    "alpha   = 0.0001\n",
    "#alpha   = 0.001\n",
    "\n",
    "gamma   = 0.95\n",
    "\n",
    "# Exploration Schedule\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "#epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes+1)) # Geometric decay\n",
    "\n",
    "# Demand Schedule\n",
    "demands = [100, 200, 400, 600, 800, 1000]\n",
    "# Session ID\n",
    "Session_ID = 'Episodes'+str(episodes)+'_Program'+program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ALvaro DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, ID, state_type, npa, memory_size, gamma, epsilon_start, epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, PER_activated, DoubleDQN, Dueling):\n",
    "        # Agent Junction ID and Controller ID\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        \n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Agent Hyperparameters\n",
    "        self.gamma = gamma                    # discount rate\n",
    "        self.epsilon = epsilon_start          # starting exploration rate\n",
    "        self.epsilon_min = epsilon_end        # final exploration rate\n",
    "        self.epsilon_decay = epsilon_decay    # decay of exploration rate\n",
    "        self.learning_rate = alpha            # learning rate\n",
    "\n",
    "        # Agent Architecture\n",
    "        self.DoubleDQN = DoubleDQN            # Double Deep Q Network Flag\n",
    "        self.Dueling = Dueling                # Dueling Q Networks Flag\n",
    "        self.PER_activated = PER_activated    # Prioritized Experience Replay Flag\n",
    "\n",
    "        # Model and target networks\n",
    "        self.copy_weights_frequency = copy_weights_frequency    # Frequency to copy weights to target network\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # Architecture Debug Messages\n",
    "        if self.DoubleDQN:\n",
    "            if self.Dueling:\n",
    "                print(\"Deploying instance of Dueling Double Deep Q Learning Agent(s)\")\n",
    "            else:\n",
    "                print(\"Deploying instance of Double Deep Q Learning Agent(s)\")\n",
    "        else:\n",
    "            if self.Dueling:\n",
    "                print(\"Deploying instance of Dueling Deep Q Learning Agent(s)\")\n",
    "            else:\n",
    "                print(\"Deploying instance of Standard Deep Q Learning Agent(s)\")\n",
    "\n",
    "        # Initial Setup of S, A, R, S_\n",
    "        self.state = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.newstate = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        # Metrics Storage Initialization\n",
    "        self.episode_reward = []\n",
    "        self.loss = []\n",
    "\n",
    "        if self.PER_activated:\n",
    "            # If PER_activated spawn BinaryTree and Memory object to store priorities and experiences\n",
    "            self.memory = PER.Memory(memory_size)\n",
    "        else:\n",
    "            # Else use the deque structure to only store experiences which will be sampled uniformly\n",
    "            self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "    # Update the Junction IDs for the agent\n",
    "    def update_IDS(self, ID, npa):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "    \n",
    "    # Agent Neural Network definition\n",
    "    def _build_model(self):\n",
    "        if self.Dueling:\n",
    "            # Architecture for the Neural Net in the Dueling Deep Q-Learning Model\n",
    "            #model = Sequential()\n",
    "            input_layer = Input(shape = (self.state_size,))\n",
    "            dense1 = Dense(24, input_dim=self.state_size, activation='relu')(input_layer)\n",
    "            #dense2 = Dense(48, activation='relu')(dense1)\n",
    "            #flatten = Flatten()(dense2)\n",
    "            fc1 = Dense(48)(dense1)\n",
    "            dueling_actions = Dense(self.action_size)(fc1)\n",
    "            fc2 = Dense(48)(dense1)\n",
    "            dueling_values = Dense(1)(fc2)\n",
    "\n",
    "            def dueling_operator(duel_input):\n",
    "                duel_v = duel_input[0]\n",
    "                duel_a = duel_input[1]\n",
    "                return (duel_v + (duel_a - K.mean(duel_a, axis = 1, keepdims = True)))\n",
    "\n",
    "            policy = Lambda(dueling_operator, name = 'policy')([dueling_values, dueling_actions])\n",
    "            model = Model(inputs=[input_layer], outputs=[policy])\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            return(model)\n",
    "        else:\n",
    "            # Architecture for the Neural Net in Deep-Q learning Model (also Double version)\n",
    "            model = Sequential()\n",
    "            model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "            model.add(Dense(48, activation='relu'))\n",
    "            model.add(Dense(self.action_size, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            return model\n",
    "    \n",
    "    # Obtain the state based on different state definitions\n",
    "    def get_state(self, state_type, state_size, Vissim):\n",
    "        if state_type == 'Queues':\n",
    "            #Obtain Queue Values (average value over the last period)\n",
    "            West_Queue  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)')\n",
    "            South_Queue = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)')\n",
    "            East_Queue  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)')\n",
    "            North_Queue = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)')\n",
    "            state = [West_Queue, South_Queue, East_Queue, North_Queue]\n",
    "            state = np.reshape(state, [1,state_size])\n",
    "            return(state)\n",
    "        elif state_type == 'Delay':\n",
    "            # Obtain Delay Values (average delay in lane * nr cars in queue)\n",
    "            West_Delay    = Vissim.Net.DelayMeasurements.ItemByKey(1).AttValue('VehDelay(Current,Last,All)') \n",
    "            West_Stopped  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QStops(Current,Last)')\n",
    "            South_Delay   = Vissim.Net.DelayMeasurements.ItemByKey(2).AttValue('VehDelay(Current,Last,All)') \n",
    "            South_Stopped = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QStops(Current,Last)')\n",
    "            East_Delay    = Vissim.Net.DelayMeasurements.ItemByKey(3).AttValue('VehDelay(Current,Last,All)') \n",
    "            East_Stopped  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QStops(Current,Last)')\n",
    "            North_Delay   = Vissim.Net.DelayMeasurements.ItemByKey(4).AttValue('VehDelay(Current,Last,All)') \n",
    "            North_Stopped = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QStops(Current,Last)')\n",
    "            \n",
    "            pre_state = [West_Delay, South_Delay, East_Delay, North_Delay, West_Stopped, South_Stopped, East_Stopped, North_Stopped]\n",
    "            pre_state = [0 if state is None else state for state in pre_state]\n",
    "            \n",
    "            state = [pre_state[0]*pre_state[4], pre_state[1]*pre_state[5], pre_state[2]*pre_state[6], pre_state[3]*pre_state[7]]\n",
    "            state = np.reshape(state, [1,state_size])\n",
    "            return(state)\n",
    "        elif state_type == 'MaxFlow':\n",
    "            pass\n",
    "        elif state_type == 'FuelConsumption':\n",
    "            pass\n",
    "        elif state_type == 'NOx':\n",
    "            pass\n",
    "        elif state_type == \"COM\":\n",
    "            pass\n",
    "    \n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        if self.PER_activated:\n",
    "            experience = (state, action, reward, next_state)\n",
    "            self.memory.store(experience)\n",
    "        else:\n",
    "            self.memory.append((state, action, reward, next_state))\n",
    "    \n",
    "    # Choosing actions\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size) \n",
    "            self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "            #print('Chosen Random Action {}'.format(action+1))\n",
    "            return action\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            action = np.argmax(act_values[0]) \n",
    "            self.signal_controller.SetAttValue('ProgNo', int(action+1))\n",
    "            #print('Chosen Not-Random Action {}'.format(action+1))\n",
    "            return action  # returns action\n",
    "    \n",
    "    def get_reward(self):\n",
    "        #reward = -np.absolute((self.newstate[0][0]-self.newstate[0][2])-(self.newstate[0][1]-self.newstate[0][3])) - \n",
    "        #reward = -np.sum(Agents[0].newstate[0])\n",
    "        reward = -np.sum([0 if state is None else state for state in self.newstate[0]])\n",
    "        #print(reward)\n",
    "\n",
    "        self.episode_reward.append(reward)\n",
    "        return reward\n",
    "    \n",
    "    def replay_single(self, batch_size, episode, loss):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "\n",
    "            if self.DoubleDQN:\n",
    "                next_action = np.argmax(self.target_model.predict(np.reshape(next_state,(1,self.state_size))), axis=1)\n",
    "                target = reward + self.gamma * self.target_model.predict(np.reshape(next_state,(1,self.state_size)))[0][next_action]\n",
    "            else:\n",
    "                target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "                # No fixed targets version\n",
    "                #target = reward + self.gamma * np.max(self.model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            self.loss.append(self.model.history.history['loss'][0])\n",
    "\n",
    "        # Exploration rate decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon += self.epsilon_decay\n",
    "        # Copy weights every 5 episodes\n",
    "        if (episode+1) % self.copy_weights_frequency == 0 and episode != 0:\n",
    "            self.copy_weights()   \n",
    "   \n",
    "    def replay_batch(self, batch_size, episode, loss):\n",
    "        state_vector = []\n",
    "        target_f_vector = []\n",
    "        absolute_errors = [] \n",
    "\n",
    "        if self.PER_activated:\n",
    "            tree_idx, minibatch, ISWeights_mb = self.memory.sample(batch_size)\n",
    "            minibatch = [item[0] for item in minibatch]\n",
    "            #return(minibatch)\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "            if self.DoubleDQN:\n",
    "                next_action = np.argmax(self.target_model.predict(np.reshape(next_state,(1,self.state_size))), axis=1)\n",
    "                target = reward + self.gamma * self.target_model.predict(np.reshape(next_state,(1,self.state_size)))[0][next_action]\n",
    "            else:\n",
    "                # Fixed Q-Target\n",
    "                target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "                # No fixed targets version\n",
    "                #target = reward + self.gamma * np.max(self.model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "\n",
    "            # This section incorporates the reward into the prediction and calculates the absolute error between old and new\n",
    "            target_f = self.model.predict(state)\n",
    "            absolute_errors.append(abs(target_f[0][action] - target)[0])\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            state_vector.append(state[0])\n",
    "            target_f_vector.append(target_f[0])\n",
    "\n",
    "        state_matrix = np.asarray(state_vector)\n",
    "        target_f_matrix = np.asarray(target_f_vector)\n",
    "\n",
    "        self.model.fit(state_matrix, target_f_matrix, epochs=1, verbose=0)\n",
    "        self.loss.append(self.model.history.history['loss'])\n",
    "\n",
    "        if self.PER_activated:\n",
    "            #Update priority\n",
    "            self.memory.batch_update(tree_idx, absolute_errors)\n",
    "\n",
    "        # Exploration rate decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon += self.epsilon_decay\n",
    "        # Copy weights every 5 episodes\n",
    "        if (episode+1) % self.copy_weights_frequency == 0 and episode != 0:\n",
    "            self.copy_weights()   \n",
    "\n",
    "    # Copy weights function\n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(\"Weights succesfully copied to Target model.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Dueling Deep Q Learning Agent(s)\n"
     ]
    }
   ],
   "source": [
    "# Load Alvaro DQN agent\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from NParser import NetworkParser\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.layers import merge, Dense, Input, Lambda\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import PER\n",
    "\n",
    "npa = NetworkParser(Vissim)\n",
    "\n",
    "Agents = [DQNAgent(state_size, action_size, ID, state_type, npa, memory_size, gamma, 0 if Demo_Mode else epsilon_start,\\\n",
    "                   epsilon_end, epsilon_decay, alpha, copy_weights_frequency, Vissim, PER_activated, DoubleDQN = False,\\\n",
    "               Dueling = True) for ID in npa.signal_controllers_ids]\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('PER_DuelingDDQN_400ep_queues_fixed_linear/Single_Cross_Straight_Episodes400_ProgramDuelingDDQN_Agent0.h5')\n",
    "Agents[0].model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-25.033817739753996, -21.594)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Alvaro DQN agent\n",
    "import Simulator_Functions as SF \n",
    "import random\n",
    "\n",
    "Get_Reward = Get_Delay\n",
    "\n",
    "# Run a Single Episode for a set simulation length\n",
    "def run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, Demo_Mode, PER_activated):\n",
    "    cycle_t = 0\n",
    "    Vissim.Simulation.SetAttValue('SimRes', 1)\n",
    "    #Vissim.Simulation.RunContinuous()\n",
    "    rewards = []\n",
    "    Queues = []\n",
    "    for time_t in range(simulation_length):\n",
    "        if cycle_t == 900:\n",
    "            for agent in Agents:\n",
    "\n",
    "                agent.newstate = agent.get_state(state_type, state_size, Vissim)\n",
    "                agent.reward   = agent.get_reward()\n",
    "                agent.remember(agent.state, agent.action, agent.reward, agent.newstate)\n",
    "                agent.action = agent.act(agent.newstate)\n",
    "                if Demo_Mode:\n",
    "                    print('Agent Reward in this cycle is : {}'.format(round(agent.reward,2)))\n",
    "\n",
    "                agent.state    = agent.newstate\n",
    "\n",
    "            cycle_t = 0\n",
    "        else:\n",
    "            cycle_t += 1\n",
    "        \n",
    "        delays = Delay_Dictionary()\n",
    "        reward = Get_Reward()              # Get the reward\n",
    "        Q_Size = Get_Total_Queue()\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        Queues.append(Q_Size)\n",
    "        \n",
    "        print(np.mean(rewards),np.mean(Queues))\n",
    "        display.clear_output(wait=True)\n",
    "            \n",
    "        # Advance the game to the next frame based on the action.\n",
    "        Vissim.Simulation.RunSingleStep()\n",
    "    # Stop the simulation    \n",
    "    Vissim.Simulation.Stop()\n",
    "\n",
    "    return np.mean(rewards), np.mean(Queues)\n",
    "\n",
    "Vissim.Simulation.Stop()\n",
    "simulation_length = 500\n",
    "run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, Demo_Mode, PER_activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vissim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e3601a8727d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSignalControllers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Vissim' is not defined"
     ]
    }
   ],
   "source": [
    "Vissim.Net.SignalControllers.GetAll()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
