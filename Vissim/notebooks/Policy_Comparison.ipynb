{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: GPU DEVICE NOT FOUND.\n"
     ]
    }
   ],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "## FA Modules\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"ERROR: GPU DEVICE NOT FOUND.\")\n",
    "\n",
    "# Import Keras is TF 1.    \n",
    "#from keras.models import load_model\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "## Data Management Modules\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "## Math Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d427aa90ad92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Our Modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mCOMServer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCOMServerDispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimulation_Environments\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mSE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Agents'"
     ]
    }
   ],
   "source": [
    "## Our Modules\n",
    "from COMServer import COMServerDispatch\n",
    "import Agents \n",
    "import Simulation_Environments as SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b64471b7c95c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reload modules for debugging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#reload(AC)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Agents' is not defined"
     ]
    }
   ],
   "source": [
    "# reload modules for debugging\n",
    "from imp import reload\n",
    "reload(Agents)\n",
    "reload(SE)\n",
    "#reload(AC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Agent():     \n",
    "    # Initialize agent with dimension of state and action space\n",
    "    def __init__(self,state_size, action_size,actions):\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.actions = actions\n",
    "\n",
    "    # Choose and action\n",
    "    def Action(self, state, actions=None):\n",
    "        if actions is None:\n",
    "            actions = self.actions\n",
    "        passactions[act_idx]\n",
    "    \n",
    "    # Learning routine\n",
    "    def Learn(self,sarsa):\n",
    "        pass\n",
    "    \n",
    "    # learn from a batch\n",
    "    def Learn_Batch(self,Sarsa, batch_size=32):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Q_learner\n",
    "'''\n",
    "Easy Q_learner Q_Function\n",
    "'''\n",
    "class Q_function(Agent):\n",
    "    def __init__(self, actions = None):\n",
    "        # Q function\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        # number of visits\n",
    "        self.N = defaultdict(lambda: defaultdict(float))\n",
    "        self.actions = actions\n",
    "\n",
    "    def Check(self,state,actions=None):\n",
    "        if actions is None :\n",
    "            actions = self.actions\n",
    "        \n",
    "        if state not in self.Q.keys():\n",
    "            for action in actions:\n",
    "                self.Q[state][action] = 0\n",
    "\n",
    "    def Max(self,state):\n",
    "        Q_maximum = np.max(list(self.Q[state].values()))\n",
    "        return Q_maximum\n",
    "\n",
    "    def Action(self,state,epsilon=0):\n",
    "        if np.random.rand() < epsilon :\n",
    "            idx = np.random.randint(len(actions))\n",
    "            action = actions[idx]\n",
    "        else :\n",
    "            self.Check(state,actions)\n",
    "            action = max(self.Q[state], key=self.Q[state].get)\n",
    "        return action\n",
    "\n",
    "    def Learn(self,sars,learning_rate=0.1,discount_factor=0.5):\n",
    "        state, action, reward, next_state = sars\n",
    "        # Check if state,action and next_state are in Q\n",
    "        self.Check(state)\n",
    "        self.Check(next_state)\n",
    "        self.N_update(state,action)\n",
    "\n",
    "        dQ = reward \\\n",
    "            + discount_factor * self.Max(next_state) \\\n",
    "            - self.Q[state][action]\n",
    "        self.Q[state][action] = self.Q[state][action] + learning_rate * dQ \n",
    "        \n",
    "        return self.Q\n",
    "\n",
    "    def N_update(self,state,action,actions=None):\n",
    "        if actions is None :\n",
    "            actions = self.actions\n",
    "        \n",
    "        if state not in self.N.keys():\n",
    "            for action in actions:\n",
    "                self.N[state][action] = 0 \n",
    "        self.N[state][action] = self.N[state][action] + 1\n",
    "        return self.N[state][action]\n",
    "\n",
    "    def Print(self):\n",
    "        for state in Q_fn.Q.keys():\n",
    "            for action in Q_fn.Q[state].keys():\n",
    "                print(state,action,Q_fn.N[state][action],Q_fn.Q[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "actions = [(1,0,1,0),\\\n",
    "            (0,1,0,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b1202ab7b745>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mEnv\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mSE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_Sim_Env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'SE' is not defined"
     ]
    }
   ],
   "source": [
    "Env= SE.Q_Sim_Env(None,model_name,vissim_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Some code to reload Simulator instance (without rebooting Vissim)\n",
    "# Vissim = Env.Vissim\n",
    "#Env = SE.Q_Sim_Env(Vissim,model_name,vissim_working_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "# MANUAL LOAD (part 1)\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "# Dispatch works better than dynamoc dispatch\n",
    "#Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "\n",
    "for _ in range(5):\n",
    "    try:\n",
    "        Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "        print('success!')\n",
    "        break\n",
    "    except:\n",
    "        print('fail')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# MANUAL LOAD (part 2)\n",
    "input_file = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\Single_Cross_Straight\\\\Single_Cross_Straight.inpx'\n",
    "Vissim.LoadNet(input_file)\n",
    "Env = SE.Q_Sim_Env(Vissim,model_name,vissim_working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "MW = Agents.MaxWeight(4,2,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actions[0]\n",
    "for _ in range(25):\n",
    "    state, reward, done = Env.Step(action)\n",
    "    action = MW.Action(state[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_fn = Q_function(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actions[0]\n",
    "state, reward, done = Env.Step(action)\n",
    "\n",
    "for _ in range(10):\n",
    "    next_state, reward, done = Env.Step(action) \n",
    "    Q_fn.Learn([tuple(state[0]),action,reward, tuple(next_state[0])])\n",
    "    state = next_state\n",
    "    action = Q_fn.Action(tuple(state[0]),1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raymond Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     13
    ]
   },
   "outputs": [],
   "source": [
    "# Raymond Agent\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "#Code adapted from http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "    def call(self, logits):\n",
    "        # sample a random categorical action from given logits\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlAC.model.action_value(state)p_policy')\n",
    "        # no tf.get_variable(), just simple Keras API\n",
    "        self.hidden1 = kl.Dense(128, activation='relu')\n",
    "        self.hidden2 = kl.Dense(128, activation='relu')\n",
    "        self.value = kl.Dense(1, name='value')\n",
    "        # logits are unnormalized log probabilities\n",
    "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "        # separate hidden layers from the same input tensor\n",
    "        hidden_logs = self.hidden1(x)\n",
    "        hidden_vals = self.hidden2(x)\n",
    "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # executes call() under the hood\n",
    "        logits, value = self.predict(obs)\n",
    "        action = self.dist.predict(logits)\n",
    "        # a simpler option, will become clear later why we don't use it\n",
    "        # action = tf.random.categorical(logits, 1)\n",
    "        return action , value\n",
    "  \n",
    "    \n",
    "class ACAgent(Agent):\n",
    "\n",
    "    def __init__(self,\\\n",
    "                 state_size,\\\n",
    "                 action_size,\\\n",
    "                 # ID,\\\n",
    "                 # state_type,\\\n",
    "                 # npa,\\\n",
    "                 n_step_size=32,\\\n",
    "                 gamma=0.99,\\\n",
    "                 alpha=0.000065): #,\n",
    "                 #Vissim):\n",
    "                \n",
    "\n",
    "\n",
    "        print(\"Deploying instance of Actor_Critic Agent(s)\")\n",
    "\n",
    "        #just temporary\n",
    "        self.epsilon=0\n",
    "\n",
    "        # Model\n",
    "        # hyperparameters for loss terms and Agent\n",
    "        self.params = {'value': 0.5, 'entropy': 0.0001, 'gamma': gamma}\n",
    "        self.model = Model(action_size)\n",
    "        self.model.compile(\n",
    "            optimizer=ko.RMSprop(lr=alpha),\n",
    "            # define separate losses for policy logits and value estimate\n",
    "            loss=[self._logits_loss, self._value_loss]\n",
    "        )\n",
    "\n",
    "#         # Agent Junction ID and Controller ID\n",
    "#         self.signal_id = ID\n",
    "#         self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "#         self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "\n",
    "        # Potential actions (compatible phases) and transitions\n",
    "        self.update_counter = 1                                 # Timesteps until next update\n",
    "        self.compatible_actions = [[0,1,0,1],[1,0,1,0]]         # Potential actions (compatible phases), 1 means green\n",
    "\n",
    "        # Internal State Traffic Control Variables\n",
    "        self.intermediate_phase = False                         # Boolean indicating an ongoing green-red or red-green transition\n",
    "        self.transition_vector = []                             # Vector that will store the transitions between updates\n",
    "\n",
    "\n",
    "        # Initial Setup of S, A, R, S_\n",
    "        self.state = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.newstate = np.reshape([0,0,0,0], [1,state_size])\n",
    "        self.action = 0\n",
    "        self.newaction = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        # Metrics Storage Initialization\n",
    "        self.episode_reward = []\n",
    "        self.loss = []\n",
    "\n",
    "        # The memory will store (state , action , reward, next_state) in a batch\n",
    "        self.memory = deque(maxlen=n_step_size)\n",
    "        self.n_step_size = n_step_size\n",
    "\n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append([state, action, reward, next_state])\n",
    "\n",
    "#     # Update the Junction IDs for the agent\n",
    "#     def update_IDS(self, ID, npa):\n",
    "#         self.signal_id = ID\n",
    "#         self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "#         self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "    # Need to test before loading to build the graph (surely an other way to do it ...)\n",
    "    def test(self):\n",
    "        _,_ = self.model.action_value(np.empty((1,self.state_size)))\n",
    "\n",
    "\n",
    "    def _value_loss(self, returns, value):\n",
    "        # value loss is typically MSE between value estimates and returns\n",
    "        return self.params['value']*kls.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, acts_and_advs, logits):\n",
    "        # a trick to input actions and advantages through same API\n",
    "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
    "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
    "        # from_logits argument ensures transformation into normalized probabilities\n",
    "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        # policy loss is defined by policy gradients, weighted by advantages\n",
    "        # note: we only calculate the loss on the actions we've actually taken\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "        # entropy loss can be calculated via CE over itself\n",
    "        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n",
    "        # here signs are flipped because optimizer minimizes\n",
    "        return policy_loss - self.params['entropy']*entropy_loss\n",
    "\n",
    "    def _returns_advantages(self, rewards, values, next_value):\n",
    "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # returns are calculated as discounted sum of future rewards\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1]\n",
    "        returns = returns[:-1]\n",
    "        # advantages are returns - baseline, value estimates in our case\n",
    "        advantages = returns - values\n",
    "        return returns, advantages\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        action, _ = self.model.action_value(state)\n",
    "        return np.squeeze(action, axis=-1)\n",
    "\n",
    "    #Performing step of gradient descent on the agent memory\n",
    "    def learn(self):\n",
    "\n",
    "\n",
    "        Sample = np.array(self.memory)\n",
    "\n",
    "        states, actions, rewards, next_state  = np.concatenate(Sample[:,0], axis=0), Sample[:,1].astype('int32'), Sample[:,2], np.concatenate(Sample[:,3], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        _, values = self.model.action_value(states)\n",
    "        values = values.squeeze()\n",
    "\n",
    "        _, next_value  = self.model.action_value(next_state)\n",
    "\n",
    "        next_value = next_value[-1]\n",
    "\n",
    "\n",
    "\n",
    "        returns, advs = self._returns_advantages(rewards, values, next_value)\n",
    "\n",
    "        # a trick to input actions and advantages through same API\n",
    "\n",
    "        acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "\n",
    "        # performs a full training step on the collected batch\n",
    "        # note: no need to mess around with gradients, Keras API handles it\n",
    "        losses = self.model.train_on_batch(states, [acts_and_advs, returns])\n",
    "\n",
    "        #print(losses)\n",
    "\n",
    "\n",
    "## comments\n",
    "    # def train(self, env, batch_sz=32, updates=1000):\n",
    "    #     # storage helpers for a single batch of data\n",
    "    #     actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "    #     rewards, dones, values = np.empty((3, batch_sz))\n",
    "    #     observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "    #     # training loop: collect samples, send to optimizer, repeat updates times\n",
    "    #     ep_rews = [0.0]\n",
    "    #     next_obs = env.reset()\n",
    "    #     for update in range(updates):\n",
    "    #         for step in range(batch_sz):\n",
    "    #             observations[step] = next_obs.copy()\n",
    "    #             actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "    #             next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "    #               ep_rews[-1] += rewards[step]\n",
    "    #             if dones[step]:\n",
    "    #                 ep_rews.append(0.0)\n",
    "    #                 next_obs = env.reset()\n",
    "\n",
    "    #           _, next_value = self.model.action_value(next_obs[None, :])\n",
    "    #         returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "    #         # a trick to input actions and advantages through same API\n",
    "    #         acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "    #         # performs a full training step on the collected batch\n",
    "    #         # note: no need to mess around with gradients, Keras API handles it\n",
    "    #         losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "    #     return ep_rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = ACAgent(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC.model.action_value(np.empty((1,AC.state_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_idx = 0\n",
    "state, reward, done = Env.Step(actions[act_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    next_state, reward, done = Env.Step(actions[act_idx]) \n",
    "    AC.remember(state,act_idx,reward, next_state)\n",
    "    state = next_state\n",
    "    act_idx = AC.model.action_value(state)[0][0]\n",
    "    AC.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raymond 2nd Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Raymond Agent II\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "#Code adapted from http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "#Code adapted from http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "    def call(self, logits):\n",
    "        # sample a random categorical action from given logits\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlp_policy')\n",
    "        # no tf.get_variable(), just simple Keras API\n",
    "\n",
    "        self.core1 = kl.Dense(32, activation='relu')\n",
    "        \n",
    "        self.value1 = kl.Dense(42, activation='relu', name='value1') #64\n",
    "        self.value2 = kl.Dense(42, activation='relu', name='value2')\n",
    "        self.value3 = kl.Dense(1, name='value3')\n",
    "        # logits are unnormalized log probabilities\n",
    "\n",
    "\n",
    "        self.logits1 = kl.Dense(42, activation='relu', name='policy_logits1')\n",
    "        self.logits2 = kl.Dense(42, activation='relu', name='policy_logits2')\n",
    "        self.logits3 = kl.Dense(num_actions, name='policy_logits3')\n",
    "\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "\n",
    "        # This it the core of the model\n",
    "        x = self.core1(x)\n",
    "        \n",
    "        # separate hidden layers from the core\n",
    "        hidden_logs = self.logits1(x)\n",
    "        hidden_logs = self.logits2(hidden_logs)\n",
    "\n",
    "        hidden_vals = self.value1(x)\n",
    "        hidden_vals = self.value2(hidden_vals)\n",
    "\n",
    "        return self.logits3(hidden_logs), self.value3(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # executes call() under the hood\n",
    "        logits, value = self.predict(obs)\n",
    "        action = self.dist.predict(logits)\n",
    "        # a simpler option, will become clear later why we don't use it\n",
    "        # action = tf.random.categorical(logits, 1)\n",
    "        return action , value\n",
    "\n",
    "\n",
    "# An working model training with entropy = 0.00001 en nstep = 32 and learn every step lr = 0.000065 gama = 0.99 \n",
    "class Modelsave1(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlp_policy')\n",
    "\n",
    "        # no tf.get_variable(), just simple Keras API\n",
    "        self.hidden1 = kl.Dense(42, activation='relu')\n",
    "        self.hidden2 = kl.Dense(42, activation='relu')\n",
    "        self.value = kl.Dense(1, name='value')\n",
    "        # logits are unnormalized log probabilities\n",
    "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "\n",
    "        # This it the core of the model\n",
    "       \n",
    "        # separate hidden layers from the core\n",
    "        hidden_logs = self.hidden1(x)\n",
    "        hidden_vals = self.hidden2(x)\n",
    "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # executes call() under the hood\n",
    "        logits, value = self.predict(obs)\n",
    "        action = self.dist.predict(logits)\n",
    "        # a simpler option, will become clear later why we don't use it\n",
    "        # action = tf.random.categorical(logits, 1)\n",
    "        return action , value\n",
    "  \n",
    "    \n",
    "class ACAgent_II:\n",
    "\n",
    "    def __init__(self,\\\n",
    "                 state_size,\\\n",
    "                 action_size,\\\n",
    "                 #ID,\\\n",
    "                 #state_type,\\\n",
    "                 #npa,\\\n",
    "                 n_step_size =32,\\\n",
    "                 gamma = 0.99,\\\n",
    "                 alpha = 0.000065,\\\n",
    "                 entropy = 0.00001 ):\n",
    "                    #Vissim):\n",
    "\n",
    "\n",
    "        print(\"Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \")\n",
    "        # agent type flag \n",
    "        self.type = 'AC'\n",
    "\n",
    "\n",
    "\n",
    "        #just temporary\n",
    "        self.epsilon = 0\n",
    "\n",
    "        self.trainstep = 0\n",
    "\n",
    "        # Model\n",
    "        # hyperparameters for loss terms and Agent\n",
    "        self.params = {'value': 0.5, 'entropy': entropy, 'gamma': gamma}\n",
    "        self.model = Modelsave1(action_size)\n",
    "        self.model.compile(\n",
    "            optimizer=ko.RMSprop(lr=alpha),\n",
    "            # define separate losses for policy logits and value estimate\n",
    "            loss=[self._logits_loss, self._value_loss]\n",
    "        )\n",
    "\n",
    "#         # Agent Junction ID and Controller ID\n",
    "#         self.signal_id = ID\n",
    "#         self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "#         self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Potential actions (compatible phases) and transitions\n",
    "        self.update_counter = 1                                 # Timesteps until next update\n",
    "        if self.action_size == 2:\n",
    "            self.compatible_actions = [[0,1,0,1],[1,0,1,0]]         # Potential actions (compatible phases), 1 means green\n",
    "        elif self.action_size == 8:\n",
    "            self.compatible_actions = [[1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                        [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                        [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                        [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                        [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                        [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                        [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                        [0,0,0,0,1,1,0,0,0,0,1,1]]\n",
    "        else:\n",
    "            raise Exception(\"ERROR: Wrong Action Size. Please review master settings and RLAgents.py\")\n",
    "        # Internal State Traffic Control Variables\n",
    "        self.intermediate_phase = False                         # Boolean indicating an ongoing green-red or red-green transition\n",
    "        self.transition_vector = []                             # Vector that will store the transitions between updates\n",
    "\n",
    "\n",
    "        # Initial Setup of S, A, R, S_\n",
    "        self.state = np.zeros((1,state_size))\n",
    "        self.newstate = np.zeros((1,state_size))\n",
    "        self.action = 0\n",
    "        self.newaction = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        # Metrics Storage Initialization\n",
    "        self.episode_reward = []\n",
    "        self.loss = []\n",
    "        self.queues_over_time = [[0,0,0,0]]\n",
    "        self.accumulated_delay= [0]\n",
    "\n",
    "\n",
    "        # The memory will store (state , action , reward, next_state) in a batch\n",
    "        self.memory = deque(maxlen=n_step_size)\n",
    "        self.n_step_size = n_step_size\n",
    "\n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append([state, action, reward, next_state])\n",
    "\n",
    "    # Update the Junction IDs for the agent\n",
    "    def update_IDS(self, ID, npa):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "    # Need to test before loading to build the graph (surely an other way to do it ...)\n",
    "    def test(self):\n",
    "        _,_ = self.model.action_value(np.empty((1,self.state_size)))\n",
    "\n",
    "\n",
    "    def _value_loss(self, returns, value):\n",
    "        # value loss is typically MSE between value estimates and returns\n",
    "        return self.params['value']*kls.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, acts_and_advs, logits):\n",
    "        # a trick to input actions and advantages through same API\n",
    "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
    "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
    "        # from_logits argument ensures transformation into normalized probabilities\n",
    "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        # policy loss is defined by policy gradients, weighted by advantages\n",
    "        # note: we only calculate the loss on the actions we've actually taken\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "        # entropy loss can be calculated via CE over itself\n",
    "        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n",
    "        # here signs are flipped because optimizer minimizes\n",
    "        return policy_loss - self.params['entropy']*entropy_loss\n",
    "\n",
    "    def _returns_advantages(self, rewards, values, next_value):\n",
    "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # returns are calculated as discounted sum of future rewards\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1]\n",
    "        returns = returns[:-1]\n",
    "        # advantages are returns - baseline, value estimates in our case\n",
    "        advantages = returns - values\n",
    "        return returns, advantages\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        action, _ = self.model.action_value(state)\n",
    "        return np.squeeze(action, axis=-1)\n",
    "\n",
    "    #Performing step of gradient descent on the agent memory\n",
    "    def learn(self):\n",
    "\n",
    "\n",
    "        Sample = np.array(self.memory)\n",
    "\n",
    "        states, actions, rewards, next_state  = np.concatenate(Sample[:,0], axis=0), Sample[:,1].astype('int32'), Sample[:,2], np.concatenate(Sample[:,3], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        _, values = self.model.action_value(states)\n",
    "        values = values.squeeze()\n",
    "\n",
    "        _, next_value  = self.model.action_value(next_state)\n",
    "\n",
    "        next_value = next_value[-1]\n",
    "\n",
    "\n",
    "\n",
    "        returns, advs = self._returns_advantages(rewards, values, next_value)\n",
    "\n",
    "        # a trick to input actions and advantages through same API\n",
    "\n",
    "        acts_and_advs = np.concatenate([actions[:, np.newaxis], advs[:, np.newaxis]], axis=-1)\n",
    "\n",
    "        # performs a full training step on the collected batch\n",
    "        # note: no need to mess around with gradients, Keras API handles it\n",
    "        losses = self.model.train_on_batch(states, [acts_and_advs, returns])\n",
    "\n",
    "        #print(losses)\n",
    "\n",
    "\n",
    "\n",
    "    # def train(self, env, batch_sz=32, updates=1000):\n",
    "    #     # storage helpers for a single batch of data\n",
    "    #     actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "    #     rewards, dones, values = np.empty((3, batch_sz))\n",
    "    #     observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "    #     # training loop: collect samples, send to optimizer, repeat updates times\n",
    "    #     ep_rews = [0.0]\n",
    "    #     next_obs = env.reset()\n",
    "    #     for update in range(updates):\n",
    "    #         for step in range(batch_sz):\n",
    "    #             observations[step] = next_obs.copy()\n",
    "    #             actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "    #             next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "    #               ep_rews[-1] += rewards[step]\n",
    "    #             if dones[step]:\n",
    "    #                 ep_rews.append(0.0)\n",
    "    #                 next_obs = env.reset()\n",
    "\n",
    "    #           _, next_value = self.model.action_value(next_obs[None, :])\n",
    "    #         returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "    #         # a trick to input actions and advantages through same API\n",
    "    #         acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "    #         # performs a full training step on the collected batch\n",
    "    #         # note: no need to mess around with gradients, Keras API handles it\n",
    "    #         losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "    #     return ep_rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = ACAgent_II(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC.model.action_value(np.empty((1,AC.state_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_idx = 0\n",
    "state, reward, done = Env.Step(actions[act_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    next_state, reward, done = Env.Step(actions[act_idx]) \n",
    "    AC.remember(state,act_idx,reward, next_state)\n",
    "    state = next_state\n",
    "    act_idx = AC.model.action_value(state)[0][0]\n",
    "    AC.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raymond Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alvaro DQN\n",
    "\n",
    "Main changes:\n",
    "\n",
    "    -- changed Action function name\n",
    "        \n",
    "    -- get state and get reward have been passed to the simulator\n",
    "    \n",
    "    \n",
    "    -- various default parameters are set \n",
    "    \n",
    "  \n",
    "    -- removed state, npa, type as that is a simulator property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Alvaro DQN agent\n",
    "'''\n",
    "Vissim dependencies are:\n",
    "    ID --    required for \n",
    "                __init__\n",
    "                update_IDS(self, ID, npa)\n",
    "    \n",
    "    npa --   required for same as ID\n",
    "    \n",
    "    Vissim -- is not even used\n",
    "\n",
    "RECOMENDATION: \n",
    "    Keep agents in a dictionary, with key as a generic ID\n",
    "    Run update_IDS as a simulation function\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "self.compatible_actions\n",
    "\n",
    "    -- actions specific to network should not be defined internally. \n",
    "    \n",
    "RECOMENDATION:\n",
    "    compatible_actions -- variable defined at __init__\n",
    "'''\n",
    "\n",
    "'''\n",
    "UNUSED VARIABLES\n",
    "\n",
    "self.accumulated_delay\n",
    "self.intermediate_phase = False                        \n",
    "self.transition_vector = [] \n",
    "Vissim\n",
    "state_type\n",
    "\n",
    "RECOMENDATION:\n",
    "    remove\n",
    "'''\n",
    "\n",
    "'''\n",
    "VARIABLES NEVER UPDATED\n",
    "\n",
    "self.episode_reward = []\n",
    "\n",
    "RECOMMENDATION:\n",
    "    remove\n",
    "'''\n",
    "\n",
    "'''\n",
    "REDUNDANT VARIABLES\n",
    "\n",
    "self.state \n",
    "self.newstate \n",
    "self.action \n",
    "self.newaction \n",
    "self.reward \n",
    "\n",
    "RECOMMENDATION\n",
    "    can keep but is not clear what that are doing\n",
    "'''\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import PER\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.losses as kls\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\\\n",
    "                 state_size,\\\n",
    "                 action_size,\\\n",
    "                 ID,\\\n",
    "                 state_type,\\\n",
    "                 npa,\\\n",
    "                 memory_size,\\\n",
    "                 gamma,\\\n",
    "                 epsilon,\\\n",
    "                 alpha,\\\n",
    "                 copy_weights_frequency,\\\n",
    "                 Vissim,\\\n",
    "                 PER_activated,\\\n",
    "                 DoubleDQN,\\\n",
    "                 Dueling):\n",
    "        \n",
    "        # Agent Junction ID and Controller ID\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Agent Hyperparameters\n",
    "        self.gamma = gamma                  # discount rate\n",
    "        self.epsilon = epsilon              # exploration rate\n",
    "        self.learning_rate = alpha          # learning rate\n",
    "\n",
    "        # Agent Architecture\n",
    "        self.DoubleDQN = DoubleDQN            # Double Deep Q Network Flag\n",
    "        self.Dueling = Dueling                # Dueling Q Networks Flag\n",
    "        self.PER_activated = PER_activated    # Prioritized Experience Replay Flag\n",
    "        self.type = 'DQN'                     # Type of the agent\n",
    "\n",
    "        # Model and target networks\n",
    "        self.copy_weights_frequency = copy_weights_frequency    # Frequency to copy weights to target network\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # Potential actions (compatible phases) and transitions\n",
    "        self.update_counter = 1                                 # Timesteps until next update\n",
    "        if self.action_size == 2:\n",
    "            self.compatible_actions = [[0,1,0,1],[1,0,1,0]]         # Potential actions (compatible phases), 1 means green\n",
    "        elif self.action_size == 4:\n",
    "        \tself.compatible_actions = [[1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                        [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                        [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                        [0,0,0,0,0,0,0,0,0,1,1,1]]\n",
    "        elif self.action_size == 8:\n",
    "            self.compatible_actions = [[1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                        [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                        [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                        [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                        [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                        [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                        [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                        [0,0,0,0,1,1,0,0,0,0,1,1]]\n",
    "        else:\n",
    "            raise Exception(\"ERROR: Wrong Action Size. Please review master settings and RLAgents.py\")\n",
    "\n",
    "        # Internal State Traffic Control Variables\n",
    "        self.intermediate_phase = False                         # Boolean indicating an ongoing green-red or red-green transition\n",
    "        self.transition_vector = []                             # Vector that will store the transitions between updates\n",
    "\n",
    "        # Architecture Debug Messages\n",
    "        if self.DoubleDQN:\n",
    "            if self.Dueling:\n",
    "                print(\"Deploying instance of Dueling Double Deep Q Learning Agent(s)\")\n",
    "            else:\n",
    "                print(\"Deploying instance of Double Deep Q Learning Agent(s)\")\n",
    "        else:\n",
    "            if self.Dueling:\n",
    "                print(\"Deploying instance of Dueling Deep Q Learning Agent(s)\")\n",
    "            else:\n",
    "                print(\"Deploying instance of Standard Deep Q Learning Agent(s)\")\n",
    "\n",
    "        # Initial Setup of S, A, R, S_\n",
    "        self.state = np.zeros((1,state_size))\n",
    "        self.newstate = np.zeros((1,state_size))\n",
    "        self.action = 0\n",
    "        self.newaction = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        # Metrics Storage Initialization\n",
    "        self.episode_reward = []\n",
    "        self.loss = []\n",
    "        self.queues_over_time = [[0,0,0,0]]\n",
    "        self.accumulated_delay= [0]\n",
    "        \n",
    "        if self.PER_activated:\n",
    "            # If PER_activated spawn BinaryTree and Memory object to store priorities and experiences\n",
    "            self.memory = PER.Memory(memory_size)\n",
    "        else:\n",
    "            # Else use the deque structure to only store experiences which will be sampled uniformly\n",
    "            self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "    # Update the Junction IDs for the agent\n",
    "    def update_IDS(self, ID, npa):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "    \n",
    "    # Agent Neural Network definition\n",
    "    def _build_model(self):\n",
    "        if self.Dueling:\n",
    "            # Architecture for the Neural Net in the Dueling Deep Q-Learning Model\n",
    "            #model = Sequential()\n",
    "            input_layer = Input(shape = (self.state_size,))\n",
    "            dense1 = Dense(64, input_dim=self.state_size, activation='relu')(input_layer)\n",
    "            #dense2 = Dense(48, activation='relu')(dense1)\n",
    "            #flatten = Flatten()(dense2)\n",
    "            fc1 = Dense(48)(dense1)\n",
    "            dueling_actions = Dense(self.action_size)(fc1)\n",
    "            fc2 = Dense(48)(dense1)\n",
    "            dueling_values = Dense(1)(fc2)\n",
    "\n",
    "            def dueling_operator(duel_input):\n",
    "                duel_v = duel_input[0]\n",
    "                duel_a = duel_input[1]\n",
    "                return (duel_v + (duel_a - K.mean(duel_a, axis = 1, keepdims = True)))\n",
    "\n",
    "            policy = Lambda(dueling_operator, name = 'policy')([dueling_values, dueling_actions])\n",
    "            model = Model(inputs=[input_layer], outputs=[policy])\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            return(model)\n",
    "        else:\n",
    "            # Architecture for the Neural Net in Deep-Q learning Model (also Double version)\n",
    "            model = Sequential()\n",
    "            model.add(Dense(42, input_dim=self.state_size, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(Dense(42, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(Dense(42, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(Dense(self.action_size, activation='linear',kernel_regularizer=regularizers.l2(0.01)))\n",
    "            \n",
    "            #model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate, epsilon =1.5*10**-4))\n",
    "            return model\n",
    "    \n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        if self.PER_activated:\n",
    "            experience = np.array([state, action, reward, next_state])\n",
    "            self.memory.store(experience)\n",
    "        else:\n",
    "            self.memory.append([state, action, reward, next_state])\n",
    "    \n",
    "    # Choosing actions\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "            #print('Chosen Random Action {}'.format(action+1))\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            action = np.argmax(act_values[0])\n",
    "            #print('Chosen Not-Random Action {}'.format(action+1))\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sample a batch of \"batch_size\" experiences and perform 1 step of gradient descent on all of them simultaneously\n",
    "    def learn_batch(self, batch_size, episode):\n",
    "        state_vector = []\n",
    "        target_f_vector = []\n",
    "        absolute_errors = [] \n",
    "\n",
    "        if self.PER_activated:\n",
    "            tree_idx, minibatch, ISWeights_mb = self.memory.sample(batch_size)\n",
    "        else:\n",
    "            idx = np.random.randint(len(self.memory), size=batch_size,dtype=int)\n",
    "            minibatch = np.array(self.memory)[idx]\n",
    "        \n",
    "        \n",
    "        state, action, reward, next_state = np.concatenate(minibatch[:,0], axis=0 ), minibatch[:,1].astype('int32') ,minibatch[:,2].reshape(batch_size,1), np.concatenate( minibatch[:,3] , axis=0 )\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.DoubleDQN:\n",
    "            next_action = np.argmax(self.model.predict(np.reshape(next_state,(batch_size,self.state_size))), axis=1)\n",
    "            target = reward + self.gamma * self.target_model.predict(np.reshape(next_state,(batch_size,self.state_size)))[np.arange(batch_size),next_action].reshape(batch_size,1)\n",
    "        else:\n",
    "            # Fixed Q-Target\n",
    "            target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(batch_size,self.state_size))),axis=1).reshape(batch_size,1)\n",
    "            print(target.shape)\n",
    "            # No fixed targets version\n",
    "            # target = reward + self.gamma * np.max(self.model.predict(np.reshape(next_state,(1,self.state_size))))    \n",
    "        \n",
    "            \n",
    "        # There should be a way to vectorize this\n",
    "        #for state, action, reward, next_state in minibatch:\n",
    "        #    if self.DoubleDQN:\n",
    "        #        next_action = np.argmax(self.model.predict(np.reshape(next_state,(1,self.state_size))), axis=1)\n",
    "        #        target = reward + self.gamma * self.target_model.predict(np.reshape(next_state,(1,self.state_size)))[0][next_action][0]\n",
    "        #    else:\n",
    "                # Fixed Q-Target\n",
    "        #        target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "                # No fixed targets version\n",
    "                # target = reward + self.gamma * np.max(self.model.predict(np.reshape(next_state,(1,self.state_size))))\n",
    "\n",
    "        # This section incorporates the reward into the prediction and calculates the absolute error between old and new\n",
    "        target_f = self.model.predict(state)\n",
    "        \n",
    "        absolute_errors = np.abs(target_f[np.arange(batch_size),action].reshape(batch_size,1)-target)\n",
    "        \n",
    "        #absolute_errors.append(abs(target_f[0][action] - target))\n",
    "        \n",
    "        target_f[np.arange(batch_size),action] = target.reshape(batch_size)\n",
    "        \n",
    "        \n",
    "        #self.model.fit(state_matrix, target_f_matrix, epochs=1, verbose=0)\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=2,batch_size=batch_size)\n",
    "        \n",
    "        self.loss.append(self.model.history.history['loss'])\n",
    "\n",
    "        if self.PER_activated:\n",
    "            #Update priority\n",
    "            self.memory.batch_update(tree_idx, absolute_errors)\n",
    "\n",
    "        # Copy weights every \"copy_weights_frequency\" episodes\n",
    "        #if (episode+1) % self.copy_weights_frequency == 0 and episode != 0:\n",
    "        #    self.copy_weights()   \n",
    "\n",
    "    # Copy weights function\n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(\"Weights succesfully copied to Target model.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "COMMENTS ON ALVARO MAIN FILE\n",
    "\n",
    "action_type \n",
    "    -- where is this used\n",
    "    \n",
    "agents_deployed\n",
    "    -- is this needed?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "PER_activated = True\n",
    "batch_size = 64\n",
    "memory_size = 1024\n",
    "alpha   = 0.000065\n",
    "gamma   = 0.95\n",
    "\n",
    "## State-Action Parameters\n",
    "state_size = 4\n",
    "action_size = 8\n",
    "\n",
    "agent_type = 'DuelingDDQN'        # DQN, DuelingDQN, DDQN, DuelingDDQN\n",
    "reward_type = 'Queues'   \n",
    "# 'Queues'          Sum of the queues for all lanes in intersection\n",
    "# 'QueuesDiff'      Difference in queue lengths in last timestep\n",
    "# 'QueuesDiffSC'    10000* QueuesDiff - Queues^2\n",
    "# 'TotalDelayDiff'\n",
    "\n",
    "state_type  = 'Queues'    # 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig'\n",
    "Random_Seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NParser import NetworkParser\n",
    "npa = NetworkParser(Vissim)\n",
    "######################################################################################################################\n",
    "## Nested data structure:\n",
    "## \n",
    "## Signal Controllers = signal_controllers[signal_controller_ids]\n",
    "## Signal Groups      = signal_groups     [signal_controller_ids] [signal_group_id]\n",
    "## Signal Heads       = signal_heads      [signal_controller_ids] [signal_heads_id]\n",
    "## Lanes              = lanes             [signal_controller_ids] [signal_heads_id] [lane_id]\n",
    "##\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "episodes = 4\n",
    "\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "if exploration_schedule == \"linear\":\n",
    "    epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "    epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "    epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "elif exploration_schedule == \"geometric\":\n",
    "    epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "    epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "else:\n",
    "    print(\"ERROR: Unrecognized choice of exploration schedule.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_weights_frequency = 2\n",
    "\n",
    "Agents = [DQNAgent(state_size,\\\n",
    "                   action_size,\\\n",
    "                   ID,\\\n",
    "                   state_type,\\\n",
    "                   npa,\\\n",
    "                   memory_size,\\\n",
    "                   gamma,\\\n",
    "                   epsilon_sequence[0],\\\n",
    "                   alpha,\\\n",
    "                   copy_weights_frequency,\\\n",
    "                   Vissim, PER_activated,\\\n",
    "                   DoubleDQN = True if agent_type == \"DDQN\" or agent_type ==\"DuelingDDQN\" else False,\\\n",
    "                   Dueling = False if agent_type == \"DQN\" or agent_type == \"DDQN\" else True)\\\n",
    "          for ID in npa.signal_controllers_ids] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "demand_list \n",
    "    -- why do I need the demand list to populate the memory? \n",
    "'''\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":600, 'm':300, 'l':150}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "seconds_per_green \n",
    "    -- do we need to control with \n",
    "    -- why is it used twice in populate_memory\n",
    "'''\n",
    "\n",
    "import Simulator_Functions as SF\n",
    "timesteps_per_second = 1\n",
    "Session_ID = 'BLAH'\n",
    "seconds_per_green = 6\n",
    "\n",
    "\n",
    "SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "memory, Agents, runflag = SF.prepopulate_memory(Agents, Vissim, state_type, reward_type, state_size, memory_size,\\\n",
    "                                                        vissim_working_directory, model_name, Session_ID,\\\n",
    "                                                        seconds_per_green, seconds_per_green, timesteps_per_second,\\\n",
    "                                                        demand_list, demand_change_timesteps, PER_activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "log_prgess \n",
    "    -- why do I need this?\n",
    "    \n",
    "COMServerReload\n",
    "    -- why do I need this?\n",
    "    \n",
    "mode \n",
    "    -- why do a need a mode flag in the internals here\n",
    "'''\n",
    "\n",
    "from COMServer import COMServerReload\n",
    "from Utilities import log_progress, pltlive\n",
    "\n",
    "simulation_length = 3600*1 + 1\n",
    "mode = 'training'\n",
    "seconds_per_yellow = 3.\n",
    "\n",
    "reward_storage = []\n",
    "best_agent_weights = []\n",
    "best_agent_memory = []\n",
    "reward_plot = np.zeros([episodes,])\n",
    "loss_plot = np.zeros([episodes,])\n",
    "\n",
    "SaveResultsAgent = False\n",
    "\n",
    "# Iterations of the simulation\n",
    "for episode in log_progress(range(episodes), every=1):\n",
    "\n",
    "    # Reload map if it has already been run (previous episode or prepopulation)\n",
    "    if episode !=0 or runflag == True:\n",
    "        Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                              simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "        # Run Network Parser and ensure agents are linked to their intersections\n",
    "        npa = NetworkParser(Vissim)\n",
    "        for index, agent in enumerate(Agents):\n",
    "            agent.update_IDS(agent.signal_id, npa)\n",
    "            agent.episode_reward = []\n",
    "\n",
    "    # Change the random seed\n",
    "    Random_Seed += 1\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "\n",
    "    # Run Episode at maximum speed\n",
    "    SF.Select_Vissim_Mode(Vissim, mode)\n",
    "    SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                              seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                              PER_activated)\n",
    "\n",
    "    # Calculate episode average reward\n",
    "    reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "    best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                          best_agent_weights, best_agent_memory,\\\n",
    "                                                          vissim_working_directory, model_name, Agents, Session_ID)\n",
    "\n",
    "\n",
    "    # Train agent with experience of episode and copy weights when necessary\n",
    "    # Update exploration rate\n",
    "    for agent in Agents: \n",
    "        for _ in range(5):\n",
    "            agent.learn_batch(batch_size, episode)\n",
    "    # Copy weights \n",
    "        if (episode+1) % agent.copy_weights_frequency == 0 and episode != 0:\n",
    "            agent.copy_weights()\n",
    "        agent.epsilon = epsilon_sequence[episode+1]\n",
    "\n",
    "    # Security save for long trainings\n",
    "    if SaveResultsAgent:\n",
    "        if (episode+1)%partial_save_at == 0:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "\n",
    "#Saving agents memory, weights and optimizer\n",
    "if SaveResultsAgent:\n",
    "    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "    print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "    Succesfully Terminated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF.green_red_to_amber(Agents[0], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.SimulationStep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Potential actions (compatible phases) and transitions\n",
    "        self.update_counter = 1                                 # Timesteps until next update\n",
    "        if self.action_size == 2:\n",
    "            self.compatible_actions = [[0,1,0,1],[1,0,1,0]]         # Potential actions (compatible phases), 1 means green\n",
    "        elif self.action_size == 4:\n",
    "        \tself.compatible_actions = [[1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                        [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                        [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                        [0,0,0,0,0,0,0,0,0,1,1,1]]\n",
    "        elif self.action_size == 8:\n",
    "            self.compatible_actions = [[1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                        [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                        [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                        [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                        [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                        [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                        [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                        [0,0,0,0,1,1,0,0,0,0,1,1]]\n",
    "        else:\n",
    "            raise Exception(\"ERROR: Wrong Action Size. Please review master settings and RLAgents.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     34
    ]
   },
   "outputs": [],
   "source": [
    "# DQN-LITE\n",
    "\n",
    "'''\n",
    "This is a light-weight version of Alvaro's DQN agent\n",
    "'''\n",
    "\n",
    "'''\n",
    "MAIN CHANGES:\n",
    "    -- is independent of network parser\n",
    "    -- no junction ID is used \n",
    "    -- no dependence on VISSIM\n",
    "    -- actions list is not maintained.\n",
    "    -- separate agents for DDQN, PER etc...\n",
    "'''\n",
    "\n",
    "PER_activated = True\n",
    "batch_size = 64\n",
    "memory_size = 1024\n",
    "alpha   = 0.000065\n",
    "gamma   = 0.95\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import PER\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.losses as kls\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\\\n",
    "                 epsilon,\\\n",
    "                 alpha = 0.000065,\\\n",
    "                 state_size=4,\\\n",
    "                 action_size=2,\\\n",
    "                 memory_size = 1024,\\\n",
    "                 gamma = 0.95,\\\n",
    "                 copy_weights_frequency =2\n",
    "                ):\n",
    "        \n",
    "\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Agent Hyperparameters\n",
    "        self.gamma = gamma                  # discount rate\n",
    "        self.epsilon = epsilon              # exploration rate\n",
    "        self.learning_rate = alpha          # learning rate\n",
    "\n",
    "        # Agent Architecture\n",
    "        self.DoubleDQN = DoubleDQN            # Double Deep Q Network Flag\n",
    "        self.Dueling = Dueling                # Dueling Q Networks Flag\n",
    "        self.PER_activated = PER_activated    # Prioritized Experience Replay Flag\n",
    "        self.type = 'DQN'                     # Type of the agent\n",
    "\n",
    "        # Model and target networks\n",
    "        self.copy_weights_frequency = copy_weights_frequency    # Frequency to copy weights to target network\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    # Agent Neural Network definition\n",
    "    def _build_model(self):\n",
    "            # Architecture for the Neural Net in Deep-Q learning Mode\n",
    "            model = Sequential()\n",
    "            model.add(Dense(42, input_dim=self.state_size, activation='tanh',kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(Dense(42, activation='tanh',kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(Dense(42, activation='tanh',kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(Dense(self.action_size, activation='linear',kernel_regularizer=regularizers.l2(0.01)))\n",
    "            \n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state):\n",
    "            self.memory.append([state, action, reward, next_state])\n",
    "    \n",
    "    # Choosing actions\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            action = np.argmax(act_values[0])\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sample a batch of \"batch_size\" experiences and perform 1 step of gradient descent on all of them simultaneously\n",
    "    def learn_batch(self, episode, batch_size):\n",
    "        state_vector = []\n",
    "        target_f_vector = []\n",
    "        absolute_errors = [] \n",
    "\n",
    "        idx = np.random.randint(len(self.memory), size=batch_size,dtype=int)\n",
    "        minibatch = np.array(self.memory)[idx]\n",
    "        \n",
    "        state, action, reward, next_state = np.concatenate(minibatch[:,0], axis=0 ), minibatch[:,1].astype('int32') ,minibatch[:,2].reshape(batch_size,1), np.concatenate( minibatch[:,3] , axis=0 )\n",
    "        \n",
    "        target = reward + self.gamma * np.max(self.target_model.predict(np.reshape(next_state,(batch_size,self.state_size))),axis=1).reshape(batch_size,1)\n",
    "\n",
    "        # This section incorporates the reward into the prediction and calculates the absolute error between old and new\n",
    "        target_f = self.model.predict(state)\n",
    "        \n",
    "        target_f[np.arange(batch_size),action] = target.reshape(batch_size)\n",
    "        \n",
    "        \n",
    "        #self.model.fit(state_matrix, target_f_matrix, epochs=1, verbose=0)\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=2,batch_size=batch_size)\n",
    "        \n",
    "        self.loss.append(self.model.history.history['loss'])\n",
    "\n",
    "#         if self.PER_activated:\n",
    "#             #Update priority\n",
    "#             self.memory.batch_update(tree_idx, absolute_errors)\n",
    "\n",
    "        # Copy weights every \"copy_weights_frequency\" episodes\n",
    "        #if (episode+1) % self.copy_weights_frequency == 0 and episode != 0:\n",
    "        #    self.copy_weights()   \n",
    "\n",
    "    # Copy weights function\n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(\"Weights succesfully copied to Target model.\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DQN_Environment_Single_Cross():\n",
    "    def __init__(self,Vissim,npa):\n",
    "        self.Vissim = Vissim\n",
    "        self.tsars_dict = defaultdict(defaultdict(int))\n",
    "        for ID in \n",
    "        \n",
    "        self.action_dict =dict()\n",
    "    \n",
    "    def step(self): \n",
    "        self.Vissim.Simulation.RunSingleStep()\n",
    "        \n",
    "    def # We should script those function later because it is only for the basic intersection here\n",
    "    \n",
    "    \n",
    "    def green_red_to_amber(agent, seconds_per_yellow,Surtrac=False):\n",
    "        # Fetch the meaning of the Actions from the compatible Actions in the Agent\n",
    "        previous_action = agent.compatible_actions[agent.action]\n",
    "        #print(\"Previous action {}:\".format(agent.action+1) + str(previous_action))\n",
    "        current_action = agent.compatible_actions[agent.newaction]\n",
    "        #print(\"Current action {}:\".format(agent.newaction+1) + str(current_action))\n",
    "\n",
    "        # Check transition vector for the whole intersection (1, 0 or -1)\n",
    "        agent.transition_vector = np.subtract(previous_action, current_action)\n",
    "        #print(\"Transition vect:\" + str(agent.transition_vector))\n",
    "\n",
    "        # Cycle through the groups and start the transition\n",
    "        for index_group, sig_group in enumerate(agent.signal_groups):\n",
    "            # If the transition vector is > 0, we are changing from GREEN to RED, so set AMBER\n",
    "            if agent.transition_vector[index_group] == 1:\n",
    "                sig_group.SetAttValue(\"SigState\", \"AMBER\")\n",
    "                #print(\"Changing Light {} to Red\".format(index_group+1))\n",
    "\n",
    "            # If the transition vector is < 0, we are changing from RED to GREEN, so set to REDAMBER\n",
    "            elif agent.transition_vector[index_group] == -1:\n",
    "                #sig_group.SetAttValue(\"SigState\", \"REDAMBER\")\n",
    "                #print(\"Changing Light {} to Green\".format(index_group+1))\n",
    "                pass\n",
    "            # If the transition vector is zero, the phase stays the same\n",
    "            elif agent.transition_vector[index_group] == 0:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"ERROR: Incongruent new phase and previous phase. Please review the code.\")\n",
    "\n",
    "        # Extend timer after transition is started\n",
    "        agent.update_counter += seconds_per_yellow\t - 1\n",
    "        if Surtrac:\n",
    "            agent.actiontime += - seconds_per_yellow                        \n",
    "        # Record that a transition is happening\n",
    "        agent.intermediate_phase = True\n",
    "\n",
    "    def amber_to_green_red(agent, seconds_per_green,Surtrac=False):\n",
    "        # Finalize the change\n",
    "        for index_group, sig_group in enumerate(agent.signal_groups):\n",
    "            # Use transition vector from previous iteration to finish the change\n",
    "            if agent.transition_vector[index_group] == 1:\n",
    "                sig_group.SetAttValue(\"SigState\", \"RED\")\n",
    "            elif agent.transition_vector[index_group] == -1:\n",
    "                sig_group.SetAttValue(\"SigState\", \"GREEN\")\n",
    "            elif agent.transition_vector[index_group] == 0:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"ERROR: Incongruent new phase and previous phase. Please review the code.\")\n",
    "\n",
    "        # Mark the transition as finished\n",
    "        agent.intermediate_phase = False\t\n",
    "        # Set timer for next update \n",
    "        if Surtrac :\n",
    "            agent.update_counter += agent.actiontime - 1\t\t\n",
    "        else :\n",
    "            agent.update_counter += seconds_per_green - 1\n",
    "    \n",
    "    def _calculate_state(Vissim, state_size):\n",
    "            #Obtain Queue Values (average value over the last period)\n",
    "            West_Queue  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)')\n",
    "            South_Queue = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)')\n",
    "            East_Queue  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)')\n",
    "            North_Queue = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)')\n",
    "            state = [West_Queue, South_Queue, East_Queue, North_Queue]\n",
    "            state = [0. if state is None else state for state in state]\n",
    "            state = np.reshape(state, [1,state_size])\n",
    "            return(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Control Unit and Cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# SCU\n",
    "\n",
    "'''\n",
    "Calculate state and reward needs updating\n",
    "    -- multi-junction case this needs updating\n",
    "'''\n",
    "\n",
    "# Compatible actions is now a dictionary \n",
    "compatible_actions = {0 : [0,1,0,1],\\\n",
    "                      1 : [1,0,1,0]}  \n",
    "\n",
    "\n",
    "'''\n",
    "Signal_Control_Unit :\n",
    "    \n",
    "    interfaces between a signal controller (at a junction) and actions (provided by an agent)\n",
    "    \n",
    "    inputs:\n",
    "    -- Vissim\n",
    "    -- Signal_Controller - a Vissim, signal controller\n",
    "    -- compatible_actions - a dictionary taking IDs to vectors non-conflicting signal groups\n",
    "    -- green_time = 10   (times are in seconds but converted to simulation steps)\n",
    "    -- redamber_time = 1\n",
    "    -- amber_time = 3\n",
    "    -- red_time = 1\n",
    "     \n",
    "    methods :\n",
    "    -- action_update():\n",
    "            initiates the change to a new action\n",
    "            \n",
    "            inputs:\n",
    "            -- action_key \n",
    "            -- green_time \n",
    "    -- update():\n",
    "            ensures each signal is correct at each simulation step\n",
    "'''\n",
    "class Signal_Control_Unit:\n",
    "    \n",
    "    def __init__(self,\\\n",
    "                 Vissim,\\\n",
    "                 Signal_Controller,\\\n",
    "                 compatible_actions,\\\n",
    "                 Signal_Groups = None,\\\n",
    "                 green_time = 40,\\\n",
    "                 redamber_time = 1,\\\n",
    "                 amber_time = 3, \\\n",
    "                 red_time = 1\\\n",
    "                ):\n",
    "        \n",
    "        # get Vissim, signal controller and its signal groups\n",
    "        self.Vissim = Vissim\n",
    "        self.signal_controller = Signal_Controller\n",
    "        \n",
    "        if Signal_Groups is None :\n",
    "            self.signal_groups = self.signal_controller.SGs\n",
    "        else :\n",
    "            self.signal_groups = Signal_Groups\n",
    "            \n",
    "        # get stae and reward parameters\n",
    "        self.state = self.calculate_state()\n",
    "        self.reward = self.calculate_reward()  \n",
    "       \n",
    "        self.compatible_actions = compatible_actions\n",
    "          \n",
    "        self.time_steps_per_second = self.Vissim.Simulation.AttValue('SimRes')\n",
    "        \n",
    "        self.green_time = green_time * self.time_steps_per_second\n",
    "        self.redamber_time = redamber_time * self.time_steps_per_second\n",
    "        self.amber_time = amber_time * self.time_steps_per_second\n",
    "        self.red_time = red_time * self.time_steps_per_second\n",
    "    \n",
    "        # implement 1st action to start\n",
    "        self.action_key = 0   # dict key of current action (we start with 0) \n",
    "        self.action_required = False # used to requests an action from agent\n",
    "        self.update_counter = 0\n",
    "        self.intermediate_phase = True # tracks when initiating a new action\n",
    "        self.action_update(self.action_key)    \n",
    "        \n",
    "\n",
    "        self.stage = \"Green\" # tracks the stage particularly when in intermediate phase.\n",
    "                             # Stages appear in order: \"Amber\" -> \"Red\" -> \"RedAmber\" -> \"Green\"\n",
    "\n",
    "            \n",
    "    '''\n",
    "    sars :\n",
    "    returns state, id of action, reward\n",
    "    '''     \n",
    "    def sars(self):\n",
    "        self.state = self.calculate_state()\n",
    "        self.reward = self.calculate_reward()\n",
    "        \n",
    "        return self.state, self.action_key, self.reward\n",
    "\n",
    "    \n",
    "    '''\n",
    "    calculate_state:\n",
    "    Alvaro's reward function needs to be more general\n",
    "    '''\n",
    "    def calculate_state(self,length=None,verbose = False):\n",
    "        \n",
    "        Queues = []\n",
    "        Lanes = []\n",
    "        for sg in self.signal_groups :\n",
    "            q = 0 \n",
    "            for sh in sg.SigHeads:\n",
    "                if (sh.Lane.AttValue('Link'),sh.Lane.AttValue('Index')) not in Lanes :\n",
    "                    Lanes.append((sh.Lane.AttValue('Link'),sh.Lane.AttValue('Index')))\n",
    "                    for veh in sh.Lane.Vehs:\n",
    "                        q += veh.AttValue('InQueue')\n",
    "            Queues.append(q)\n",
    "            # Summarize queue size in each lane\n",
    "            if verbose :\n",
    "                print(self.signal_controller.AttValue('No'),sg.AttValue('No'),q)\n",
    "            \n",
    "        # now reshape\n",
    "        if length is not None :\n",
    "            state = np.reshape(Queues,[1,length])\n",
    "        else :\n",
    "            state = np.reshape(Queues,[1,len(Queues)])\n",
    "        \n",
    "        return (state)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    calculate_reward:\n",
    "    Alvaro's reward function needs to be more general\n",
    "    '''\n",
    "    def calculate_reward(self):\n",
    "        state = self.calculate_state()\n",
    "        reward = -np.sum(state)\n",
    "        \n",
    "        return reward\n",
    " \n",
    "\n",
    "    '''\n",
    "    action_update :\n",
    "    initiates a new action\n",
    "        inputs:\n",
    "        -- id of action\n",
    "        -- green_time, if specified by agent (in seconds)\n",
    "    '''    \n",
    "    def action_update(self,action_key,green_time=None):\n",
    "        self.intermediate_phase = True # initate intermediate_phase\n",
    "        self.update_counter = 0 # set update counter zero (will get reset at self.update() )\n",
    "        self.action_key = action_key\n",
    "        self.current_action = self.compatible_actions[action_key] \n",
    "        self.new_colors = [ 2*val for val in self.current_action] # converts action to 0,1,2 range\n",
    "        \n",
    "        if green_time is not None:\n",
    "            self.green_time = green_time * self.time_steps_per_second\n",
    "        \n",
    "\n",
    "    # internal helper function\n",
    "    # red = 0, amber/redamber = 1 and green = 2\n",
    "    def _color_convert(self,color):\n",
    "        if color == \"RED\" :\n",
    "            return 0\n",
    "        elif color == \"GREEN\" :\n",
    "            return 2\n",
    "        else :\n",
    "            return 1\n",
    "\n",
    "        \n",
    "    '''\n",
    "    _color_changer :\n",
    "    Internal function\n",
    "    Changes color of a signal group\n",
    "        inputs:\n",
    "        -- signal group\n",
    "        -- new_color : 2 = green / 0 = red\n",
    "        -- stage : what stage all lights in the controller are.\n",
    "    '''          \n",
    "    def _color_changer(self,signal_group,new_color,stage):\n",
    "        #Get the current color\n",
    "        current_color = self._color_convert(signal_group.AttValue(\"SigState\"))\n",
    "        change = new_color-current_color\n",
    "        \n",
    "        # want green but currently red\n",
    "        if change == -2 and stage == \"Green\" :\n",
    "            signal_group.SetAttValue(\"SigState\", \"AMBER\")\n",
    "        \n",
    "        # want red but currently amber\n",
    "        # if just gone red need on second before green change\n",
    "        elif change == -1 and stage == \"Amber\" :\n",
    "            signal_group.SetAttValue(\"SigState\", \"RED\")\n",
    "        \n",
    "        # want green but currently red \n",
    "        elif change == 2 and stage == \"Red\" :\n",
    "            signal_group.SetAttValue(\"SigState\", \"REDAMBER\")\n",
    "                \n",
    "        # want green but currently redamber\n",
    "        elif change == 1 and stage == \"RedAmber\":\n",
    "            signal_group.SetAttValue(\"SigState\", \"GREEN\")\n",
    "        \n",
    "        # if both red or green pass (i.e. no change keep green)\n",
    "        elif change == 0 :\n",
    "            pass\n",
    "    \n",
    "\n",
    "    '''\n",
    "    _stage_changer :\n",
    "    Internal function\n",
    "    \n",
    "    Track controllers stage (in the stages of Amber->Red->RedAmber-Green) \n",
    "    and time for each transtion\n",
    "    \n",
    "        inputs:\n",
    "        -- stage\n",
    "        \n",
    "    Nb. stage is a controller method while color is a sg property\n",
    "    '''\n",
    "    def _stage_changer(self,stage):\n",
    "        \n",
    "        if stage == \"Green\" :\n",
    "            time = self.amber_time\n",
    "            self.stage = \"Amber\" \n",
    "    \n",
    "        elif stage == \"Amber\" :\n",
    "            time = self.red_time\n",
    "            self.stage = \"Red\"\n",
    "        \n",
    "        elif stage == \"Red\" :\n",
    "            time = self.redamber_time\n",
    "            self.stage = \"RedAmber\"\n",
    "                \n",
    "        # want green but currently redamber\n",
    "        elif stage == \"RedAmber\" :\n",
    "            time = self.green_time\n",
    "            self.stage = \"Green\"\n",
    "        \n",
    "        return time\n",
    "                \n",
    "        \n",
    "    '''\n",
    "    update :\n",
    "    \n",
    "    returns True if action required (otherwise is None)\n",
    "    \n",
    "    implements cycle at each signal group\n",
    "    and updates the stage of the controllers.\n",
    "    \n",
    "    (writen so multiple controllers can be updated in parallel)\n",
    "    (Computational Overhead should be lower than before)\n",
    "    \n",
    "    '''   \n",
    "    def update(self):\n",
    "    \n",
    "        self.update_counter -= 1\n",
    "        \n",
    "        # These 'if' clauses mean update computation only happens if needed\n",
    "        if self.update_counter <= 0 :\n",
    "            # if update counter just went zero \n",
    "            # then ask for an action \n",
    "            if self.intermediate_phase is False :\n",
    "                self.action_required = True \n",
    "                return self.action_required\n",
    "                    \n",
    "            # if during a change\n",
    "            # then make the change\n",
    "            if self.intermediate_phase is True : \n",
    "                self.action_reqired = False\n",
    "                \n",
    "                # Get light color right for each signal group\n",
    "                for sg in self.signal_groups :\n",
    "                    ID = sg.AttValue('No')-1\n",
    "                    self._color_changer(sg,self.new_colors[ID],self.stage)\n",
    "                        \n",
    "                # change the current stage and get time the stage last for\n",
    "                time = self._stage_changer(self.stage)\n",
    "                self.update_counter = time\n",
    "                    \n",
    "                # if full transition (Amber->Red->RedAmber-Green) to green done  \n",
    "                if self.stage == \"Green\" :\n",
    "                    self.intermediate_phase = False # record current action is implemented\n",
    "                 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Basic Simulation Environment\n",
    "'''\n",
    "It is assumed that vissim is loaded\n",
    "'''\n",
    "\n",
    "class Simulation_Environment():\n",
    "    def __init__(self,\\\n",
    "                 Loaded_Vissim,\\\n",
    "                 sim_length=3601,\\\n",
    "                 timesteps_per_second=12,\\\n",
    "                 Quick_Mode=True):\n",
    "        \n",
    "        self.Vissim = Loaded_Vissim\n",
    "        self.Quick_Mode(Quick_Mode)\n",
    "            \n",
    "        \n",
    "    def Step(self,action,sim_steps=10):\n",
    "        \n",
    "        state = _calculate_state(self)\n",
    "        reward = _calculate_reward(self)\n",
    "    \n",
    "        return  state, reward, done\n",
    "        \n",
    "    def _calculate_state(self):\n",
    "        pass\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        pass\n",
    "    \n",
    "    def Quick_Mode(self,Quick_Mode):\n",
    "        self.Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",Quick_Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Env_SPU(Simulation_Environment):\n",
    "    def __init__(self,\\\n",
    "                 Loaded_Vissim,\\\n",
    "                 sim_length=3601,\\\n",
    "                 timesteps_per_second=1,\\\n",
    "                ):\n",
    "    \n",
    "        # Load init from Parent Class\n",
    "        super(Env_SPU,self).__init__(\n",
    "                 Loaded_Vissim,\\\n",
    "                 sim_length,\\\n",
    "                 timesteps_per_second,\\\n",
    "                 delete_results,\\\n",
    "                 verbose)\n",
    "        \n",
    "            def __init__(self,\\\n",
    "                 Vissim,\\\n",
    "                 Signal_Controller,\\\n",
    "                 compatible_actions,\\\n",
    "                 Signal_Groups = None,\\\n",
    "                 green_time = 40,\\\n",
    "                 redamber_time = 1,\\\n",
    "                 amber_time = 3, \\\n",
    "                 red_time = 1\\\n",
    "                ):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triple Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load Network\n",
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "#vissim_working_directory = 'C:\\\\Users\\\\acabrejasegea\\\\OneDrive - The Alan Turing Institute\\\\Desktop\\\\ATI\\\\0_TMF\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "compatible_actions = { 0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                       1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                       2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                       3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                       4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                       5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                       6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                       7 : [0,0,0,0,1,1,0,0,0,0,1,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MANUAL LOAD (part 2)\n",
    "input_file = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\Single_Cross_Triple\\\\Single_Cross_Triple.inpx'\n",
    "Vissim.LoadNet(input_file)\n",
    "Env = SE.Q_Sim_Env(Vissim,model_name,vissim_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "CC = Cyclic_Control(8)\n",
    "\n",
    "SPU = Signal_Control_Unit(Vissim,\\\n",
    "                          Vissim.Net.SignalControllers[0],\\\n",
    "                          compatible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Main loop\n",
    "for _ in range(500):  \n",
    "    SCU.Vissim.Simulation.RunSingleStep()\n",
    "    action_required = SCU.update()\n",
    "    if action_required :\n",
    "        new_action = CC.choose_action()\n",
    "        SCU.action_update(new_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "# Reload Vissim\n",
    "for _ in range(5):\n",
    "    try:\n",
    "        Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "        print('success!')\n",
    "        break\n",
    "    except:\n",
    "        print('fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load Network\n",
    "#input_file = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\Balance\\\\Balance.inpx'\n",
    "#input_file = 'C:\\\\Users\\\\Public\\\\Documents\\\\PTV Vision\\\\PTV Vissim 11\\\\Examples Training\\\\Signal Control\\\\UTC - Workflow PTV Balance PTV Epics\\\\03 AFTER In PTV Vissim\\\\PTV Balance PTV Epics Vision Suite Workflow.inpx'\n",
    "input_file = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\NSW\\\\Balance_NSW\\\\Balance.inpx'\n",
    "Vissim.LoadNet(input_file)\n",
    "#Env = SE.Q_Sim_Env(Vissim,model_name,vissim_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Simulation_Environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-352-38faec962a11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m '''\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mEnv_MultiAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSimulation_Environment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     def __init__(self,\\\n\u001b[0;32m      8\u001b[0m                  \u001b[0mLoaded_Vissim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Simulation_Environment' is not defined"
     ]
    }
   ],
   "source": [
    "# Environment Class\n",
    "'''\n",
    "THIS IS A WORK IN PROGRESS MULTIAGENT SIMULATION ENVIRONMENT\n",
    "'''\n",
    "\n",
    "class Env_MultiAgent(Simulation_Environment):\n",
    "    def __init__(self,\\\n",
    "                 Loaded_Vissim,\\\n",
    "                 Controllers_Actions,\\\n",
    "                 Signal_Controllers=None,\\\n",
    "                 sim_length=3601,\\\n",
    "                 timesteps_per_second=1,\\\n",
    "                ):\n",
    "    \n",
    "        # Load init from Parent Class\n",
    "        super(Env_MultiAgent,self).__init__(\n",
    "                 Loaded_Vissim,\\\n",
    "                 sim_length,\\\n",
    "                 timesteps_per_second)\n",
    "        \n",
    "\n",
    "        self.Vissim = Loaded_Vissim\n",
    "\n",
    "\n",
    "        #self.Quick_Mode(Quick_Mode)\n",
    "        \n",
    "        # Signal_Controllers is all controllers or a specified list    \n",
    "        if Signal_Controllers is not None :\n",
    "            self.Signal_Controllers = Signal_Controllers\n",
    "        else : \n",
    "            self.Signal_Controllers = Vissim.Net.SignalControllers\n",
    "        \n",
    "        self.Controllers_Actions = Controllers_Actions \n",
    "        self.SCUs = self._Load_SCUs()\n",
    "        \n",
    "    '''\n",
    "    _Load_SCUs :\n",
    "        provides a dictionary with at the SCUs\n",
    "    '''\n",
    "    def _Load_SCUs(self):\n",
    "        \n",
    "        SCUs = dict()\n",
    "        \n",
    "        for idx, sc in enumerate(self.Signal_Controllers):\n",
    "            SCUs[idx] = Signal_Control_Unit(Vissim, sc, self.Controllers_Actions[idx])\n",
    "        \n",
    "        return SCUs\n",
    "    \n",
    "    '''\n",
    "    Make a Step\n",
    "    '''   \n",
    "    def Step(self,action):\n",
    "        self.Vissim.Simulation.RunSingleStep()\n",
    "        \n",
    "        Sars = dict()\n",
    "        \n",
    "        for idx, scu in self.SCUs:\n",
    "            scu.action_required = SCU.update()\n",
    "            if action_required :\n",
    "                Sars[idx] = scu.sars()\n",
    "        \n",
    "        if len(actions_required) > 0 :\n",
    "            return True, Sars\n",
    "        else:\n",
    "            return False, None\n",
    "    \n",
    "    def sar(self):\n",
    "        sar_all = dict()\n",
    "        \n",
    "        for idx, scu in self.SCUs:\n",
    "            sar_all[idx] = (scu._calculate_state, scu.action_key, scu._calculate_reward) \n",
    "        \n",
    "        return sar_all\n",
    "        \n",
    "    def _calculate_state(self):\n",
    "        '''\n",
    "        THIS SHOULD GET QUEUE LENGTH FOR EACH Signal Control Group\n",
    "        Debug because of multiple signal heads\n",
    "        '''\n",
    "\n",
    "        pass\n",
    "    \n",
    "#     def _calculate_reward(self):\n",
    "#         pass\n",
    "    \n",
    "#     def Quick_Mode(self,Quick_Mode):\n",
    "#         self.Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",Quick_Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Env_MultiAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-353-42990af849c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mMultiAgent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mCyclic_Control\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSignal_Controllers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m Env =  Env_MultiAgent(\\\n\u001b[0m\u001b[0;32m      7\u001b[0m                  \u001b[0mVissim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                  \u001b[0mControllers_Actions\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Env_MultiAgent' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Controllers, agents and Environment\n",
    "Signal_Controllers = Vissim.Net.SignalControllers\n",
    "\n",
    "MultiAgent = { idx : Cyclic_Control(len(sc.SGs)) for idx, sc in enumerate(Signal_Controllers) }\n",
    "\n",
    "Env =  Env_MultiAgent(\\\n",
    "                 Vissim,\\\n",
    "                 Controllers_Actions\\\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load up an SPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_Controller = Vissim.Net.SignalControllers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Signal_Controller.AttValue('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Signal_Controller.SGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_actions ={   0 : [ 1, 0, 0, 1, 0, 0,0,0,0,0,0,0],\n",
    "            1 : [ 0, 1, 0, 0, 1, 0,0,0,0,0,0,0],\n",
    "            2 : [ 0, 0, 1, 0, 0, 1,0,0,0,0,0,0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC = Cyclic_Control(len(compatible_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCU = Signal_Control_Unit(\n",
    "                 Vissim,\\\n",
    "                 Signal_Controller,\\\n",
    "                 compatible_actions,\\\n",
    "                 Signal_Groups = None,\\\n",
    "                 green_time = 10,\\\n",
    "                 redamber_time = 1,\\\n",
    "                 amber_time = 3, \\\n",
    "                 red_time = 1\\\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SCU.signal_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCU.action_update(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "for _ in range(1000):  \n",
    "    SCU.Vissim.Simulation.RunSingleStep()\n",
    "    action_required = SCU.update()\n",
    "    if action_required :\n",
    "        new_action = CC.choose_action()\n",
    "        SCU.action_update(new_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compatible_actions[SCU.action_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Multijunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_actions = [None,None]\n",
    "Signal_Controller = [None, None]\n",
    "Signal_Groups = [None, None]\n",
    "SCU = [None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_actions[0] ={   0 : [ 1, 0, 0, 1, 0, 0],\n",
    "            1 : [ 0, 1, 0, 0, 1, 0],\n",
    "            2 : [ 0, 0, 1, 0, 0, 1]\n",
    "        }\n",
    "Signal_Controller[0] = Vissim.Net.SignalControllers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_Groups[0] = []\n",
    "for i in range(6):\n",
    "    Signal_Groups[0].append(Signal_Controller[0].SGs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "scu1 = Signal_Control_Unit(\n",
    "                 Vissim,\\\n",
    "                 Signal_Controller[0],\\\n",
    "                 sc_actions[0],\\\n",
    "                 Signal_Groups = Signal_Groups[0],\\\n",
    "                 green_time = 5,\\\n",
    "                 redamber_time = 1,\\\n",
    "                 amber_time = 3, \\\n",
    "                 red_time = 1\\\n",
    "                )\n",
    "SCU[0] =scu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_actions[1] ={   0 : [ 1, 0, 0, 1, 0, 0],\n",
    "            1 : [ 0, 1, 0, 0, 1, 0],\n",
    "            2 : [ 0, 0, 1, 0, 0, 1]\n",
    "        }\n",
    "Signal_Controller[1] = Vissim.Net.SignalControllers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_Groups[1] = []\n",
    "for i in range(6):\n",
    "    Signal_Groups[1].append(Signal_Controller[1].SGs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "scu2 = Signal_Control_Unit(\n",
    "                 Vissim,\\\n",
    "                 Signal_Controller[1],\\\n",
    "                 sc_actions[1],\\\n",
    "                 Signal_Groups = Signal_Groups[1],\\\n",
    "                 green_time = 5,\\\n",
    "                 redamber_time = 1,\\\n",
    "                 amber_time = 3, \\\n",
    "                 red_time = 1\\\n",
    "                )\n",
    "SCU[1] =scu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC = [ Cyclic_Control(3),  Cyclic_Control(3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "for _ in range(1000):  \n",
    "    Vissim.Simulation.RunSingleStep()\n",
    "   # time.sleep(0.3)\n",
    "    for i,scu in enumerate(SCU) :\n",
    "        action_required = scu.update()\n",
    "        if action_required :\n",
    "            new_action = CC[i].choose_action()\n",
    "            scu.action_update(new_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug All Junctions cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# all controller actions\n",
    "Controllers_Actions =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {   0 : [ 1, 0, 0, 1, 0, 0,],\n",
    "            1 : [ 0, 1, 0, 0, 1, 0,],\n",
    "            2 : [ 0, 0, 1, 0, 0, 1,]\n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {   0 : [ 1, 0, 0, 1, 0, 0,],\n",
    "            1 : [ 0, 1, 0, 0, 1, 0,],\n",
    "            2 : [ 0, 0, 1, 0, 0, 1,]\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {\n",
    "            0 : [1,0,0,0,1,0,0,0],\n",
    "            1 : [0,1,0,0,0,1,0,0],\n",
    "            2 : [0,0,1,0,0,0,1,0],\n",
    "            3 : [0,0,0,1,0,0,0,1]\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {\n",
    "            0 : [1,0,0,1,0,0],\n",
    "            1 : [0,1,0,0,1,0],\n",
    "            2 : [0,0,1,0,0,1],\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {\n",
    "            0 : [1,1,0,0],\n",
    "            1 : [0,1,1,0],\n",
    "            2 : [1,0,0,1]\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {\n",
    "            0 : [1,0,1],\n",
    "            1 : [0,1,0]\n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {\n",
    "            0 : [1,0,1,0],\n",
    "            1 : [0,1,0,1]\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {\n",
    "            0 : [1,0,0,0,0],\n",
    "            1 : [0,1,0,1,0],\n",
    "            2 : [0,0,1,0,1]\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {\n",
    "            0 : [1,0,1],\n",
    "            1 : [0,1,0],\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {\n",
    "            0 : [1,0,0,0],\n",
    "            1 : [0,1,0,1],\n",
    "            1 : [0,0,1,0],\n",
    "        },\n",
    "    # Controller 15\n",
    "     10 : {   \n",
    "            0 : [1,0,1,0],\\\n",
    "            1 : [0,1,0,1]\\\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {   \n",
    "            0 : [1,0,1,0],\\\n",
    "            1 : [0,1,0,1]\\\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {\n",
    "            0 : [1,0,0,0,0],\n",
    "            1 : [0,1,0,0,1],\n",
    "            2 : [0,0,1,0,0],\n",
    "            3 : [0,0,0,1,1],\n",
    "            4 : [0,1,0,0,1]\n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {\n",
    "            0 : [1,0,0,1],\n",
    "            1 : [1,0,1,0],\n",
    "            2 : [0,1,0,0]\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_scs = len(Controllers_Actions)\n",
    "\n",
    "Signal_Controller = []\n",
    "for idx in range(number_of_scs):\n",
    "    Signal_Controller.append(Vissim.Net.SignalControllers[idx])\n",
    "\n",
    "Signal_Groups = [ [] for _ in range(number_of_scs)]\n",
    "\n",
    "for i, SGs in enumerate(Signal_Groups) :  \n",
    "    for j in range(len(Controllers_Actions[i][0])):\n",
    "        SGs.append(Signal_Controller[i].SGs[j])\n",
    "        #print(Signal_Controller[i].SGs[j].AttValue('No'))\n",
    "\n",
    "SCU =  []\n",
    "\n",
    "for i in range(number_of_scs):\n",
    "    SCU.append(\\\n",
    "              Signal_Control_Unit(\n",
    "                 Vissim,\\\n",
    "                 Signal_Controller[i],\\\n",
    "                 Controllers_Actions[i],\\\n",
    "                 Signal_Groups = Signal_Groups[i],\\\n",
    "                 green_time = 5,\\\n",
    "                 redamber_time = 1,\\\n",
    "                 amber_time = 3, \\\n",
    "                 red_time = 1\\\n",
    "                )\\\n",
    "              )\n",
    "\n",
    "CC =[]\n",
    "for i, acts in enumerate(Controllers_Actions):\n",
    "    cycle_size = len(Controllers_Actions[i])\n",
    "    CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, 'VISSIM.Vissim.1100', 'AttValue failed: Object 1 - 1: SG1L: Attribute Signal state is no subject to changes.', None, 0, -2147352567), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-362-64b5deff5a2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m    \u001b[1;31m# time.sleep(0.3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSCU\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0maction_required\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;31m# state = scu.calculate_state()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-349-37fa23e78d41>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0msg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal_groups\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                     \u001b[0mID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_color_changer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_colors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[1;31m# change the current stage and get time the stage last for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-349-37fa23e78d41>\u001b[0m in \u001b[0;36m_color_changer\u001b[1;34m(self, signal_group, new_color, stage)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# want green but currently red\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mchange\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Green\"\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m             \u001b[0msignal_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SigState\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"AMBER\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# want red but currently amber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mSetAttValue\u001b[1;34m(self, Attribut, arg1)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, 'VISSIM.Vissim.1100', 'AttValue failed: Object 1 - 1: SG1L: Attribute Signal state is no subject to changes.', None, 0, -2147352567), None)"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "for _ in range(4000):  \n",
    "    Vissim.Simulation.RunSingleStep()\n",
    "   # time.sleep(0.3)\n",
    "    for i, scu in enumerate(SCU) :\n",
    "        action_required = scu.update()\n",
    "        if action_required :\n",
    "            # state = scu.calculate_state()\n",
    "            new_action = CC[i].choose_action()\n",
    "            scu.action_update(new_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCU[0].action_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 0, 0, 1, 0, 0], 1: [0, 1, 0, 0, 1, 0], 2: [0, 0, 1, 0, 0, 1]}"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCU[0].compatible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCU[0].current_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 0, 2, 0, 0]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCU[0].new_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for sg in SCU[0].signal_groups:\n",
    "    print(sg.AttValue('No'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<COMObject <unknown>>, <COMObject <unknown>>, <COMObject <unknown>>]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Signal_Groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Idea for main control loop\n",
    "'''\n",
    "\n",
    "action_required, scu_sars = Env.Step()\n",
    "\n",
    "if action_required :\n",
    "    for idx, sar in enumerate(scu_sars):\n",
    "        state, action, reward = sar\n",
    "        action = Agent.choose_action(state)\n",
    "        Env.SCUs[idx]. action\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Vissim.Net.QueueCounters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Vissim.Net.Links)):\n",
    "    print(len(Vissim.Net.Links[i].QueueCounters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Simulation.RunSingleStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env_SPU(Simulation_Environment):\n",
    "    def __init__(self,\\\n",
    "                 Loaded_Vissim,\\\n",
    "                 sim_length=3601,\\\n",
    "                 timesteps_per_second=1,\\\n",
    "                ):\n",
    "    \n",
    "        # Load init from Parent Class\n",
    "        super(Env_SPU,self).__init__(\n",
    "                 Loaded_Vissim,\\\n",
    "                 sim_length,\\\n",
    "                 timesteps_per_second,\\\n",
    "                 delete_results,\\\n",
    "                 verbose)\n",
    "        \n",
    "            def __init__(self,\\\n",
    "                 Vissim,\\\n",
    "                 Signal_Controller,\\\n",
    "                 compatible_actions,\\\n",
    "                 Signal_Groups = None,\\\n",
    "                 green_time = 40,\\\n",
    "                 redamber_time = 1,\\\n",
    "                 amber_time = 3, \\\n",
    "                 red_time = 1\\\n",
    "                ):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vissim.Net.SignalControllers[0].SGs[4].SigHeads[0].Lane.AttValue('Link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "YOU ARE HERE\n",
    "'''\n",
    "for veh in Vissim.Net.SignalControllers[0].SGs[4].SigHeads[0].Lane.Vehs:\n",
    "    print(veh.AttValue('InQueue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "THIS SHOULD GET QUEUE LENGTH FOR EACH Signal Control Group\n",
    "Debug because of multiple signal heads\n",
    "'''\n",
    "sc = Vissim.Net.SignalControllers[7]\n",
    "Queues = []\n",
    "Lanes = []\n",
    "for sg in sc.SGs :\n",
    "    q = 0 \n",
    "    for sh in sg.SigHeads:\n",
    "        if (sh.Lane.AttValue('Link'),sh.Lane.AttValue('Index')) not in Lanes :\n",
    "            Lanes.append((sh.Lane.AttValue('Link'),sh.Lane.AttValue('Index')))\n",
    "            for veh in sh.Lane.Vehs:\n",
    "                q += veh.AttValue('InQueue')\n",
    "    Queues.append(q)\n",
    "    print(sc.AttValue('No'),sg.AttValue('No'),q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_required, scu_sars = Env.Step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions for Balance Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Controllers_Actions =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {   0 : [ 1, 0, 0, 1, 0, 0,],\n",
    "            1 : [ 0, 1, 0, 0, 1, 0,],\n",
    "            2 : [ 0, 0, 1, 0, 0, 1,]\n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {   0 : [ 1, 0, 0, 1, 0, 0,],\n",
    "            1 : [ 0, 1, 0, 0, 1, 0,],\n",
    "            2 : [ 0, 0, 1, 0, 0, 1,]\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {\n",
    "            0 : [1,0,0,0,1,0,0,0],\n",
    "            1 : [0,1,0,0,0,1,0,0],\n",
    "            2 : [0,0,1,0,0,0,1,0],\n",
    "            3 : [0,0,0,1,0,0,0,1]\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {\n",
    "            0 : [1,0,0,1,0,0],\n",
    "            1 : [0,1,0,0,1,0],\n",
    "            2 : [0,0,1,0,0,1],\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {\n",
    "            0 : [1,1,0,0],\n",
    "            1 : [0,1,1,0],\n",
    "            2 : [0,0,0,1]\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {\n",
    "            0 : [1,0,1],\n",
    "            1 : [0,1,0]\n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {\n",
    "            0 : [1,0,1,0],\n",
    "            1 : [0,1,0,1]\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {\n",
    "            0 : [1,0,0,0,0],\n",
    "            1 : [0,1,0,1,0],\n",
    "            2 : [0,0,1,0,1]\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {\n",
    "            0 : [1,0,1],\n",
    "            1 : [0,1,0],\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {\n",
    "            0 : [1,0,1],\n",
    "            1 : [0,1,0],\n",
    "        },\n",
    "    # Controller 15\n",
    "     10 : {   \n",
    "            0 : [1,0,1,0],\\\n",
    "            1 : [0,1,0,1]\\\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {   \n",
    "            0 : [1,0,1,0],\\\n",
    "            1 : [0,1,0,1]\\\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {\n",
    "            0 : [1,0,0,0,0],\n",
    "            1 : [0,1,0,0,1],\n",
    "            2 : [0,0,1,0,0],\n",
    "            3 : [0,0,0,1,1],\n",
    "            4 : [0,1,0,0,1]\n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {\n",
    "            0 : [1,0,0,1],\n",
    "            1 : [1,0,1,0],\n",
    "            2 : [0,1,0,0]\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_2 \\\n",
    "= \\\n",
    "{   0 : [ 1, 0, 0, 1, 0, 0,],\\\n",
    "    1 : [ 0, 1, 0, 0, 1, 0,],\\\n",
    "    2 : [ 0, 1, 0, 0, 1, 0,]\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_3 = Actions_Controller_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_4 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,0,0,0,1,0,0,0],\n",
    "    1 : [0,1,0,0,0,1,0,0],\n",
    "    2 : [0,0,1,0,0,0,1,0],\n",
    "    3 : [0,0,0,1,0,0,0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_5 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,0,0,1,0,0],\n",
    "    1 : [0,1,0,0,1,0],\n",
    "    2 : [0,0,1,0,0,1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_6 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,1,0,0],\n",
    "    1 : [0,1,1,0],\n",
    "    2 : [0,0,0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_8 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,0,1],\n",
    "    1 : [0,1,0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sg in sc_0.SGs :\n",
    "    print( sg.AttValue('SigState'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sg in enumerate(SG) :\n",
    "    if idx == 0 or idx == 3 :\n",
    "        sg.SetAttValue('SigState','GREEN')\n",
    "    else :\n",
    "        sg.SetAttValue('SigState','RED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sc_0.SGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPU.Vissim.Simulation.RunSingleStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG = [ sg for idx, sg in enumerate(sc_0.SGs) if idx < 6 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sc in Vissim.Net.SignalControllers :\n",
    "    for sg in sc.SGs :\n",
    "        print(sc.AttValue('No'),sg.AttValue('No'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_9 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,0,1,0],\n",
    "    1 : [0,1,0,1]\n",
    "}\n",
    "\n",
    "Actions_Controller_10 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,0,0,0,0],\n",
    "    1 : [0,1,0,1,0],\n",
    "    2 : [0,0,1,0,1]\n",
    "}\n",
    "\n",
    "Actions_Controller_12 = \\\n",
    "{\n",
    "    0 : [1,0,1],\n",
    "    1 : [0,1,0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_13 = Actions_Controller_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_15 \\\n",
    "= \\\n",
    "{   0 : [1,0,1,0],\\\n",
    "    1 : [0,1,0,1]\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_16 = Actions_Controller_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_17 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,0,0,0,0],\n",
    "    1 : [0,1,0,0,1],\n",
    "    2 : [0,0,1,0,0],\n",
    "    3 : [0,0,0,1,1],\n",
    "    4 : [0,1,0,0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_33 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,0,0,1],\n",
    "    1 : [1,0,1,0],\n",
    "    2 : [0,1,0,0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Controller_Actions[13][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions_Controller_9 \\\n",
    "=\\\n",
    "{\n",
    "    0 : [1,0,1,0],\n",
    "    1 : [0,1,0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC = Cyclic_Control(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.choose_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sg in Vissim.Net.SignalControllers[0].SGs :\n",
    "    for sh in sg.SigHeads :\n",
    "        print(sg.AttValue('No'), sh.AttValue('Lane'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCU = Signal_Control_Unit(Env.Vissim,\\\n",
    "                          Env.Vissim.Net.SignalControllers[0],\\\n",
    "                          compatible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(500):  \n",
    "    SCU.Vissim.Simulation.RunSingleStep()\n",
    "    action_required = SCU.update()\n",
    "    if action_required :\n",
    "        new_action = CC.choose_action()\n",
    "        SCU.action_update(new_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Single Episode for a set simulation length\n",
    "def run_simulation_episode(Agents, Vissim, state_type, reward_type,\\\n",
    " state_size, simulation_length, timesteps_per_second, seconds_per_green,\\\n",
    "  seconds_per_yellow, demand_list, demand_change_timesteps, mode, PER_activated, Surtrac = False, AC = False):\n",
    "\t\n",
    "\tfor time_t in range(simulation_length):\n",
    "\n",
    "\t\t# Change demand every 450 seconds.\n",
    "\t\tif time_t % demand_change_timesteps == 0:\n",
    "\t\t\tchange_demand(Vissim, demand_list, demand_change_timesteps, time_t)\n",
    "\t\t\n",
    "\t\t# Pass the control over to COM\n",
    "\t\tif time_t == 1:\n",
    "\t\t\tfor agent in Agents:\n",
    "\t\t\t\tfor group in agent.signal_groups:\n",
    "\t\t\t\t\tgroup.SetAttValue('ContrByCOM',1)\n",
    "\n",
    "\t\t# Cycle through all agents and update them\n",
    "\t\tAgents_update(Agents, Vissim, state_type,reward_type, state_size, seconds_per_green, seconds_per_yellow, mode, time_t, Surtrac = Surtrac , AC = AC )\n",
    "\t\t\n",
    "\t\t# Advance the game to the next second (proportionally to the simulator resolution).\n",
    "\t\tfor _ in range(0, timesteps_per_second):\n",
    "\t\t\tVissim.Simulation.RunSingleStep()\n",
    "\n",
    "\t\tif mode == \"test\":\n",
    "\t\t\tfor agent in Agents:\n",
    "\t\t\t\tagent.queues_over_time.append(get_queue_lengths(Vissim, agent))\n",
    "\t\t\t\tagent.accumulated_delay.append(agent.accumulated_delay[-1]+get_delay_timestep(Vissim))\n",
    "\n",
    "\t# Reconfigure agents for the start of the next episode\n",
    "\tfor agent in Agents:\n",
    "\t\tagent.update_counter = 1\n",
    "\t\tagent.intermediate_phase = False\n",
    "\t\tagent.action = 0\n",
    "\t# Stop the simulation    \n",
    "\tVissim.Simulation.Stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Agents_update(Agents, Vissim, state_type, reward_type, state_size, seconds_per_green, seconds_per_yellow, mode, time_t, Surtrac = False, AC = False):\n",
    "\t\n",
    "\tfor index, agent in enumerate(Agents):\n",
    "\t\t# Check if agent needs to update\n",
    "\t\tif agent.update_counter > 0:\n",
    "\t\t\t# If it doesn't, substract 1 timestep from the counter and skip the rest\n",
    "\t\t\tagent.update_counter -= 1\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Update the agent\n",
    "\t\telif agent.update_counter == 0:\n",
    "\t\t\tif mode\t == \"debug\":\n",
    "\t\t\t\tprint(\"Update at time {}\".format(time_t))\n",
    "\t\t\t# Make sure the agent is not in the middle of a transition\n",
    "\t\t\tif agent.intermediate_phase == False:\n",
    "\t\t\t\t# Compute the current State and store it in the agent\n",
    "\t\t\t\tagent.newstate = calculate_state(Vissim, state_type, state_size)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reward generated by last Action and store it in the agent\n",
    "\t\t\t\tagent.reward   = calculate_reward(Vissim, reward_type)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif not Surtrac :\n",
    "\t\t\t\t# Commit previous State, previous Action, Reward generated and current State to memory\n",
    "\t\t\t\t\tagent.remember(agent.state, agent.action, agent.reward, agent.newstate)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tagent.episode_reward.append(agent.reward)\t\t\n",
    "\t\t\t\t#print(agent.newstate)\n",
    "\n",
    "\t\t\t\t# Compute the new Action and store it in the agent\n",
    "\t\t\t\tagent.newaction = agent.choose_action(agent.newstate)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# In Demonstration Mode, show the Reward of the last cycle\n",
    "\t\t\t\tif mode == \"demo\":\n",
    "\t\t\t\t\tprint('Agent Reward in this cycle is : {}'.format(round(agent.reward,2)))\n",
    "\n",
    "\t\t\t\t# If the same Action is chosen\n",
    "\t\t\t\tif agent.newaction == agent.action:\n",
    "\t\t\t\t\t# Extend Timer (do nothing)\n",
    "\t\t\t\t\tif Surtrac:\n",
    "\t\t\t\t\t\tagent.update_counter += agent.actiontime - 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tagent.update_counter += seconds_per_green - 1\n",
    "\t\t\t\t\t#print(\"Extended Phase\")\n",
    "\n",
    "\t\t\t\t# If a different Action is chosen\n",
    "\t\t\t\telif agent.newaction != agent.action:\n",
    "\t\t\t\t\t# Transition from green to amber and from red to redamber\n",
    "\t\t\t\t\tgreen_red_to_amber(agent, seconds_per_yellow,Surtrac)\n",
    "\t\t\t\t\t#print(\"Finished Transition\")\n",
    "\n",
    "\t\t\t# If the agent is in the middle of a transition\n",
    "\t\t\telif agent.intermediate_phase == True:\n",
    "\t\t\t\t# Transition from amber to red and from redamber to green\n",
    "\t\t\t\tamber_to_green_red(agent, seconds_per_green, Surtrac)\n",
    "\n",
    "\t\t\t# Update internal State\n",
    "\t\t\tagent.state  = agent.newstate\n",
    "\t\t\t# Update internal Action\n",
    "\t\t\tagent.action = agent.newaction  \n",
    "\t\t\t\n",
    "\t\t\t# Training during the episode\n",
    "\t\t\tif mode == 'training' and AC :\n",
    "\t\t\t\tagent.trainstep += 1\n",
    "\t\t\t\tif len(agent.memory) == agent.n_step_size and agent.trainstep >= 1:\n",
    "\t\t\t\t\tagent.learn()\n",
    "\t\t\t\t\tagent.trainstep = 0\t\t\t\t\n",
    "\t\t# Error protection against negative update counters\n",
    "\t\telse:\n",
    "\t\t\traise Exception(\"ERROR: Update Counter for agent {} is negative. Please investigate.\".format(index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
