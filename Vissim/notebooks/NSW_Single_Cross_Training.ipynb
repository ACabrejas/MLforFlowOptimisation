{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt to train a single cross junction with multiple agents\n",
    "\n",
    "\n",
    "We use our new enviroment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# add folder up to gain enviroments above\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Vissim_env_class as Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Allow for reload of key modules\n",
    "# for debugging purposes\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "# MANUAL LOAD (part 1)\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "# Dispatch works better than dynamoc dispatch\n",
    "#Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "\n",
    "for _ in range(5):\n",
    "    try:\n",
    "        Vissim = com.Dispatch(\"Vissim.Vissim\")\n",
    "        print('success!')\n",
    "        break\n",
    "    except:\n",
    "        print('fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MANUAL LOAD (part 2)\n",
    "input_file = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\Vissim_Networks\\\\Single_Cross_Straight\\\\Single_Cross_Straight.inpx'\n",
    "Vissim.LoadNet(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "code_folding": [
     173,
     179,
     217,
     257
    ]
   },
   "outputs": [],
   "source": [
    "# vissim env class\n",
    "import numpy as np\n",
    "from NParser import NetworkParser\n",
    "from Vissim_SCU_class import Signal_Control_Unit\n",
    "import win32com.client\n",
    "import os\n",
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "# The environment class , \n",
    "class vissim_env():\n",
    "\n",
    "    \"\"\"\n",
    "    -Load the model\n",
    "        - it need the controller actions to be defined by hand\n",
    "    -Deploy the SCU\n",
    "    -\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\\\n",
    "                 controllers_actions,\\\n",
    "                 model_name=None,\\\n",
    "                 vissim_working_directory=None,\\\n",
    "                 sim_length=361, \\\n",
    "                 Vissim = None,\\\n",
    "                 timesteps_per_second = 1,\\\n",
    "                 mode = 'training',\\\n",
    "                 delete_results = True,\\\n",
    "                 verbose = True,\\\n",
    "                 green_time = 5):\n",
    "\n",
    "        # Model parameters\n",
    "        self.model_name = model_name\n",
    "        self.vissim_working_directory = vissim_working_directory\n",
    "        self.controllers_actions = controllers_actions\n",
    "\n",
    "\n",
    "        # Simulation parameters\n",
    "        self.sim_length = sim_length\n",
    "        self.global_counter = 0\n",
    "\n",
    "        self.mode = mode\n",
    "        self.timesteps_per_second = timesteps_per_second\n",
    "        self.green_time = green_time\n",
    "\n",
    "        # Evaluation parameters\n",
    "        self.delete_results = delete_results\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # ComServerDisp\n",
    "        if Vissim is None:\n",
    "            self.Vissim, _, _, _ = COMServerDispatch(model_name, vissim_working_directory, self.sim_length,\\\n",
    "            self.timesteps_per_second, delete_results = self.delete_results, verbose = self.verbose)\n",
    "        # Hand over open Vissim\n",
    "        else:\n",
    "            self.Vissim = Vissim\n",
    "            self.Vissim.Simulation.Stop()\n",
    "            self.Vissim.Simulation.SetAttValue('SimPeriod', sim_length)\n",
    "            if delete_results == True:\n",
    "                # Delete all previous simulation runs first:\n",
    "                for simRun in Vissim.Net.SimulationRuns:\n",
    "                    self.Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "\n",
    "        self.done = False\n",
    "\n",
    "        # The parser can be a methode of the environment\n",
    "        self.npa = NetworkParser(self.Vissim) \n",
    "\n",
    "        self.select_mode()\n",
    "\n",
    "        # Simulate one step and give the control to COM\n",
    "        for _ in range(self.timesteps_per_second):\n",
    "            self.Vissim.Simulation.RunSingleStep()\n",
    "            self.global_counter += 1\n",
    "\n",
    "\n",
    "        for SC in self.npa.signal_controllers_ids:\n",
    "            for group in self.npa.signal_groups[SC]:\n",
    "                group.SetAttValue('ContrByCOM',1)\n",
    "\n",
    "\n",
    "        # Create a dictionnary of SCUs each scu control a signal controller\n",
    "\n",
    "\n",
    "        tic = time()\n",
    "        self._Load_SCUs()\n",
    "        tac = time()\n",
    "        #print(tac-tic)\n",
    "\n",
    "    '''\n",
    "    _Load_SCUs :\n",
    "        provides a dictionary with at the SCUs\n",
    "        # Need to find later a way to give different green / yellow time to each SCUs\n",
    "    '''\n",
    "    def _Load_SCUs(self):\n",
    "\n",
    "        self.SCUs = dict()\n",
    "\n",
    "        for idx, sc in enumerate(self.npa.signal_controllers):\n",
    "            self.SCUs[idx] = Signal_Control_Unit(\\\n",
    "                         self.Vissim,\\\n",
    "                         sc,\\\n",
    "                         self.controllers_actions[idx],\\\n",
    "                         Signal_Groups = None,\\\n",
    "                         green_time = self.green_time,\\\n",
    "                         redamber_time = 1,\\\n",
    "                         amber_time = 3, \\\n",
    "                         red_time = 1\\\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "    # -Function to get the SCUs to later deploy agent on them\n",
    "    def get_SCU(self):\n",
    "        return(self.SCUs)\n",
    "\n",
    "    # does a step in the simulator\n",
    "    # INPUT a dictionary of action\n",
    "    # return a dictionnary of (state, action, reward, next_state , done) the key will be the SCU's key\n",
    "    def step(self, actions):\n",
    "        self.Vissim.Simulation.RunSingleStep()\n",
    "        self.global_counter += 1\n",
    "        if self.global_counter > (self.sim_length-1) * self.timesteps_per_second:\n",
    "            self.done = True\n",
    "            self.global_counter = 0\n",
    "        else :\n",
    "            self.done = False\n",
    "\n",
    "        if not self.Vissim.Simulation.AttValue('IsRunning') :\n",
    "            self.Vissim.Simulation.RunSingleStep()\n",
    "        \n",
    "        Sarsd = dict()\n",
    "\n",
    "        for idx, scu in self.SCUs.items():\n",
    "            if scu.action_required :\n",
    "                tic = time()\n",
    "                scu.action_update(actions[idx])\n",
    "                tac = time()\n",
    "                #print('action_update')\n",
    "                #print(tac-tic)\n",
    "\n",
    "            #tic = time()\n",
    "            if not self.done :\n",
    "                scu.update()\n",
    "            #tac = time()\n",
    "            #print('update')\n",
    "            #print(tac-tic)\n",
    "\n",
    "            if scu.action_required or self.done :\n",
    "                Sarsd[idx] = scu.sars()+[self.done]\n",
    "\n",
    "\n",
    "        if len(Sarsd) > 0 :\n",
    "            return True, Sarsd\n",
    "        else:\n",
    "            return False, None\n",
    "\n",
    "\n",
    "    # reset the environnement\n",
    "    def reset(self):\n",
    "        ## Connecting the COM Server => Open a new Vissim Window:\n",
    "        # Server should only be dispatched in first run. Otherwise reload model.\n",
    "        # Setting Working Directory\n",
    "\n",
    "\n",
    "        COMServerReload(self.Vissim, self.model_name, self.vissim_working_directory, self.sim_length, self.timesteps_per_second, self.delete_results)\n",
    "        self.npa = NetworkParser(self.Vissim) \n",
    "        self.select_mode()\n",
    "\n",
    "        # Simulate one step and give the control to COM\n",
    "        for _ in range(self.timesteps_per_second):\n",
    "            self.Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "        for SC in self.npa.signal_controllers_ids:\n",
    "            for group in self.npa.signal_groups[SC]:\n",
    "                group.SetAttValue('ContrByCOM', 1)\n",
    "\n",
    "        self._Load_SCUs()\n",
    "        self.done = False\n",
    "\n",
    "\n",
    "\n",
    "    # Set mode to training, demo, debugging\n",
    "    def select_mode(self):\n",
    "        \n",
    "        self.Vissim.Simulation.Stop()\n",
    "        # Select the mode for the metric collection \n",
    "\n",
    "        # In test mode all the data is stored (The simulation will be slow)\n",
    "        if self.mode == 'test' :\n",
    "            #This select quickmode and simulation resolution\n",
    "            #self.timesteps_per_second = 10\n",
    "            self.Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)\n",
    "            Vissim.ResumeUpdateGUI(False)\n",
    "            self.Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\", 1)\n",
    "            self.Vissim.Simulation.SetAttValue('SimRes', self.timesteps_per_second)\n",
    "            self.Vissim.SuspendUpdateGUI()  \n",
    "\n",
    "            # set the data mesurement\n",
    "            self.Vissim.Evaluation.SetAttValue('DataCollCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('DataCollInterval', 1)\n",
    "\n",
    "            # set the delay mesurement\n",
    "            self.Vissim.Evaluation.SetAttValue('DelaysCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('DelaysInterval', 1)\n",
    "\n",
    "            # set the data mesurement for each link\n",
    "            self.Vissim.Evaluation.SetAttValue('LinkResCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('LinkResInterval', 1)\n",
    "\n",
    "            # set the data mesurement for each node\n",
    "            self.Vissim.Evaluation.SetAttValue('NodeResCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('NodeResInterval', 1)\n",
    "\n",
    "            # set the queues mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('QueuesCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('QueuesInterval', 1)\n",
    "\n",
    "            # set the vehicles perf mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('VehNetPerfCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('VehNetPerfInterval', 1)\n",
    "\n",
    "            # set the vehicles travel time mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('VehTravTmsCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('VehTravTmsInterval', 1)\n",
    "\n",
    "        # In demo mode we only use the queue counter for the moment\n",
    "        elif self.mode == 'demo' :\n",
    "            \n",
    "            Vissim.ResumeUpdateGUI(True)\n",
    "            Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",0)\n",
    "            \n",
    "            #This select the simulation resolution\n",
    "            #self.timesteps_per_second = 10\n",
    "            self.Vissim.Simulation.SetAttValue('SimRes', self.timesteps_per_second)\n",
    "\n",
    "\n",
    "            # set the data mesurement\n",
    "            self.Vissim.Evaluation.SetAttValue('DataCollCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('DataCollInterval', 99999)\n",
    "\n",
    "            # set the delay mesurement\n",
    "            self.Vissim.Evaluation.SetAttValue('DelaysCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('DelaysInterval', 99999)\n",
    "\n",
    "            # set the data mesurement for each link\n",
    "            self.Vissim.Evaluation.SetAttValue('LinkResCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('LinkResInterval', 99999)\n",
    "\n",
    "            # set the data mesurement for each node\n",
    "            self.Vissim.Evaluation.SetAttValue('NodeResCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('NodeResInterval', 99999)\n",
    "\n",
    "\n",
    "            # set the queues mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('QueuesCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('QueuesInterval', 3)\n",
    "\n",
    "            # set the vehicles perf mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('VehNetPerfCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('VehNetPerfInterval', 99999)\n",
    "\n",
    "            # set the vehicles travel time mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('VehTravTmsCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('VehTravTmsInterval', 99999)\n",
    "\n",
    "        # In demo mode we only use the queue counter and the delay counter for the moment    \n",
    "        elif self.mode == 'training' :\n",
    "\n",
    "            #This select quickmode and simulation resolution\n",
    "            #self.timesteps_per_second = 1\n",
    "            self.Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)\n",
    "            Vissim.ResumeUpdateGUI(False)\n",
    "            self.Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",1)\n",
    "            self.Vissim.Simulation.SetAttValue('SimRes', self.timesteps_per_second)\n",
    "            self.Vissim.SuspendUpdateGUI()  \n",
    "\n",
    "\n",
    "            # set the data mesurement\n",
    "            self.Vissim.Evaluation.SetAttValue('DataCollCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('DataCollInterval', 3)\n",
    "\n",
    "            # set the delay mesurement\n",
    "            self.Vissim.Evaluation.SetAttValue('DelaysCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('DelaysInterval', 99999)\n",
    "\n",
    "            # set the data mesurement for each link\n",
    "            self.Vissim.Evaluation.SetAttValue('LinkResCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('LinkResInterval', 99999)\n",
    "\n",
    "            # set the data mesurement for each node\n",
    "            self.Vissim.Evaluation.SetAttValue('NodeResCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('NodeResInterval', 99999)\n",
    "\n",
    "\n",
    "            # set the queues mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('QueuesCollectData', True)\n",
    "            self.Vissim.Evaluation.SetAttValue('QueuesInterval', 3)\n",
    "\n",
    "            # set the vehicles perf mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('VehNetPerfCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('VehNetPerfInterval', 99999)\n",
    "\n",
    "            # set the vehicles travel time mesurement \n",
    "            self.Vissim.Evaluation.SetAttValue('VehTravTmsCollectData', False)\n",
    "            self.Vissim.Evaluation.SetAttValue('VehTravTmsInterval', 99999)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def COMServerDispatch(model_name, vissim_working_directory, sim_length, timesteps_per_second, delete_results = True, verbose = True):\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            ## Connecting the COM Server => Open a new Vissim Window:\n",
    "            # Server should only be dispatched in first run. Otherwise reload model.\n",
    "            # Setting Working Directory\n",
    "            if verbose:\n",
    "                print ('Working Directory set to: ' + vissim_working_directory)\n",
    "                # Check Chache\n",
    "                print ('Generating Cache...')\n",
    "\n",
    "            # Vissim = win32com.client.gencache.EnsureDispatch(\"Vissim.Vissim\") \n",
    "            #Vissim = win32com.client.dynamic.Dispatch(\"Vissim.Vissim\") \n",
    "            Vissim = win32com.client.Dispatch(\"Vissim.Vissim\")\n",
    "\n",
    "            if verbose:\n",
    "                print ('Cache generated.\\n')\n",
    "                print ('****************************')\n",
    "                print ('*   COM Server dispatched  *')\n",
    "                print ('****************************\\n')\n",
    "            cache_flag = True\n",
    "\n",
    "            ## Load the Network:\n",
    "            Filename = os.path.join(vissim_working_directory, model_name, (model_name+'.inpx'))\n",
    "\n",
    "            if verbose:\n",
    "                print ('Attempting to load Model File: ' + model_name+'.inpx ...')\n",
    "\n",
    "            if os.path.exists(Filename):\n",
    "                Vissim.LoadNet(Filename)\n",
    "            else:\n",
    "                raise Exception(\"ERROR: Could not find Model file: {}\".format(Filename))\n",
    "\n",
    "            if verbose:\n",
    "                print ('Load process successful')\n",
    "\n",
    "            ## Setting Simulation End\n",
    "            Vissim.Simulation.SetAttValue('SimPeriod', sim_length)\n",
    "\n",
    "            if verbose:\n",
    "                print ('Simulation length set to '+str(sim_length) + ' seconds.')\n",
    "\n",
    "            ## If a fresh start is needed\n",
    "            if delete_results == True:\n",
    "                # Delete all previous simulation runs first:\n",
    "                for simRun in Vissim.Net.SimulationRuns:\n",
    "                    Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "                if verbose:\n",
    "                    print ('Results from Previous Simulations: Deleted. Fresh Start Available.')\n",
    "\n",
    "            #Pre-fetch objects for stability\n",
    "            Simulation = Vissim.Simulation\n",
    "            if verbose:\n",
    "                print ('Fetched and containerized Simulation Object')\n",
    "            Network = Vissim.Net\n",
    "\n",
    "            if verbose:\n",
    "                print ('Fetched and containerized Network Object \\n')\n",
    "                print ('*******************************************************')\n",
    "                print ('*                                                     *')\n",
    "                print ('*                 SETUP COMPLETE                      *')\n",
    "                print ('*                                                     *')\n",
    "                print ('*******************************************************\\n')\n",
    "            else:\n",
    "                print('Server Dispatched.')\n",
    "            return(Vissim, Simulation, Network, cache_flag)\n",
    "        # If loading fails\n",
    "        except:\n",
    "            if _ != 4:\n",
    "                print(\"Failed load attempt \" +str(_+1)+ \"/5. Re-attempting.\")\n",
    "            elif _ == 4:\n",
    "                raise Exception(\"Failed 5th loading attempt. Please restart program. TERMINATING NOW.\")\n",
    "\n",
    "\n",
    "def COMServerReload(Vissim, model_name, vissim_working_directory, simulation_length, timesteps_per_second, delete_results):\n",
    "    ## Connecting the COM Server => Open a new Vissim Window:\n",
    "    # Server should only be dispatched in first run. Otherwise reload model.\n",
    "    # Setting Working Directory\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            ## Load the Network:\n",
    "            Filename = os.path.join(vissim_working_directory, model_name, (model_name+'.inpx'))\n",
    "\n",
    "            Vissim.LoadNet(Filename)\n",
    "\n",
    "            ## Setting Simulation End\n",
    "            Vissim.Simulation.SetAttValue('SimPeriod', simulation_length)\n",
    "            ## If a fresh start is needed\n",
    "            if delete_results == True:\n",
    "                # Delete all previous simulation runs first:\n",
    "                for simRun in Vissim.Net.SimulationRuns:\n",
    "                    Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "                #print ('Results from Previous Simulations: Deleted. Fresh Start Available.')\n",
    "\n",
    "            #Pre-fetch objects for stability\n",
    "\n",
    "            #print('Reloading complete. Executing new episode...')\n",
    "            return()\n",
    "        # If loading fails\n",
    "        except:\n",
    "            if _ != 4:\n",
    "                print(\"Failed load attempt \" +str(_+1)+ \"/5. Re-attempting.\")\n",
    "            elif _ == 4:\n",
    "                raise Exception(\"Failed 5th loading attempt. Please restart program. TERMINATING NOW.\")\n",
    "                quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCU Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# SCU Class\n",
    "import numpy as np\n",
    "import time as t\n",
    "\n",
    "\n",
    "'''\n",
    "Signal_Control_Unit :\n",
    "\n",
    "    interfaces between a signal controller (at a junction) and actions (provided by an agent)\n",
    "\n",
    "    inputs:\n",
    "    -- Vissim\n",
    "    -- Signal_Controller - a Vissim, signal controller\n",
    "    -- compatible_actions - a dictionary taking IDs to vectors non-conflicting signal groups\n",
    "    -- green_time = 10   (times are in seconds but converted to simulation steps)\n",
    "    -- redamber_time = 1\n",
    "    -- amber_time = 3\n",
    "    -- red_time = 1\n",
    "\n",
    "    methods :\n",
    "    -- action_update():\n",
    "            initiates the change to a new action\n",
    "\n",
    "            inputs:\n",
    "            -- action_key \n",
    "            -- green_time \n",
    "    -- update():\n",
    "            ensures each signal is correct at each simulation step\n",
    "'''\n",
    "class Signal_Control_Unit:\n",
    "\n",
    "    def __init__(self,\\\n",
    "                 Vissim,\\\n",
    "                 Signal_Controller,\\\n",
    "                 compatible_actions,\\\n",
    "                 Signal_Groups = None,\\\n",
    "                 green_time = 10,\\\n",
    "                 redamber_time = 1,\\\n",
    "                 amber_time = 3, \\\n",
    "                 red_time = 1\\\n",
    "                ):\n",
    "\n",
    "        # get Vissim, signal controller and its signal groups\n",
    "        self.Vissim = Vissim\n",
    "        self.signal_controller = Signal_Controller\n",
    "\n",
    "        if Signal_Groups is None :\n",
    "            self.signal_groups = self.signal_controller.SGs\n",
    "        else :\n",
    "            self.signal_groups = Signal_Groups\n",
    "\n",
    "        # get stae and reward parameters\n",
    "        self.state = self.calculate_state()\n",
    "        self.next_state = None\n",
    "        self.reward = self.calculate_reward()  \n",
    "\n",
    "        self.compatible_actions = compatible_actions\n",
    "\n",
    "        self.time_steps_per_second = self.Vissim.Simulation.AttValue('SimRes')\n",
    "\n",
    "        self.green_time = green_time * self.time_steps_per_second # the green time is in step\n",
    "        self.redamber_time = redamber_time * self.time_steps_per_second\n",
    "        self.amber_time = amber_time * self.time_steps_per_second\n",
    "        self.red_time = red_time * self.time_steps_per_second\n",
    "\n",
    "        # implement 1st action to start\n",
    "        self.action_key = 0   # dict key of current action (we start with 0) \n",
    "        self.next_action_key = 0\n",
    "\n",
    "        self.action_required = False # used to requests an action from agent\n",
    "        self.update_counter = 1\n",
    "        self.intermediate_phase = True # tracks when initiating a new action\n",
    "        self.action_update(self.action_key)    \n",
    "\n",
    "\n",
    "        self.stage = \"Green\" # tracks the stage particularly when in intermediate phase.\n",
    "                             # Stages appear in order: \"Amber\" -> \"Red\" -> \"RedAmber\" -> \"Green\"\n",
    "\n",
    "\n",
    "    '''\n",
    "    update :\n",
    "\n",
    "    returns True if action required (otherwise is None)\n",
    "\n",
    "    implements cycle at each signal group\n",
    "    and updates the stage of the controllers.\n",
    "\n",
    "    (writen so multiple controllers can be updated in parallel)\n",
    "    (Computational Overhead should be lower than before)\n",
    "\n",
    "    '''   \n",
    "    def update(self):\n",
    "\n",
    "        self.update_counter -= 1\n",
    "\n",
    "        # These 'if' clauses mean update computation only happens if needed\n",
    "        if self.update_counter == 0. :\n",
    "            # if update counter just went zero \n",
    "            # then ask for an action \n",
    "            if self.intermediate_phase is False :\n",
    "                self.action_required = True \n",
    "\n",
    "                # Comment this out because it slow they are not implemented yet and are very slow\n",
    "\n",
    "                #self.next_state = self.calculate_state()\n",
    "                #self.reward = self.calculate_reward()\n",
    "\n",
    "            # if during a change\n",
    "            # then make the change\n",
    "            if self.intermediate_phase is True : \n",
    "                self.action_required = False\n",
    "\n",
    "                # Get light color right for each signal group\n",
    "                for sg in self.signal_groups :\n",
    "\n",
    "                    ID = sg.AttValue('No')-1\n",
    "                    #tic = t.time()\n",
    "                    self._color_changer(sg, self.new_colors[ID], self.stage)\n",
    "                    #tac = t.time()\n",
    "                    #print('_color_changer')\n",
    "                    #print(tac-tic)\n",
    "\n",
    "                # change the current stage and get time the stage last for\n",
    "                time = self._stage_changer(self.stage)\n",
    "                self.update_counter = time\n",
    "\n",
    "                # if full transition (Amber->Red->RedAmber-Green) to green done  \n",
    "                if self.stage == \"Green\" :\n",
    "                    self.intermediate_phase = False # record current action is implemented   \n",
    "\n",
    "\n",
    "    '''\n",
    "    sars :\n",
    "    returns state, id of action, reward, next state\n",
    "    '''     \n",
    "    def sars(self):\n",
    "\n",
    "        self.next_state = self.calculate_state()\n",
    "        self.reward = self.calculate_reward(self.next_state)\n",
    "        \n",
    "        sars =  [self.state, self.action_key, self.reward, self.next_state]\n",
    "\n",
    "        self.state = self.next_state\n",
    "        self.action = self.next_action_key\n",
    "\n",
    "        return(sars)\n",
    "\n",
    "\n",
    "    '''\n",
    "    calculate_state:\n",
    "    Alvaro's reward function needs to be more general\n",
    "    '''\n",
    "    def calculate_state(self, length = None, verbose = False):\n",
    "\n",
    "        # mesure the time taken to do this action\n",
    "        #tic = t.time()\n",
    "\n",
    "        Queues = []\n",
    "        Lanes = []\n",
    "        for sg in self.signal_groups :\n",
    "            q = 0 \n",
    "            for sh in sg.SigHeads:\n",
    "                if (sh.Lane.AttValue('Link'),sh.Lane.AttValue('Index')) not in Lanes :\n",
    "                    Lanes.append((sh.Lane.AttValue('Link'),sh.Lane.AttValue('Index')))\n",
    "                    for veh in sh.Lane.Vehs:\n",
    "                        q += veh.AttValue('InQueue')\n",
    "            Queues.append(q)\n",
    "            # Summarize queue size in each lane\n",
    "            if verbose :\n",
    "                print(self.signal_controller.AttValue('No'),sg.AttValue('No'),q)\n",
    "\n",
    "        # now reshape\n",
    "        if length is not None :\n",
    "            state = np.reshape(Queues,[1,length])\n",
    "        else :\n",
    "            state = np.reshape(Queues,[1,len(Queues)])\n",
    "\n",
    "        #tac = t.time()\n",
    "        #print(tac-tic)\n",
    "\n",
    "        return (state)\n",
    "\n",
    "\n",
    "    '''\n",
    "    calculate_reward:\n",
    "    Alvaro's reward function needs to be more general\n",
    "    '''\n",
    "    def calculate_reward(self,state=None):\n",
    "        if state is None:\n",
    "            state = self.calculate_state()\n",
    "        reward = -np.sum(state)\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    '''\n",
    "    action_update :\n",
    "    initiates a new action\n",
    "        inputs:\n",
    "        -- id of action\n",
    "        -- green_time, if specified by agent (in seconds)\n",
    "    '''    \n",
    "    def action_update(self, next_action_key, green_time=None):\n",
    "        self.intermediate_phase = True # initate intermediate_phase\n",
    "        self.update_counter = 1 # set update counter zero (will get reset at self.update() )\n",
    "        self.next_action_key = next_action_key\n",
    "        self.current_action = self.compatible_actions[next_action_key] \n",
    "        self.new_colors = [ 2*val for val in self.current_action] # converts action to 0,1,2 range\n",
    "\n",
    "        if green_time is not None:\n",
    "            self.green_time = green_time * self.time_steps_per_second\n",
    "\n",
    "        self.action_required = False\n",
    "\n",
    "\n",
    "    # internal helper function\n",
    "    # red = 0, amber/redamber = 1 and green = 2\n",
    "    def _color_convert(self,color):\n",
    "        if color == \"RED\" :\n",
    "            return 0\n",
    "        elif color == \"GREEN\" :\n",
    "            return 2\n",
    "        else :\n",
    "            return 1\n",
    "\n",
    "\n",
    "    '''\n",
    "    _color_changer :\n",
    "    Internal function\n",
    "    Changes color of a signal group\n",
    "        inputs:\n",
    "        -- signal group\n",
    "        -- new_color : 2 = green / 0 = red\n",
    "        -- stage : what stage all lights in the controller are.\n",
    "    '''          \n",
    "    def _color_changer(self,signal_group,new_color,stage):\n",
    "        #Get the current color\n",
    "\n",
    "        #print('current_color')\n",
    "        #tic = t.time()\n",
    "        current_color = self._color_convert(signal_group.AttValue(\"SigState\"))\n",
    "        #tac = t.time()\n",
    "        #print(tac-tic)\n",
    "        change = new_color-current_color\n",
    "\n",
    "        # want green but currently red\n",
    "        if change == -2 and stage == \"Green\" :\n",
    "            signal_group.SetAttValue(\"SigState\", \"AMBER\")\n",
    "\n",
    "        # want red but currently amber\n",
    "        # if just gone red need on second before green change\n",
    "        elif change == -1 and stage == \"Amber\" :\n",
    "            signal_group.SetAttValue(\"SigState\", \"RED\")\n",
    "\n",
    "        # want green but currently red \n",
    "        elif change == 2 and stage == \"Red\" :\n",
    "            signal_group.SetAttValue(\"SigState\", \"REDAMBER\")\n",
    "\n",
    "        # want green but currently redamber\n",
    "        elif change == 1 and stage == \"RedAmber\":\n",
    "            signal_group.SetAttValue(\"SigState\", \"GREEN\")\n",
    "\n",
    "        # if both red or green pass (i.e. no change keep green)\n",
    "        elif change == 0 :\n",
    "            pass\n",
    "\n",
    "\n",
    "    '''\n",
    "    _stage_changer :\n",
    "    Internal function\n",
    "\n",
    "    Track controllers stage (in the stages of Amber->Red->RedAmber-Green) \n",
    "    and time for each transtion\n",
    "\n",
    "        inputs:\n",
    "        -- stage\n",
    "\n",
    "    Nb. stage is a controller method while color is a sg property\n",
    "    '''\n",
    "    def _stage_changer(self,stage):\n",
    "\n",
    "        if stage == \"Green\" :\n",
    "            time = self.amber_time\n",
    "            self.stage = \"Amber\" \n",
    "\n",
    "        elif stage == \"Amber\" :\n",
    "            time = self.red_time\n",
    "            self.stage = \"Red\"\n",
    "\n",
    "\n",
    "        # what is this red stage ? a stage where all the light are red ?\n",
    "        elif stage == \"Red\" :\n",
    "            time = self.redamber_time\n",
    "            self.stage = \"RedAmber\"\n",
    "\n",
    "        # want green but currently redamber\n",
    "        elif stage == \"RedAmber\" :\n",
    "            time = self.green_time\n",
    "            self.stage = \"Green\"\n",
    "\n",
    "        return time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Cyclic Controllers\n",
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generic Agent\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "#############################\n",
    "## THIS IS A GENERIC AGENT ##\n",
    "#############################\n",
    "class Agent():     \n",
    "    # Initialize agent with dimension of state and action space\n",
    "    def __init__(self,state_size, action_size,actions):\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.actions = actions\n",
    "\n",
    "    # Choose and action\n",
    "    def Action(self, state, actions=None):\n",
    "        if actions is None:\n",
    "            actions = self.actions\n",
    "        pass\n",
    "    \n",
    "    # Learning routine\n",
    "    def Learn(self,sarsa):\n",
    "        pass\n",
    "    \n",
    "    # learn from a batch\n",
    "    def Learn_Batch(self,Sarsa, batch_size=32):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MaxWeight Agent \n",
    "'''\n",
    "MaxWeight Agent\n",
    "'''\n",
    "\n",
    "class MaxWeight():\n",
    "    \n",
    "    # Initialize agent with dimension of state and action space\n",
    "    def __init__(self,state_size, action_size,actions):\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.actions = actions\n",
    "        \n",
    "    def choose_action(self,state,actions=None):\n",
    "        if actions is None:\n",
    "            actions=self.actions\n",
    "            \n",
    "        opt_val = 0\n",
    "        for idx, act in actions.items() : \n",
    "            val = np.dot(act,state)\n",
    "            if val >= opt_val :\n",
    "                opt_val = val\n",
    "                opt_idx = idx\n",
    "        return opt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Q learner\n",
    "'''\n",
    "Easy Q_learner\n",
    "'''\n",
    "class Q_function():\n",
    "    def __init__(self, actions = None):\n",
    "        # Q function\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        # number of visits\n",
    "        self.N = defaultdict(lambda: defaultdict(float))\n",
    "        self.actions = actions\n",
    "\n",
    "    def Check(self,state,actions=None):\n",
    "        if actions is None :\n",
    "            actions = self.actions\n",
    "        \n",
    "        if state not in self.Q.keys():\n",
    "            for action in actions:\n",
    "                self.Q[state][action] = 0\n",
    "\n",
    "    def Max(self,state):\n",
    "        Q_maximum = np.max(list(self.Q[state].values()))\n",
    "        return Q_maximum\n",
    "\n",
    "    def Action(self,state,epsilon=0):\n",
    "        if np.random.rand() < epsilon :\n",
    "            idx = np.random.randint(len(self.actions))\n",
    "            action = self.actions[idx]\n",
    "        else :\n",
    "            self.Check(state,self.actions)\n",
    "            action = max(self.Q[state], key=self.Q[state].get)\n",
    "        return action\n",
    "\n",
    "    def Learn(self,sars,learning_rate=0.1,discount_factor=0.5):\n",
    "        state, action, reward, next_state = sars\n",
    "        # Check if state,action and next_state are in Q\n",
    "        self.Check(state)\n",
    "        self.Check(next_state)\n",
    "        self.N_update(state,action)\n",
    "\n",
    "        dQ = reward \\\n",
    "            + discount_factor * self.Max(next_state) \\\n",
    "            - self.Q[state][action]\n",
    "        self.Q[state][action] = self.Q[state][action] + learning_rate * dQ \n",
    "        \n",
    "        return self.Q\n",
    "\n",
    "    def N_update(self,state,action,actions=None):\n",
    "        if actions is None :\n",
    "            actions = self.actions\n",
    "        \n",
    "        if state not in self.N.keys():\n",
    "            for action in actions:\n",
    "                self.N[state][action] = 0 \n",
    "        self.N[state][action] = self.N[state][action] + 1\n",
    "        return self.N[state][action]\n",
    "\n",
    "    def Print(self):\n",
    "        for state in Q_fn.Q.keys():\n",
    "            for action in Q_fn.Q[state].keys():\n",
    "                print(state,action,Q_fn.N[state][action],Q_fn.Q[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main bits and pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here neil\n"
     ]
    }
   ],
   "source": [
    "# Load up environment\n",
    "Controllers_Actions =\\\n",
    "{\\\n",
    "    0 : {   0 : [1, 0, 1, 0],\n",
    "            1 : [0, 1, 0, 1],\n",
    "        },\n",
    "}\n",
    "\n",
    "env = vissim_env(controllers_actions=Controllers_Actions,\\\n",
    "                 Vissim=Vissim,\\\n",
    "                 timesteps_per_second=10,\\\n",
    "                 green_time = 3\\\n",
    "                )\n",
    "\n",
    "env.timesteps_per_second\n",
    "\n",
    "env.mode = 'demo'\n",
    "env.select_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclic Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load agents\n",
    "CC = []    \n",
    "for i, acts in enumerate(Controllers_Actions):\n",
    "    cycle_size = len(Controllers_Actions[i])\n",
    "    CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# picks up first key from each controller\n",
    "actions = dict()\n",
    "for key, val in Controllers_Actions.items():\n",
    "    actions[key] = next(iter(val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "\n",
    "            # in order to find the next action you need to evaluate the next state because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load Agents\n",
    "MWs = []    \n",
    "for i, acts in Controllers_Actions.items():\n",
    "    action_size = len(acts)\n",
    "    state_size =  len(acts[0])\n",
    "    \n",
    "    MWs.append(MaxWeight(state_size,action_size,acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Main loop\n",
    "for _ in range(1000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "\n",
    "            # in order to find the next action you need to evaluate the next state because it is the current state of the simulator\n",
    "            actions[idx] = MWs[idx].choose_action(ns[0])\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load Agents\n",
    "Qs = []    \n",
    "for i, acts in Controllers_Actions.items():\n",
    "    actions = list(acts.keys())\n",
    "    Qs.append(Q_function(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Main loop\n",
    "for _ in range(1000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            Qs[idx].Learn([np.array2string(s),a,r,np.array2string(ns)])\n",
    "            # in order to find the next action you need to evaluate the next state because it is the current state of the simulator\n",
    "            actions[idx] = Qs[idx].Action(np.array2string(ns),epsilon=0.5)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raymond's Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Raymond Agent II\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "#Code adapted from http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "#Code adapted from http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "    def call(self, logits):\n",
    "        # sample a random categorical action from given logits\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlp_policy')\n",
    "        # no tf.get_variable(), just simple Keras API\n",
    "\n",
    "        self.core1 = kl.Dense(32, activation='relu')\n",
    "        \n",
    "        self.value1 = kl.Dense(42, activation='relu', name='value1') #64\n",
    "        self.value2 = kl.Dense(42, activation='relu', name='value2')\n",
    "        self.value3 = kl.Dense(1, name='value3')\n",
    "        # logits are unnormalized log probabilities\n",
    "\n",
    "\n",
    "        self.logits1 = kl.Dense(42, activation='relu', name='policy_logits1')\n",
    "        self.logits2 = kl.Dense(42, activation='relu', name='policy_logits2')\n",
    "        self.logits3 = kl.Dense(num_actions, name='policy_logits3')\n",
    "\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "\n",
    "        # This it the core of the model\n",
    "        x = self.core1(x)\n",
    "        \n",
    "        # separate hidden layers from the core\n",
    "        hidden_logs = self.logits1(x)\n",
    "        hidden_logs = self.logits2(hidden_logs)\n",
    "\n",
    "        hidden_vals = self.value1(x)\n",
    "        hidden_vals = self.value2(hidden_vals)\n",
    "\n",
    "        return self.logits3(hidden_logs), self.value3(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # executes call() under the hood\n",
    "        logits, value = self.predict(obs)\n",
    "        action = self.dist.predict(logits)\n",
    "        # a simpler option, will become clear later why we don't use it\n",
    "        # action = tf.random.categorical(logits, 1)\n",
    "        return action , value\n",
    "\n",
    "\n",
    "# An working model training with entropy = 0.00001 en nstep = 32 and learn every step lr = 0.000065 gama = 0.99 \n",
    "class Modelsave1(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlp_policy')\n",
    "\n",
    "        # no tf.get_variable(), just simple Keras API\n",
    "        self.hidden1 = kl.Dense(42, activation='relu')\n",
    "        self.hidden2 = kl.Dense(42, activation='relu')\n",
    "        self.value = kl.Dense(1, name='value')\n",
    "        # logits are unnormalized log probabilities\n",
    "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "\n",
    "        # This it the core of the model\n",
    "       \n",
    "        # separate hidden layers from the core\n",
    "        hidden_logs = self.hidden1(x)\n",
    "        hidden_vals = self.hidden2(x)\n",
    "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # executes call() under the hood\n",
    "        logits, value = self.predict(obs)\n",
    "        action = self.dist.predict(logits)\n",
    "        # a simpler option, will become clear later why we don't use it\n",
    "        # action = tf.random.categorical(logits, 1)\n",
    "        return action , value\n",
    "  \n",
    "    \n",
    "class ACAgent_II:\n",
    "\n",
    "    def __init__(self,\\\n",
    "                 state_size,\\\n",
    "                 action_size,\\\n",
    "                 #ID,\\\n",
    "                 #state_type,\\\n",
    "                 #npa,\\\n",
    "                 n_step_size =32,\\\n",
    "                 gamma = 0.99,\\\n",
    "                 alpha = 0.000065,\\\n",
    "                 entropy = 0.00001 ):\n",
    "                    #Vissim):\n",
    "\n",
    "\n",
    "        print(\"Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \")\n",
    "        # agent type flag \n",
    "        self.type = 'AC'\n",
    "\n",
    "\n",
    "\n",
    "        #just temporary\n",
    "        self.epsilon = 0\n",
    "\n",
    "        self.trainstep = 0\n",
    "\n",
    "        # Model\n",
    "        # hyperparameters for loss terms and Agent\n",
    "        self.params = {'value': 0.5, 'entropy': entropy, 'gamma': gamma}\n",
    "        self.model = Modelsave1(action_size)\n",
    "        self.model.compile(\n",
    "            optimizer=ko.RMSprop(lr=alpha),\n",
    "            # define separate losses for policy logits and value estimate\n",
    "            loss=[self._logits_loss, self._value_loss]\n",
    "        )\n",
    "\n",
    "#         # Agent Junction ID and Controller ID\n",
    "#         self.signal_id = ID\n",
    "#         self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "#         self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "        # Number of states, action space and memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Potential actions (compatible phases) and transitions\n",
    "        self.update_counter = 1                                 # Timesteps until next update\n",
    "        if self.action_size == 2:\n",
    "            self.compatible_actions = [[0,1,0,1],[1,0,1,0]]         # Potential actions (compatible phases), 1 means green\n",
    "        elif self.action_size == 8:\n",
    "            self.compatible_actions = [[1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                        [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                        [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                        [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                        [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                        [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                        [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                        [0,0,0,0,1,1,0,0,0,0,1,1]]\n",
    "        else:\n",
    "            raise Exception(\"ERROR: Wrong Action Size. Please review master settings and RLAgents.py\")\n",
    "        # Internal State Traffic Control Variables\n",
    "        self.intermediate_phase = False                         # Boolean indicating an ongoing green-red or red-green transition\n",
    "        self.transition_vector = []                             # Vector that will store the transitions between updates\n",
    "\n",
    "\n",
    "        # Initial Setup of S, A, R, S_\n",
    "        self.state = np.zeros((1,state_size))\n",
    "        self.newstate = np.zeros((1,state_size))\n",
    "        self.action = 0\n",
    "        self.newaction = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        # Metrics Storage Initialization\n",
    "        self.episode_reward = []\n",
    "        self.loss = []\n",
    "        self.queues_over_time = [[0,0,0,0]]\n",
    "        self.accumulated_delay= [0]\n",
    "\n",
    "\n",
    "        # The memory will store (state , action , reward, next_state) in a batch\n",
    "        self.memory = deque(maxlen=n_step_size)\n",
    "        self.n_step_size = n_step_size\n",
    "\n",
    "    # Add memory on the right, if over memory limit, pop leftmost item\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append([state, action, reward, next_state])\n",
    "\n",
    "    # Update the Junction IDs for the agent\n",
    "    def update_IDS(self, ID, npa):\n",
    "        self.signal_id = ID\n",
    "        self.signal_controller = npa.signal_controllers[self.signal_id]\n",
    "        self.signal_groups = npa.signal_groups[self.signal_id]\n",
    "\n",
    "    # Need to test before loading to build the graph (surely an other way to do it ...)\n",
    "    def test(self):\n",
    "        _,_ = self.model.action_value(np.empty((1,self.state_size)))\n",
    "\n",
    "\n",
    "    def _value_loss(self, returns, value):\n",
    "        # value loss is typically MSE between value estimates and returns\n",
    "        return self.params['value']*kls.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, acts_and_advs, logits):\n",
    "        # a trick to input actions and advantages through same API\n",
    "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
    "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
    "        # from_logits argument ensures transformation into normalized probabilities\n",
    "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        # policy loss is defined by policy gradients, weighted by advantages\n",
    "        # note: we only calculate the loss on the actions we've actually taken\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "        # entropy loss can be calculated via CE over itself\n",
    "        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n",
    "        # here signs are flipped because optimizer minimizes\n",
    "        return policy_loss - self.params['entropy']*entropy_loss\n",
    "\n",
    "    def _returns_advantages(self, rewards, values, next_value):\n",
    "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # returns are calculated as discounted sum of future rewards\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1]\n",
    "        returns = returns[:-1]\n",
    "        # advantages are returns - baseline, value estimates in our case\n",
    "        advantages = returns - values\n",
    "        return returns, advantages\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        action, _ = self.model.action_value(state)\n",
    "        return np.squeeze(action, axis=-1)\n",
    "\n",
    "    #Performing step of gradient descent on the agent memory\n",
    "    def learn(self):\n",
    "\n",
    "\n",
    "        Sample = np.array(self.memory)\n",
    "\n",
    "        states, actions, rewards, next_state  = np.concatenate(Sample[:,0], axis=0), Sample[:,1].astype('int32'), Sample[:,2], np.concatenate(Sample[:,3], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        _, values = self.model.action_value(states)\n",
    "        values = values.squeeze()\n",
    "\n",
    "        _, next_value  = self.model.action_value(next_state)\n",
    "\n",
    "        next_value = next_value[-1]\n",
    "\n",
    "\n",
    "\n",
    "        returns, advs = self._returns_advantages(rewards, values, next_value)\n",
    "\n",
    "        # a trick to input actions and advantages through same API\n",
    "\n",
    "        acts_and_advs = np.concatenate([actions[:, np.newaxis], advs[:, np.newaxis]], axis=-1)\n",
    "\n",
    "        # performs a full training step on the collected batch\n",
    "        # note: no need to mess around with gradients, Keras API handles it\n",
    "        losses = self.model.train_on_batch(states, [acts_and_advs, returns])\n",
    "\n",
    "        #print(losses)\n",
    "\n",
    "\n",
    "\n",
    "    # def train(self, env, batch_sz=32, updates=1000):\n",
    "    #     # storage helpers for a single batch of data\n",
    "    #     actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "    #     rewards, dones, values = np.empty((3, batch_sz))\n",
    "    #     observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "    #     # training loop: collect samples, send to optimizer, repeat updates times\n",
    "    #     ep_rews = [0.0]\n",
    "    #     next_obs = env.reset()\n",
    "    #     for update in range(updates):\n",
    "    #         for step in range(batch_sz):\n",
    "    #             observations[step] = next_obs.copy()\n",
    "    #             actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "    #             next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "    #               ep_rews[-1] += rewards[step]\n",
    "    #             if dones[step]:\n",
    "    #                 ep_rews.append(0.0)\n",
    "    #                 next_obs = env.reset()\n",
    "\n",
    "    #           _, next_value = self.model.action_value(next_obs[None, :])\n",
    "    #         returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "    #         # a trick to input actions and advantages through same API\n",
    "    #         acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "    #         # performs a full training step on the collected batch\n",
    "    #         # note: no need to mess around with gradients, Keras API handles it\n",
    "    #         losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "    #     return ep_rews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    }
   ],
   "source": [
    "AC = ACAgent_II(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load up environment\n",
    "Controllers_Actions =\\\n",
    "{\\\n",
    "    0 : {   0 : [1, 0, 1, 0],\n",
    "            1 : [0, 1, 0, 1],\n",
    "        },\n",
    "}\n",
    "\n",
    "env = vissim_env(controllers_actions=Controllers_Actions,\\\n",
    "                 Vissim=Vissim,\\\n",
    "                 timesteps_per_second=1,\\\n",
    "                 green_time = 3,\\\n",
    "                 sim_length= 1001\n",
    "                )\n",
    "\n",
    "env.timesteps_per_second\n",
    "\n",
    "env.mode = 'training'\n",
    "env.select_mode()\n",
    "\n",
    "# picks up first key from each controller\n",
    "actions = dict()\n",
    "for key, val in Controllers_Actions.items():\n",
    "    actions[key] = next(iter(val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-45.045045045045065\n",
      "-43.22522522522523\n",
      "-45.54954954954954\n",
      "-45.10810810810812\n",
      "-46.89189189189188\n",
      "-45.42342342342342\n",
      "-46.738738738738746\n",
      "-46.67567567567568\n",
      "-46.58558558558558\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "av_reward = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required :\n",
    "        #print(action_required, (toc-tuc) - (toc-tic) )\n",
    "        s,a,r,ns,d = SARSDs[0]\n",
    "        #print(d)\n",
    "        #tic =  time()\n",
    "        AC.remember(s,a,r,ns)\n",
    "        #tac =  time()\n",
    "        actions[0]=int(AC.choose_action(ns))\n",
    "        #toc =  time()\n",
    "        AC.learn()\n",
    "        # This could be internalized\n",
    "        if d :\n",
    "            print(av_reward)\n",
    "            t=0\n",
    "            av_reward = 0\n",
    "            env.Vissim.Simulation.Stop()\n",
    "            for _ in range(10):\n",
    "                env.Vissim.Simulation.RunSingleStep()\n",
    "            env.done = False\n",
    "        else:\n",
    "            t +=1 \n",
    "            av_reward += ( r - av_reward ) / t\n",
    "        \n",
    "        #print(tac-tic,toc-tac,tuc-toc)\n",
    "        #print(action_required, toc-tic, tuc-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for sg in env.npa.signal_controllers[0].SGs:\n",
    "    print(sg.AttValue(\"SigState\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, 'VISSIM.Vissim.1100', 'AttValue failed: Object 3 - 1: South: Attribute Signal state is no subject to changes.', None, 0, -2147352567), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-1f9ca3f5c50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal_controllers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SigState\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Green\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mSetAttValue\u001b[1;34m(self, Attribut, arg1)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, 'VISSIM.Vissim.1100', 'AttValue failed: Object 3 - 1: South: Attribute Signal state is no subject to changes.', None, 0, -2147352567), None)"
     ]
    }
   ],
   "source": [
    "env.npa.signal_controllers[0].SGs[0].SetAttValue(\"SigState\",\"Green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.Vissim.Simulation.AttValue('IsRunning'):\n",
    "    print('hello')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
