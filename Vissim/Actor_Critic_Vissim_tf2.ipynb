{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import math\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from Actor_Critic_Agents import ACAgent\n",
    "\n",
    "\n",
    "\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PER\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes \n",
    "episodes = 400\n",
    "partial_save_at = 25 #50\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "Surtrac = False\n",
    "AC = True\n",
    "PER_activated = False\n",
    "\n",
    "\n",
    "alpha   = 0.0000005\n",
    "gamma   = 0.85  #0.85 # 0.99\n",
    "entropy = 0.000001 # exploration\n",
    "value = 5 #0.5 # weight attributed in the value loss during gradient descent\n",
    "\n",
    "n_sample = 10 # number of sample for the value check\n",
    "horizon = 100 # horizon of the value check, the number of step forward uses to compute the return\n",
    "\n",
    "# In order to reduce entropy during training (not implemented yet)\n",
    "reduce_entropy = False\n",
    "reduce_entropy_every = 1000\n",
    "\n",
    "n_step_size = 8 #16 # number of step in the n step learning 32\n",
    "# Do not work if n_step_size is aver 31\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1 # worked with 2400\n",
    "\n",
    "\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = (2,4,22)  #5 # 4 queues or 5 queues + signal state    #49 53 (1,8,6) for conv\n",
    "action_size = 2\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP) # worked with 600\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":600, 'm':300, 'l':150}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\n"
     ]
    }
   ],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"retraining\" = continue the training of previous agent\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Network Model Parameters\n",
    "model_name  = 'Single_Cross_Mod2'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Triple_Mod'\n",
    "# 'Single_Cross_Mod2'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "agent_type = 'AC' # DQN, DuelingDQN, DDQN, DuelingDDQN AC\n",
    "reward_type = 'Queues'\n",
    "\n",
    "state_type  = 'CellsT'\n",
    "#CellsSpeedOccSig'    # 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig' CellsSpeedSig \n",
    "#CellsSpeedOccSig 'CellsOccSig' 'CellsT'\n",
    "# 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig'\n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = True\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Loading the best agent during demo and training\n",
    "best = True\n",
    "\n",
    "\n",
    "\n",
    "# Session ID\n",
    "#Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "\n",
    "# Adding the state type to the Session_ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_State_\"+state_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "print(Session_ID)\n",
    "\n",
    "if mode == 'demo' :\n",
    "    simulation_length = 3601\n",
    "    demand_list = [[demand['l'], demand['l']]]\n",
    "    demand_change_timesteps = simulation_length\n",
    "\n",
    "if mode == 'test' : \n",
    "    simulation_length = 3601\n",
    "    demand_change_timesteps = 450\n",
    "    demand = {\"h\":800, 'm':400, 'l':200}\n",
    "    demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "                  [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "                  [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "                  [demand['l'], demand['h']], [demand['l'], demand['m']]]\n",
    "    Random_Seed = 1\n",
    "    # Loading the best agent\n",
    "    best = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Mod2.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"modelconv\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value_conv1 (Conv2D)         multiple                  16960     \n",
      "_________________________________________________________________\n",
      "value_conv2 (Conv2D)         multiple                  24608     \n",
      "_________________________________________________________________\n",
      "value_conv3 (Conv2D)         multiple                  6160      \n",
      "_________________________________________________________________\n",
      "value1 (Dense)               multiple                  1056      \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  528       \n",
      "_________________________________________________________________\n",
      "value_last (Dense)           multiple                  17        \n",
      "_________________________________________________________________\n",
      "value_flat (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "policy_conv1 (Conv2D)        multiple                  8480      \n",
      "_________________________________________________________________\n",
      "policy_conv2 (Conv2D)        multiple                  12320     \n",
      "_________________________________________________________________\n",
      "policy_conv3 (Conv2D)        multiple                  6160      \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1056      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  528       \n",
      "_________________________________________________________________\n",
      "policy_last (Dense)          multiple                  34        \n",
      "_________________________________________________________________\n",
      "policy_flat (Flatten)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 77,907\n",
      "Trainable params: 77,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corected\n",
      "Deployed 1 agent(s) of the Class AC.\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f53142a19334a40baa28a68e91e4699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=400)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/400, Epsilon:0, Average reward: -285.1\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -0.0, -0.0, -1.0, -1.0, -1.0, -0.0, -1.0, -0.0, -0.0] \n",
      " [-3358.0, -28.0, -42.0, -3342.0, -3382.0, -56.0, -2730.0, -3379.0, -290.0, -154.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.84, 1.04], [0.0, -0.0], [0.0, -0.0], [-0.72, 0.92], [-1.36, 2.21], [-0.42, 0.36], [-0.12, 0.12], [-0.69, 1.12], [-0.01, 0.01], [-0.01, 0.13]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 2/400, Epsilon:0, Average reward: -313.76\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -1.0, -0.0, -0.0, -1.0, -1.0, -1.0, -1.0, -1.0] \n",
      " [-3486.0, -3405.0, -3333.0, -2437.0, -806.0, -286.0, -3427.0, -196.0, -3451.0, -3399.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.48, 1.35], [-1.0, 1.42], [-1.1, 1.42], [-0.24, 0.7], [-0.28, 0.4], [-0.25, 1.23], [-0.63, 1.39], [-0.29, 0.9], [-1.04, 2.54], [-0.38, 0.8]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 3/400, Epsilon:0, Average reward: -300.37\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -0.0, -1.0, -0.0, -1.0, -1.0, -2.0, -1.0, -1.0] \n",
      " [-3418.0, -735.0, -3416.0, -3412.0, -2969.0, -3462.0, -3424.0, -3458.0, -40.0, -3455.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.98, 1.02], [-0.52, 0.85], [-0.68, 0.58], [-0.6, 0.9], [-0.38, 0.56], [-0.42, 0.45], [-1.03, 1.82], [-0.3, 1.03], [-0.14, 0.65], [-0.43, 1.11]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 4/400, Epsilon:0, Average reward: -258.18\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -1.0, -2.0, -0.0, -1.0, -0.0, -0.0, -2.0, -0.0] \n",
      " [-1148.0, -522.0, -3261.0, -536.0, -23.0, -393.0, -764.0, -2502.0, -3588.0, -581.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.1, 0.36], [-0.6, 1.12], [-0.24, 0.49], [-0.1, 0.79], [-0.52, 0.77], [-0.47, 0.83], [-0.08, 0.29], [-0.43, 0.73], [-0.4, 1.28], [-0.43, 0.67]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 5/400, Epsilon:0, Average reward: -298.86\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -1.0, -0.0, -0.0, -1.0, -1.0, -1.0, -0.0, -2.0, -2.0] \n",
      " [-3460.0, -3413.0, -57.0, -838.0, -3389.0, -49.0, -2159.0, -64.0, -3410.0, -1537.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.91, 1.75], [-0.4, 0.55], [-0.0, 0.01], [-0.01, 0.02], [-0.32, 0.28], [-0.21, 0.34], [-0.62, 0.92], [-0.29, 0.4], [-0.33, 1.25], [-0.52, 1.2]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 6/400, Epsilon:0, Average reward: -219.77\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0] \n",
      " [-855.0, -3488.0, -3257.0, -855.0, -1145.0, -289.0, -3350.0, -58.0, -3446.0, -988.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.03, 0.14], [-0.16, 0.42], [-1.01, 1.02], [-0.78, 0.86], [-0.4, 0.68], [-0.35, 0.35], [-0.69, 1.04], [-0.18, 0.46], [-1.48, 2.25], [-0.41, 1.07]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 7/400, Epsilon:0, Average reward: -328.87\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -3.0, -1.0] \n",
      " [-3423.0, -1889.0, -3385.0, -2355.0, -1936.0, -2798.0, -2854.0, -2263.0, -86.0, -26.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.33, 0.82], [-0.64, 0.61], [-0.18, 0.41], [-0.46, 0.87], [-0.24, 0.52], [-0.23, 0.38], [-0.35, 0.33], [-0.28, 0.3], [-0.62, 0.75], [-0.05, 0.56]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 8/400, Epsilon:0, Average reward: -345.17\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -3.0] \n",
      " [-45.0, -335.0, -3469.0, -2203.0, -3416.0, -2372.0, -3388.0, -3449.0, -39.0, -3218.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.37, 0.48], [-0.55, 0.79], [-0.11, 0.46], [-0.46, 0.92], [-0.28, 0.22], [-0.49, 0.88], [-0.38, 0.93], [-0.13, 0.21], [0.07, 0.37], [-0.77, 0.6]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 9/400, Epsilon:0, Average reward: -330.95\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -1.0, -1.0, -1.0, -1.0, -1.0, -3.0, -1.0, -2.0, -1.0] \n",
      " [-3442.0, -2119.0, -3392.0, -3479.0, -2502.0, -3394.0, -3431.0, -1303.0, -2275.0, -1788.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.48, 0.85], [-0.18, 0.16], [-0.05, 0.27], [-0.12, 0.38], [-0.12, 0.32], [-0.23, 0.31], [-0.69, 1.35], [-0.04, 0.15], [-0.61, 1.13], [-0.16, 0.3]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 10/400, Epsilon:0, Average reward: -178.44\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -3.0, -3.0, -0.0, -2.0, -2.0, -1.0, -0.0, -3.0, -2.0] \n",
      " [-8.0, -411.0, -137.0, -61.0, -411.0, -309.0, -3040.0, -78.0, -2583.0, -36.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.0, -0.0], [-0.54, 0.53], [-0.06, 0.31], [0.0, -0.0], [-0.4, 0.68], [-0.42, 1.2], [-0.11, 0.47], [-0.01, 0.03], [-0.39, 1.04], [-0.2, 0.4]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 11/400, Epsilon:0, Average reward: -249.52\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -3.0, -2.0, -4.0, -1.0, -2.0, -0.0, -2.0, -1.0, -4.0] \n",
      " [-3540.0, -3556.0, -537.0, -129.0, -1449.0, -3403.0, -1496.0, -3456.0, -1265.0, -3332.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.52, 1.4], [-0.28, 0.41], [-0.28, 0.44], [-0.5, 1.39], [-0.11, 0.19], [-0.34, 0.77], [-0.08, 0.13], [-0.36, 1.03], [-0.22, 0.44], [-0.59, 1.38]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352562, 'Nombre de paramètres non valide.', None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bc44a88d308e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m             SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n\u001b[0;32m    104\u001b[0m                                       \u001b[0mseconds_per_green\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_yellow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdemand_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdemand_change_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                                       PER_activated,Surtrac = Surtrac)\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;31m# Calculate episode average reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mrun_simulation_episode\u001b[1;34m(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second, seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode, PER_activated, Surtrac)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;31m# Cycle through all agents and update them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mAgents_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVissim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_green\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds_per_yellow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSurtrac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSurtrac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mAgents_update\u001b[1;34m(Agents, Vissim, state_type, reward_type, state_size, seconds_per_green, seconds_per_yellow, mode, time_t, Surtrac)\u001b[0m\n\u001b[0;32m     87\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_phase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                                 \u001b[1;31m# Compute the current State and store it in the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                                 \u001b[1;31m# Reward generated by last Action and store it in the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Simulator_Functions.py\u001b[0m in \u001b[0;36mcalculate_state\u001b[1;34m(Vissim, state_type, state_size, action)\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[0mSpeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDetectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mDetector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDetectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                         \u001b[0mSpeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VehSpeed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                         \u001b[0mOccupancy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'OccupRate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mAttValue\u001b[1;34m(self, Attribut, arg1)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352562, 'Nombre de paramètres non valide.', None, None)"
     ]
    }
   ],
   "source": [
    "# Have to find a way to reduce entropy over time entropy = exploration\n",
    "\n",
    "## Converging network\n",
    "# - reward queue, state queue\n",
    "# converging with updates every steps and entropy = 0.00001 and 1 core layer of 42\n",
    "# converging well with updates every steps and entropy = 0.00001 and no core\n",
    "\n",
    "# - reward queue state queues + sig\n",
    "# converging well with updates every steps and entropy = 0.00001 and no core\n",
    "\n",
    "# 64 is a good number\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                simulation_length, timesteps_per_second,\\\n",
    "                                                                delete_results = True, verbose = True)\n",
    "    SF.Select_Vissim_Mode(Vissim,mode)\n",
    "    \n",
    "    runflag = True\n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['AC'] :\n",
    "        Agents = [ACAgent(state_size, action_size, ID, state_type, npa, n_step_size, gamma, alpha, entropy, value, Vissim) for ID in npa.signal_controllers_ids] \n",
    "        for agent in Agents:\n",
    "            # to initialise the computational graph ot the model (I am sure there is a better way to to this)\n",
    "            agent.test()\n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents , reward_storage = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = best)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0\n",
    "                \n",
    "    # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        \n",
    "        \n",
    "        Vissim = None\n",
    "    \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\" or mode == \"retraining\":\n",
    "        print(\"Training\")\n",
    "        \n",
    "            \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                \n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "        \n",
    "            # Run Episode at maximum speed\n",
    "            \n",
    "            SF.Select_Vissim_Mode(Vissim, mode)\n",
    "            \n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated,Surtrac = Surtrac)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "            \n",
    "            for index, agent in enumerate(Agents):\n",
    "                predicted_values, true_values, logit0, logits = agent.value_check(horizon, n_sample)\n",
    "                print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(index, predicted_values, true_values))\n",
    "                print (\"Agent {} : Logits on those states : \\n {}\" .format(index, logits))\n",
    "                print (\"Agent {} : Logits on the 0 state : \\n {}\" .format(index, logit0))\n",
    "               \n",
    "        \n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "            \n",
    "            # line to reduce the entropy of the actor_critic.\n",
    "            if reduce_entropy:\n",
    "                pass\n",
    "            \n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()\n",
    "\n",
    "print(reward_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
