import numpy as np
import os
import pickle
from keras.models import load_model

# Set Fastest Mode in Simulator
def Set_Quickmode(Vissim, timesteps_per_second):
	# Set speed parameters in Vissim
	Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)
	Vissim.Graphics.CurrentNetworkWindow.SetAttValue("QuickMode",1)
	Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)
	Vissim.SuspendUpdateGUI()  

# Run a Single Episode for a set simulation length
def run_simulation_episode(Agents, Vissim, state_type, state_size, simulation_length, timesteps_per_second, seconds_per_update, Demo_Mode, PER_activated):

	# During each timestep in the simulation
	for time_t in range(simulation_length):

		# Cycle through all agents
		for index, agent in enumerate(Agents):

			# Check if agent needs to update
			if agent.update_counter > 0:
				# If it doesn't, substract 1 timestep from the counter and skip the rest
				agent.update_counter -= 1
				continue

			# Update the agent
			elif agent.update_counter == 0:
				print("time to update!!")
				# Make sure the agent is not in the middle of a transition
				if agent.intermediate_phase == False:
					# Compute the current State and store it in the agent
					agent.newstate = calculate_state(Vissim, state_type, state_size)
					# Reward generated by last Action and store it in the agent
					agent.reward   = calculate_reward(agent)
					# Commit previous State, previous Action, Reward generated and current State to memory
					agent.remember(agent.state, agent.action, agent.reward, agent.newstate)
					# Compute the new Action and store it in the agent
					agent.newaction = agent.choose_action(agent.newstate)
					# In Demonstration Mode, show the Reward of the last cycle
					if mode == "demo":
						print('Agent Reward in this cycle is : {}'.format(round(agent.reward,2)))

					# If the same Action is chosen
					if agent.newaction == agent.action:
						# Extend Timer (do nothing)
						agent.update_counter += seconds_per_update * timesteps_per_second
						
					# If a different Action is chosen
					elif agent.newaction != agent.action:
						# Fetch the meaning of the Actions from the compatible Actions in the Agent
						previous_action = agent.compatible_actions[agent.action]
						current_action = agent.compatible_actions[agent.newaction]
						# Check transition vector for the whole intersection (1, 0 or -1)
						agent.transition_vector = np.subtract(previous_action, current_action)

						# Cycle through the groups and start the transition
						for index_group, sig_group in enumerate(agent.signal_groups):
							# If the transition vector is > 0, we are changing from GREEN to RED, so set AMBER
							if agent.transition_vector[index_group] == 1:
								sig_group.SetAttValue("SigState", "AMBER")
							# If the transition vector is < 0, we are changing from RED to GREEN, so set to REDAMBER
							elif agent.transition_vector[index_group] == -1:
								sig_group.SetAttValue("SigState", "REDAMBER")
							# If the transition vector is zero, the phase stays the same
							elif agent.transition_vector[index_group] == 0:
								pass
							else:
								print("ERROR: Incongruent new phase and previous phase. Please review the code.")
								break
						# Extend timer after transition is started
						agent.update_counter += seconds_per_update 
						# Record that a transition is happening
						agent.intermediate_phase = True

				# If the agent is in the middle of a transition
				elif agent.intermediate_phase == True:
					# Finalize the change
					for index_group, sig_group in enumerate(agent.signal_groups):
						# Use transition vector from previous iteration to finish the change
						if agent.transition_vector[index_group] == 1:
							sig_group.SetAttValue("SigState", "RED")
						elif agent.transition_vector[index_group] == -1:
							sig_group.SetAttValue("SigState", "GREEN")
						elif agent.transition_vector[index_group] == 0:
							pass
						else:
							print("ERROR: Incongruent new phase and previous phase. Please review the code.")
							break
					# Mark the transition as finished
					agent.intermediate_phase = False	
					# Set timer for next update 
					agent.update_counter += seconds_per_update

				# Update internal State
				agent.state  = agent.newstate
				# Update internal Action
				agent.action = agent.newaction  

			# Error protection against negative update counters
			else:
				print("ERROR: Update Counter for agent {} is negative. Please investigate.".format(index))
            
		# Advance the game to the next second (proportionally to the simulator resolution).
		for _ in range(0, timesteps_per_second):
			Vissim.Simulation.RunSingleStep()
	# Stop the simulation    
	Vissim.Simulation.Stop()

def calculate_state(Vissim, state_type, state_size):
	if state_type == 'Queues':
    	#Obtain Queue Values (average value over the last period)
		West_Queue  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QLen(Current,Last)')
		South_Queue = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QLen(Current,Last)')
		East_Queue  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QLen(Current,Last)')
		North_Queue = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QLen(Current,Last)')
		state = [West_Queue, South_Queue, East_Queue, North_Queue]
		state = np.reshape(state, [1,state_size])
		return(state)
	elif state_type == 'Delay':
		# Obtain Delay Values (average delay in lane * nr cars in queue)
		West_Delay    = Vissim.Net.DelayMeasurements.ItemByKey(1).AttValue('VehDelay(Current,Last,All)') 
		West_Stopped  = Vissim.Net.QueueCounters.ItemByKey(1).AttValue('QStops(Current,Last)')
		South_Delay   = Vissim.Net.DelayMeasurements.ItemByKey(2).AttValue('VehDelay(Current,Last,All)') 
		South_Stopped = Vissim.Net.QueueCounters.ItemByKey(2).AttValue('QStops(Current,Last)')
		East_Delay    = Vissim.Net.DelayMeasurements.ItemByKey(3).AttValue('VehDelay(Current,Last,All)') 
		East_Stopped  = Vissim.Net.QueueCounters.ItemByKey(3).AttValue('QStops(Current,Last)')
		North_Delay   = Vissim.Net.DelayMeasurements.ItemByKey(4).AttValue('VehDelay(Current,Last,All)') 
		North_Stopped = Vissim.Net.QueueCounters.ItemByKey(4).AttValue('QStops(Current,Last)')

		pre_state = [West_Delay, South_Delay, East_Delay, North_Delay, West_Stopped, South_Stopped, East_Stopped, North_Stopped]
		pre_state = [0 if state is None else state for state in pre_state]
        
		state = [pre_state[0]*pre_state[4], pre_state[1]*pre_state[5], pre_state[2]*pre_state[6], pre_state[3]*pre_state[7]]
		state = np.reshape(state, [1,state_size])
		return(state)
	elif state_type == 'MaxFlow':
		pass
	elif state_type == 'FuelConsumption':
		pass
	elif state_type == 'NOx':
		pass
	elif state_type == "COM":
		pass

def calculate_reward(agent):
        reward = -np.sum([0 if state is None else state for state in agent.newstate[0]])
        #print(reward)
        agent.episode_reward.append(reward)
        return reward

def PER_prepopulate_memory(Agents, Vissim, state_type, state_size, memory_size, vissim_working_directory, model_name):
	memory = []
	# Chech if suitable file exists
	PER_prepopulation_filename =  os.path.join(vissim_working_directory, model_name, 'PER_Prepopulation_length_'+ str(memory_size) +'.p')
	PER_prepopulation_exists = os.path.isfile(PER_prepopulation_filename)
	# If it does, process it into the memory
	if PER_prepopulation_exists:
		print("Previous Experience Found")
		for agent in Agents:
			memory = pickle.load(open(PER_prepopulation_filename, 'rb'))
			for s,a,r,s_ in memory:
				agent.remember(s,a,r,s_)
	# Otherwise generate random data, store it in the agent and generate the file above
	else:
		memory_full = False
		cycle_t = 0
		while not memory_full:
			if cycle_t == 900:
				for agent in Agents:
					agent.newstate = agent.get_state(state_type, state_size, Vissim)
					agent.reward   = agent.get_reward()
	
					memory.append((agent.state, agent.action, agent.reward, agent.newstate))
					agent.remember(agent.state, agent.action, agent.reward, agent.newstate)
	
					agent.action   = agent.act(agent.newstate)
					agent.state    = agent.newstate
					if len(memory) == memory_size:
						memory_full = True
				cycle_t = 0
			else:
				cycle_t += 1
 	         
			# Advance the game to the next frame based on the action.
			Vissim.Simulation.RunSingleStep()
		# Dump random transitions into pickle file for later prepopulation of PER
		pickle.dump(memory, open(PER_prepopulation_filename, 'wb'))
		# Stop the simulation   
		Vissim.Simulation.Stop()
	return(memory)

# Average reward across agents after an episode
def average_reward(reward_storage, Agents, episode, episodes):
	average_reward = []
	for agent in Agents:
		average_agent_reward = np.average(agent.episode_reward)
		average_reward.append(average_agent_reward)
	average_reward = np.average(average_reward)
	reward_storage.append(average_reward)
    
	if len(Agents)>1:
			# Print the score and break out of the loop
			print("Episode: {}/{}, Epsilon:{}, Average reward: {}".format(episode+1, episodes, np.round(Agents[0].epsilon,2),np.round(average_reward,2)))
			print("Prediction for [5000,0,5000,0] is: {}".format(Agents[0].model.predict(np.reshape([5000,0,5000,0], [1,4]))))
			for agent in enumerate(Agents):
				print("Agent {}, Average agent reward: {}".format(agent, average_reward[index]))
	else:
		print("Episode: {}/{}, Epsilon:{}, Average reward: {}".format(episode+1, episodes, np.round(Agents[0].epsilon,2), np.round(average_reward,2)))
		print("Prediction for [500,0,500,0] is: {}".format(Agents[0].model.predict(np.reshape([500,0,500,0], [1,4]))))
	return(reward_storage, average_reward)

# Reload agents
def load_agents(vissim_working_directory, model_name, Agents, Session_ID, best):
	print('Loading Pre-Trained Agent, Architecture, Optimizer and Memory.')
	for index, agent in enumerate(Agents):
		Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'.h5')
		agent.model = load_model(Filename)
		if best:
			Memory_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_BestAgent'+str(index)+'_Memory'+'.p')
		else:
			Memory_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'_Memory'+'.p')
		agent.memory = pickle.load(open(Memory_Filename, 'rb'))
		Training_Progress_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'_Training'+'.p')
		agent.memory = pickle.load(open(Training_Progress_Filename, 'rb'))
		Loss_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'_Loss'+'.p')
		agent.Loss = pickle.load(open(Loss_Filename, 'rb'))
		
	print('Items successfully loaded.')
	return(Agents)

# Save agents
def save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage):
	for index,agent in enumerate(Agents):    
		Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'.h5')
		print('Saving architecture, weights and optimizer state for agent-{}'.format(index))
		agent.model.save(Filename)
		Memory_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'_Memory'+'.p')
		print('Dumping agent-{} memory into pickle file'.format(index))
		pickle.dump(agent.memory, open(Memory_Filename, 'wb'))
		Training_Progress_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'_Training'+'.p')
		print('Dumping Training Results into pickle file.')
		pickle.dump(reward_storage, open(Training_Progress_Filename, 'wb'))
		Loss_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_Agent'+str(index)+'_Loss'+'.p')
		print('Dumping Loss Results into pickle file.')
		pickle.dump(agent.loss, open(Loss_Filename, 'wb'))

# Save the agent producing best reward
def best_agent(reward_storage, average_reward, best_agent_weights, vissim_working_directory, model_name, Agents, Session_ID):
	if average_reward == np.max(reward_storage):
		for index, agent in enumerate(Agents):
			best_agent_weights = agent.memory
			Memory_Filename = os.path.join(vissim_working_directory, model_name, model_name+'_'+ Session_ID + '_BestAgent'+str(index)+'_Memory'+'.p')
			pickle.dump(agent.memory, open(Memory_Filename, 'wb'))
			print("New best agent found. Saved in {}".format(Memory_Filename))
	return(best_agent_weights)

def change_demand():
	for vehicle_input in len(Network.VehicleInputs):
		Vissim.Net.VehicleInputs.ItemByKey(vehicle_input).SetAttValue('Volume(1)', demands[np.random.randint(0,len(demands)-1)])    
        