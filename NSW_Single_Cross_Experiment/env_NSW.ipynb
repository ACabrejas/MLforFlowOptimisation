{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Modules that we need\n",
    "from Vissim_env_class import environment, Load_Vissim\n",
    "from MasterAC_Agent import MasterAC_Agent\n",
    "from MasterDQN_Agent import MasterDQN_Agent\n",
    "\n",
    "# General Libraries\n",
    "import numpy as np \n",
    "import pylab as plt\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "vissim \\\n",
    "= \\\n",
    "Load_Vissim(\n",
    "Path_to_network = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\NSW_Single_Cross_Experiment\\\\Single_Cross_Straight\\\\',\\\n",
    "inpx_Filename = 'Single_Cross_Straight.inpx',\\\n",
    "layx_Filename = 'Single_Cross_Straight.layx',\\\n",
    "attempts=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single Cross AC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory=\\\n",
    "'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\n",
    "\\\\Documents\\\\MLforFlowOptimisation\\\\NSW_Single_Cross_Experiment\\\\'\n",
    "\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Straigth_AC\"\n",
    "\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "        'queues_counter_ID' : [1,2,3,4]}\n",
    "        },\n",
    " 'demand' : { 'default' : [800, 800, 800, 800],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.00001\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 0.5\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 1\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  288       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  1176      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  25        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  288       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1176      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  50        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 3,003\n",
      "Trainable params: 3,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents\\\n",
    "= MasterAC_Agent(model_name, \n",
    "                 vissim_working_directory, \n",
    "                 sim_length, \n",
    "                 Single_Cross_Straight_dictionary,\n",
    "                 n_step_size, \n",
    "                 gamma, alpha, entropy, value,\n",
    "                 Random_Seed = Random_Seed, \n",
    "                 timesteps_per_second = 1, \n",
    "                 Session_ID = Session_ID, \n",
    "                 verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample,\n",
    "                save_location = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Desktop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.1 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1024 16:11:56.002338 21752 deprecation.py:323] From C:\\Users\\nwalton\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 101\n",
      "Episode 1 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-46.0, -46.0, -30.0, -46.0, -46.0, -46.0, -46.0, -46.0, -46.0, -46.0] \n",
      " [-9473.0, -9473.0, -8675.0, -9473.0, -9473.0, -9473.0, -9473.0, -9473.0, -9473.0, -9473.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -489.18\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 102\n",
      "Episode 2 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-70.0, -70.0, -70.0, -70.0, -70.0, -70.0, -70.0, -70.0, -70.0, -70.0] \n",
      " [-9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -484.26\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 103\n",
      "Episode 3 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-93.0, -93.0, -93.0, -36.0, -93.0, -93.0, -93.0, -93.0, -93.0, -93.0] \n",
      " [-9504.0, -9504.0, -9504.0, -6559.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -489.41\n",
      "Random Seed Set to 104\n",
      "Episode 4 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-121.0, -121.0, -121.0, -121.0, -121.0, -121.0, -121.0, -121.0, -121.0, -6.0] \n",
      " [-9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -4441.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.98, 0.02]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -488.19\n",
      "Random Seed Set to 105\n",
      "Episode 5 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-148.0, -148.0, -148.0, -148.0, -148.0, -148.0, -148.0, -148.0, -148.0, -148.0] \n",
      " [-9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -481.98\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 106\n",
      "Episode 6 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-178.0, -178.0, -178.0, -178.0, -178.0, -178.0, -178.0, -178.0, -178.0, -4.0] \n",
      " [-9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -2617.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.73, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -470.79\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 107\n",
      "Episode 7 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-211.0, -211.0, -211.0, -211.0, -211.0, -211.0, -211.0, -211.0, -211.0, -211.0] \n",
      " [-9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -487.22\n",
      "Random Seed Set to 108\n",
      "Episode 8 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-244.0, -244.0, -244.0, -244.0, -244.0, -244.0, -244.0, -244.0, -76.0, -244.0] \n",
      " [-9399.0, -9399.0, -9399.0, -9399.0, -9399.0, -9399.0, -9399.0, -9399.0, -5715.0, -9399.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -482.26\n",
      "Random Seed Set to 109\n",
      "Episode 9 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-279.0, -269.0, -279.0, -279.0, -279.0, -279.0, -87.0, -17.0, -279.0, -279.0] \n",
      " [-9401.0, -9344.0, -9401.0, -9401.0, -9401.0, -9401.0, -5924.0, -4129.0, -9401.0, -9401.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -476.77\n",
      "Random Seed Set to 110\n",
      "Episode 10 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-314.0, -314.0, -310.0, -138.0, -314.0, -80.0, -314.0, -314.0, -314.0, -314.0] \n",
      " [-9442.0, -9442.0, -9439.0, -6694.0, -9442.0, -5031.0, -9442.0, -9442.0, -9442.0, -9442.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -478.32\n",
      "Random Seed Set to 111\n",
      "Episode 11 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-351.0, -351.0, -351.0, -351.0, -351.0, -351.0, -351.0, -351.0, -351.0, -351.0] \n",
      " [-9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -485.89\n",
      "Random Seed Set to 112\n",
      "Episode 12 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-391.0, -391.0, -391.0, -391.0, -391.0, -40.0, -391.0, -391.0, -391.0, -391.0] \n",
      " [-9405.0, -9405.0, -9405.0, -9405.0, -9405.0, -4771.0, -9405.0, -9405.0, -9405.0, -9405.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -484.66\n",
      "Random Seed Set to 113\n",
      "Episode 13 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-431.0, -431.0, -431.0, -431.0, -431.0, -425.0, -431.0, -431.0, -312.0, -431.0] \n",
      " [-9410.0, -9410.0, -9410.0, -9410.0, -9410.0, -9413.0, -9410.0, -9410.0, -8474.0, -9410.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -477.95\n",
      "Random Seed Set to 114\n",
      "Episode 14 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-473.0, -473.0, -473.0, -473.0, -473.0, -473.0, -473.0, -473.0, -473.0, -473.0] \n",
      " [-9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -479.71\n",
      "Random Seed Set to 115\n",
      "Episode 15 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-516.0, -372.0, -516.0, -516.0, -516.0, -516.0, -516.0, -516.0, -516.0, -516.0] \n",
      " [-9323.0, -8091.0, -9323.0, -9323.0, -9323.0, -9323.0, -9323.0, -9323.0, -9323.0, -9323.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -476.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 116\n",
      "Episode 16 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-575.0, -575.0, -575.0, -575.0, -575.0, -575.0, -575.0, -575.0, -575.0, -575.0] \n",
      " [-9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -492.3\n",
      "Random Seed Set to 117\n",
      "Episode 17 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-621.0, -621.0, -621.0, -621.0, -621.0, -621.0, -621.0, -621.0, -540.0, -621.0] \n",
      " [-9426.0, -9426.0, -9426.0, -9426.0, -9426.0, -9426.0, -9426.0, -9426.0, -9121.0, -9426.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -479.09\n",
      "Random Seed Set to 118\n",
      "Episode 18 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-672.0, -672.0, -672.0, -672.0, -672.0, -1.0, -672.0, -102.0, -154.0, -157.0] \n",
      " [-9396.0, -9396.0, -9396.0, -9396.0, -9396.0, -2237.0, -9396.0, -4647.0, -5483.0, -5907.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.5, 0.5], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -481.11\n",
      "Random Seed Set to 119\n",
      "Episode 19 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-734.0, -734.0, -734.0, -734.0, -734.0, -137.0, -734.0, -734.0, -734.0, -734.0] \n",
      " [-9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -4326.0, -9465.0, -9465.0, -9465.0, -9465.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -483.38\n",
      "Random Seed Set to 120\n",
      "Episode 20 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-786.0, -786.0, -786.0, -786.0, -786.0, -786.0, -48.0, -786.0, -786.0, -786.0] \n",
      " [-9389.0, -9389.0, -9389.0, -9389.0, -9389.0, -9389.0, -3534.0, -9389.0, -9389.0, -9389.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.97, 0.03], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -474.76\n",
      "Random Seed Set to 121\n",
      "Episode 21 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-859.0, -859.0, -787.0, -859.0, -859.0, -859.0, -859.0, -859.0, -859.0, -859.0] \n",
      " [-9475.0, -9475.0, -9270.0, -9475.0, -9475.0, -9475.0, -9475.0, -9475.0, -9475.0, -9475.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -482.52\n",
      "Random Seed Set to 122\n",
      "Episode 22 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-920.0, -920.0, -920.0, -920.0, -920.0, -920.0, -920.0, -920.0, -920.0, -920.0] \n",
      " [-9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -480.86\n",
      "Random Seed Set to 123\n",
      "Episode 23 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-998.0, -998.0, -975.0, -998.0, -998.0, -998.0, -998.0, -998.0, -998.0, -998.0] \n",
      " [-9433.0, -9433.0, -9381.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -482.41\n",
      "Random Seed Set to 124\n",
      "Episode 24 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1086.0, -1086.0, -219.0, -1086.0, -1086.0, -1086.0, -1086.0, -1086.0, -1086.0, -1086.0] \n",
      " [-9532.0, -9532.0, -5081.0, -9532.0, -9532.0, -9532.0, -9532.0, -9532.0, -9532.0, -9532.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -490.49\n",
      "Random Seed Set to 125\n",
      "Episode 25 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1148.0, -1148.0, -1148.0, -1148.0, -1141.0, -1148.0, -1148.0, -1148.0, -1148.0, -1148.0] \n",
      " [-9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -485.51\n",
      "Random Seed Set to 126\n",
      "Episode 26 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0] \n",
      " [-9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -478.63\n",
      "Random Seed Set to 127\n",
      "Episode 27 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0] \n",
      " [-9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -486.98\n",
      "Random Seed Set to 128\n",
      "Episode 28 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1410.0, -1355.0, -1410.0, -1410.0, -1410.0, -1410.0, -843.0, -1410.0, -1410.0, -1410.0] \n",
      " [-9411.0, -9353.0, -9411.0, -9411.0, -9411.0, -9411.0, -7840.0, -9411.0, -9411.0, -9411.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -473.45\n",
      "Random Seed Set to 129\n",
      "Episode 29 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -1504.0, -1504.0, -1504.0, -1504.0, -1504.0, -1504.0, -512.0, -76.0, -1504.0] \n",
      " [-2001.0, -9394.0, -9394.0, -9394.0, -9394.0, -9394.0, -9394.0, -5583.0, -2879.0, -9394.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.53, 0.47], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.77, 0.23], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -473.94\n",
      "Random Seed Set to 130\n",
      "Episode 30 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1614.0, -162.0, -1614.0, -1614.0, -1614.0, -1614.0, -1614.0, -1614.0, -1614.0, -1614.0] \n",
      " [-9467.0, -3740.0, -9467.0, -9467.0, -9467.0, -9467.0, -9467.0, -9467.0, -9467.0, -9467.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.98, 0.02], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -482.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 131\n",
      "Episode 31 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1731.0, -1553.0, -1731.0, -701.0, -1731.0, -1731.0, -1731.0, -210.0, -1731.0, -1731.0] \n",
      " [-9530.0, -9375.0, -9530.0, -6574.0, -9530.0, -9530.0, -9530.0, -3679.0, -9530.0, -9530.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -481.57\n",
      "Random Seed Set to 132\n",
      "Episode 32 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1813.0, -1813.0, -1813.0, -1813.0, -1426.0, -1692.0, -10.0, -1813.0, -1813.0, -1813.0] \n",
      " [-9393.0, -9393.0, -9393.0, -9393.0, -8779.0, -9299.0, -2396.0, -9393.0, -9393.0, -9393.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.71, 0.29], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -471.22\n",
      "Random Seed Set to 133\n",
      "Episode 33 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1952.0, -1952.0, -1952.0, -1952.0, -1639.0, -1952.0, -1952.0, -1952.0, -1952.0, -1952.0] \n",
      " [-9500.0, -9500.0, -9500.0, -9500.0, -9058.0, -9500.0, -9500.0, -9500.0, -9500.0, -9500.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -486.48\n",
      "Random Seed Set to 134\n",
      "Episode 34 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0] \n",
      " [-9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -477.96\n",
      "Random Seed Set to 135\n",
      "Episode 35 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-800.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0] \n",
      " [-6265.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -481.62\n",
      "Random Seed Set to 136\n",
      "Episode 36 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -2320.0, -2320.0, -2320.0, -2320.0, -184.0, -2320.0, -655.0, -2320.0, -2320.0] \n",
      " [-2497.0, -9463.0, -9463.0, -9463.0, -9463.0, -3647.0, -9463.0, -5843.0, -9463.0, -9463.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.78, 0.22], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.93, 0.07], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -479.99\n",
      "Random Seed Set to 137\n",
      "Episode 37 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0] \n",
      " [-9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -487.29\n",
      "Random Seed Set to 138\n",
      "Episode 38 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -475.0] \n",
      " [-9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -4632.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -481.89\n",
      "Random Seed Set to 139\n",
      "Episode 39 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0] \n",
      " [-9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -481.97\n",
      "Random Seed Set to 140\n",
      "Episode 40 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2875.0, -2875.0, -2875.0, -2875.0, -2875.0, -2875.0, -2875.0, -2875.0, -2706.0, -2875.0] \n",
      " [-9433.0, -9434.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0, -9392.0, -8240.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.22, 0.78], [0.14, 0.86]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -463.78\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 141\n",
      "Episode 41 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-280.0, -69.0, -138.0, -105.0, -185.0, -54.0, -69.0, -93.0, -126.0, -72.0] \n",
      " [-705.0, -681.0, -598.0, -579.0, -373.0, -381.0, -583.0, -428.0, -538.0, -617.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.38, 0.62], [1.0, 0.0], [1.0, 0.0], [0.28, 0.72], [0.17, 0.83], [0.42, 0.58], [0.41, 0.59], [0.37, 0.63], [0.35, 0.65], [0.36, 0.64]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -31.6\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 142\n",
      "Episode 42 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-66.0, -73.0, -81.0, -67.0, -113.0, -170.0, -5.0, -125.0, -62.0, -43.0] \n",
      " [-495.0, -531.0, -412.0, -506.0, -533.0, -595.0, -452.0, -517.0, -474.0, -591.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.38, 0.62], [0.98, 0.02], [0.42, 0.58], [0.43, 0.57], [0.14, 0.86], [0.35, 0.65], [0.49, 0.51], [0.99, 0.01], [1.0, 0.0], [0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -27.65\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 143\n",
      "Episode 43 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-148.0, -14.0, -197.0, -117.0, -117.0, -78.0, -41.0, -5.0, -347.0, -45.0] \n",
      " [-314.0, -462.0, -383.0, -405.0, -479.0, -347.0, -585.0, -366.0, -316.0, -395.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.2, 0.8], [0.83, 0.17], [0.11, 0.89], [0.35, 0.65], [0.96, 0.04], [0.31, 0.69], [0.99, 0.01], [0.49, 0.51], [0.01, 0.99], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -24.07\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 144\n",
      "Episode 44 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-38.0, -234.0, -21.0, -229.0, -64.0, -187.0, -145.0, -15.0, -133.0, -34.0] \n",
      " [-442.0, -617.0, -495.0, -482.0, -312.0, -451.0, -449.0, -534.0, -347.0, -509.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.99, 0.01], [0.36, 0.64], [0.82, 0.18], [1.0, 0.0], [0.42, 0.58], [0.05, 0.95], [0.12, 0.88], [0.9, 0.1], [1.0, 0.0], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -24.34\n",
      "Random Seed Set to 145\n",
      "Episode 45 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-91.0, -29.0, -67.0, -48.0, -71.0, -44.0, -34.0, -282.0, -31.0, -81.0] \n",
      " [-326.0, -433.0, -423.0, -495.0, -501.0, -551.0, -368.0, -311.0, -579.0, -465.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [0.38, 0.62], [0.99, 0.01], [0.06, 0.94], [1.0, 0.0], [0.27, 0.73]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -24.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 146\n",
      "Episode 46 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-51.0, -11.0, -528.0, -110.0, -48.0, -192.0, -90.0, -174.0, -35.0, -278.0] \n",
      " [-482.0, -380.0, -667.0, -366.0, -822.0, -346.0, -436.0, -452.0, -517.0, -494.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.34, 0.66], [0.82, 0.18], [0.0, 1.0], [0.17, 0.83], [0.35, 0.65], [0.02, 0.98], [0.99, 0.01], [0.04, 0.96], [0.37, 0.63], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Average Reward for Agent 0 this episode : -25.1\n",
      "Random Seed Set to 147\n",
      "Episode 47 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-42.0, -65.0, -63.0, -90.0, -55.0, -300.0, -143.0, -61.0, -16.0, -7.0] \n",
      " [-343.0, -297.0, -423.0, -346.0, -377.0, -686.0, -470.0, -386.0, -409.0, -457.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.31, 0.69], [0.26, 0.74], [1.0, 0.0], [0.14, 0.86], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.95, 0.05], [0.61, 0.39]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Average Reward for Agent 0 this episode : -23.36\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 148\n",
      "Episode 48 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -435.0, -181.0, -55.0, -104.0, -94.0, -226.0, -131.0, -48.0, -154.0] \n",
      " [-415.0, -655.0, -771.0, -477.0, -495.0, -302.0, -348.0, -474.0, -443.0, -366.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.51, 0.49], [0.0, 1.0], [0.01, 0.99], [1.0, 0.0], [0.97, 0.03], [1.0, 0.0], [0.03, 0.97], [0.05, 0.95], [0.26, 0.74], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Average Reward for Agent 0 this episode : -24.43\n",
      "Random Seed Set to 149\n",
      "Episode 49 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-190.0, -135.0, -48.0, -51.0, -58.0, -48.0, -74.0, -263.0, -77.0, -54.0] \n",
      " [-336.0, -364.0, -418.0, -533.0, -489.0, -418.0, -390.0, -407.0, -410.0, -429.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.99], [0.03, 0.97], [0.24, 0.76], [1.0, 0.0], [0.31, 0.69], [0.24, 0.76], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.25, 0.75]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.52 0.48]\n",
      "Average Reward for Agent 0 this episode : -22.56\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 150\n",
      "Episode 50 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18.0, -156.0, -151.0, -136.0, -375.0, -19.0, -41.0, -51.0, -140.0, -32.0] \n",
      " [-452.0, -433.0, -512.0, -482.0, -618.0, -401.0, -356.0, -391.0, -332.0, -473.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.98, 0.02], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [1.0, 0.0], [0.02, 0.98], [0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.52 0.48]\n",
      "Average Reward for Agent 0 this episode : -23.3\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Saving at :  C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Desktop\\Episode50Agent0_Weights.h5\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Random Seed Set to 151\n",
      "Episode 51 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-72.0, -226.0, -21.0, -99.0, -195.0, -139.0, -106.0, -74.0, -8.0, -24.0] \n",
      " [-429.0, -281.0, -518.0, -375.0, -439.0, -473.0, -468.0, -256.0, -568.0, -454.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.86, 0.14], [0.99, 0.01], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.67, 0.33], [0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.53 0.47]\n",
      "Average Reward for Agent 0 this episode : -25.8\n",
      "Random Seed Set to 152\n",
      "Episode 52 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-201.0, -161.0, -195.0, -51.0, -136.0, -88.0, -107.0, -247.0, -66.0, -104.0] \n",
      " [-323.0, -659.0, -561.0, -614.0, -498.0, -358.0, -399.0, -383.0, -611.0, -384.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.01, 0.99], [0.0, 1.0], [0.19, 0.81], [0.16, 0.84], [1.0, 0.0], [0.03, 0.97], [0.0, 1.0], [1.0, 0.0], [0.04, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.53 0.47]\n",
      "Average Reward for Agent 0 this episode : -25.04\n",
      "Random Seed Set to 153\n",
      "Episode 53 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-44.0, -31.0, -197.0, -104.0, -35.0, -38.0, -7.0, -279.0, -7.0, -275.0] \n",
      " [-539.0, -487.0, -533.0, -576.0, -462.0, -362.0, -391.0, -422.0, -455.0, -353.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.3, 0.7], [0.99, 0.01], [0.65, 0.35], [0.03, 0.97], [1.0, 0.0], [1.0, 0.0], [0.54, 0.46], [0.0, 1.0], [0.54, 0.46], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.54 0.46]\n",
      "Average Reward for Agent 0 this episode : -24.96\n",
      "Random Seed Set to 154\n",
      "Episode 54 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-103.0, -67.0, -133.0, -280.0, -236.0, -204.0, -67.0, -245.0, -183.0, -98.0] \n",
      " [-404.0, -548.0, -459.0, -535.0, -467.0, -621.0, -548.0, -627.0, -458.0, -329.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.97], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.01, 0.99], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.02, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.55 0.45]\n",
      "Average Reward for Agent 0 this episode : -25.45\n",
      "Random Seed Set to 155\n",
      "Episode 55 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-36.0, -15.0, -93.0, -161.0, -83.0, -102.0, -26.0, -103.0, -250.0, -60.0] \n",
      " [-411.0, -371.0, -321.0, -488.0, -397.0, -476.0, -528.0, -425.0, -605.0, -532.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.91, 0.09], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.03, 0.97], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.55 0.45]\n",
      "Average Reward for Agent 0 this episode : -24.93\n",
      "Random Seed Set to 156\n",
      "Episode 56 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -99.0, -94.0, -101.0, -25.0, -659.0, -598.0, -7.0, -101.0, -262.0] \n",
      " [-528.0, -338.0, -490.0, -432.0, -397.0, -489.0, -696.0, -412.0, -432.0, -344.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.75, 0.25], [0.02, 0.98], [0.02, 0.98], [0.02, 0.98], [1.0, 0.0], [1.0, 0.0], [0.14, 0.86], [0.56, 0.44], [0.02, 0.98], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.56 0.44]\n",
      "Average Reward for Agent 0 this episode : -24.89\n",
      "Random Seed Set to 157\n",
      "Episode 57 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-35.0, -38.0, -154.0, -99.0, -45.0, -7.0, -43.0, -170.0, -186.0, -25.0] \n",
      " [-363.0, -392.0, -332.0, -341.0, -515.0, -306.0, -409.0, -600.0, -728.0, -350.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.02, 0.98], [1.0, 0.0], [0.56, 0.44], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.56 0.44]\n",
      "Average Reward for Agent 0 this episode : -24.1\n",
      "Random Seed Set to 158\n",
      "Episode 58 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -272.0, -393.0, -347.0, -234.0, -193.0, -67.0, -35.0, -347.0, -159.0] \n",
      " [-397.0, -365.0, -310.0, -506.0, -443.0, -316.0, -432.0, -403.0, -355.0, -509.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.57, 0.43], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.04, 0.96], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.57 0.43]\n",
      "Average Reward for Agent 0 this episode : -22.97\n",
      "Random Seed Set to 159\n",
      "Episode 59 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-31.0, -272.0, -10.0, -114.0, -136.0, -120.0, -63.0, -46.0, -8.0, -107.0] \n",
      " [-394.0, -565.0, -521.0, -555.0, -721.0, -547.0, -333.0, -489.0, -611.0, -466.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.8, 0.2], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.09, 0.91], [1.0, 0.0], [0.58, 0.42], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.58 0.42]\n",
      "Average Reward for Agent 0 this episode : -24.38\n",
      "Random Seed Set to 160\n",
      "Episode 60 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-407.0, -37.0, -71.0, -50.0, -162.0, -47.0, -10.0, -171.0, -47.0, -94.0] \n",
      " [-496.0, -238.0, -428.0, -622.0, -348.0, -315.0, -595.0, -310.0, -403.0, -556.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.08, 0.92], [0.0, 1.0], [1.0, 0.0], [0.82, 0.18], [1.0, 0.0], [1.0, 0.0], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.59 0.41]\n",
      "Average Reward for Agent 0 this episode : -22.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 161\n",
      "Episode 61 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-297.0, -274.0, -76.0, -150.0, -122.0, -67.0, -47.0, -57.0, -50.0, -59.0] \n",
      " [-623.0, -449.0, -244.0, -439.0, -636.0, -385.0, -378.0, -294.0, -342.0, -557.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.25, 0.75], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.59 0.41]\n",
      "Average Reward for Agent 0 this episode : -23.59\n",
      "Random Seed Set to 162\n",
      "Episode 62 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-289.0, -218.0, -70.0, -276.0, -108.0, -102.0, -152.0, -293.0, -257.0, -248.0] \n",
      " [-673.0, -387.0, -515.0, -593.0, -414.0, -389.0, -556.0, -567.0, -669.0, -482.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.9], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.6 0.4]\n",
      "Average Reward for Agent 0 this episode : -25.38\n",
      "Random Seed Set to 163\n",
      "Episode 63 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-65.0, -50.0, -161.0, -46.0, -471.0, -434.0, -203.0, -104.0, -90.0, -128.0] \n",
      " [-500.0, -420.0, -484.0, -374.0, -538.0, -394.0, -353.0, -540.0, -398.0, -250.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.61 0.39]\n",
      "Average Reward for Agent 0 this episode : -20.87\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 164\n",
      "Episode 64 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-63.0, -423.0, -60.0, -9.0, -59.0, -225.0, -465.0, -45.0, -254.0, -133.0] \n",
      " [-406.0, -496.0, -503.0, -415.0, -317.0, -417.0, -740.0, -391.0, -658.0, -310.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.97], [0.01, 0.99], [1.0, 0.0], [0.61, 0.39], [0.04, 0.96], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.61 0.39]\n",
      "Average Reward for Agent 0 this episode : -25.01\n",
      "Random Seed Set to 165\n",
      "Episode 65 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-78.0, -169.0, -72.0, -169.0, -493.0, -278.0, -90.0, -47.0, -57.0, -9.0] \n",
      " [-438.0, -388.0, -764.0, -388.0, -606.0, -422.0, -248.0, -698.0, -516.0, -322.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.62, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.62 0.38]\n",
      "Average Reward for Agent 0 this episode : -25.02\n",
      "Random Seed Set to 166\n",
      "Episode 66 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-75.0, -109.0, -69.0, -70.0, -317.0, -246.0, -101.0, -186.0, -136.0, -194.0] \n",
      " [-465.0, -409.0, -529.0, -450.0, -546.0, -382.0, -416.0, -458.0, -408.0, -508.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.85, 0.15], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.63 0.37]\n",
      "Average Reward for Agent 0 this episode : -24.78\n",
      "Random Seed Set to 167\n",
      "Episode 67 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-237.0, -329.0, -135.0, -289.0, -13.0, -85.0, -112.0, -211.0, -132.0, -59.0] \n",
      " [-627.0, -404.0, -317.0, -340.0, -390.0, -636.0, -462.0, -397.0, -380.0, -414.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.92, 0.08], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.64 0.36]\n",
      "Average Reward for Agent 0 this episode : -24.75\n",
      "Random Seed Set to 168\n",
      "Episode 68 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-197.0, -77.0, -374.0, -9.0, -160.0, -60.0, -231.0, -164.0, -718.0, -193.0] \n",
      " [-561.0, -586.0, -491.0, -439.0, -534.0, -460.0, -470.0, -530.0, -352.0, -290.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.69, 0.31], [1.0, 0.0], [0.0, 1.0], [0.65, 0.35], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.65 0.35]\n",
      "Average Reward for Agent 0 this episode : -24.49\n",
      "Random Seed Set to 169\n",
      "Episode 69 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-337.0, -225.0, -178.0, -261.0, -204.0, -177.0, -42.0, -325.0, -55.0, -50.0] \n",
      " [-482.0, -503.0, -510.0, -471.0, -386.0, -448.0, -417.0, -420.0, -348.0, -455.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.02, 0.98], [0.05, 0.95], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.66 0.34]\n",
      "Average Reward for Agent 0 this episode : -23.06\n",
      "Random Seed Set to 170\n",
      "Episode 70 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-213.0, -207.0, -96.0, -96.0, -261.0, -143.0, -131.0, -218.0, -59.0, -14.0] \n",
      " [-308.0, -308.0, -775.0, -775.0, -267.0, -448.0, -301.0, -584.0, -302.0, -681.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.01, 0.99], [0.94, 0.06]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.67 0.33]\n",
      "Average Reward for Agent 0 this episode : -23.55\n",
      "Random Seed Set to 171\n",
      "Episode 71 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-104.0, -108.0, -116.0, -91.0, -10.0, -144.0, -85.0, -181.0, -96.0, -10.0] \n",
      " [-430.0, -293.0, -508.0, -400.0, -376.0, -337.0, -363.0, -434.0, -453.0, -356.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.68, 0.32], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.68, 0.32]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.68 0.32]\n",
      "Average Reward for Agent 0 this episode : -22.3\n",
      "Random Seed Set to 172\n",
      "Episode 72 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-139.0, -237.0, -117.0, -118.0, -142.0, -51.0, -381.0, -259.0, -169.0, -14.0] \n",
      " [-379.0, -353.0, -358.0, -420.0, -329.0, -275.0, -318.0, -333.0, -272.0, -336.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.69 0.31]\n",
      "Average Reward for Agent 0 this episode : -20.22\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 173\n",
      "Episode 73 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-26.0, -212.0, -107.0, -115.0, -10.0, -55.0, -22.0, -324.0, -74.0, -107.0] \n",
      " [-350.0, -326.0, -435.0, -432.0, -312.0, -299.0, -314.0, -378.0, -452.0, -360.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.7, 0.3], [0.01, 0.99], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.7 0.3]\n",
      "Average Reward for Agent 0 this episode : -21.79\n",
      "Random Seed Set to 174\n",
      "Episode 74 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-241.0, -131.0, -657.0, -170.0, -15.0, -176.0, -222.0, -251.0, -70.0, -284.0] \n",
      " [-545.0, -665.0, -458.0, -396.0, -287.0, -456.0, -686.0, -440.0, -286.0, -585.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.97, 0.03], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.7 0.3]\n",
      "Average Reward for Agent 0 this episode : -25.49\n",
      "Random Seed Set to 175\n",
      "Episode 75 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-175.0, -111.0, -186.0, -80.0, -314.0, -135.0, -191.0, -200.0, -38.0, -334.0] \n",
      " [-485.0, -421.0, -524.0, -465.0, -379.0, -425.0, -473.0, -507.0, -512.0, -416.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.98, 0.02], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.72 0.28]\n",
      "Average Reward for Agent 0 this episode : -22.9\n",
      "Random Seed Set to 176\n",
      "Episode 76 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-276.0, -77.0, -121.0, -39.0, -83.0, -119.0, -222.0, -153.0, -338.0, -117.0] \n",
      " [-453.0, -373.0, -310.0, -622.0, -678.0, -359.0, -313.0, -380.0, -712.0, -454.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.73 0.27]\n",
      "Average Reward for Agent 0 this episode : -24.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 177\n",
      "Episode 77 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-270.0, -126.0, -146.0, -100.0, -69.0, -16.0, -87.0, -65.0, -139.0, -120.0] \n",
      " [-373.0, -490.0, -257.0, -274.0, -263.0, -327.0, -238.0, -281.0, -224.0, -494.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.98, 0.02], [1.0, 0.0], [0.01, 0.99], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.74 0.26]\n",
      "Average Reward for Agent 0 this episode : -20.18\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 178\n",
      "Episode 78 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-164.0, -44.0, -306.0, -344.0, -62.0, -316.0, -212.0, -277.0, -166.0, -103.0] \n",
      " [-501.0, -449.0, -504.0, -500.0, -602.0, -435.0, -405.0, -347.0, -382.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.02, 0.98], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.03, 0.97], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.75 0.25]\n",
      "Average Reward for Agent 0 this episode : -22.6\n",
      "Random Seed Set to 179\n",
      "Episode 79 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-141.0, -16.0, -185.0, -352.0, -151.0, -201.0, -185.0, -66.0, -192.0, -132.0] \n",
      " [-330.0, -405.0, -419.0, -510.0, -323.0, -325.0, -416.0, -389.0, -465.0, -524.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.99, 0.01], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.76 0.24]\n",
      "Average Reward for Agent 0 this episode : -24.18\n",
      "Random Seed Set to 180\n",
      "Episode 80 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-194.0, -120.0, -302.0, -83.0, -44.0, -57.0, -355.0, -289.0, -618.0, -184.0] \n",
      " [-611.0, -358.0, -380.0, -494.0, -384.0, -459.0, -439.0, -493.0, -483.0, -307.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.76 0.24]\n",
      "Average Reward for Agent 0 this episode : -23.56\n",
      "Random Seed Set to 181\n",
      "Episode 81 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -129.0, -76.0, -61.0, -204.0, -129.0, -132.0, -415.0, -160.0, -499.0] \n",
      " [-329.0, -478.0, -556.0, -377.0, -392.0, -478.0, -382.0, -619.0, -247.0, -537.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.77, 0.23], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.59, 0.41], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.77 0.23]\n",
      "Average Reward for Agent 0 this episode : -22.99\n",
      "Random Seed Set to 182\n",
      "Episode 82 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-147.0, -17.0, -138.0, -31.0, -37.0, -57.0, -225.0, -125.0, -340.0, -44.0] \n",
      " [-618.0, -461.0, -480.0, -409.0, -346.0, -382.0, -329.0, -284.0, -507.0, -450.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.99, 0.01], [0.0, 1.0], [0.52, 0.48], [0.75, 0.25], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.79 0.21]\n",
      "Average Reward for Agent 0 this episode : -24.07\n",
      "Random Seed Set to 183\n",
      "Episode 83 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-181.0, -220.0, -33.0, -58.0, -334.0, -193.0, -78.0, -107.0, -17.0, -553.0] \n",
      " [-507.0, -617.0, -564.0, -514.0, -521.0, -314.0, -527.0, -550.0, -436.0, -392.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.8 0.2]\n",
      "Average Reward for Agent 0 this episode : -23.81\n",
      "Random Seed Set to 184\n",
      "Episode 84 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -183.0, -313.0, -332.0, -100.0, -72.0, -171.0, -107.0, -221.0, -305.0] \n",
      " [-406.0, -228.0, -285.0, -394.0, -424.0, -237.0, -513.0, -510.0, -444.0, -486.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.81, 0.19], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.81 0.19]\n",
      "Average Reward for Agent 0 this episode : -24.09\n",
      "Random Seed Set to 185\n",
      "Episode 85 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-103.0, -120.0, -119.0, -325.0, -321.0, -319.0, -262.0, -114.0, -365.0, -200.0] \n",
      " [-559.0, -307.0, -453.0, -384.0, -378.0, -676.0, -526.0, -454.0, -713.0, -373.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.88, 0.12], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.81 0.19]\n",
      "Average Reward for Agent 0 this episode : -22.25\n",
      "Random Seed Set to 186\n",
      "Episode 86 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-409.0, -158.0, -18.0, -44.0, -449.0, -449.0, -150.0, -82.0, -258.0, -75.0] \n",
      " [-404.0, -364.0, -359.0, -379.0, -342.0, -342.0, -467.0, -312.0, -362.0, -430.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.16, 0.84], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.82 0.18]\n",
      "Average Reward for Agent 0 this episode : -21.56\n",
      "Random Seed Set to 187\n",
      "Episode 87 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-42.0, -108.0, -23.0, -295.0, -348.0, -84.0, -62.0, -64.0, -137.0, -259.0] \n",
      " [-309.0, -317.0, -340.0, -376.0, -530.0, -333.0, -441.0, -465.0, -297.0, -312.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.53, 0.47], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.83 0.17]\n",
      "Average Reward for Agent 0 this episode : -21.76\n",
      "Random Seed Set to 188\n",
      "Episode 88 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-164.0, -297.0, -216.0, -196.0, -220.0, -312.0, -77.0, -135.0, -85.0, -87.0] \n",
      " [-423.0, -471.0, -510.0, -468.0, -361.0, -706.0, -371.0, -294.0, -468.0, -418.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.84 0.16]\n",
      "Average Reward for Agent 0 this episode : -22.39\n",
      "Random Seed Set to 189\n",
      "Episode 89 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-143.0, -38.0, -318.0, -211.0, -152.0, -84.0, -522.0, -298.0, -249.0, -85.0] \n",
      " [-414.0, -389.0, -390.0, -635.0, -341.0, -462.0, -431.0, -611.0, -467.0, -423.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.85 0.15]\n",
      "Average Reward for Agent 0 this episode : -22.33\n",
      "Random Seed Set to 190\n",
      "Episode 90 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-222.0, -378.0, -181.0, -196.0, -139.0, -222.0, -203.0, -402.0, -121.0, -141.0] \n",
      " [-529.0, -523.0, -338.0, -378.0, -382.0, -529.0, -609.0, -511.0, -311.0, -569.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.86 0.14]\n",
      "Average Reward for Agent 0 this episode : -24.58\n",
      "Random Seed Set to 191\n",
      "Episode 91 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-143.0, -311.0, -72.0, -84.0, -177.0, -356.0, -95.0, -104.0, -267.0, -159.0] \n",
      " [-322.0, -346.0, -480.0, -433.0, -354.0, -534.0, -288.0, -358.0, -370.0, -664.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.9], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.09, 0.91], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.87 0.13]\n",
      "Average Reward for Agent 0 this episode : -21.11\n",
      "Random Seed Set to 192\n",
      "Episode 92 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-150.0, -248.0, -117.0, -125.0, -70.0, -266.0, -167.0, -441.0, -209.0, -175.0] \n",
      " [-325.0, -392.0, -344.0, -435.0, -406.0, -372.0, -621.0, -435.0, -481.0, -397.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.88 0.12]\n",
      "Average Reward for Agent 0 this episode : -24.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 193\n",
      "Episode 93 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-179.0, -228.0, -69.0, -96.0, -95.0, -107.0, -65.0, -292.0, -206.0, -127.0] \n",
      " [-422.0, -451.0, -311.0, -339.0, -321.0, -373.0, -449.0, -412.0, -484.0, -337.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.77, 0.23], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.88 0.12]\n",
      "Average Reward for Agent 0 this episode : -21.9\n",
      "Random Seed Set to 194\n",
      "Episode 94 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-112.0, -111.0, -156.0, -93.0, -14.0, -250.0, -473.0, -132.0, -68.0, -296.0] \n",
      " [-430.0, -431.0, -624.0, -450.0, -301.0, -443.0, -458.0, -535.0, -398.0, -553.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.89, 0.11], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.89 0.11]\n",
      "Average Reward for Agent 0 this episode : -23.36\n",
      "Random Seed Set to 195\n",
      "Episode 95 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-515.0, -136.0, -14.0, -276.0, -234.0, -76.0, -135.0, -280.0, -272.0, -72.0] \n",
      " [-390.0, -425.0, -420.0, -599.0, -542.0, -464.0, -501.0, -459.0, -377.0, -409.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.9, 0.1], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.9 0.1]\n",
      "Average Reward for Agent 0 this episode : -23.02\n",
      "Random Seed Set to 196\n",
      "Episode 96 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-69.0, -797.0, -221.0, -367.0, -14.0, -112.0, -78.0, -85.0, -75.0, -171.0] \n",
      " [-354.0, -495.0, -391.0, -407.0, -438.0, -273.0, -286.0, -597.0, -420.0, -306.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.91, 0.09], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.91 0.09]\n",
      "Average Reward for Agent 0 this episode : -21.18\n",
      "Random Seed Set to 197\n",
      "Episode 97 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-143.0, -276.0, -443.0, -144.0, -92.0, -107.0, -300.0, -236.0, -92.0, -78.0] \n",
      " [-763.0, -341.0, -645.0, -538.0, -671.0, -449.0, -792.0, -731.0, -366.0, -422.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.92 0.08]\n",
      "Average Reward for Agent 0 this episode : -26.42\n",
      "Random Seed Set to 198\n",
      "Episode 98 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -535.0, -449.0, -176.0, -50.0, -98.0, -43.0, -291.0, -101.0, -39.0] \n",
      " [-381.0, -383.0, -390.0, -420.0, -472.0, -404.0, -308.0, -488.0, -489.0, -356.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.92, 0.08], [1.0, 0.0], [0.05, 0.95], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.92, 0.08]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.92 0.08]\n",
      "Average Reward for Agent 0 this episode : -22.76\n",
      "Random Seed Set to 199\n",
      "Episode 99 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-111.0, -149.0, -246.0, -254.0, -264.0, -66.0, -149.0, -54.0, -158.0, -229.0] \n",
      " [-392.0, -459.0, -455.0, -599.0, -398.0, -425.0, -452.0, -438.0, -426.0, -458.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.93 0.07]\n",
      "Average Reward for Agent 0 this episode : -25.01\n",
      "Random Seed Set to 200\n",
      "Episode 100 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-65.0, -355.0, -92.0, -308.0, -281.0, -184.0, -274.0, -68.0, -400.0, -192.0] \n",
      " [-354.0, -415.0, -571.0, -297.0, -441.0, -396.0, -602.0, -481.0, -401.0, -508.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.94 0.06]\n",
      "Average Reward for Agent 0 this episode : -24.25\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Saving at :  C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Desktop\\Episode100Agent0_Weights.h5\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.train(100,vissim=vissim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.15 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.demo(vissim=vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - delete\n",
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  288       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  1176      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  25        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  288       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1176      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  50        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 3,003\n",
      "Trainable params: 3,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "loaded from  C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Documents\\MLforFlowOptimisation\\NSW_Single_Cross_Experiment\\Single_Cross_Straight\\Agents_Results\\Neil_Results\\Neils_Good_Agents\\Oct24\\Episode100Agent0_Weights.h5\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.load(episode=100,\n",
    "                                          best=False, \n",
    "load_location='C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\NSW_Single_Cross_Experiment\\\\Single_Cross_Straight\\\\Agents_Results\\\\Neil_Results\\\\Neils_Good_Agents\\\\Oct24'\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "vissim_working_directory='C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\Vissim\\\\'\n",
    "\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"DuelingDQN\"\n",
    "Session_ID = \"Single_Cross_Straigth_DuelingDQN20c0\"\n",
    "\n",
    "# all controller actions\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "         'queues_counter_ID' : [1,2,3,4]  }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 300\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MasterDQN_Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e3febf4c5239>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mSingle_Cross_Straight_MultiDQN_Agents\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m MasterDQN_Agent(model_name, \n\u001b[0m\u001b[0;32m      3\u001b[0m                 \u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0msim_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mSingle_Cross_Straight_dictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MasterDQN_Agent' is not defined"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents =\\\n",
    "MasterDQN_Agent(model_name, \n",
    "                vissim_working_directory, \n",
    "                sim_length, \n",
    "                Single_Cross_Straight_dictionary,\n",
    "                'default_actions',\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, \n",
    "                batch_size, copy_weights_frequency, epsilon_sequence,\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, \n",
    "                Session_ID = Session_ID, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Experience: Found. Loading into agents\n",
      "Previous Experience: Successfully loaded file from:\n",
      "C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Documents\\MLforFlowOptimisation\\Vissim\\Single_Cross_Straight\\Agents_Results\\DuelingDQN\\Single_Cross_Straigth_DuelingDQN20c0\\Agent0_PERPre_1000.p\n"
     ]
    }
   ],
   "source": [
    "#Single_Cross_Straight_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Documents\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Working Directory set to: C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Documents\\MLforFlowOptimisation\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.1 seconds.\n",
      "\n",
      "start\n",
      "Random Seed Set to 101\n",
      "Episode 1: Finished running.\n",
      "Agent 0, Average Reward: -38.41\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 1s - loss: 744.2011\n",
      "Reducing exploration for all agents to 0.9772\n",
      "\n",
      "Episode 2: Starting computation.\n",
      "Random Seed Set to 102\n",
      "Episode 2: Finished running.\n",
      "Agent 0, Average Reward: -40.22\n",
      "Epoch 1/1\n",
      " - 0s - loss: 854.4097\n",
      "Reducing exploration for all agents to 0.9548\n",
      "\n",
      "Episode 3: Starting computation.\n",
      "Random Seed Set to 103\n",
      "Episode 3: Finished running.\n",
      "Agent 0, Average Reward: -43.07\n",
      "Epoch 1/1\n",
      " - 0s - loss: 793.4293\n",
      "Reducing exploration for all agents to 0.933\n",
      "\n",
      "Episode 4: Starting computation.\n",
      "Random Seed Set to 104\n",
      "Episode 4: Finished running.\n",
      "Agent 0, Average Reward: -38.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 793.6385\n",
      "Reducing exploration for all agents to 0.9117\n",
      "\n",
      "Episode 5: Starting computation.\n",
      "Random Seed Set to 105\n",
      "Episode 5: Finished running.\n",
      "Agent 0, Average Reward: -34.83\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 705.1667\n",
      "Reducing exploration for all agents to 0.8909\n",
      "\n",
      "Episode 6: Starting computation.\n",
      "Random Seed Set to 106\n",
      "Episode 6: Finished running.\n",
      "Agent 0, Average Reward: -40.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 686.2171\n",
      "Reducing exploration for all agents to 0.8706\n",
      "\n",
      "Episode 7: Starting computation.\n",
      "Random Seed Set to 107\n",
      "Episode 7: Finished running.\n",
      "Agent 0, Average Reward: -37.28\n",
      "Epoch 1/1\n",
      " - 0s - loss: 528.1699\n",
      "Reducing exploration for all agents to 0.8507\n",
      "\n",
      "Episode 8: Starting computation.\n",
      "Random Seed Set to 108\n",
      "Episode 8: Finished running.\n",
      "Agent 0, Average Reward: -35.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 602.2899\n",
      "Reducing exploration for all agents to 0.8313\n",
      "\n",
      "Episode 9: Starting computation.\n",
      "Random Seed Set to 109\n",
      "Episode 9: Finished running.\n",
      "Agent 0, Average Reward: -33.05\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 316.7727\n",
      "Reducing exploration for all agents to 0.8123\n",
      "\n",
      "Episode 10: Starting computation.\n",
      "Random Seed Set to 110\n",
      "Episode 10: Finished running.\n",
      "Agent 0, Average Reward: -35.58\n",
      "Epoch 1/1\n",
      " - 0s - loss: 276.9485\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.7937\n",
      "\n",
      "Episode 11: Starting computation.\n",
      "Random Seed Set to 111\n",
      "Episode 11: Finished running.\n",
      "Agent 0, Average Reward: -30.83\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 738.9574\n",
      "Reducing exploration for all agents to 0.7756\n",
      "\n",
      "Episode 12: Starting computation.\n",
      "Random Seed Set to 112\n",
      "Episode 12: Finished running.\n",
      "Agent 0, Average Reward: -28.66\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 483.9535\n",
      "Reducing exploration for all agents to 0.7579\n",
      "\n",
      "Episode 13: Starting computation.\n",
      "Random Seed Set to 113\n",
      "Episode 13: Finished running.\n",
      "Agent 0, Average Reward: -33.38\n",
      "Epoch 1/1\n",
      " - 0s - loss: 467.2921\n",
      "Reducing exploration for all agents to 0.7406\n",
      "\n",
      "Episode 14: Starting computation.\n",
      "Random Seed Set to 114\n",
      "Episode 14: Finished running.\n",
      "Agent 0, Average Reward: -34.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 397.4078\n",
      "Reducing exploration for all agents to 0.7237\n",
      "\n",
      "Episode 15: Starting computation.\n",
      "Random Seed Set to 115\n",
      "Episode 15: Finished running.\n",
      "Agent 0, Average Reward: -30.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 328.8647\n",
      "Reducing exploration for all agents to 0.7071\n",
      "\n",
      "Episode 16: Starting computation.\n",
      "Random Seed Set to 116\n",
      "Episode 16: Finished running.\n",
      "Agent 0, Average Reward: -32.38\n",
      "Epoch 1/1\n",
      " - 0s - loss: 239.5042\n",
      "Reducing exploration for all agents to 0.691\n",
      "\n",
      "Episode 17: Starting computation.\n",
      "Random Seed Set to 117\n",
      "Episode 17: Finished running.\n",
      "Agent 0, Average Reward: -28.12\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 422.2842\n",
      "Reducing exploration for all agents to 0.6752\n",
      "\n",
      "Episode 18: Starting computation.\n",
      "Random Seed Set to 118\n",
      "Episode 18: Finished running.\n",
      "Agent 0, Average Reward: -36.69\n",
      "Epoch 1/1\n",
      " - 0s - loss: 339.2178\n",
      "Reducing exploration for all agents to 0.6598\n",
      "\n",
      "Episode 19: Starting computation.\n",
      "Random Seed Set to 119\n",
      "Episode 19: Finished running.\n",
      "Agent 0, Average Reward: -28.85\n",
      "Epoch 1/1\n",
      " - 0s - loss: 468.7333\n",
      "Reducing exploration for all agents to 0.6447\n",
      "\n",
      "Episode 20: Starting computation.\n",
      "Random Seed Set to 120\n",
      "Episode 20: Finished running.\n",
      "Agent 0, Average Reward: -30.99\n",
      "Epoch 1/1\n",
      " - 0s - loss: 422.5314\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.63\n",
      "\n",
      "Episode 21: Starting computation.\n",
      "Random Seed Set to 121\n",
      "Episode 21: Finished running.\n",
      "Agent 0, Average Reward: -27.11\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 648.7977\n",
      "Reducing exploration for all agents to 0.6156\n",
      "\n",
      "Episode 22: Starting computation.\n",
      "Random Seed Set to 122\n",
      "Episode 22: Finished running.\n",
      "Agent 0, Average Reward: -28.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 832.1935\n",
      "Reducing exploration for all agents to 0.6015\n",
      "\n",
      "Episode 23: Starting computation.\n",
      "Random Seed Set to 123\n",
      "Episode 23: Finished running.\n",
      "Agent 0, Average Reward: -24.99\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 547.1431\n",
      "Reducing exploration for all agents to 0.5878\n",
      "\n",
      "Episode 24: Starting computation.\n",
      "Random Seed Set to 124\n",
      "Episode 24: Finished running.\n",
      "Agent 0, Average Reward: -26.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 511.8903\n",
      "Reducing exploration for all agents to 0.5744\n",
      "\n",
      "Episode 25: Starting computation.\n",
      "Random Seed Set to 125\n",
      "Episode 25: Finished running.\n",
      "Agent 0, Average Reward: -24.26\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 497.0876\n",
      "Reducing exploration for all agents to 0.5613\n",
      "\n",
      "Episode 26: Starting computation.\n",
      "Random Seed Set to 126\n",
      "Episode 26: Finished running.\n",
      "Agent 0, Average Reward: -28.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 678.3939\n",
      "Reducing exploration for all agents to 0.5484\n",
      "\n",
      "Episode 27: Starting computation.\n",
      "Random Seed Set to 127\n",
      "Episode 27: Finished running.\n",
      "Agent 0, Average Reward: -31.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 593.4026\n",
      "Reducing exploration for all agents to 0.5359\n",
      "\n",
      "Episode 28: Starting computation.\n",
      "Random Seed Set to 128\n",
      "Episode 28: Finished running.\n",
      "Agent 0, Average Reward: -29.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 562.9448\n",
      "Reducing exploration for all agents to 0.5237\n",
      "\n",
      "Episode 29: Starting computation.\n",
      "Random Seed Set to 129\n",
      "Episode 29: Finished running.\n",
      "Agent 0, Average Reward: -31.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 521.4076\n",
      "Reducing exploration for all agents to 0.5117\n",
      "\n",
      "Episode 30: Starting computation.\n",
      "Random Seed Set to 130\n",
      "Episode 30: Finished running.\n",
      "Agent 0, Average Reward: -26.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 680.8503\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.5\n",
      "\n",
      "Episode 31: Starting computation.\n",
      "Random Seed Set to 131\n",
      "Episode 31: Finished running.\n",
      "Agent 0, Average Reward: -27.98\n",
      "Epoch 1/1\n",
      " - 0s - loss: 848.4825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.4886\n",
      "\n",
      "Episode 32: Starting computation.\n",
      "Random Seed Set to 132\n",
      "Episode 32: Finished running.\n",
      "Agent 0, Average Reward: -24.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 671.7003\n",
      "Reducing exploration for all agents to 0.4775\n",
      "\n",
      "Episode 33: Starting computation.\n",
      "Random Seed Set to 133\n",
      "Episode 33: Finished running.\n",
      "Agent 0, Average Reward: -29.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 554.5353\n",
      "Reducing exploration for all agents to 0.4665\n",
      "\n",
      "Episode 34: Starting computation.\n",
      "Random Seed Set to 134\n",
      "Episode 34: Finished running.\n",
      "Agent 0, Average Reward: -26.1\n",
      "Epoch 1/1\n",
      " - 0s - loss: 615.3406\n",
      "Reducing exploration for all agents to 0.4559\n",
      "\n",
      "Episode 35: Starting computation.\n",
      "Random Seed Set to 135\n",
      "Episode 35: Finished running.\n",
      "Agent 0, Average Reward: -27.1\n",
      "Epoch 1/1\n",
      " - 0s - loss: 515.2579\n",
      "Reducing exploration for all agents to 0.4455\n",
      "\n",
      "Episode 36: Starting computation.\n",
      "Random Seed Set to 136\n",
      "Episode 36: Finished running.\n",
      "Agent 0, Average Reward: -25.63\n",
      "Epoch 1/1\n",
      " - 0s - loss: 442.4352\n",
      "Reducing exploration for all agents to 0.4353\n",
      "\n",
      "Episode 37: Starting computation.\n",
      "Random Seed Set to 137\n",
      "Episode 37: Finished running.\n",
      "Agent 0, Average Reward: -28.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 701.3026\n",
      "Reducing exploration for all agents to 0.4254\n",
      "\n",
      "Episode 38: Starting computation.\n",
      "Random Seed Set to 138\n",
      "Episode 38: Finished running.\n",
      "Agent 0, Average Reward: -26.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 598.0809\n",
      "Reducing exploration for all agents to 0.4157\n",
      "\n",
      "Episode 39: Starting computation.\n",
      "Random Seed Set to 139\n",
      "Episode 39: Finished running.\n",
      "Agent 0, Average Reward: -27.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 457.9291\n",
      "Reducing exploration for all agents to 0.4062\n",
      "\n",
      "Episode 40: Starting computation.\n",
      "Random Seed Set to 140\n",
      "Episode 40: Finished running.\n",
      "Agent 0, Average Reward: -20.21\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 593.9211\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.3969\n",
      "\n",
      "Episode 41: Starting computation.\n",
      "Random Seed Set to 141\n",
      "Episode 41: Finished running.\n",
      "Agent 0, Average Reward: -28.49\n",
      "Epoch 1/1\n",
      " - 0s - loss: 455.7337\n",
      "Reducing exploration for all agents to 0.3878\n",
      "\n",
      "Episode 42: Starting computation.\n",
      "Random Seed Set to 142\n",
      "Episode 42: Finished running.\n",
      "Agent 0, Average Reward: -28.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 849.1451\n",
      "Reducing exploration for all agents to 0.379\n",
      "\n",
      "Episode 43: Starting computation.\n",
      "Random Seed Set to 143\n",
      "Episode 43: Finished running.\n",
      "Agent 0, Average Reward: -24.38\n",
      "Epoch 1/1\n",
      " - 0s - loss: 703.3147\n",
      "Reducing exploration for all agents to 0.3703\n",
      "\n",
      "Episode 44: Starting computation.\n",
      "Random Seed Set to 144\n",
      "Episode 44: Finished running.\n",
      "Agent 0, Average Reward: -25.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 608.4990\n",
      "Reducing exploration for all agents to 0.3618\n",
      "\n",
      "Episode 45: Starting computation.\n",
      "Random Seed Set to 145\n",
      "Episode 45: Finished running.\n",
      "Agent 0, Average Reward: -27.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 761.8960\n",
      "Reducing exploration for all agents to 0.3536\n",
      "\n",
      "Episode 46: Starting computation.\n",
      "Random Seed Set to 146\n",
      "Episode 46: Finished running.\n",
      "Agent 0, Average Reward: -25.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 537.2686\n",
      "Reducing exploration for all agents to 0.3455\n",
      "\n",
      "Episode 47: Starting computation.\n",
      "Random Seed Set to 147\n",
      "Episode 47: Finished running.\n",
      "Agent 0, Average Reward: -26.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 845.2934\n",
      "Reducing exploration for all agents to 0.3376\n",
      "\n",
      "Episode 48: Starting computation.\n",
      "Random Seed Set to 148\n",
      "Episode 48: Finished running.\n",
      "Agent 0, Average Reward: -25.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 651.8780\n",
      "Reducing exploration for all agents to 0.3299\n",
      "\n",
      "Episode 49: Starting computation.\n",
      "Random Seed Set to 149\n",
      "Episode 49: Finished running.\n",
      "Agent 0, Average Reward: -25.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 747.6407\n",
      "Reducing exploration for all agents to 0.3224\n",
      "\n",
      "Episode 50: Starting computation.\n",
      "Random Seed Set to 150\n",
      "Episode 50: Finished running.\n",
      "Agent 0, Average Reward: -25.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 637.3870\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.315\n",
      "\n",
      "Episode 51: Starting computation.\n",
      "Random Seed Set to 151\n",
      "Episode 51: Finished running.\n",
      "Agent 0, Average Reward: -28.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 545.3072\n",
      "Reducing exploration for all agents to 0.3078\n",
      "\n",
      "Episode 52: Starting computation.\n",
      "Random Seed Set to 152\n",
      "Episode 52: Finished running.\n",
      "Agent 0, Average Reward: -25.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 544.3998\n",
      "Reducing exploration for all agents to 0.3008\n",
      "\n",
      "Episode 53: Starting computation.\n",
      "Random Seed Set to 153\n",
      "Episode 53: Finished running.\n",
      "Agent 0, Average Reward: -24.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 478.0096\n",
      "Reducing exploration for all agents to 0.2939\n",
      "\n",
      "Episode 54: Starting computation.\n",
      "Random Seed Set to 154\n",
      "Episode 54: Finished running.\n",
      "Agent 0, Average Reward: -29.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 581.0249\n",
      "Reducing exploration for all agents to 0.2872\n",
      "\n",
      "Episode 55: Starting computation.\n",
      "Random Seed Set to 155\n",
      "Episode 55: Finished running.\n",
      "Agent 0, Average Reward: -28.98\n",
      "Epoch 1/1\n",
      " - 0s - loss: 601.4576\n",
      "Reducing exploration for all agents to 0.2806\n",
      "\n",
      "Episode 56: Starting computation.\n",
      "Random Seed Set to 156\n",
      "Episode 56: Finished running.\n",
      "Agent 0, Average Reward: -27.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 537.8897\n",
      "Reducing exploration for all agents to 0.2742\n",
      "\n",
      "Episode 57: Starting computation.\n",
      "Random Seed Set to 157\n",
      "Episode 57: Finished running.\n",
      "Agent 0, Average Reward: -25.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 515.5845\n",
      "Reducing exploration for all agents to 0.268\n",
      "\n",
      "Episode 58: Starting computation.\n",
      "Random Seed Set to 158\n",
      "Episode 58: Finished running.\n",
      "Agent 0, Average Reward: -23.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 398.9902\n",
      "Reducing exploration for all agents to 0.2619\n",
      "\n",
      "Episode 59: Starting computation.\n",
      "Random Seed Set to 159\n",
      "Episode 59: Finished running.\n",
      "Agent 0, Average Reward: -25.91\n",
      "Epoch 1/1\n",
      " - 0s - loss: 414.9603\n",
      "Reducing exploration for all agents to 0.2559\n",
      "\n",
      "Episode 60: Starting computation.\n",
      "Random Seed Set to 160\n",
      "Episode 60: Finished running.\n",
      "Agent 0, Average Reward: -24.07\n",
      "Epoch 1/1\n",
      " - 0s - loss: 421.7999\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.25\n",
      "\n",
      "Episode 61: Starting computation.\n",
      "Random Seed Set to 161\n",
      "Episode 61: Finished running.\n",
      "Agent 0, Average Reward: -25.39\n",
      "Epoch 1/1\n",
      " - 0s - loss: 553.2224\n",
      "Reducing exploration for all agents to 0.2443\n",
      "\n",
      "Episode 62: Starting computation.\n",
      "Random Seed Set to 162\n",
      "Episode 62: Finished running.\n",
      "Agent 0, Average Reward: -27.87\n",
      "Epoch 1/1\n",
      " - 0s - loss: 684.9187\n",
      "Reducing exploration for all agents to 0.2387\n",
      "\n",
      "Episode 63: Starting computation.\n",
      "Random Seed Set to 163\n",
      "Episode 63: Finished running.\n",
      "Agent 0, Average Reward: -26.01\n",
      "Epoch 1/1\n",
      " - 0s - loss: 647.1422\n",
      "Reducing exploration for all agents to 0.2333\n",
      "\n",
      "Episode 64: Starting computation.\n",
      "Random Seed Set to 164\n",
      "Episode 64: Finished running.\n",
      "Agent 0, Average Reward: -25.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 554.2194\n",
      "Reducing exploration for all agents to 0.228\n",
      "\n",
      "Episode 65: Starting computation.\n",
      "Random Seed Set to 165\n",
      "Episode 65: Finished running.\n",
      "Agent 0, Average Reward: -26.99\n",
      "Epoch 1/1\n",
      " - 0s - loss: 542.4656\n",
      "Reducing exploration for all agents to 0.2228\n",
      "\n",
      "Episode 66: Starting computation.\n",
      "Random Seed Set to 166\n",
      "Episode 66: Finished running.\n",
      "Agent 0, Average Reward: -25.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 561.5977\n",
      "Reducing exploration for all agents to 0.2177\n",
      "\n",
      "Episode 67: Starting computation.\n",
      "Random Seed Set to 167\n",
      "Episode 67: Finished running.\n",
      "Agent 0, Average Reward: -26.58\n",
      "Epoch 1/1\n",
      " - 0s - loss: 597.6855\n",
      "Reducing exploration for all agents to 0.2127\n",
      "\n",
      "Episode 68: Starting computation.\n",
      "Random Seed Set to 168\n",
      "Episode 68: Finished running.\n",
      "Agent 0, Average Reward: -23.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 512.4525\n",
      "Reducing exploration for all agents to 0.2078\n",
      "\n",
      "Episode 69: Starting computation.\n",
      "Random Seed Set to 169\n",
      "Episode 69: Finished running.\n",
      "Agent 0, Average Reward: -24.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 508.3672\n",
      "Reducing exploration for all agents to 0.2031\n",
      "\n",
      "Episode 70: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 170\n",
      "Episode 70: Finished running.\n",
      "Agent 0, Average Reward: -24.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 445.5176\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.1985\n",
      "\n",
      "Episode 71: Starting computation.\n",
      "Random Seed Set to 171\n",
      "Episode 71: Finished running.\n",
      "Agent 0, Average Reward: -26.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 437.8929\n",
      "Reducing exploration for all agents to 0.1939\n",
      "\n",
      "Episode 72: Starting computation.\n",
      "Random Seed Set to 172\n",
      "Episode 72: Finished running.\n",
      "Agent 0, Average Reward: -23.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 642.1141\n",
      "Reducing exploration for all agents to 0.1895\n",
      "\n",
      "Episode 73: Starting computation.\n",
      "Random Seed Set to 173\n",
      "Episode 73: Finished running.\n",
      "Agent 0, Average Reward: -25.43\n",
      "Epoch 1/1\n",
      " - 0s - loss: 491.7093\n",
      "Reducing exploration for all agents to 0.1852\n",
      "\n",
      "Episode 74: Starting computation.\n",
      "Random Seed Set to 174\n",
      "Episode 74: Finished running.\n",
      "Agent 0, Average Reward: -25.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 483.3109\n",
      "Reducing exploration for all agents to 0.1809\n",
      "\n",
      "Episode 75: Starting computation.\n",
      "Random Seed Set to 175\n",
      "Episode 75: Finished running.\n",
      "Agent 0, Average Reward: -25.08\n",
      "Epoch 1/1\n",
      " - 0s - loss: 330.7947\n",
      "Reducing exploration for all agents to 0.1768\n",
      "\n",
      "Episode 76: Starting computation.\n",
      "Random Seed Set to 176\n",
      "Episode 76: Finished running.\n",
      "Agent 0, Average Reward: -22.98\n",
      "Epoch 1/1\n",
      " - 0s - loss: 676.2294\n",
      "Reducing exploration for all agents to 0.1728\n",
      "\n",
      "Episode 77: Starting computation.\n",
      "Random Seed Set to 177\n",
      "Episode 77: Finished running.\n",
      "Agent 0, Average Reward: -23.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 465.0503\n",
      "Reducing exploration for all agents to 0.1688\n",
      "\n",
      "Episode 78: Starting computation.\n",
      "Random Seed Set to 178\n",
      "Episode 78: Finished running.\n",
      "Agent 0, Average Reward: -23.71\n",
      "Epoch 1/1\n",
      " - 0s - loss: 528.7678\n",
      "Reducing exploration for all agents to 0.165\n",
      "\n",
      "Episode 79: Starting computation.\n",
      "Random Seed Set to 179\n",
      "Episode 79: Finished running.\n",
      "Agent 0, Average Reward: -26.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 611.3318\n",
      "Reducing exploration for all agents to 0.1612\n",
      "\n",
      "Episode 80: Starting computation.\n",
      "Random Seed Set to 180\n",
      "Episode 80: Finished running.\n",
      "Agent 0, Average Reward: -25.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 548.1476\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.1575\n",
      "\n",
      "Episode 81: Starting computation.\n",
      "Random Seed Set to 181\n",
      "Episode 81: Finished running.\n",
      "Agent 0, Average Reward: -23.81\n",
      "Epoch 1/1\n",
      " - 0s - loss: 402.0203\n",
      "Reducing exploration for all agents to 0.1539\n",
      "\n",
      "Episode 82: Starting computation.\n",
      "Random Seed Set to 182\n",
      "Episode 82: Finished running.\n",
      "Agent 0, Average Reward: -23.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 534.6998\n",
      "Reducing exploration for all agents to 0.1504\n",
      "\n",
      "Episode 83: Starting computation.\n",
      "Random Seed Set to 183\n",
      "Episode 83: Finished running.\n",
      "Agent 0, Average Reward: -26.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 468.0789\n",
      "Reducing exploration for all agents to 0.147\n",
      "\n",
      "Episode 84: Starting computation.\n",
      "Random Seed Set to 184\n",
      "Episode 84: Finished running.\n",
      "Agent 0, Average Reward: -22.08\n",
      "Epoch 1/1\n",
      " - 0s - loss: 567.2115\n",
      "Reducing exploration for all agents to 0.1436\n",
      "\n",
      "Episode 85: Starting computation.\n",
      "Random Seed Set to 185\n",
      "Episode 85: Finished running.\n",
      "Agent 0, Average Reward: -24.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 427.7388\n",
      "Reducing exploration for all agents to 0.1403\n",
      "\n",
      "Episode 86: Starting computation.\n",
      "Random Seed Set to 186\n",
      "Episode 86: Finished running.\n",
      "Agent 0, Average Reward: -22.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 495.4708\n",
      "Reducing exploration for all agents to 0.1371\n",
      "\n",
      "Episode 87: Starting computation.\n",
      "Random Seed Set to 187\n",
      "Episode 87: Finished running.\n",
      "Agent 0, Average Reward: -23.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 561.9041\n",
      "Reducing exploration for all agents to 0.134\n",
      "\n",
      "Episode 88: Starting computation.\n",
      "Random Seed Set to 188\n",
      "Episode 88: Finished running.\n",
      "Agent 0, Average Reward: -24.43\n",
      "Epoch 1/1\n",
      " - 0s - loss: 489.3388\n",
      "Reducing exploration for all agents to 0.1309\n",
      "\n",
      "Episode 89: Starting computation.\n",
      "Random Seed Set to 189\n",
      "Episode 89: Finished running.\n",
      "Agent 0, Average Reward: -23.1\n",
      "Epoch 1/1\n",
      " - 0s - loss: 605.9335\n",
      "Reducing exploration for all agents to 0.1279\n",
      "\n",
      "Episode 90: Starting computation.\n",
      "Random Seed Set to 190\n",
      "Episode 90: Finished running.\n",
      "Agent 0, Average Reward: -26.08\n",
      "Epoch 1/1\n",
      " - 0s - loss: 414.4537\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.125\n",
      "\n",
      "Episode 91: Starting computation.\n",
      "Random Seed Set to 191\n",
      "Episode 91: Finished running.\n",
      "Agent 0, Average Reward: -24.02\n",
      "Epoch 1/1\n",
      " - 0s - loss: 397.6594\n",
      "Reducing exploration for all agents to 0.1222\n",
      "\n",
      "Episode 92: Starting computation.\n",
      "Random Seed Set to 192\n",
      "Episode 92: Finished running.\n",
      "Agent 0, Average Reward: -27.16\n",
      "Epoch 1/1\n",
      " - 0s - loss: 457.3413\n",
      "Reducing exploration for all agents to 0.1194\n",
      "\n",
      "Episode 93: Starting computation.\n",
      "Random Seed Set to 193\n",
      "Episode 93: Finished running.\n",
      "Agent 0, Average Reward: -23.0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 436.6559\n",
      "Reducing exploration for all agents to 0.1167\n",
      "\n",
      "Episode 94: Starting computation.\n",
      "Random Seed Set to 194\n",
      "Episode 94: Finished running.\n",
      "Agent 0, Average Reward: -25.19\n",
      "Epoch 1/1\n",
      " - 0s - loss: 605.6030\n",
      "Reducing exploration for all agents to 0.114\n",
      "\n",
      "Episode 95: Starting computation.\n",
      "Random Seed Set to 195\n",
      "Episode 95: Finished running.\n",
      "Agent 0, Average Reward: -24.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 382.9248\n",
      "Reducing exploration for all agents to 0.1114\n",
      "\n",
      "Episode 96: Starting computation.\n",
      "Random Seed Set to 196\n",
      "Episode 96: Finished running.\n",
      "Agent 0, Average Reward: -23.51\n",
      "Epoch 1/1\n",
      " - 0s - loss: 489.5871\n",
      "Reducing exploration for all agents to 0.1088\n",
      "\n",
      "Episode 97: Starting computation.\n",
      "Random Seed Set to 197\n",
      "Episode 97: Finished running.\n",
      "Agent 0, Average Reward: -26.36\n",
      "Epoch 1/1\n",
      " - 0s - loss: 423.2744\n",
      "Reducing exploration for all agents to 0.1064\n",
      "\n",
      "Episode 98: Starting computation.\n",
      "Random Seed Set to 198\n",
      "Episode 98: Finished running.\n",
      "Agent 0, Average Reward: -24.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 532.2355\n",
      "Reducing exploration for all agents to 0.1039\n",
      "\n",
      "Episode 99: Starting computation.\n",
      "Random Seed Set to 199\n",
      "Episode 99: Finished running.\n",
      "Agent 0, Average Reward: -26.06\n",
      "Epoch 1/1\n",
      " - 0s - loss: 513.5271\n",
      "Reducing exploration for all agents to 0.1016\n",
      "\n",
      "Episode 100: Starting computation.\n",
      "Random Seed Set to 200\n",
      "Episode 100: Finished running.\n",
      "Agent 0, Average Reward: -22.71\n",
      "Epoch 1/1\n",
      " - 0s - loss: 443.5008\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0992\n",
      "\n",
      "Episode 101: Starting computation.\n",
      "Random Seed Set to 201\n",
      "Episode 101: Finished running.\n",
      "Agent 0, Average Reward: -22.92\n",
      "Epoch 1/1\n",
      " - 0s - loss: 429.8430\n",
      "Reducing exploration for all agents to 0.097\n",
      "\n",
      "Episode 102: Starting computation.\n",
      "Random Seed Set to 202\n",
      "Episode 102: Finished running.\n",
      "Agent 0, Average Reward: -22.97\n",
      "Epoch 1/1\n",
      " - 0s - loss: 425.9800\n",
      "Reducing exploration for all agents to 0.0948\n",
      "\n",
      "Episode 103: Starting computation.\n",
      "Random Seed Set to 203\n",
      "Episode 103: Finished running.\n",
      "Agent 0, Average Reward: -27.34\n",
      "Epoch 1/1\n",
      " - 0s - loss: 596.7869\n",
      "Reducing exploration for all agents to 0.0926\n",
      "\n",
      "Episode 104: Starting computation.\n",
      "Random Seed Set to 204\n",
      "Episode 104: Finished running.\n",
      "Agent 0, Average Reward: -24.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 365.5438\n",
      "Reducing exploration for all agents to 0.0905\n",
      "\n",
      "Episode 105: Starting computation.\n",
      "Random Seed Set to 205\n",
      "Episode 105: Finished running.\n",
      "Agent 0, Average Reward: -20.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 362.2841\n",
      "Reducing exploration for all agents to 0.0884\n",
      "\n",
      "Episode 106: Starting computation.\n",
      "Random Seed Set to 206\n",
      "Episode 106: Finished running.\n",
      "Agent 0, Average Reward: -24.51\n",
      "Epoch 1/1\n",
      " - 0s - loss: 420.0668\n",
      "Reducing exploration for all agents to 0.0864\n",
      "\n",
      "Episode 107: Starting computation.\n",
      "Random Seed Set to 207\n",
      "Episode 107: Finished running.\n",
      "Agent 0, Average Reward: -23.19\n",
      "Epoch 1/1\n",
      " - 0s - loss: 415.5591\n",
      "Reducing exploration for all agents to 0.0844\n",
      "\n",
      "Episode 108: Starting computation.\n",
      "Random Seed Set to 208\n",
      "Episode 108: Finished running.\n",
      "Agent 0, Average Reward: -23.32\n",
      "Epoch 1/1\n",
      " - 0s - loss: 462.7146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0825\n",
      "\n",
      "Episode 109: Starting computation.\n",
      "Random Seed Set to 209\n",
      "Episode 109: Finished running.\n",
      "Agent 0, Average Reward: -23.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 659.5028\n",
      "Reducing exploration for all agents to 0.0806\n",
      "\n",
      "Episode 110: Starting computation.\n",
      "Random Seed Set to 210\n",
      "Episode 110: Finished running.\n",
      "Agent 0, Average Reward: -21.55\n",
      "Epoch 1/1\n",
      " - 0s - loss: 268.8184\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0788\n",
      "\n",
      "Episode 111: Starting computation.\n",
      "Random Seed Set to 211\n",
      "Episode 111: Finished running.\n",
      "Agent 0, Average Reward: -25.68\n",
      "Epoch 1/1\n",
      " - 0s - loss: 558.3965\n",
      "Reducing exploration for all agents to 0.077\n",
      "\n",
      "Episode 112: Starting computation.\n",
      "Random Seed Set to 212\n",
      "Episode 112: Finished running.\n",
      "Agent 0, Average Reward: -22.58\n",
      "Epoch 1/1\n",
      " - 0s - loss: 520.2795\n",
      "Reducing exploration for all agents to 0.0752\n",
      "\n",
      "Episode 113: Starting computation.\n",
      "Random Seed Set to 213\n",
      "Episode 113: Finished running.\n",
      "Agent 0, Average Reward: -21.3\n",
      "Epoch 1/1\n",
      " - 0s - loss: 455.9832\n",
      "Reducing exploration for all agents to 0.0735\n",
      "\n",
      "Episode 114: Starting computation.\n",
      "Random Seed Set to 214\n",
      "Episode 114: Finished running.\n",
      "Agent 0, Average Reward: -26.19\n",
      "Epoch 1/1\n",
      " - 0s - loss: 313.7018\n",
      "Reducing exploration for all agents to 0.0718\n",
      "\n",
      "Episode 115: Starting computation.\n",
      "Random Seed Set to 215\n",
      "Episode 115: Finished running.\n",
      "Agent 0, Average Reward: -23.49\n",
      "Epoch 1/1\n",
      " - 0s - loss: 360.2845\n",
      "Reducing exploration for all agents to 0.0702\n",
      "\n",
      "Episode 116: Starting computation.\n",
      "Random Seed Set to 216\n",
      "Episode 116: Finished running.\n",
      "Agent 0, Average Reward: -25.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 308.9501\n",
      "Reducing exploration for all agents to 0.0686\n",
      "\n",
      "Episode 117: Starting computation.\n",
      "Random Seed Set to 217\n",
      "Episode 117: Finished running.\n",
      "Agent 0, Average Reward: -25.68\n",
      "Epoch 1/1\n",
      " - 0s - loss: 317.8629\n",
      "Reducing exploration for all agents to 0.067\n",
      "\n",
      "Episode 118: Starting computation.\n",
      "Random Seed Set to 218\n",
      "Episode 118: Finished running.\n",
      "Agent 0, Average Reward: -24.29\n",
      "Epoch 1/1\n",
      " - 0s - loss: 279.1570\n",
      "Reducing exploration for all agents to 0.0655\n",
      "\n",
      "Episode 119: Starting computation.\n",
      "Random Seed Set to 219\n",
      "Episode 119: Finished running.\n",
      "Agent 0, Average Reward: -24.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 349.6550\n",
      "Reducing exploration for all agents to 0.064\n",
      "\n",
      "Episode 120: Starting computation.\n",
      "Random Seed Set to 220\n",
      "Episode 120: Finished running.\n",
      "Agent 0, Average Reward: -24.0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 392.4192\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0625\n",
      "\n",
      "Episode 121: Starting computation.\n",
      "Random Seed Set to 221\n",
      "Episode 121: Finished running.\n",
      "Agent 0, Average Reward: -21.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 478.6041\n",
      "Reducing exploration for all agents to 0.0611\n",
      "\n",
      "Episode 122: Starting computation.\n",
      "Random Seed Set to 222\n",
      "Episode 122: Finished running.\n",
      "Agent 0, Average Reward: -24.77\n",
      "Epoch 1/1\n",
      " - 0s - loss: 323.4181\n",
      "Reducing exploration for all agents to 0.0597\n",
      "\n",
      "Episode 123: Starting computation.\n",
      "Random Seed Set to 223\n",
      "Episode 123: Finished running.\n",
      "Agent 0, Average Reward: -23.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 418.9871\n",
      "Reducing exploration for all agents to 0.0583\n",
      "\n",
      "Episode 124: Starting computation.\n",
      "Random Seed Set to 224\n",
      "Episode 124: Finished running.\n",
      "Agent 0, Average Reward: -24.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 303.1163\n",
      "Reducing exploration for all agents to 0.057\n",
      "\n",
      "Episode 125: Starting computation.\n",
      "Random Seed Set to 225\n",
      "Episode 125: Finished running.\n",
      "Agent 0, Average Reward: -26.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 435.3592\n",
      "Reducing exploration for all agents to 0.0557\n",
      "\n",
      "Episode 126: Starting computation.\n",
      "Random Seed Set to 226\n",
      "Episode 126: Finished running.\n",
      "Agent 0, Average Reward: -23.37\n",
      "Epoch 1/1\n",
      " - 0s - loss: 395.3164\n",
      "Reducing exploration for all agents to 0.0544\n",
      "\n",
      "Episode 127: Starting computation.\n",
      "Random Seed Set to 227\n",
      "Episode 127: Finished running.\n",
      "Agent 0, Average Reward: -23.69\n",
      "Epoch 1/1\n",
      " - 0s - loss: 468.5871\n",
      "Reducing exploration for all agents to 0.0532\n",
      "\n",
      "Episode 128: Starting computation.\n",
      "Random Seed Set to 228\n",
      "Episode 128: Finished running.\n",
      "Agent 0, Average Reward: -24.55\n",
      "Epoch 1/1\n",
      " - 0s - loss: 374.2935\n",
      "Reducing exploration for all agents to 0.052\n",
      "\n",
      "Episode 129: Starting computation.\n",
      "Random Seed Set to 229\n",
      "Episode 129: Finished running.\n",
      "Agent 0, Average Reward: -24.56\n",
      "Epoch 1/1\n",
      " - 0s - loss: 500.9972\n",
      "Reducing exploration for all agents to 0.0508\n",
      "\n",
      "Episode 130: Starting computation.\n",
      "Random Seed Set to 230\n",
      "Episode 130: Finished running.\n",
      "Agent 0, Average Reward: -21.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 436.9784\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0496\n",
      "\n",
      "Episode 131: Starting computation.\n",
      "Random Seed Set to 231\n",
      "Episode 131: Finished running.\n",
      "Agent 0, Average Reward: -21.76\n",
      "Epoch 1/1\n",
      " - 0s - loss: 306.9564\n",
      "Reducing exploration for all agents to 0.0485\n",
      "\n",
      "Episode 132: Starting computation.\n",
      "Random Seed Set to 232\n",
      "Episode 132: Finished running.\n",
      "Agent 0, Average Reward: -23.1\n",
      "Epoch 1/1\n",
      " - 0s - loss: 377.7148\n",
      "Reducing exploration for all agents to 0.0474\n",
      "\n",
      "Episode 133: Starting computation.\n",
      "Random Seed Set to 233\n",
      "Episode 133: Finished running.\n",
      "Agent 0, Average Reward: -22.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 462.5190\n",
      "Reducing exploration for all agents to 0.0463\n",
      "\n",
      "Episode 134: Starting computation.\n",
      "Random Seed Set to 234\n",
      "Episode 134: Finished running.\n",
      "Agent 0, Average Reward: -22.08\n",
      "Epoch 1/1\n",
      " - 0s - loss: 324.5751\n",
      "Reducing exploration for all agents to 0.0452\n",
      "\n",
      "Episode 135: Starting computation.\n",
      "Random Seed Set to 235\n",
      "Episode 135: Finished running.\n",
      "Agent 0, Average Reward: -19.52\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 349.6867\n",
      "Reducing exploration for all agents to 0.0442\n",
      "\n",
      "Episode 136: Starting computation.\n",
      "Random Seed Set to 236\n",
      "Episode 136: Finished running.\n",
      "Agent 0, Average Reward: -23.48\n",
      "Epoch 1/1\n",
      " - 0s - loss: 344.6093\n",
      "Reducing exploration for all agents to 0.0432\n",
      "\n",
      "Episode 137: Starting computation.\n",
      "Random Seed Set to 237\n",
      "Episode 137: Finished running.\n",
      "Agent 0, Average Reward: -22.28\n",
      "Epoch 1/1\n",
      " - 0s - loss: 368.7545\n",
      "Reducing exploration for all agents to 0.0422\n",
      "\n",
      "Episode 138: Starting computation.\n",
      "Random Seed Set to 238\n",
      "Episode 138: Finished running.\n",
      "Agent 0, Average Reward: -24.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 490.8608\n",
      "Reducing exploration for all agents to 0.0412\n",
      "\n",
      "Episode 139: Starting computation.\n",
      "Random Seed Set to 239\n",
      "Episode 139: Finished running.\n",
      "Agent 0, Average Reward: -23.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 353.1603\n",
      "Reducing exploration for all agents to 0.0403\n",
      "\n",
      "Episode 140: Starting computation.\n",
      "Random Seed Set to 240\n",
      "Episode 140: Finished running.\n",
      "Agent 0, Average Reward: -22.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 271.0688\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0394\n",
      "\n",
      "Episode 141: Starting computation.\n",
      "Random Seed Set to 241\n",
      "Episode 141: Finished running.\n",
      "Agent 0, Average Reward: -24.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 294.7619\n",
      "Reducing exploration for all agents to 0.0385\n",
      "\n",
      "Episode 142: Starting computation.\n",
      "Random Seed Set to 242\n",
      "Episode 142: Finished running.\n",
      "Agent 0, Average Reward: -23.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 261.6049\n",
      "Reducing exploration for all agents to 0.0376\n",
      "\n",
      "Episode 143: Starting computation.\n",
      "Random Seed Set to 243\n",
      "Episode 143: Finished running.\n",
      "Agent 0, Average Reward: -22.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 231.9949\n",
      "Reducing exploration for all agents to 0.0367\n",
      "\n",
      "Episode 144: Starting computation.\n",
      "Random Seed Set to 244\n",
      "Episode 144: Finished running.\n",
      "Agent 0, Average Reward: -24.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 398.1440\n",
      "Reducing exploration for all agents to 0.0359\n",
      "\n",
      "Episode 145: Starting computation.\n",
      "Random Seed Set to 245\n",
      "Episode 145: Finished running.\n",
      "Agent 0, Average Reward: -21.87\n",
      "Epoch 1/1\n",
      " - 0s - loss: 311.2915\n",
      "Reducing exploration for all agents to 0.0351\n",
      "\n",
      "Episode 146: Starting computation.\n",
      "Random Seed Set to 246\n",
      "Episode 146: Finished running.\n",
      "Agent 0, Average Reward: -21.06\n",
      "Epoch 1/1\n",
      " - 0s - loss: 313.9108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0343\n",
      "\n",
      "Episode 147: Starting computation.\n",
      "Random Seed Set to 247\n",
      "Episode 147: Finished running.\n",
      "Agent 0, Average Reward: -23.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 264.8424\n",
      "Reducing exploration for all agents to 0.0335\n",
      "\n",
      "Episode 148: Starting computation.\n",
      "Random Seed Set to 248\n",
      "Episode 148: Finished running.\n",
      "Agent 0, Average Reward: -24.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 372.8139\n",
      "Reducing exploration for all agents to 0.0327\n",
      "\n",
      "Episode 149: Starting computation.\n",
      "Random Seed Set to 249\n",
      "Episode 149: Finished running.\n",
      "Agent 0, Average Reward: -21.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 298.2141\n",
      "Reducing exploration for all agents to 0.032\n",
      "\n",
      "Episode 150: Starting computation.\n",
      "Random Seed Set to 250\n",
      "Episode 150: Finished running.\n",
      "Agent 0, Average Reward: -23.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 396.5721\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0313\n",
      "\n",
      "Episode 151: Starting computation.\n",
      "Random Seed Set to 251\n",
      "Episode 151: Finished running.\n",
      "Agent 0, Average Reward: -22.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 332.7176\n",
      "Reducing exploration for all agents to 0.0305\n",
      "\n",
      "Episode 152: Starting computation.\n",
      "Random Seed Set to 252\n",
      "Episode 152: Finished running.\n",
      "Agent 0, Average Reward: -25.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 283.2924\n",
      "Reducing exploration for all agents to 0.0298\n",
      "\n",
      "Episode 153: Starting computation.\n",
      "Random Seed Set to 253\n",
      "Episode 153: Finished running.\n",
      "Agent 0, Average Reward: -23.55\n",
      "Epoch 1/1\n",
      " - 0s - loss: 469.5703\n",
      "Reducing exploration for all agents to 0.0292\n",
      "\n",
      "Episode 154: Starting computation.\n",
      "Random Seed Set to 254\n",
      "Episode 154: Finished running.\n",
      "Agent 0, Average Reward: -24.55\n",
      "Epoch 1/1\n",
      " - 0s - loss: 437.5830\n",
      "Reducing exploration for all agents to 0.0285\n",
      "\n",
      "Episode 155: Starting computation.\n",
      "Random Seed Set to 255\n",
      "Episode 155: Finished running.\n",
      "Agent 0, Average Reward: -23.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 485.9217\n",
      "Reducing exploration for all agents to 0.0278\n",
      "\n",
      "Episode 156: Starting computation.\n",
      "Random Seed Set to 256\n",
      "Episode 156: Finished running.\n",
      "Agent 0, Average Reward: -25.04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 379.9815\n",
      "Reducing exploration for all agents to 0.0272\n",
      "\n",
      "Episode 157: Starting computation.\n",
      "Random Seed Set to 257\n",
      "Episode 157: Finished running.\n",
      "Agent 0, Average Reward: -22.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 249.0294\n",
      "Reducing exploration for all agents to 0.0266\n",
      "\n",
      "Episode 158: Starting computation.\n",
      "Random Seed Set to 258\n",
      "Episode 158: Finished running.\n",
      "Agent 0, Average Reward: -25.04\n",
      "Epoch 1/1\n",
      " - 0s - loss: 225.3690\n",
      "Reducing exploration for all agents to 0.026\n",
      "\n",
      "Episode 159: Starting computation.\n",
      "Random Seed Set to 259\n",
      "Episode 159: Finished running.\n",
      "Agent 0, Average Reward: -23.46\n",
      "Epoch 1/1\n",
      " - 0s - loss: 278.3218\n",
      "Reducing exploration for all agents to 0.0254\n",
      "\n",
      "Episode 160: Starting computation.\n",
      "Random Seed Set to 260\n",
      "Episode 160: Finished running.\n",
      "Agent 0, Average Reward: -26.27\n",
      "Epoch 1/1\n",
      " - 0s - loss: 209.5037\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0248\n",
      "\n",
      "Episode 161: Starting computation.\n",
      "Random Seed Set to 261\n",
      "Episode 161: Finished running.\n",
      "Agent 0, Average Reward: -22.58\n",
      "Epoch 1/1\n",
      " - 0s - loss: 371.5023\n",
      "Reducing exploration for all agents to 0.0242\n",
      "\n",
      "Episode 162: Starting computation.\n",
      "Random Seed Set to 262\n",
      "Episode 162: Finished running.\n",
      "Agent 0, Average Reward: -25.83\n",
      "Epoch 1/1\n",
      " - 0s - loss: 303.6976\n",
      "Reducing exploration for all agents to 0.0237\n",
      "\n",
      "Episode 163: Starting computation.\n",
      "Random Seed Set to 263\n",
      "Episode 163: Finished running.\n",
      "Agent 0, Average Reward: -21.36\n",
      "Epoch 1/1\n",
      " - 0s - loss: 276.9714\n",
      "Reducing exploration for all agents to 0.0231\n",
      "\n",
      "Episode 164: Starting computation.\n",
      "Random Seed Set to 264\n",
      "Episode 164: Finished running.\n",
      "Agent 0, Average Reward: -23.74\n",
      "Epoch 1/1\n",
      " - 0s - loss: 412.5808\n",
      "Reducing exploration for all agents to 0.0226\n",
      "\n",
      "Episode 165: Starting computation.\n",
      "Random Seed Set to 265\n",
      "Episode 165: Finished running.\n",
      "Agent 0, Average Reward: -24.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 309.7028\n",
      "Reducing exploration for all agents to 0.0221\n",
      "\n",
      "Episode 166: Starting computation.\n",
      "Random Seed Set to 266\n",
      "Episode 166: Finished running.\n",
      "Agent 0, Average Reward: -22.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 375.2465\n",
      "Reducing exploration for all agents to 0.0216\n",
      "\n",
      "Episode 167: Starting computation.\n",
      "Random Seed Set to 267\n",
      "Episode 167: Finished running.\n",
      "Agent 0, Average Reward: -25.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 445.3717\n",
      "Reducing exploration for all agents to 0.0211\n",
      "\n",
      "Episode 168: Starting computation.\n",
      "Random Seed Set to 268\n",
      "Episode 168: Finished running.\n",
      "Agent 0, Average Reward: -26.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 447.0198\n",
      "Reducing exploration for all agents to 0.0206\n",
      "\n",
      "Episode 169: Starting computation.\n",
      "Random Seed Set to 269\n",
      "Episode 169: Finished running.\n",
      "Agent 0, Average Reward: -24.67\n",
      "Epoch 1/1\n",
      " - 0s - loss: 393.5346\n",
      "Reducing exploration for all agents to 0.0202\n",
      "\n",
      "Episode 170: Starting computation.\n",
      "Random Seed Set to 270\n",
      "Episode 170: Finished running.\n",
      "Agent 0, Average Reward: -24.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 347.8588\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0197\n",
      "\n",
      "Episode 171: Starting computation.\n",
      "Random Seed Set to 271\n",
      "Episode 171: Finished running.\n",
      "Agent 0, Average Reward: -22.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 370.5475\n",
      "Reducing exploration for all agents to 0.0192\n",
      "\n",
      "Episode 172: Starting computation.\n",
      "Random Seed Set to 272\n",
      "Episode 172: Finished running.\n",
      "Agent 0, Average Reward: -25.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 258.7852\n",
      "Reducing exploration for all agents to 0.0188\n",
      "\n",
      "Episode 173: Starting computation.\n",
      "Random Seed Set to 273\n",
      "Episode 173: Finished running.\n",
      "Agent 0, Average Reward: -24.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 426.4155\n",
      "Reducing exploration for all agents to 0.0184\n",
      "\n",
      "Episode 174: Starting computation.\n",
      "Random Seed Set to 274\n",
      "Episode 174: Finished running.\n",
      "Agent 0, Average Reward: -25.59\n",
      "Epoch 1/1\n",
      " - 0s - loss: 366.6707\n",
      "Reducing exploration for all agents to 0.018\n",
      "\n",
      "Episode 175: Starting computation.\n",
      "Random Seed Set to 275\n",
      "Episode 175: Finished running.\n",
      "Agent 0, Average Reward: -23.91\n",
      "Epoch 1/1\n",
      " - 0s - loss: 431.9582\n",
      "Reducing exploration for all agents to 0.0175\n",
      "\n",
      "Episode 176: Starting computation.\n",
      "Random Seed Set to 276\n",
      "Episode 176: Finished running.\n",
      "Agent 0, Average Reward: -25.19\n",
      "Epoch 1/1\n",
      " - 0s - loss: 371.3232\n",
      "Reducing exploration for all agents to 0.0171\n",
      "\n",
      "Episode 177: Starting computation.\n",
      "Random Seed Set to 277\n",
      "Episode 177: Finished running.\n",
      "Agent 0, Average Reward: -27.1\n",
      "Epoch 1/1\n",
      " - 0s - loss: 373.2931\n",
      "Reducing exploration for all agents to 0.0168\n",
      "\n",
      "Episode 178: Starting computation.\n",
      "Random Seed Set to 278\n",
      "Episode 178: Finished running.\n",
      "Agent 0, Average Reward: -26.24\n",
      "Epoch 1/1\n",
      " - 0s - loss: 450.7252\n",
      "Reducing exploration for all agents to 0.0164\n",
      "\n",
      "Episode 179: Starting computation.\n",
      "Random Seed Set to 279\n",
      "Episode 179: Finished running.\n",
      "Agent 0, Average Reward: -25.68\n",
      "Epoch 1/1\n",
      " - 0s - loss: 437.0028\n",
      "Reducing exploration for all agents to 0.016\n",
      "\n",
      "Episode 180: Starting computation.\n",
      "Random Seed Set to 280\n",
      "Episode 180: Finished running.\n",
      "Agent 0, Average Reward: -25.17\n",
      "Epoch 1/1\n",
      " - 0s - loss: 354.1334\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0156\n",
      "\n",
      "Episode 181: Starting computation.\n",
      "Random Seed Set to 281\n",
      "Episode 181: Finished running.\n",
      "Agent 0, Average Reward: -23.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 344.4692\n",
      "Reducing exploration for all agents to 0.0153\n",
      "\n",
      "Episode 182: Starting computation.\n",
      "Random Seed Set to 282\n",
      "Episode 182: Finished running.\n",
      "Agent 0, Average Reward: -24.81\n",
      "Epoch 1/1\n",
      " - 0s - loss: 379.6720\n",
      "Reducing exploration for all agents to 0.0149\n",
      "\n",
      "Episode 183: Starting computation.\n",
      "Random Seed Set to 283\n",
      "Episode 183: Finished running.\n",
      "Agent 0, Average Reward: -25.59\n",
      "Epoch 1/1\n",
      " - 0s - loss: 405.0099\n",
      "Reducing exploration for all agents to 0.0146\n",
      "\n",
      "Episode 184: Starting computation.\n",
      "Random Seed Set to 284\n",
      "Episode 184: Finished running.\n",
      "Agent 0, Average Reward: -24.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 351.9498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0143\n",
      "\n",
      "Episode 185: Starting computation.\n",
      "Random Seed Set to 285\n",
      "Episode 185: Finished running.\n",
      "Agent 0, Average Reward: -27.83\n",
      "Epoch 1/1\n",
      " - 0s - loss: 500.8638\n",
      "Reducing exploration for all agents to 0.0139\n",
      "\n",
      "Episode 186: Starting computation.\n",
      "Random Seed Set to 286\n",
      "Episode 186: Finished running.\n",
      "Agent 0, Average Reward: -23.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 344.7976\n",
      "Reducing exploration for all agents to 0.0136\n",
      "\n",
      "Episode 187: Starting computation.\n",
      "Random Seed Set to 287\n",
      "Episode 187: Finished running.\n",
      "Agent 0, Average Reward: -26.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 257.4502\n",
      "Reducing exploration for all agents to 0.0133\n",
      "\n",
      "Episode 188: Starting computation.\n",
      "Random Seed Set to 288\n",
      "Episode 188: Finished running.\n",
      "Agent 0, Average Reward: -24.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 293.2469\n",
      "Reducing exploration for all agents to 0.013\n",
      "\n",
      "Episode 189: Starting computation.\n",
      "Random Seed Set to 289\n",
      "Episode 189: Finished running.\n",
      "Agent 0, Average Reward: -26.5\n",
      "Epoch 1/1\n",
      " - 0s - loss: 358.4521\n",
      "Reducing exploration for all agents to 0.0127\n",
      "\n",
      "Episode 190: Starting computation.\n",
      "Random Seed Set to 290\n",
      "Episode 190: Finished running.\n",
      "Agent 0, Average Reward: -25.54\n",
      "Epoch 1/1\n",
      " - 0s - loss: 380.0481\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0124\n",
      "\n",
      "Episode 191: Starting computation.\n",
      "Random Seed Set to 291\n",
      "Episode 191: Finished running.\n",
      "Agent 0, Average Reward: -24.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 268.9028\n",
      "Reducing exploration for all agents to 0.0121\n",
      "\n",
      "Episode 192: Starting computation.\n",
      "Random Seed Set to 292\n",
      "Episode 192: Finished running.\n",
      "Agent 0, Average Reward: -24.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 362.9098\n",
      "Reducing exploration for all agents to 0.0118\n",
      "\n",
      "Episode 193: Starting computation.\n",
      "Random Seed Set to 293\n",
      "Episode 193: Finished running.\n",
      "Agent 0, Average Reward: -25.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 428.0286\n",
      "Reducing exploration for all agents to 0.0116\n",
      "\n",
      "Episode 194: Starting computation.\n",
      "Random Seed Set to 294\n",
      "Episode 194: Finished running.\n",
      "Agent 0, Average Reward: -26.28\n",
      "Epoch 1/1\n",
      " - 0s - loss: 388.5689\n",
      "Reducing exploration for all agents to 0.0113\n",
      "\n",
      "Episode 195: Starting computation.\n",
      "Random Seed Set to 295\n",
      "Episode 195: Finished running.\n",
      "Agent 0, Average Reward: -25.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 380.6327\n",
      "Reducing exploration for all agents to 0.0111\n",
      "\n",
      "Episode 196: Starting computation.\n",
      "Random Seed Set to 296\n",
      "Episode 196: Finished running.\n",
      "Agent 0, Average Reward: -24.19\n",
      "Epoch 1/1\n",
      " - 0s - loss: 373.7996\n",
      "Reducing exploration for all agents to 0.0108\n",
      "\n",
      "Episode 197: Starting computation.\n",
      "Random Seed Set to 297\n",
      "Episode 197: Finished running.\n",
      "Agent 0, Average Reward: -24.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 394.2114\n",
      "Reducing exploration for all agents to 0.0106\n",
      "\n",
      "Episode 198: Starting computation.\n",
      "Random Seed Set to 298\n",
      "Episode 198: Finished running.\n",
      "Agent 0, Average Reward: -27.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 362.8264\n",
      "Reducing exploration for all agents to 0.0103\n",
      "\n",
      "Episode 199: Starting computation.\n",
      "Random Seed Set to 299\n",
      "Episode 199: Finished running.\n",
      "Agent 0, Average Reward: -26.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 306.9126\n",
      "Reducing exploration for all agents to 0.0101\n",
      "\n",
      "Episode 200: Starting computation.\n",
      "Random Seed Set to 300\n",
      "Episode 200: Finished running.\n",
      "Agent 0, Average Reward: -22.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 357.0176\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0098\n",
      "\n",
      "Episode 201: Starting computation.\n",
      "Random Seed Set to 301\n",
      "Episode 201: Finished running.\n",
      "Agent 0, Average Reward: -22.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 268.4222\n",
      "Reducing exploration for all agents to 0.0096\n",
      "\n",
      "Episode 202: Starting computation.\n",
      "Random Seed Set to 302\n",
      "Episode 202: Finished running.\n",
      "Agent 0, Average Reward: -23.65\n",
      "Epoch 1/1\n",
      " - 0s - loss: 234.0703\n",
      "Reducing exploration for all agents to 0.0094\n",
      "\n",
      "Episode 203: Starting computation.\n",
      "Random Seed Set to 303\n",
      "Episode 203: Finished running.\n",
      "Agent 0, Average Reward: -24.39\n",
      "Epoch 1/1\n",
      " - 0s - loss: 430.4374\n",
      "Reducing exploration for all agents to 0.0092\n",
      "\n",
      "Episode 204: Starting computation.\n",
      "Random Seed Set to 304\n",
      "Episode 204: Finished running.\n",
      "Agent 0, Average Reward: -22.95\n",
      "Epoch 1/1\n",
      " - 0s - loss: 293.1619\n",
      "Reducing exploration for all agents to 0.009\n",
      "\n",
      "Episode 205: Starting computation.\n",
      "Random Seed Set to 305\n",
      "Episode 205: Finished running.\n",
      "Agent 0, Average Reward: -23.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 229.9513\n",
      "Reducing exploration for all agents to 0.0088\n",
      "\n",
      "Episode 206: Starting computation.\n",
      "Random Seed Set to 306\n",
      "Episode 206: Finished running.\n",
      "Agent 0, Average Reward: -27.38\n",
      "Epoch 1/1\n",
      " - 0s - loss: 400.9517\n",
      "Reducing exploration for all agents to 0.0086\n",
      "\n",
      "Episode 207: Starting computation.\n",
      "Random Seed Set to 307\n",
      "Episode 207: Finished running.\n",
      "Agent 0, Average Reward: -26.13\n",
      "Epoch 1/1\n",
      " - 0s - loss: 321.7689\n",
      "Reducing exploration for all agents to 0.0084\n",
      "\n",
      "Episode 208: Starting computation.\n",
      "Random Seed Set to 308\n",
      "Episode 208: Finished running.\n",
      "Agent 0, Average Reward: -25.65\n",
      "Epoch 1/1\n",
      " - 0s - loss: 313.8484\n",
      "Reducing exploration for all agents to 0.0082\n",
      "\n",
      "Episode 209: Starting computation.\n",
      "Random Seed Set to 309\n",
      "Episode 209: Finished running.\n",
      "Agent 0, Average Reward: -27.12\n",
      "Epoch 1/1\n",
      " - 0s - loss: 327.3316\n",
      "Reducing exploration for all agents to 0.008\n",
      "\n",
      "Episode 210: Starting computation.\n",
      "Random Seed Set to 310\n",
      "Episode 210: Finished running.\n",
      "Agent 0, Average Reward: -25.21\n",
      "Epoch 1/1\n",
      " - 0s - loss: 225.7999\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0078\n",
      "\n",
      "Episode 211: Starting computation.\n",
      "Random Seed Set to 311\n",
      "Episode 211: Finished running.\n",
      "Agent 0, Average Reward: -22.87\n",
      "Epoch 1/1\n",
      " - 0s - loss: 483.5312\n",
      "Reducing exploration for all agents to 0.0076\n",
      "\n",
      "Episode 212: Starting computation.\n",
      "Random Seed Set to 312\n",
      "Episode 212: Finished running.\n",
      "Agent 0, Average Reward: -24.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 243.0648\n",
      "Reducing exploration for all agents to 0.0075\n",
      "\n",
      "Episode 213: Starting computation.\n",
      "Random Seed Set to 313\n",
      "Episode 213: Finished running.\n",
      "Agent 0, Average Reward: -27.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 239.0961\n",
      "Reducing exploration for all agents to 0.0073\n",
      "\n",
      "Episode 214: Starting computation.\n",
      "Random Seed Set to 314\n",
      "Episode 214: Finished running.\n",
      "Agent 0, Average Reward: -26.71\n",
      "Epoch 1/1\n",
      " - 0s - loss: 311.5644\n",
      "Reducing exploration for all agents to 0.0071\n",
      "\n",
      "Episode 215: Starting computation.\n",
      "Random Seed Set to 315\n",
      "Episode 215: Finished running.\n",
      "Agent 0, Average Reward: -28.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 322.3540\n",
      "Reducing exploration for all agents to 0.007\n",
      "\n",
      "Episode 216: Starting computation.\n",
      "Random Seed Set to 316\n",
      "Episode 216: Finished running.\n",
      "Agent 0, Average Reward: -26.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 414.8218\n",
      "Reducing exploration for all agents to 0.0068\n",
      "\n",
      "Episode 217: Starting computation.\n",
      "Random Seed Set to 317\n",
      "Episode 217: Finished running.\n",
      "Agent 0, Average Reward: -27.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 373.5017\n",
      "Reducing exploration for all agents to 0.0066\n",
      "\n",
      "Episode 218: Starting computation.\n",
      "Random Seed Set to 318\n",
      "Episode 218: Finished running.\n",
      "Agent 0, Average Reward: -25.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 361.6765\n",
      "Reducing exploration for all agents to 0.0065\n",
      "\n",
      "Episode 219: Starting computation.\n",
      "Random Seed Set to 319\n",
      "Episode 219: Finished running.\n",
      "Agent 0, Average Reward: -24.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 303.3741\n",
      "Reducing exploration for all agents to 0.0063\n",
      "\n",
      "Episode 220: Starting computation.\n",
      "Random Seed Set to 320\n",
      "Episode 220: Finished running.\n",
      "Agent 0, Average Reward: -28.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 201.0288\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0062\n",
      "\n",
      "Episode 221: Starting computation.\n",
      "Random Seed Set to 321\n",
      "Episode 221: Finished running.\n",
      "Agent 0, Average Reward: -26.72\n",
      "Epoch 1/1\n",
      " - 0s - loss: 379.3798\n",
      "Reducing exploration for all agents to 0.0061\n",
      "\n",
      "Episode 222: Starting computation.\n",
      "Random Seed Set to 322\n",
      "Episode 222: Finished running.\n",
      "Agent 0, Average Reward: -28.47\n",
      "Epoch 1/1\n",
      " - 0s - loss: 443.4753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0059\n",
      "\n",
      "Episode 223: Starting computation.\n",
      "Random Seed Set to 323\n",
      "Episode 223: Finished running.\n",
      "Agent 0, Average Reward: -27.06\n",
      "Epoch 1/1\n",
      " - 0s - loss: 286.3363\n",
      "Reducing exploration for all agents to 0.0058\n",
      "\n",
      "Episode 224: Starting computation.\n",
      "Random Seed Set to 324\n",
      "Episode 224: Finished running.\n",
      "Agent 0, Average Reward: -24.51\n",
      "Epoch 1/1\n",
      " - 0s - loss: 195.6272\n",
      "Reducing exploration for all agents to 0.0057\n",
      "\n",
      "Episode 225: Starting computation.\n",
      "Random Seed Set to 325\n",
      "Episode 225: Finished running.\n",
      "Agent 0, Average Reward: -24.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 217.3148\n",
      "Reducing exploration for all agents to 0.0055\n",
      "\n",
      "Episode 226: Starting computation.\n",
      "Random Seed Set to 326\n",
      "Episode 226: Finished running.\n",
      "Agent 0, Average Reward: -28.3\n",
      "Epoch 1/1\n",
      " - 0s - loss: 250.2586\n",
      "Reducing exploration for all agents to 0.0054\n",
      "\n",
      "Episode 227: Starting computation.\n",
      "Random Seed Set to 327\n",
      "Episode 227: Finished running.\n",
      "Agent 0, Average Reward: -26.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 262.9486\n",
      "Reducing exploration for all agents to 0.0053\n",
      "\n",
      "Episode 228: Starting computation.\n",
      "Random Seed Set to 328\n",
      "Episode 228: Finished running.\n",
      "Agent 0, Average Reward: -26.35\n",
      "Epoch 1/1\n",
      " - 0s - loss: 334.0390\n",
      "Reducing exploration for all agents to 0.0052\n",
      "\n",
      "Episode 229: Starting computation.\n",
      "Random Seed Set to 329\n",
      "Episode 229: Finished running.\n",
      "Agent 0, Average Reward: -28.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 190.8602\n",
      "Reducing exploration for all agents to 0.005\n",
      "\n",
      "Episode 230: Starting computation.\n",
      "Random Seed Set to 330\n",
      "Episode 230: Finished running.\n",
      "Agent 0, Average Reward: -28.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 366.1729\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0049\n",
      "\n",
      "Episode 231: Starting computation.\n",
      "Random Seed Set to 331\n",
      "Episode 231: Finished running.\n",
      "Agent 0, Average Reward: -27.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 298.6233\n",
      "Reducing exploration for all agents to 0.0048\n",
      "\n",
      "Episode 232: Starting computation.\n",
      "Random Seed Set to 332\n",
      "Episode 232: Finished running.\n",
      "Agent 0, Average Reward: -28.18\n",
      "Epoch 1/1\n",
      " - 0s - loss: 413.3531\n",
      "Reducing exploration for all agents to 0.0047\n",
      "\n",
      "Episode 233: Starting computation.\n",
      "Random Seed Set to 333\n",
      "Episode 233: Finished running.\n",
      "Agent 0, Average Reward: -27.4\n",
      "Epoch 1/1\n",
      " - 0s - loss: 248.7789\n",
      "Reducing exploration for all agents to 0.0046\n",
      "\n",
      "Episode 234: Starting computation.\n",
      "Random Seed Set to 334\n",
      "Episode 234: Finished running.\n",
      "Agent 0, Average Reward: -27.53\n",
      "Epoch 1/1\n",
      " - 0s - loss: 335.5114\n",
      "Reducing exploration for all agents to 0.0045\n",
      "\n",
      "Episode 235: Starting computation.\n",
      "Random Seed Set to 335\n",
      "Episode 235: Finished running.\n",
      "Agent 0, Average Reward: -27.06\n",
      "Epoch 1/1\n",
      " - 0s - loss: 306.7733\n",
      "Reducing exploration for all agents to 0.0044\n",
      "\n",
      "Episode 236: Starting computation.\n",
      "Random Seed Set to 336\n",
      "Episode 236: Finished running.\n",
      "Agent 0, Average Reward: -26.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 305.6196\n",
      "Reducing exploration for all agents to 0.0043\n",
      "\n",
      "Episode 237: Starting computation.\n",
      "Random Seed Set to 337\n",
      "Episode 237: Finished running.\n",
      "Agent 0, Average Reward: -27.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 179.4987\n",
      "Reducing exploration for all agents to 0.0042\n",
      "\n",
      "Episode 238: Starting computation.\n",
      "Random Seed Set to 338\n",
      "Episode 238: Finished running.\n",
      "Agent 0, Average Reward: -26.51\n",
      "Epoch 1/1\n",
      " - 0s - loss: 261.7854\n",
      "Reducing exploration for all agents to 0.0041\n",
      "\n",
      "Episode 239: Starting computation.\n",
      "Random Seed Set to 339\n",
      "Episode 239: Finished running.\n",
      "Agent 0, Average Reward: -25.97\n",
      "Epoch 1/1\n",
      " - 0s - loss: 418.9044\n",
      "Reducing exploration for all agents to 0.004\n",
      "\n",
      "Episode 240: Starting computation.\n",
      "Random Seed Set to 340\n",
      "Episode 240: Finished running.\n",
      "Agent 0, Average Reward: -26.73\n",
      "Epoch 1/1\n",
      " - 0s - loss: 314.8487\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0039\n",
      "\n",
      "Episode 241: Starting computation.\n",
      "Random Seed Set to 341\n",
      "Episode 241: Finished running.\n",
      "Agent 0, Average Reward: -28.28\n",
      "Epoch 1/1\n",
      " - 0s - loss: 330.0473\n",
      "Reducing exploration for all agents to 0.0038\n",
      "\n",
      "Episode 242: Starting computation.\n",
      "Random Seed Set to 342\n",
      "Episode 242: Finished running.\n",
      "Agent 0, Average Reward: -27.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 227.7697\n",
      "Reducing exploration for all agents to 0.0037\n",
      "\n",
      "Episode 243: Starting computation.\n",
      "Random Seed Set to 343\n",
      "Episode 243: Finished running.\n",
      "Agent 0, Average Reward: -24.21\n",
      "Epoch 1/1\n",
      " - 0s - loss: 230.2774\n",
      "Reducing exploration for all agents to 0.0036\n",
      "\n",
      "Episode 244: Starting computation.\n",
      "Random Seed Set to 344\n",
      "Episode 244: Finished running.\n",
      "Agent 0, Average Reward: -25.01\n",
      "Epoch 1/1\n",
      " - 0s - loss: 301.9745\n",
      "Reducing exploration for all agents to 0.0036\n",
      "\n",
      "Episode 245: Starting computation.\n",
      "Random Seed Set to 345\n",
      "Episode 245: Finished running.\n",
      "Agent 0, Average Reward: -27.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 270.9395\n",
      "Reducing exploration for all agents to 0.0035\n",
      "\n",
      "Episode 246: Starting computation.\n",
      "Random Seed Set to 346\n",
      "Episode 246: Finished running.\n",
      "Agent 0, Average Reward: -27.7\n",
      "Epoch 1/1\n",
      " - 0s - loss: 236.8592\n",
      "Reducing exploration for all agents to 0.0034\n",
      "\n",
      "Episode 247: Starting computation.\n",
      "Random Seed Set to 347\n",
      "Episode 247: Finished running.\n",
      "Agent 0, Average Reward: -25.63\n",
      "Epoch 1/1\n",
      " - 0s - loss: 351.7054\n",
      "Reducing exploration for all agents to 0.0033\n",
      "\n",
      "Episode 248: Starting computation.\n",
      "Random Seed Set to 348\n",
      "Episode 248: Finished running.\n",
      "Agent 0, Average Reward: -27.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 326.9293\n",
      "Reducing exploration for all agents to 0.0032\n",
      "\n",
      "Episode 249: Starting computation.\n",
      "Random Seed Set to 349\n",
      "Episode 249: Finished running.\n",
      "Agent 0, Average Reward: -27.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 293.5251\n",
      "Reducing exploration for all agents to 0.0032\n",
      "\n",
      "Episode 250: Starting computation.\n",
      "Random Seed Set to 350\n",
      "Episode 250: Finished running.\n",
      "Agent 0, Average Reward: -28.09\n",
      "Epoch 1/1\n",
      " - 0s - loss: 280.5581\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0031\n",
      "\n",
      "Episode 251: Starting computation.\n",
      "Random Seed Set to 351\n",
      "Episode 251: Finished running.\n",
      "Agent 0, Average Reward: -28.97\n",
      "Epoch 1/1\n",
      " - 0s - loss: 362.9039\n",
      "Reducing exploration for all agents to 0.003\n",
      "\n",
      "Episode 252: Starting computation.\n",
      "Random Seed Set to 352\n",
      "Episode 252: Finished running.\n",
      "Agent 0, Average Reward: -30.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 276.5705\n",
      "Reducing exploration for all agents to 0.003\n",
      "\n",
      "Episode 253: Starting computation.\n",
      "Random Seed Set to 353\n",
      "Episode 253: Finished running.\n",
      "Agent 0, Average Reward: -29.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 320.5349\n",
      "Reducing exploration for all agents to 0.0029\n",
      "\n",
      "Episode 254: Starting computation.\n",
      "Random Seed Set to 354\n",
      "Episode 254: Finished running.\n",
      "Agent 0, Average Reward: -27.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 481.8638\n",
      "Reducing exploration for all agents to 0.0028\n",
      "\n",
      "Episode 255: Starting computation.\n",
      "Random Seed Set to 355\n",
      "Episode 255: Finished running.\n",
      "Agent 0, Average Reward: -28.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 303.3762\n",
      "Reducing exploration for all agents to 0.0028\n",
      "\n",
      "Episode 256: Starting computation.\n",
      "Random Seed Set to 356\n",
      "Episode 256: Finished running.\n",
      "Agent 0, Average Reward: -26.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 289.6409\n",
      "Reducing exploration for all agents to 0.0027\n",
      "\n",
      "Episode 257: Starting computation.\n",
      "Random Seed Set to 357\n",
      "Episode 257: Finished running.\n",
      "Agent 0, Average Reward: -26.79\n",
      "Epoch 1/1\n",
      " - 0s - loss: 315.4619\n",
      "Reducing exploration for all agents to 0.0026\n",
      "\n",
      "Episode 258: Starting computation.\n",
      "Random Seed Set to 358\n",
      "Episode 258: Finished running.\n",
      "Agent 0, Average Reward: -23.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 299.3313\n",
      "Reducing exploration for all agents to 0.0026\n",
      "\n",
      "Episode 259: Starting computation.\n",
      "Random Seed Set to 359\n",
      "Episode 259: Finished running.\n",
      "Agent 0, Average Reward: -25.23\n",
      "Epoch 1/1\n",
      " - 0s - loss: 322.4083\n",
      "Reducing exploration for all agents to 0.0025\n",
      "\n",
      "Episode 260: Starting computation.\n",
      "Random Seed Set to 360\n",
      "Episode 260: Finished running.\n",
      "Agent 0, Average Reward: -24.68\n",
      "Epoch 1/1\n",
      " - 0s - loss: 246.1879\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0025\n",
      "\n",
      "Episode 261: Starting computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 361\n",
      "Episode 261: Finished running.\n",
      "Agent 0, Average Reward: -25.86\n",
      "Epoch 1/1\n",
      " - 0s - loss: 280.8742\n",
      "Reducing exploration for all agents to 0.0024\n",
      "\n",
      "Episode 262: Starting computation.\n",
      "Random Seed Set to 362\n",
      "Episode 262: Finished running.\n",
      "Agent 0, Average Reward: -26.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 294.8068\n",
      "Reducing exploration for all agents to 0.0024\n",
      "\n",
      "Episode 263: Starting computation.\n",
      "Random Seed Set to 363\n",
      "Episode 263: Finished running.\n",
      "Agent 0, Average Reward: -25.89\n",
      "Epoch 1/1\n",
      " - 0s - loss: 238.5182\n",
      "Reducing exploration for all agents to 0.0023\n",
      "\n",
      "Episode 264: Starting computation.\n",
      "Random Seed Set to 364\n",
      "Episode 264: Finished running.\n",
      "Agent 0, Average Reward: -27.9\n",
      "Epoch 1/1\n",
      " - 0s - loss: 367.2678\n",
      "Reducing exploration for all agents to 0.0022\n",
      "\n",
      "Episode 265: Starting computation.\n",
      "Random Seed Set to 365\n",
      "Episode 265: Finished running.\n",
      "Agent 0, Average Reward: -27.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 460.1028\n",
      "Reducing exploration for all agents to 0.0022\n",
      "\n",
      "Episode 266: Starting computation.\n",
      "Random Seed Set to 366\n",
      "Episode 266: Finished running.\n",
      "Agent 0, Average Reward: -25.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 276.3472\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 267: Starting computation.\n",
      "Random Seed Set to 367\n",
      "Episode 267: Finished running.\n",
      "Agent 0, Average Reward: -25.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 225.3952\n",
      "Reducing exploration for all agents to 0.0021\n",
      "\n",
      "Episode 268: Starting computation.\n",
      "Random Seed Set to 368\n",
      "Episode 268: Finished running.\n",
      "Agent 0, Average Reward: -26.33\n",
      "Epoch 1/1\n",
      " - 0s - loss: 434.1299\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 269: Starting computation.\n",
      "Random Seed Set to 369\n",
      "Episode 269: Finished running.\n",
      "Agent 0, Average Reward: -26.55\n",
      "Epoch 1/1\n",
      " - 0s - loss: 275.3490\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 270: Starting computation.\n",
      "Random Seed Set to 370\n",
      "Episode 270: Finished running.\n",
      "Agent 0, Average Reward: -29.6\n",
      "Epoch 1/1\n",
      " - 0s - loss: 392.5522\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.002\n",
      "\n",
      "Episode 271: Starting computation.\n",
      "Random Seed Set to 371\n",
      "Episode 271: Finished running.\n",
      "Agent 0, Average Reward: -30.16\n",
      "Epoch 1/1\n",
      " - 0s - loss: 441.2409\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 272: Starting computation.\n",
      "Random Seed Set to 372\n",
      "Episode 272: Finished running.\n",
      "Agent 0, Average Reward: -29.45\n",
      "Epoch 1/1\n",
      " - 0s - loss: 412.7814\n",
      "Reducing exploration for all agents to 0.0019\n",
      "\n",
      "Episode 273: Starting computation.\n",
      "Random Seed Set to 373\n",
      "Episode 273: Finished running.\n",
      "Agent 0, Average Reward: -29.82\n",
      "Epoch 1/1\n",
      " - 0s - loss: 358.2336\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 274: Starting computation.\n",
      "Random Seed Set to 374\n",
      "Episode 274: Finished running.\n",
      "Agent 0, Average Reward: -29.03\n",
      "Epoch 1/1\n",
      " - 0s - loss: 591.6938\n",
      "Reducing exploration for all agents to 0.0018\n",
      "\n",
      "Episode 275: Starting computation.\n",
      "Random Seed Set to 375\n",
      "Episode 275: Finished running.\n",
      "Agent 0, Average Reward: -27.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 319.1964\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 276: Starting computation.\n",
      "Random Seed Set to 376\n",
      "Episode 276: Finished running.\n",
      "Agent 0, Average Reward: -29.25\n",
      "Epoch 1/1\n",
      " - 0s - loss: 421.4685\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 277: Starting computation.\n",
      "Random Seed Set to 377\n",
      "Episode 277: Finished running.\n",
      "Agent 0, Average Reward: -26.2\n",
      "Epoch 1/1\n",
      " - 0s - loss: 482.3486\n",
      "Reducing exploration for all agents to 0.0017\n",
      "\n",
      "Episode 278: Starting computation.\n",
      "Random Seed Set to 378\n",
      "Episode 278: Finished running.\n",
      "Agent 0, Average Reward: -24.83\n",
      "Epoch 1/1\n",
      " - 0s - loss: 435.7567\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 279: Starting computation.\n",
      "Random Seed Set to 379\n",
      "Episode 279: Finished running.\n",
      "Agent 0, Average Reward: -23.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 407.8422\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 280: Starting computation.\n",
      "Random Seed Set to 380\n",
      "Episode 280: Finished running.\n",
      "Agent 0, Average Reward: -24.66\n",
      "Epoch 1/1\n",
      " - 0s - loss: 279.7557\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.0016\n",
      "\n",
      "Episode 281: Starting computation.\n",
      "Random Seed Set to 381\n",
      "Episode 281: Finished running.\n",
      "Agent 0, Average Reward: -26.08\n",
      "Epoch 1/1\n",
      " - 0s - loss: 308.7753\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 282: Starting computation.\n",
      "Random Seed Set to 382\n",
      "Episode 282: Finished running.\n",
      "Agent 0, Average Reward: -23.88\n",
      "Epoch 1/1\n",
      " - 0s - loss: 309.7820\n",
      "Reducing exploration for all agents to 0.0015\n",
      "\n",
      "Episode 283: Starting computation.\n",
      "Random Seed Set to 383\n",
      "Episode 283: Finished running.\n",
      "Agent 0, Average Reward: -29.69\n",
      "Epoch 1/1\n",
      " - 0s - loss: 332.8183\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 284: Starting computation.\n",
      "Random Seed Set to 384\n",
      "Episode 284: Finished running.\n",
      "Agent 0, Average Reward: -24.05\n",
      "Epoch 1/1\n",
      " - 0s - loss: 225.2603\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 285: Starting computation.\n",
      "Random Seed Set to 385\n",
      "Episode 285: Finished running.\n",
      "Agent 0, Average Reward: -27.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 336.0614\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 286: Starting computation.\n",
      "Random Seed Set to 386\n",
      "Episode 286: Finished running.\n",
      "Agent 0, Average Reward: -24.52\n",
      "Epoch 1/1\n",
      " - 0s - loss: 258.8532\n",
      "Reducing exploration for all agents to 0.0014\n",
      "\n",
      "Episode 287: Starting computation.\n",
      "Random Seed Set to 387\n",
      "Episode 287: Finished running.\n",
      "Agent 0, Average Reward: -25.76\n",
      "Epoch 1/1\n",
      " - 0s - loss: 355.1905\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 288: Starting computation.\n",
      "Random Seed Set to 388\n",
      "Episode 288: Finished running.\n",
      "Agent 0, Average Reward: -25.46\n",
      "Epoch 1/1\n",
      " - 0s - loss: 249.1852\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 289: Starting computation.\n",
      "Random Seed Set to 389\n",
      "Episode 289: Finished running.\n",
      "Agent 0, Average Reward: -25.15\n",
      "Epoch 1/1\n",
      " - 0s - loss: 188.6345\n",
      "Reducing exploration for all agents to 0.0013\n",
      "\n",
      "Episode 290: Starting computation.\n",
      "Random Seed Set to 390\n",
      "Episode 290: Finished running.\n",
      "Agent 0, Average Reward: -27.61\n",
      "Epoch 1/1\n",
      " - 0s - loss: 346.2060\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 291: Starting computation.\n",
      "Random Seed Set to 391\n",
      "Episode 291: Finished running.\n",
      "Agent 0, Average Reward: -28.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 316.0686\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 292: Starting computation.\n",
      "Random Seed Set to 392\n",
      "Episode 292: Finished running.\n",
      "Agent 0, Average Reward: -28.14\n",
      "Epoch 1/1\n",
      " - 0s - loss: 429.1921\n",
      "Reducing exploration for all agents to 0.0012\n",
      "\n",
      "Episode 293: Starting computation.\n",
      "Random Seed Set to 393\n",
      "Episode 293: Finished running.\n",
      "Agent 0, Average Reward: -28.32\n",
      "Epoch 1/1\n",
      " - 0s - loss: 334.8301\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 294: Starting computation.\n",
      "Random Seed Set to 394\n",
      "Episode 294: Finished running.\n",
      "Agent 0, Average Reward: -26.94\n",
      "Epoch 1/1\n",
      " - 0s - loss: 332.2610\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 295: Starting computation.\n",
      "Random Seed Set to 395\n",
      "Episode 295: Finished running.\n",
      "Agent 0, Average Reward: -26.64\n",
      "Epoch 1/1\n",
      " - 0s - loss: 330.1996\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 296: Starting computation.\n",
      "Random Seed Set to 396\n",
      "Episode 296: Finished running.\n",
      "Agent 0, Average Reward: -26.92\n",
      "Epoch 1/1\n",
      " - 0s - loss: 362.1945\n",
      "Reducing exploration for all agents to 0.0011\n",
      "\n",
      "Episode 297: Starting computation.\n",
      "Random Seed Set to 397\n",
      "Episode 297: Finished running.\n",
      "Agent 0, Average Reward: -29.11\n",
      "Epoch 1/1\n",
      " - 0s - loss: 537.5299\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 298: Starting computation.\n",
      "Random Seed Set to 398\n",
      "Episode 298: Finished running.\n",
      "Agent 0, Average Reward: -27.84\n",
      "Epoch 1/1\n",
      " - 0s - loss: 385.2741\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 299: Starting computation.\n",
      "Random Seed Set to 399\n",
      "Episode 299: Finished running.\n",
      "Agent 0, Average Reward: -26.44\n",
      "Epoch 1/1\n",
      " - 0s - loss: 360.5397\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 300: Starting computation.\n",
      "Random Seed Set to 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300: Finished running.\n",
      "Agent 0, Average Reward: -28.4\n",
      "Epoch 1/1\n",
      " - 0s - loss: 330.5846\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Reducing exploration for all agents to 0.001\n",
      "\n",
      "Episode 301: Starting computation.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving architecture, weights and optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "#Single_Cross_Straight_MultiDQN_Agents.save(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.load(300,False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.43 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.demo(vissim=vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERSECTION 0: SETTING UP AGENT\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (None, 24)           144         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 24)           600         dense_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 24)           600         dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 24)           600         dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 1)            25          dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 2)            50          dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_101[0][0]                  \n",
      "                                                                 dense_99[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,019\n",
      "Trainable params: 2,019\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Deep Q Learning Agent(s) at Intersection 0\n",
      "\n",
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: test\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.16 seconds.\n",
      "\n",
      "INTERSECTION 0: SETTING UP AGENT\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 24)           144         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (None, 24)           600         dense_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_112 (Dense)               (None, 24)           600         dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_110 (Dense)               (None, 24)           600         dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_113 (Dense)               (None, 1)            25          dense_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_111 (Dense)               (None, 2)            50          dense_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_113[0][0]                  \n",
      "                                                                 dense_111[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,019\n",
      "Trainable params: 2,019\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deployed instance of Dueling Deep Q Learning Agent(s) at Intersection 0\n",
      "\n",
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-c81c9d5b994e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mSingle_Cross_Straight_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mSingle_Cross_Straight_MultiDQN_Agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvissim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Documents\\MLforFlowOptimisation\\NSW_Single_Cross_Experiment\\MasterDQN_Agent.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, episode, best)\u001b[0m\n\u001b[0;32m    454\u001b[0m                 \"\"\"\n\u001b[0;32m    455\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m                         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvissim_working_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m                         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_episode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - The Alan Turing Institute\\Documents\\MLforFlowOptimisation\\NSW_Single_Cross_Experiment\\General_agent.py\u001b[0m in \u001b[0;36mload_agent\u001b[1;34m(self, vissim_working_directory, model_name, agent_type, Session_ID, episode, best, load_location, memory_required)\u001b[0m\n\u001b[0;32m    230\u001b[0m                                                         'Episode'+ str(episode) +'Agent'+str(self.ID)+'.h5')\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmemory_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    275\u001b[0m         ]\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_weight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m           logging.warning('Error in loading the saved optimizer '\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m    127\u001b[0m             'provided weight shape ' + str(w.shape))\n\u001b[0;32m    128\u001b[0m       \u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   2782\u001b[0m         \u001b[0massign_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2783\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2784\u001b[1;33m       \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vissimgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = []    \n",
    "\n",
    "End_of_simulation = 3600\n",
    "no_of_simulations = 1\n",
    "demands = [200, 400, 600, 800, 1000, 1200]\n",
    "\n",
    "vissim.Simulation.Stop()\n",
    "Vissim = vissim\n",
    "Vissim.Graphics.CurrentNetworkWindow.SetAttValue(\"QuickMode\",1)\n",
    "Vissim.SuspendUpdateGUI()\n",
    "Vissim.Simulation.SetAttValue('SimPeriod', End_of_simulation)\n",
    "Vissim.Simulation.SetAttValue('UseMaxSimSpeed', True)\n",
    "\n",
    "for demand in demands :\n",
    "    for simRun in Vissim.Net.SimulationRuns:\n",
    "        Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "    \n",
    "    for i in range(10):\n",
    "        Single_Cross_Straight_dictionary['demand'][i] = [demand,demand,demand,demand]\n",
    "    \n",
    "        Single_Cross_Straight_MultiDQN_Agents =\\\n",
    "        MasterDQN_Agent(model_name, \n",
    "                        vissim_working_directory, \n",
    "                        sim_length, \n",
    "                        Single_Cross_Straight_dictionary,\n",
    "                        'default_actions',\n",
    "                        gamma, alpha, agent_type, memory_size, PER_activated, \n",
    "                        batch_size, copy_weights_frequency, epsilon_sequence,\n",
    "                        Random_Seed = Random_Seed, timesteps_per_second = 1, \n",
    "                        Session_ID = Session_ID, verbose = True)\n",
    "\n",
    "    \n",
    "        Single_Cross_Straight_MultiDQN_Agents.load(300,False) \n",
    "    \n",
    "        Single_Cross_Straight_MultiDQN_Agents.test(vissim=Vissim)\n",
    "    \n",
    "    data.append([demand, Vissim.Net.DelayMeasurements.GetMultiAttValues('VehDelay(Avg,Avg,All)')])\n",
    "    print(data[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: test\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.16 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiDQN_Agents.test(vissim=Vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vissim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0ace32f2516b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDelayMeasurements\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetMultiAttValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VehDelay(Avg,Avg,All)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Vissim' is not defined"
     ]
    }
   ],
   "source": [
    "Vissim.Net.DelayMeasurements.GetMultiAttValues('VehDelay(Avg,Avg,All)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
