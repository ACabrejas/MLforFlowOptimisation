{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This completes a start to finish training for Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Modules that we need\n",
    "from Vissim_env_class import environment, Load_Vissim\n",
    "from MasterAC_Agent import MasterAC_Agent\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "vissim \\\n",
    "= \\\n",
    "Load_Vissim(\n",
    "Path_to_network = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\NSW_Single_Cross_Experiment\\\\Single_Cross_Straight\\\\',\\\n",
    "inpx_Filename = 'Single_Cross_Straight.inpx',\\\n",
    "layx_Filename = 'Single_Cross_Straight.layx',\\\n",
    "attempts=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Cross AC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory=\\\n",
    "'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\n",
    "\\\\Documents\\\\MLforFlowOptimisation\\\\NSW_Single_Cross_Experiment\\\\'\n",
    "\n",
    "#vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "agent_type = \"AC\"\n",
    "Session_ID = \"Single_Cross_Straigth_AC\"\n",
    "\n",
    "\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'default_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    "         \n",
    "         'all_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         'agent_type' : agent_type,\n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues',\n",
    "        'queues_counter_ID' : [1,2,3,4]}\n",
    "        },\n",
    " 'demand' : { 'default' : [800, 800, 800, 800],\n",
    "             \n",
    "             0 : [200,200,200,200],\n",
    "             1 : [400,400,400,400],\n",
    "             2 : [900,500,900,500],\n",
    "             3 : [1000,500,1000,500],\n",
    "             4 : [700,500,700,500],\n",
    "             5 : [500,700,500,700],\n",
    "             6 : [500,1000,500,1000],\n",
    "             7 : [500,900,500,900],\n",
    "             8 : [400,400,400,400],\n",
    "             9 : [200,200,200,200]\n",
    "            }\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "alpha = 0.00001\n",
    "\n",
    "\n",
    "value = 0.5\n",
    "entropy = 0.5\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "reduce_entropy_every = 100\n",
    "Random_Seed = 1\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  288       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  1176      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  25        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  288       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1176      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  50        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 3,003\n",
      "Trainable params: 3,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents\\\n",
    "= MasterAC_Agent(model_name, \n",
    "                 vissim_working_directory, \n",
    "                 sim_length, \n",
    "                 Single_Cross_Straight_dictionary,\\\n",
    "                n_step_size, gamma, alpha, entropy, value, \\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, Session_ID = Session_ID, verbose = True, \\\n",
    "                 horizon = horizon, n_sample = n_sample,\n",
    "                save_location = 'C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Desktop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.1 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1024 16:11:56.002338 21752 deprecation.py:323] From C:\\Users\\nwalton\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 101\n",
      "Episode 1 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-46.0, -46.0, -30.0, -46.0, -46.0, -46.0, -46.0, -46.0, -46.0, -46.0] \n",
      " [-9473.0, -9473.0, -8675.0, -9473.0, -9473.0, -9473.0, -9473.0, -9473.0, -9473.0, -9473.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -489.18\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 102\n",
      "Episode 2 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-70.0, -70.0, -70.0, -70.0, -70.0, -70.0, -70.0, -70.0, -70.0, -70.0] \n",
      " [-9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0, -9424.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -484.26\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 103\n",
      "Episode 3 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-93.0, -93.0, -93.0, -36.0, -93.0, -93.0, -93.0, -93.0, -93.0, -93.0] \n",
      " [-9504.0, -9504.0, -9504.0, -6559.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -489.41\n",
      "Random Seed Set to 104\n",
      "Episode 4 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-121.0, -121.0, -121.0, -121.0, -121.0, -121.0, -121.0, -121.0, -121.0, -6.0] \n",
      " [-9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -9472.0, -4441.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.98, 0.02]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -488.19\n",
      "Random Seed Set to 105\n",
      "Episode 5 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-148.0, -148.0, -148.0, -148.0, -148.0, -148.0, -148.0, -148.0, -148.0, -148.0] \n",
      " [-9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0, -9392.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -481.98\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 106\n",
      "Episode 6 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-178.0, -178.0, -178.0, -178.0, -178.0, -178.0, -178.0, -178.0, -178.0, -4.0] \n",
      " [-9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -9427.0, -2617.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.73, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -470.79\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 107\n",
      "Episode 7 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-211.0, -211.0, -211.0, -211.0, -211.0, -211.0, -211.0, -211.0, -211.0, -211.0] \n",
      " [-9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0, -9398.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -487.22\n",
      "Random Seed Set to 108\n",
      "Episode 8 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-244.0, -244.0, -244.0, -244.0, -244.0, -244.0, -244.0, -244.0, -76.0, -244.0] \n",
      " [-9399.0, -9399.0, -9399.0, -9399.0, -9399.0, -9399.0, -9399.0, -9399.0, -5715.0, -9399.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -482.26\n",
      "Random Seed Set to 109\n",
      "Episode 9 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-279.0, -269.0, -279.0, -279.0, -279.0, -279.0, -87.0, -17.0, -279.0, -279.0] \n",
      " [-9401.0, -9344.0, -9401.0, -9401.0, -9401.0, -9401.0, -5924.0, -4129.0, -9401.0, -9401.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -476.77\n",
      "Random Seed Set to 110\n",
      "Episode 10 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-314.0, -314.0, -310.0, -138.0, -314.0, -80.0, -314.0, -314.0, -314.0, -314.0] \n",
      " [-9442.0, -9442.0, -9439.0, -6694.0, -9442.0, -5031.0, -9442.0, -9442.0, -9442.0, -9442.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -478.32\n",
      "Random Seed Set to 111\n",
      "Episode 11 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-351.0, -351.0, -351.0, -351.0, -351.0, -351.0, -351.0, -351.0, -351.0, -351.0] \n",
      " [-9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0, -9406.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -485.89\n",
      "Random Seed Set to 112\n",
      "Episode 12 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-391.0, -391.0, -391.0, -391.0, -391.0, -40.0, -391.0, -391.0, -391.0, -391.0] \n",
      " [-9405.0, -9405.0, -9405.0, -9405.0, -9405.0, -4771.0, -9405.0, -9405.0, -9405.0, -9405.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -484.66\n",
      "Random Seed Set to 113\n",
      "Episode 13 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-431.0, -431.0, -431.0, -431.0, -431.0, -425.0, -431.0, -431.0, -312.0, -431.0] \n",
      " [-9410.0, -9410.0, -9410.0, -9410.0, -9410.0, -9413.0, -9410.0, -9410.0, -8474.0, -9410.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -477.95\n",
      "Random Seed Set to 114\n",
      "Episode 14 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-473.0, -473.0, -473.0, -473.0, -473.0, -473.0, -473.0, -473.0, -473.0, -473.0] \n",
      " [-9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0, -9365.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -479.71\n",
      "Random Seed Set to 115\n",
      "Episode 15 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-516.0, -372.0, -516.0, -516.0, -516.0, -516.0, -516.0, -516.0, -516.0, -516.0] \n",
      " [-9323.0, -8091.0, -9323.0, -9323.0, -9323.0, -9323.0, -9323.0, -9323.0, -9323.0, -9323.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -476.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 116\n",
      "Episode 16 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-575.0, -575.0, -575.0, -575.0, -575.0, -575.0, -575.0, -575.0, -575.0, -575.0] \n",
      " [-9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0, -9504.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -492.3\n",
      "Random Seed Set to 117\n",
      "Episode 17 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-621.0, -621.0, -621.0, -621.0, -621.0, -621.0, -621.0, -621.0, -540.0, -621.0] \n",
      " [-9426.0, -9426.0, -9426.0, -9426.0, -9426.0, -9426.0, -9426.0, -9426.0, -9121.0, -9426.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -479.09\n",
      "Random Seed Set to 118\n",
      "Episode 18 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-672.0, -672.0, -672.0, -672.0, -672.0, -1.0, -672.0, -102.0, -154.0, -157.0] \n",
      " [-9396.0, -9396.0, -9396.0, -9396.0, -9396.0, -2237.0, -9396.0, -4647.0, -5483.0, -5907.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.5, 0.5], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -481.11\n",
      "Random Seed Set to 119\n",
      "Episode 19 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-734.0, -734.0, -734.0, -734.0, -734.0, -137.0, -734.0, -734.0, -734.0, -734.0] \n",
      " [-9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -4326.0, -9465.0, -9465.0, -9465.0, -9465.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -483.38\n",
      "Random Seed Set to 120\n",
      "Episode 20 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-786.0, -786.0, -786.0, -786.0, -786.0, -786.0, -48.0, -786.0, -786.0, -786.0] \n",
      " [-9389.0, -9389.0, -9389.0, -9389.0, -9389.0, -9389.0, -3534.0, -9389.0, -9389.0, -9389.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.97, 0.03], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -474.76\n",
      "Random Seed Set to 121\n",
      "Episode 21 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-859.0, -859.0, -787.0, -859.0, -859.0, -859.0, -859.0, -859.0, -859.0, -859.0] \n",
      " [-9475.0, -9475.0, -9270.0, -9475.0, -9475.0, -9475.0, -9475.0, -9475.0, -9475.0, -9475.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -482.52\n",
      "Random Seed Set to 122\n",
      "Episode 22 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-920.0, -920.0, -920.0, -920.0, -920.0, -920.0, -920.0, -920.0, -920.0, -920.0] \n",
      " [-9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0, -9395.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -480.86\n",
      "Random Seed Set to 123\n",
      "Episode 23 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-998.0, -998.0, -975.0, -998.0, -998.0, -998.0, -998.0, -998.0, -998.0, -998.0] \n",
      " [-9433.0, -9433.0, -9381.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -482.41\n",
      "Random Seed Set to 124\n",
      "Episode 24 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1086.0, -1086.0, -219.0, -1086.0, -1086.0, -1086.0, -1086.0, -1086.0, -1086.0, -1086.0] \n",
      " [-9532.0, -9532.0, -5081.0, -9532.0, -9532.0, -9532.0, -9532.0, -9532.0, -9532.0, -9532.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -490.49\n",
      "Random Seed Set to 125\n",
      "Episode 25 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1148.0, -1148.0, -1148.0, -1148.0, -1141.0, -1148.0, -1148.0, -1148.0, -1148.0, -1148.0] \n",
      " [-9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -485.51\n",
      "Random Seed Set to 126\n",
      "Episode 26 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0, -1227.0] \n",
      " [-9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0, -9352.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -478.63\n",
      "Random Seed Set to 127\n",
      "Episode 27 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0, -1318.0] \n",
      " [-9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0, -9390.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -486.98\n",
      "Random Seed Set to 128\n",
      "Episode 28 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1410.0, -1355.0, -1410.0, -1410.0, -1410.0, -1410.0, -843.0, -1410.0, -1410.0, -1410.0] \n",
      " [-9411.0, -9353.0, -9411.0, -9411.0, -9411.0, -9411.0, -7840.0, -9411.0, -9411.0, -9411.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -473.45\n",
      "Random Seed Set to 129\n",
      "Episode 29 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -1504.0, -1504.0, -1504.0, -1504.0, -1504.0, -1504.0, -512.0, -76.0, -1504.0] \n",
      " [-2001.0, -9394.0, -9394.0, -9394.0, -9394.0, -9394.0, -9394.0, -5583.0, -2879.0, -9394.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.53, 0.47], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.77, 0.23], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -473.94\n",
      "Random Seed Set to 130\n",
      "Episode 30 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1614.0, -162.0, -1614.0, -1614.0, -1614.0, -1614.0, -1614.0, -1614.0, -1614.0, -1614.0] \n",
      " [-9467.0, -3740.0, -9467.0, -9467.0, -9467.0, -9467.0, -9467.0, -9467.0, -9467.0, -9467.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.98, 0.02], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -482.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 131\n",
      "Episode 31 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1731.0, -1553.0, -1731.0, -701.0, -1731.0, -1731.0, -1731.0, -210.0, -1731.0, -1731.0] \n",
      " [-9530.0, -9375.0, -9530.0, -6574.0, -9530.0, -9530.0, -9530.0, -3679.0, -9530.0, -9530.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -481.57\n",
      "Random Seed Set to 132\n",
      "Episode 32 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1813.0, -1813.0, -1813.0, -1813.0, -1426.0, -1692.0, -10.0, -1813.0, -1813.0, -1813.0] \n",
      " [-9393.0, -9393.0, -9393.0, -9393.0, -8779.0, -9299.0, -2396.0, -9393.0, -9393.0, -9393.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.71, 0.29], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -471.22\n",
      "Random Seed Set to 133\n",
      "Episode 33 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1952.0, -1952.0, -1952.0, -1952.0, -1639.0, -1952.0, -1952.0, -1952.0, -1952.0, -1952.0] \n",
      " [-9500.0, -9500.0, -9500.0, -9500.0, -9058.0, -9500.0, -9500.0, -9500.0, -9500.0, -9500.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -486.48\n",
      "Random Seed Set to 134\n",
      "Episode 34 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0, -2047.0] \n",
      " [-9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0, -9386.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -477.96\n",
      "Random Seed Set to 135\n",
      "Episode 35 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-800.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0, -2189.0] \n",
      " [-6265.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0, -9465.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -481.62\n",
      "Random Seed Set to 136\n",
      "Episode 36 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-16.0, -2320.0, -2320.0, -2320.0, -2320.0, -184.0, -2320.0, -655.0, -2320.0, -2320.0] \n",
      " [-2497.0, -9463.0, -9463.0, -9463.0, -9463.0, -3647.0, -9463.0, -5843.0, -9463.0, -9463.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.78, 0.22], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.93, 0.07], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -479.99\n",
      "Random Seed Set to 137\n",
      "Episode 37 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0, -2443.0] \n",
      " [-9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0, -9409.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -487.29\n",
      "Random Seed Set to 138\n",
      "Episode 38 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -2573.0, -475.0] \n",
      " [-9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -9380.0, -4632.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -481.89\n",
      "Random Seed Set to 139\n",
      "Episode 39 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0, -2747.0] \n",
      " [-9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0, -9483.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -481.97\n",
      "Random Seed Set to 140\n",
      "Episode 40 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2875.0, -2875.0, -2875.0, -2875.0, -2875.0, -2875.0, -2875.0, -2875.0, -2706.0, -2875.0] \n",
      " [-9433.0, -9434.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0, -9433.0, -9392.0, -8240.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.14, 0.86], [0.22, 0.78], [0.14, 0.86]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -463.78\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 141\n",
      "Episode 41 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-280.0, -69.0, -138.0, -105.0, -185.0, -54.0, -69.0, -93.0, -126.0, -72.0] \n",
      " [-705.0, -681.0, -598.0, -579.0, -373.0, -381.0, -583.0, -428.0, -538.0, -617.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.38, 0.62], [1.0, 0.0], [1.0, 0.0], [0.28, 0.72], [0.17, 0.83], [0.42, 0.58], [0.41, 0.59], [0.37, 0.63], [0.35, 0.65], [0.36, 0.64]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -31.6\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 142\n",
      "Episode 42 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-66.0, -73.0, -81.0, -67.0, -113.0, -170.0, -5.0, -125.0, -62.0, -43.0] \n",
      " [-495.0, -531.0, -412.0, -506.0, -533.0, -595.0, -452.0, -517.0, -474.0, -591.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.38, 0.62], [0.98, 0.02], [0.42, 0.58], [0.43, 0.57], [0.14, 0.86], [0.35, 0.65], [0.49, 0.51], [0.99, 0.01], [1.0, 0.0], [0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -27.65\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 143\n",
      "Episode 43 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-148.0, -14.0, -197.0, -117.0, -117.0, -78.0, -41.0, -5.0, -347.0, -45.0] \n",
      " [-314.0, -462.0, -383.0, -405.0, -479.0, -347.0, -585.0, -366.0, -316.0, -395.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.2, 0.8], [0.83, 0.17], [0.11, 0.89], [0.35, 0.65], [0.96, 0.04], [0.31, 0.69], [0.99, 0.01], [0.49, 0.51], [0.01, 0.99], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Average Reward for Agent 0 this episode : -24.07\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 144\n",
      "Episode 44 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-38.0, -234.0, -21.0, -229.0, -64.0, -187.0, -145.0, -15.0, -133.0, -34.0] \n",
      " [-442.0, -617.0, -495.0, -482.0, -312.0, -451.0, -449.0, -534.0, -347.0, -509.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.99, 0.01], [0.36, 0.64], [0.82, 0.18], [1.0, 0.0], [0.42, 0.58], [0.05, 0.95], [0.12, 0.88], [0.9, 0.1], [1.0, 0.0], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -24.34\n",
      "Random Seed Set to 145\n",
      "Episode 45 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-91.0, -29.0, -67.0, -48.0, -71.0, -44.0, -34.0, -282.0, -31.0, -81.0] \n",
      " [-326.0, -433.0, -423.0, -495.0, -501.0, -551.0, -368.0, -311.0, -579.0, -465.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [0.38, 0.62], [0.99, 0.01], [0.06, 0.94], [1.0, 0.0], [0.27, 0.73]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Average Reward for Agent 0 this episode : -24.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 146\n",
      "Episode 46 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-51.0, -11.0, -528.0, -110.0, -48.0, -192.0, -90.0, -174.0, -35.0, -278.0] \n",
      " [-482.0, -380.0, -667.0, -366.0, -822.0, -346.0, -436.0, -452.0, -517.0, -494.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.34, 0.66], [0.82, 0.18], [0.0, 1.0], [0.17, 0.83], [0.35, 0.65], [0.02, 0.98], [0.99, 0.01], [0.04, 0.96], [0.37, 0.63], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Average Reward for Agent 0 this episode : -25.1\n",
      "Random Seed Set to 147\n",
      "Episode 47 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-42.0, -65.0, -63.0, -90.0, -55.0, -300.0, -143.0, -61.0, -16.0, -7.0] \n",
      " [-343.0, -297.0, -423.0, -346.0, -377.0, -686.0, -470.0, -386.0, -409.0, -457.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.31, 0.69], [0.26, 0.74], [1.0, 0.0], [0.14, 0.86], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.95, 0.05], [0.61, 0.39]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Average Reward for Agent 0 this episode : -23.36\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 148\n",
      "Episode 48 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -435.0, -181.0, -55.0, -104.0, -94.0, -226.0, -131.0, -48.0, -154.0] \n",
      " [-415.0, -655.0, -771.0, -477.0, -495.0, -302.0, -348.0, -474.0, -443.0, -366.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.51, 0.49], [0.0, 1.0], [0.01, 0.99], [1.0, 0.0], [0.97, 0.03], [1.0, 0.0], [0.03, 0.97], [0.05, 0.95], [0.26, 0.74], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.51 0.49]\n",
      "Average Reward for Agent 0 this episode : -24.43\n",
      "Random Seed Set to 149\n",
      "Episode 49 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-190.0, -135.0, -48.0, -51.0, -58.0, -48.0, -74.0, -263.0, -77.0, -54.0] \n",
      " [-336.0, -364.0, -418.0, -533.0, -489.0, -418.0, -390.0, -407.0, -410.0, -429.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.01, 0.99], [0.03, 0.97], [0.24, 0.76], [1.0, 0.0], [0.31, 0.69], [0.24, 0.76], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.25, 0.75]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.52 0.48]\n",
      "Average Reward for Agent 0 this episode : -22.56\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 150\n",
      "Episode 50 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18.0, -156.0, -151.0, -136.0, -375.0, -19.0, -41.0, -51.0, -140.0, -32.0] \n",
      " [-452.0, -433.0, -512.0, -482.0, -618.0, -401.0, -356.0, -391.0, -332.0, -473.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.98, 0.02], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [1.0, 0.0], [0.02, 0.98], [0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.52 0.48]\n",
      "Average Reward for Agent 0 this episode : -23.3\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Saving at :  C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Desktop\\Episode50Agent0_Weights.h5\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Random Seed Set to 151\n",
      "Episode 51 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-72.0, -226.0, -21.0, -99.0, -195.0, -139.0, -106.0, -74.0, -8.0, -24.0] \n",
      " [-429.0, -281.0, -518.0, -375.0, -439.0, -473.0, -468.0, -256.0, -568.0, -454.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.86, 0.14], [0.99, 0.01], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.67, 0.33], [0.99, 0.01]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.53 0.47]\n",
      "Average Reward for Agent 0 this episode : -25.8\n",
      "Random Seed Set to 152\n",
      "Episode 52 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-201.0, -161.0, -195.0, -51.0, -136.0, -88.0, -107.0, -247.0, -66.0, -104.0] \n",
      " [-323.0, -659.0, -561.0, -614.0, -498.0, -358.0, -399.0, -383.0, -611.0, -384.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.01, 0.99], [0.0, 1.0], [0.19, 0.81], [0.16, 0.84], [1.0, 0.0], [0.03, 0.97], [0.0, 1.0], [1.0, 0.0], [0.04, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.53 0.47]\n",
      "Average Reward for Agent 0 this episode : -25.04\n",
      "Random Seed Set to 153\n",
      "Episode 53 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-44.0, -31.0, -197.0, -104.0, -35.0, -38.0, -7.0, -279.0, -7.0, -275.0] \n",
      " [-539.0, -487.0, -533.0, -576.0, -462.0, -362.0, -391.0, -422.0, -455.0, -353.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.3, 0.7], [0.99, 0.01], [0.65, 0.35], [0.03, 0.97], [1.0, 0.0], [1.0, 0.0], [0.54, 0.46], [0.0, 1.0], [0.54, 0.46], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.54 0.46]\n",
      "Average Reward for Agent 0 this episode : -24.96\n",
      "Random Seed Set to 154\n",
      "Episode 54 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-103.0, -67.0, -133.0, -280.0, -236.0, -204.0, -67.0, -245.0, -183.0, -98.0] \n",
      " [-404.0, -548.0, -459.0, -535.0, -467.0, -621.0, -548.0, -627.0, -458.0, -329.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.97], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.01, 0.99], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.02, 0.98]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.55 0.45]\n",
      "Average Reward for Agent 0 this episode : -25.45\n",
      "Random Seed Set to 155\n",
      "Episode 55 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-36.0, -15.0, -93.0, -161.0, -83.0, -102.0, -26.0, -103.0, -250.0, -60.0] \n",
      " [-411.0, -371.0, -321.0, -488.0, -397.0, -476.0, -528.0, -425.0, -605.0, -532.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.91, 0.09], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.03, 0.97], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.55 0.45]\n",
      "Average Reward for Agent 0 this episode : -24.93\n",
      "Random Seed Set to 156\n",
      "Episode 56 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -99.0, -94.0, -101.0, -25.0, -659.0, -598.0, -7.0, -101.0, -262.0] \n",
      " [-528.0, -338.0, -490.0, -432.0, -397.0, -489.0, -696.0, -412.0, -432.0, -344.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.75, 0.25], [0.02, 0.98], [0.02, 0.98], [0.02, 0.98], [1.0, 0.0], [1.0, 0.0], [0.14, 0.86], [0.56, 0.44], [0.02, 0.98], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.56 0.44]\n",
      "Average Reward for Agent 0 this episode : -24.89\n",
      "Random Seed Set to 157\n",
      "Episode 57 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-35.0, -38.0, -154.0, -99.0, -45.0, -7.0, -43.0, -170.0, -186.0, -25.0] \n",
      " [-363.0, -392.0, -332.0, -341.0, -515.0, -306.0, -409.0, -600.0, -728.0, -350.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.02, 0.98], [1.0, 0.0], [0.56, 0.44], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.56 0.44]\n",
      "Average Reward for Agent 0 this episode : -24.1\n",
      "Random Seed Set to 158\n",
      "Episode 58 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -272.0, -393.0, -347.0, -234.0, -193.0, -67.0, -35.0, -347.0, -159.0] \n",
      " [-397.0, -365.0, -310.0, -506.0, -443.0, -316.0, -432.0, -403.0, -355.0, -509.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.57, 0.43], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.04, 0.96], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.57 0.43]\n",
      "Average Reward for Agent 0 this episode : -22.97\n",
      "Random Seed Set to 159\n",
      "Episode 59 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-31.0, -272.0, -10.0, -114.0, -136.0, -120.0, -63.0, -46.0, -8.0, -107.0] \n",
      " [-394.0, -565.0, -521.0, -555.0, -721.0, -547.0, -333.0, -489.0, -611.0, -466.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.8, 0.2], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.09, 0.91], [1.0, 0.0], [0.58, 0.42], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.58 0.42]\n",
      "Average Reward for Agent 0 this episode : -24.38\n",
      "Random Seed Set to 160\n",
      "Episode 60 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-407.0, -37.0, -71.0, -50.0, -162.0, -47.0, -10.0, -171.0, -47.0, -94.0] \n",
      " [-496.0, -238.0, -428.0, -622.0, -348.0, -315.0, -595.0, -310.0, -403.0, -556.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.08, 0.92], [0.0, 1.0], [1.0, 0.0], [0.82, 0.18], [1.0, 0.0], [1.0, 0.0], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.59 0.41]\n",
      "Average Reward for Agent 0 this episode : -22.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 161\n",
      "Episode 61 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-297.0, -274.0, -76.0, -150.0, -122.0, -67.0, -47.0, -57.0, -50.0, -59.0] \n",
      " [-623.0, -449.0, -244.0, -439.0, -636.0, -385.0, -378.0, -294.0, -342.0, -557.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.25, 0.75], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.59 0.41]\n",
      "Average Reward for Agent 0 this episode : -23.59\n",
      "Random Seed Set to 162\n",
      "Episode 62 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-289.0, -218.0, -70.0, -276.0, -108.0, -102.0, -152.0, -293.0, -257.0, -248.0] \n",
      " [-673.0, -387.0, -515.0, -593.0, -414.0, -389.0, -556.0, -567.0, -669.0, -482.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.9], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.6 0.4]\n",
      "Average Reward for Agent 0 this episode : -25.38\n",
      "Random Seed Set to 163\n",
      "Episode 63 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-65.0, -50.0, -161.0, -46.0, -471.0, -434.0, -203.0, -104.0, -90.0, -128.0] \n",
      " [-500.0, -420.0, -484.0, -374.0, -538.0, -394.0, -353.0, -540.0, -398.0, -250.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.61 0.39]\n",
      "Average Reward for Agent 0 this episode : -20.87\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 164\n",
      "Episode 64 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-63.0, -423.0, -60.0, -9.0, -59.0, -225.0, -465.0, -45.0, -254.0, -133.0] \n",
      " [-406.0, -496.0, -503.0, -415.0, -317.0, -417.0, -740.0, -391.0, -658.0, -310.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.03, 0.97], [0.01, 0.99], [1.0, 0.0], [0.61, 0.39], [0.04, 0.96], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.61 0.39]\n",
      "Average Reward for Agent 0 this episode : -25.01\n",
      "Random Seed Set to 165\n",
      "Episode 65 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-78.0, -169.0, -72.0, -169.0, -493.0, -278.0, -90.0, -47.0, -57.0, -9.0] \n",
      " [-438.0, -388.0, -764.0, -388.0, -606.0, -422.0, -248.0, -698.0, -516.0, -322.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.62, 0.38]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.62 0.38]\n",
      "Average Reward for Agent 0 this episode : -25.02\n",
      "Random Seed Set to 166\n",
      "Episode 66 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-75.0, -109.0, -69.0, -70.0, -317.0, -246.0, -101.0, -186.0, -136.0, -194.0] \n",
      " [-465.0, -409.0, -529.0, -450.0, -546.0, -382.0, -416.0, -458.0, -408.0, -508.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.85, 0.15], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.63 0.37]\n",
      "Average Reward for Agent 0 this episode : -24.78\n",
      "Random Seed Set to 167\n",
      "Episode 67 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-237.0, -329.0, -135.0, -289.0, -13.0, -85.0, -112.0, -211.0, -132.0, -59.0] \n",
      " [-627.0, -404.0, -317.0, -340.0, -390.0, -636.0, -462.0, -397.0, -380.0, -414.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.92, 0.08], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.64 0.36]\n",
      "Average Reward for Agent 0 this episode : -24.75\n",
      "Random Seed Set to 168\n",
      "Episode 68 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-197.0, -77.0, -374.0, -9.0, -160.0, -60.0, -231.0, -164.0, -718.0, -193.0] \n",
      " [-561.0, -586.0, -491.0, -439.0, -534.0, -460.0, -470.0, -530.0, -352.0, -290.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.69, 0.31], [1.0, 0.0], [0.0, 1.0], [0.65, 0.35], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.65 0.35]\n",
      "Average Reward for Agent 0 this episode : -24.49\n",
      "Random Seed Set to 169\n",
      "Episode 69 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-337.0, -225.0, -178.0, -261.0, -204.0, -177.0, -42.0, -325.0, -55.0, -50.0] \n",
      " [-482.0, -503.0, -510.0, -471.0, -386.0, -448.0, -417.0, -420.0, -348.0, -455.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.02, 0.98], [0.05, 0.95], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.66 0.34]\n",
      "Average Reward for Agent 0 this episode : -23.06\n",
      "Random Seed Set to 170\n",
      "Episode 70 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-213.0, -207.0, -96.0, -96.0, -261.0, -143.0, -131.0, -218.0, -59.0, -14.0] \n",
      " [-308.0, -308.0, -775.0, -775.0, -267.0, -448.0, -301.0, -584.0, -302.0, -681.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.01, 0.99], [0.94, 0.06]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.67 0.33]\n",
      "Average Reward for Agent 0 this episode : -23.55\n",
      "Random Seed Set to 171\n",
      "Episode 71 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-104.0, -108.0, -116.0, -91.0, -10.0, -144.0, -85.0, -181.0, -96.0, -10.0] \n",
      " [-430.0, -293.0, -508.0, -400.0, -376.0, -337.0, -363.0, -434.0, -453.0, -356.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.68, 0.32], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.68, 0.32]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.68 0.32]\n",
      "Average Reward for Agent 0 this episode : -22.3\n",
      "Random Seed Set to 172\n",
      "Episode 72 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-139.0, -237.0, -117.0, -118.0, -142.0, -51.0, -381.0, -259.0, -169.0, -14.0] \n",
      " [-379.0, -353.0, -358.0, -420.0, -329.0, -275.0, -318.0, -333.0, -272.0, -336.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.96, 0.04]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.69 0.31]\n",
      "Average Reward for Agent 0 this episode : -20.22\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 173\n",
      "Episode 73 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-26.0, -212.0, -107.0, -115.0, -10.0, -55.0, -22.0, -324.0, -74.0, -107.0] \n",
      " [-350.0, -326.0, -435.0, -432.0, -312.0, -299.0, -314.0, -378.0, -452.0, -360.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.7, 0.3], [0.01, 0.99], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.7 0.3]\n",
      "Average Reward for Agent 0 this episode : -21.79\n",
      "Random Seed Set to 174\n",
      "Episode 74 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-241.0, -131.0, -657.0, -170.0, -15.0, -176.0, -222.0, -251.0, -70.0, -284.0] \n",
      " [-545.0, -665.0, -458.0, -396.0, -287.0, -456.0, -686.0, -440.0, -286.0, -585.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.97, 0.03], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.7 0.3]\n",
      "Average Reward for Agent 0 this episode : -25.49\n",
      "Random Seed Set to 175\n",
      "Episode 75 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-175.0, -111.0, -186.0, -80.0, -314.0, -135.0, -191.0, -200.0, -38.0, -334.0] \n",
      " [-485.0, -421.0, -524.0, -465.0, -379.0, -425.0, -473.0, -507.0, -512.0, -416.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.98, 0.02], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.72 0.28]\n",
      "Average Reward for Agent 0 this episode : -22.9\n",
      "Random Seed Set to 176\n",
      "Episode 76 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-276.0, -77.0, -121.0, -39.0, -83.0, -119.0, -222.0, -153.0, -338.0, -117.0] \n",
      " [-453.0, -373.0, -310.0, -622.0, -678.0, -359.0, -313.0, -380.0, -712.0, -454.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.73 0.27]\n",
      "Average Reward for Agent 0 this episode : -24.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 177\n",
      "Episode 77 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-270.0, -126.0, -146.0, -100.0, -69.0, -16.0, -87.0, -65.0, -139.0, -120.0] \n",
      " [-373.0, -490.0, -257.0, -274.0, -263.0, -327.0, -238.0, -281.0, -224.0, -494.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.98, 0.02], [1.0, 0.0], [0.01, 0.99], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.74 0.26]\n",
      "Average Reward for Agent 0 this episode : -20.18\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "Random Seed Set to 178\n",
      "Episode 78 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-164.0, -44.0, -306.0, -344.0, -62.0, -316.0, -212.0, -277.0, -166.0, -103.0] \n",
      " [-501.0, -449.0, -504.0, -500.0, -602.0, -435.0, -405.0, -347.0, -382.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.02, 0.98], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.03, 0.97], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.75 0.25]\n",
      "Average Reward for Agent 0 this episode : -22.6\n",
      "Random Seed Set to 179\n",
      "Episode 79 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-141.0, -16.0, -185.0, -352.0, -151.0, -201.0, -185.0, -66.0, -192.0, -132.0] \n",
      " [-330.0, -405.0, -419.0, -510.0, -323.0, -325.0, -416.0, -389.0, -465.0, -524.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.99, 0.01], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.76 0.24]\n",
      "Average Reward for Agent 0 this episode : -24.18\n",
      "Random Seed Set to 180\n",
      "Episode 80 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-194.0, -120.0, -302.0, -83.0, -44.0, -57.0, -355.0, -289.0, -618.0, -184.0] \n",
      " [-611.0, -358.0, -380.0, -494.0, -384.0, -459.0, -439.0, -493.0, -483.0, -307.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.76 0.24]\n",
      "Average Reward for Agent 0 this episode : -23.56\n",
      "Random Seed Set to 181\n",
      "Episode 81 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -129.0, -76.0, -61.0, -204.0, -129.0, -132.0, -415.0, -160.0, -499.0] \n",
      " [-329.0, -478.0, -556.0, -377.0, -392.0, -478.0, -382.0, -619.0, -247.0, -537.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.77, 0.23], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.59, 0.41], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.77 0.23]\n",
      "Average Reward for Agent 0 this episode : -22.99\n",
      "Random Seed Set to 182\n",
      "Episode 82 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-147.0, -17.0, -138.0, -31.0, -37.0, -57.0, -225.0, -125.0, -340.0, -44.0] \n",
      " [-618.0, -461.0, -480.0, -409.0, -346.0, -382.0, -329.0, -284.0, -507.0, -450.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.99, 0.01], [0.0, 1.0], [0.52, 0.48], [0.75, 0.25], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.01, 0.99]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.79 0.21]\n",
      "Average Reward for Agent 0 this episode : -24.07\n",
      "Random Seed Set to 183\n",
      "Episode 83 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-181.0, -220.0, -33.0, -58.0, -334.0, -193.0, -78.0, -107.0, -17.0, -553.0] \n",
      " [-507.0, -617.0, -564.0, -514.0, -521.0, -314.0, -527.0, -550.0, -436.0, -392.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.8 0.2]\n",
      "Average Reward for Agent 0 this episode : -23.81\n",
      "Random Seed Set to 184\n",
      "Episode 84 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -183.0, -313.0, -332.0, -100.0, -72.0, -171.0, -107.0, -221.0, -305.0] \n",
      " [-406.0, -228.0, -285.0, -394.0, -424.0, -237.0, -513.0, -510.0, -444.0, -486.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.81, 0.19], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.81 0.19]\n",
      "Average Reward for Agent 0 this episode : -24.09\n",
      "Random Seed Set to 185\n",
      "Episode 85 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-103.0, -120.0, -119.0, -325.0, -321.0, -319.0, -262.0, -114.0, -365.0, -200.0] \n",
      " [-559.0, -307.0, -453.0, -384.0, -378.0, -676.0, -526.0, -454.0, -713.0, -373.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.88, 0.12], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.81 0.19]\n",
      "Average Reward for Agent 0 this episode : -22.25\n",
      "Random Seed Set to 186\n",
      "Episode 86 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-409.0, -158.0, -18.0, -44.0, -449.0, -449.0, -150.0, -82.0, -258.0, -75.0] \n",
      " [-404.0, -364.0, -359.0, -379.0, -342.0, -342.0, -467.0, -312.0, -362.0, -430.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.16, 0.84], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.82 0.18]\n",
      "Average Reward for Agent 0 this episode : -21.56\n",
      "Random Seed Set to 187\n",
      "Episode 87 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-42.0, -108.0, -23.0, -295.0, -348.0, -84.0, -62.0, -64.0, -137.0, -259.0] \n",
      " [-309.0, -317.0, -340.0, -376.0, -530.0, -333.0, -441.0, -465.0, -297.0, -312.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.53, 0.47], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.83 0.17]\n",
      "Average Reward for Agent 0 this episode : -21.76\n",
      "Random Seed Set to 188\n",
      "Episode 88 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-164.0, -297.0, -216.0, -196.0, -220.0, -312.0, -77.0, -135.0, -85.0, -87.0] \n",
      " [-423.0, -471.0, -510.0, -468.0, -361.0, -706.0, -371.0, -294.0, -468.0, -418.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.84 0.16]\n",
      "Average Reward for Agent 0 this episode : -22.39\n",
      "Random Seed Set to 189\n",
      "Episode 89 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-143.0, -38.0, -318.0, -211.0, -152.0, -84.0, -522.0, -298.0, -249.0, -85.0] \n",
      " [-414.0, -389.0, -390.0, -635.0, -341.0, -462.0, -431.0, -611.0, -467.0, -423.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.85 0.15]\n",
      "Average Reward for Agent 0 this episode : -22.33\n",
      "Random Seed Set to 190\n",
      "Episode 90 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-222.0, -378.0, -181.0, -196.0, -139.0, -222.0, -203.0, -402.0, -121.0, -141.0] \n",
      " [-529.0, -523.0, -338.0, -378.0, -382.0, -529.0, -609.0, -511.0, -311.0, -569.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.86 0.14]\n",
      "Average Reward for Agent 0 this episode : -24.58\n",
      "Random Seed Set to 191\n",
      "Episode 91 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-143.0, -311.0, -72.0, -84.0, -177.0, -356.0, -95.0, -104.0, -267.0, -159.0] \n",
      " [-322.0, -346.0, -480.0, -433.0, -354.0, -534.0, -288.0, -358.0, -370.0, -664.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.1, 0.9], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.09, 0.91], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.87 0.13]\n",
      "Average Reward for Agent 0 this episode : -21.11\n",
      "Random Seed Set to 192\n",
      "Episode 92 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-150.0, -248.0, -117.0, -125.0, -70.0, -266.0, -167.0, -441.0, -209.0, -175.0] \n",
      " [-325.0, -392.0, -344.0, -435.0, -406.0, -372.0, -621.0, -435.0, -481.0, -397.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.88 0.12]\n",
      "Average Reward for Agent 0 this episode : -24.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed Set to 193\n",
      "Episode 93 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-179.0, -228.0, -69.0, -96.0, -95.0, -107.0, -65.0, -292.0, -206.0, -127.0] \n",
      " [-422.0, -451.0, -311.0, -339.0, -321.0, -373.0, -449.0, -412.0, -484.0, -337.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.77, 0.23], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.88 0.12]\n",
      "Average Reward for Agent 0 this episode : -21.9\n",
      "Random Seed Set to 194\n",
      "Episode 94 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-112.0, -111.0, -156.0, -93.0, -14.0, -250.0, -473.0, -132.0, -68.0, -296.0] \n",
      " [-430.0, -431.0, -624.0, -450.0, -301.0, -443.0, -458.0, -535.0, -398.0, -553.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.89, 0.11], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.89 0.11]\n",
      "Average Reward for Agent 0 this episode : -23.36\n",
      "Random Seed Set to 195\n",
      "Episode 95 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-515.0, -136.0, -14.0, -276.0, -234.0, -76.0, -135.0, -280.0, -272.0, -72.0] \n",
      " [-390.0, -425.0, -420.0, -599.0, -542.0, -464.0, -501.0, -459.0, -377.0, -409.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.9, 0.1], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.9 0.1]\n",
      "Average Reward for Agent 0 this episode : -23.02\n",
      "Random Seed Set to 196\n",
      "Episode 96 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-69.0, -797.0, -221.0, -367.0, -14.0, -112.0, -78.0, -85.0, -75.0, -171.0] \n",
      " [-354.0, -495.0, -391.0, -407.0, -438.0, -273.0, -286.0, -597.0, -420.0, -306.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.91, 0.09], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.91 0.09]\n",
      "Average Reward for Agent 0 this episode : -21.18\n",
      "Random Seed Set to 197\n",
      "Episode 97 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-143.0, -276.0, -443.0, -144.0, -92.0, -107.0, -300.0, -236.0, -92.0, -78.0] \n",
      " [-763.0, -341.0, -645.0, -538.0, -671.0, -449.0, -792.0, -731.0, -366.0, -422.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.92 0.08]\n",
      "Average Reward for Agent 0 this episode : -26.42\n",
      "Random Seed Set to 198\n",
      "Episode 98 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -535.0, -449.0, -176.0, -50.0, -98.0, -43.0, -291.0, -101.0, -39.0] \n",
      " [-381.0, -383.0, -390.0, -420.0, -472.0, -404.0, -308.0, -488.0, -489.0, -356.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.92, 0.08], [1.0, 0.0], [0.05, 0.95], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.92, 0.08]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.92 0.08]\n",
      "Average Reward for Agent 0 this episode : -22.76\n",
      "Random Seed Set to 199\n",
      "Episode 99 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-111.0, -149.0, -246.0, -254.0, -264.0, -66.0, -149.0, -54.0, -158.0, -229.0] \n",
      " [-392.0, -459.0, -455.0, -599.0, -398.0, -425.0, -452.0, -438.0, -426.0, -458.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.93 0.07]\n",
      "Average Reward for Agent 0 this episode : -25.01\n",
      "Random Seed Set to 200\n",
      "Episode 100 is finished\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-65.0, -355.0, -92.0, -308.0, -281.0, -184.0, -274.0, -68.0, -400.0, -192.0] \n",
      " [-354.0, -415.0, -571.0, -297.0, -441.0, -396.0, -602.0, -481.0, -401.0, -508.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.94 0.06]\n",
      "Average Reward for Agent 0 this episode : -24.25\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Saving at :  C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Desktop\\Episode100Agent0_Weights.h5\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.train(100,vissim=vissim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links, Lanes and Vehicle Inputs.\n",
      "\n",
      "Setting Simulation mode to: demo\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 0.15 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.demo(vissim=vissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - delete\n",
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  288       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  1176      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  25        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  288       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1176      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  50        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 3,003\n",
      "Trainable params: 3,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "loaded from  C:\\Users\\nwalton\\OneDrive - The Alan Turing Institute\\Documents\\MLforFlowOptimisation\\NSW_Single_Cross_Experiment\\Single_Cross_Straight\\Agents_Results\\Neil_Results\\Neils_Good_Agents\\Oct24\\Episode100Agent0_Weights.h5\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Single_Cross_Straight_MultiAC_Agents.load(episode=100,\n",
    "                                          best=False, \n",
    "load_location='C:\\\\Users\\\\nwalton\\\\OneDrive - The Alan Turing Institute\\\\Documents\\\\MLforFlowOptimisation\\\\NSW_Single_Cross_Experiment\\\\Single_Cross_Straight\\\\Agents_Results\\\\Neil_Results\\\\Neils_Good_Agents\\\\Oct24'\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
